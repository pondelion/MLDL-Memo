{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRhac/IJaWCM4WXa79PcgO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install colabgymrender imageio==2.4.1 atari-py==0.2.6 gym==0.17.3\n","!apt-get install x11-utils > /dev/null 2>&1 \n","!pip install pyglet > /dev/null 2>&1 \n","!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","!pip install ptan pytorch-ignite pybullet\n","!pip uninstall -y torch torchvision torchaudio\n","!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4u_YoOF2bt9W","executionInfo":{"status":"ok","timestamp":1673899833906,"user_tz":-540,"elapsed":241999,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"65636404-fbe9-4a04-a6ae-c7ddafa14f1c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting colabgymrender\n","  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting imageio==2.4.1\n","  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting atari-py==0.2.6\n","  Downloading atari-py-0.2.6.tar.gz (790 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m790.2/790.2 KB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.1) (1.21.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.1) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from atari-py==0.2.6) (1.15.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from gym==0.17.3) (1.7.3)\n","Collecting pyglet<=1.5.0,>=1.4.0\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.8/dist-packages (from colabgymrender) (0.2.3.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.16.0)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.8/dist-packages (from moviepy->colabgymrender) (4.64.1)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'atari-py' candidate (version 0.2.6 at https://files.pythonhosted.org/packages/43/dd/2721f34a89dc520d2e09363fd23d110a33bbab2399e50fdced6eb2ed2157/atari-py-0.2.6.tar.gz#sha256=6249ad5079b0489e87eb44e65485bb1b07cc1b5af729f1ee52ece749503ceb1d (from https://pypi.org/simple/atari-py/))\n","Reason for being yanked: re-release with new wheels\u001b[0m\u001b[33m\n","\u001b[0mBuilding wheels for collected packages: imageio, atari-py, gym, colabgymrender\n","  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303886 sha256=5a7087473bbb038d14cb2788e424a1893e9e34cd11f203769db5babee38f305e\n","  Stored in directory: /root/.cache/pip/wheels/be/7b/04/4d8d56f1d503e5c404f0de6018c0cfa592c71588a39b49e002\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-0.2.6-cp38-cp38-linux_x86_64.whl size=3093198 sha256=7ed66ce29c8701a2e9e1368d314e61734d6282db649d326358589ffb04bc3b31\n","  Stored in directory: /root/.cache/pip/wheels/7f/5e/27/2e90b9887063d82ee2f9f8b2f8db76bb2290aa281dc40449c8\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654651 sha256=fcbc9e15055a8648929dee5f5679540020519083a779668154f8ebb47560ac62\n","  Stored in directory: /root/.cache/pip/wheels/84/40/e7/14efb9870cfc92ac236d78cb721dce614ddec9666c8a5e0a35\n","  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3130 sha256=aa282c2b422e311f071ad4038514316a6cf33d082758796c01bd765de3343602\n","  Stored in directory: /root/.cache/pip/wheels/e4/d2/e1/cc1c940178ad92438325422b51c3e8c3d927b9ef8381da8840\n","Successfully built imageio atari-py gym colabgymrender\n","Installing collected packages: pyglet, imageio, cloudpickle, atari-py, gym, colabgymrender\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.9.0\n","    Uninstalling imageio-2.9.0:\n","      Successfully uninstalled imageio-2.9.0\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.0\n","    Uninstalling cloudpickle-2.2.0:\n","      Successfully uninstalled cloudpickle-2.2.0\n","  Attempting uninstall: atari-py\n","    Found existing installation: atari-py 0.2.9\n","    Uninstalling atari-py-0.2.9:\n","      Successfully uninstalled atari-py-0.2.9\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed atari-py-0.2.6 cloudpickle-1.6.0 colabgymrender-1.1.0 gym-0.17.3 imageio-2.4.1 pyglet-1.5.0\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.13).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (2.0.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (22.2.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (3.0.9)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (0.38.4)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ptan\n","  Downloading ptan-0.7.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-ignite\n","  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pybullet\n","  Downloading pybullet-3.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch==1.7.0\n","  Downloading torch-1.7.0-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from ptan) (0.17.3)\n","Requirement already satisfied: atari-py in /usr/local/lib/python3.8/dist-packages (from ptan) (0.2.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ptan) (1.21.6)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from ptan) (4.6.0.66)\n","Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from torch==1.7.0->ptan) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.7.0->ptan) (4.4.0)\n","Collecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from atari-py->ptan) (1.15.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from gym->ptan) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from gym->ptan) (1.7.3)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->ptan) (1.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->pytorch-ignite) (3.0.9)\n","Building wheels for collected packages: ptan\n","  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ptan: filename=ptan-0.7-py3-none-any.whl size=23504 sha256=c7df13f673ba9a50de0441a66e21f0a787b98e272694c5ca693fd8426aac5428\n","  Stored in directory: /root/.cache/pip/wheels/4e/c0/b7/67750bb433409ebfe80cefcb7403aec8014372ff47d5efe8fb\n","Successfully built ptan\n","Installing collected packages: pybullet, dataclasses, torch, pytorch-ignite, ptan\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.7.0 which is incompatible.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.7.0 which is incompatible.\n","torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dataclasses-0.6 ptan-0.7 pybullet-3.2.5 pytorch-ignite-0.4.10 torch-1.7.0\n","Found existing installation: torch 1.7.0\n","Uninstalling torch-1.7.0:\n","  Successfully uninstalled torch-1.7.0\n","Found existing installation: torchvision 0.14.0+cu116\n","Uninstalling torchvision-0.14.0+cu116:\n","  Successfully uninstalled torchvision-0.14.0+cu116\n","Found existing installation: torchaudio 0.13.0+cu116\n","Uninstalling torchaudio-0.13.0+cu116:\n","  Successfully uninstalled torchaudio-0.13.0+cu116\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl (1977.9 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1977925632 bytes == 0x3d3c000 @  0x7fad9b1e31e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2472411136 bytes == 0x79b88000 @  0x7fad9b1e4615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1977925632 bytes == 0x3d3c000 @  0x7fad9b1e31e7 0x4d30a0 0x5dede2 0x6758aa 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4fe318 0x5da092 0x62042c 0x5d8d8c 0x561f80 0x4fd2db 0x4997c7 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m913.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp38-cp38-linux_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n","Installing collected packages: torch, torchvision, torchaudio\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.13.1+cu116 which is incompatible.\n","ptan 0.7 requires torch==1.7.0, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.13.1+cu116 torchaudio-0.13.1+cu116 torchvision-0.14.1+cu116\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZaFOx4OUygBu","executionInfo":{"status":"ok","timestamp":1673899844800,"user_tz":-540,"elapsed":5968,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"61664ed4-2748-4ce4-d3a9-eded3440cc8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n","Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2293760/45929032 bytes (5.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4988928/45929032 bytes (10.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7610368/45929032 bytes (16.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9617408/45929032 bytes (20.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12222464/45929032 bytes (26.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14098432/45929032 bytes (30.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16842752/45929032 bytes (36.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19447808/45929032 bytes (42.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21667840/45929032 bytes (47.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24256512/45929032 bytes (52.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27230208/45929032 bytes (59.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30236672/45929032 bytes (65.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32661504/45929032 bytes (71.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35012608/45929032 bytes (76.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37937152/45929032 bytes (82.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40656896/45929032 bytes (88.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42745856/45929032 bytes (93.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45613056/45929032 bytes (99.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"]}],"source":["import collections\n","from typing import Union\n","import math\n","import random\n","from copy import deepcopy\n","from typing import Optional\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","import torch.distributions as distr\n","import gym\n","from gym.spaces import Box\n","from gym.wrappers import FrameStack\n","import pybullet_envs\n","from colabgymrender.recorder import Recorder\n","from fastprogress import progress_bar as pb\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["class NoisyLinear(nn.Linear):\n","\n","    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n","        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n","        w = torch.full((out_features, in_features), sigma_init)\n","        self._sigma_weight = nn.Parameter(w)\n","        z = torch.zeros(out_features, in_features)\n","        self.register_buffer(\"epsilon_weight\", z)\n","        if bias:\n","            w = torch.full((out_features,), sigma_init)\n","            self._sigma_bias = nn.Parameter(w)\n","            z = torch.zeros(out_features)\n","            self.register_buffer(\"epsilon_bias\", z)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        std = math.sqrt(3 / self.in_features)\n","        self.weight.data.uniform_(-std, std)\n","        self.bias.data.uniform_(-std, std)\n","\n","    def forward(self, input):\n","        self.epsilon_weight.normal_()\n","        bias = self.bias\n","        if bias is not None:\n","            self.epsilon_bias.normal_()\n","            bias = bias + self._sigma_bias * \\\n","                   self.epsilon_bias.data\n","        v = self._sigma_weight * self.epsilon_weight.data + \\\n","            self.weight\n","        return F.linear(input, v, bias)"],"metadata":{"id":"tUrcNe6qcE62","executionInfo":{"status":"ok","timestamp":1673899844800,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class SACActor(nn.Module):\n","\n","    def __init__(self, obs_size, act_size, hidden_size: int = 64):\n","        super(SACActor, self).__init__()\n","\n","        self.mu = nn.Sequential(\n","            nn.Linear(obs_size, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, act_size),\n","            nn.Tanh(),\n","        )\n","        self.logstd = nn.Parameter(torch.zeros(act_size))\n","\n","    def forward(self, x):\n","        return self.mu(x)"],"metadata":{"id":"ozSnnjd1lKmH","executionInfo":{"status":"ok","timestamp":1673900146162,"user_tz":-540,"elapsed":254,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["class SACCritic(nn.Module):\n","\n","    def __init__(self, obs_size, hidden_size: int = 64):\n","        super(SACCritic, self).__init__()\n","\n","        self.value = nn.Sequential(\n","            nn.Linear(obs_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","    def forward(self, x):\n","        return self.value(x)"],"metadata":{"id":"8gwNk0IZlQKm","executionInfo":{"status":"ok","timestamp":1673899845112,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class SACTwinQ(nn.Module):\n","\n","    def __init__(self, obs_size, act_size, hidden_size: int = 64):\n","        super(SACTwinQ, self).__init__()\n","\n","        self.q1 = nn.Sequential(\n","            nn.Linear(obs_size + act_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","        self.q2 = nn.Sequential(\n","            nn.Linear(obs_size + act_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","    def forward(self, obs, act):\n","        x = torch.cat([obs, act], dim=1)\n","        return self.q1(x), self.q2(x)"],"metadata":{"id":"WiInxNGblTRs","executionInfo":{"status":"ok","timestamp":1673899845112,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class NStepPriorityReplayBuffer:\n","\n","    def __init__(\n","        self,\n","        max_size: int,\n","        prob_alpha: float = 0.6,\n","        beta_start: float = 0.4,\n","        beta_frames: float = 100000,\n","        n_step: int = 4,\n","        gamma: float = 0.99,\n","    ):\n","        self._prob_alpha = prob_alpha\n","        self._max_size = max_size\n","        self._pos = 0\n","        self._buf = []\n","        self._priorities = np.zeros((max_size,), dtype=np.float32)\n","        self._beta_start = beta_start\n","        self._beta = beta_start\n","        self._beta_frames = beta_frames\n","        self._n_step = n_step\n","        self._gamma = gamma\n","        self._total_discounted_rewards = np.array([np.nan]*max_size)\n","        self._last_states = [np.nan]*max_size\n","\n","    def update_bata(self, idx) -> None:\n","        beta = self._beta_start + idx * (1.0 - self._beta_start) / self._beta_frames\n","        self._beta = min(1.0, beta)\n","        return self._beta\n","\n","    def __len__(self):\n","        return len(self._buf)\n","\n","    def append(\n","        self,\n","        state: np.ndarray,\n","        action: int,\n","        reward: Union[int, float],\n","        done: bool,\n","        next_state: np.ndarray,\n","    ) -> None:\n","        max_prio = self._priorities.max() if self._buf else 1.0\n","        if len(self._buf) < self._max_size:\n","            self._buf.append(\n","                (state, action, reward, done, next_state)\n","            )\n","        else:\n","            self._buf[self._pos] = (state, action, reward, done, next_state)\n","        self._priorities[self._pos] = max_prio\n","\n","        if len(self._buf) >= self._n_step:\n","            dis_r = 0.0\n","            last_state = self._buf[self._pos][0]\n","            for i in range(self._n_step):\n","                state, _, r, done, _ = self._buf[self._pos - i]\n","                dis_r = r + self._gamma * dis_r\n","                if done:\n","                    last_state = state\n","                self._total_discounted_rewards[self._pos - i] = dis_r\n","                self._last_states[self._pos - i] = last_state\n","            \n","            for i in range(self._n_step-1):\n","                done = self._buf[self._pos - i][3]\n","                if done:\n","                    break\n","                self._total_discounted_rewards[self._pos - i] = np.nan\n","                self._last_states[self._pos - i] = np.nan\n","\n","        self._pos = (self._pos + 1) % self._max_size\n","\n","    def sample(self, size: int):\n","        sample_target_indices = np.where(~np.isnan(self._total_discounted_rewards[:len(self._buf)]))[0]\n","        # prios = self._priorities[sample_target_indices]  #self._priorities if len(self._buf) == self._max_size else self._priorities[:self._pos]\n","        prios = self._priorities\n","        probs = prios * self._prob_alpha\n","        # probs /= np.nan_to_num(probs, 0.0).sum()\n","        probs /= probs[sample_target_indices].sum()\n","        sampled_indices = np.random.choice(\n","            sample_target_indices,\n","            # np.where(~np.isnan(self._total_discounted_rewards[:len(self._buf)]))[0],\n","            size, p=probs[sample_target_indices]\n","        )\n","        states, actions, rewards, dones, next_states = zip(*[self._buf[idx] for idx in sampled_indices])\n","        states = np.array(states)\n","        actions = np.array(actions)\n","        rewards = np.array(rewards)\n","        dones = np.array(dones)\n","        next_states = np.array(next_states)\n","        total_discounted_rewards = self._total_discounted_rewards[sampled_indices]\n","        last_states = np.stack([self._last_states[idx] for idx in sampled_indices])\n","        total = len(self._buf)\n","        weights = np.array((total * probs[sampled_indices]) ** (-self._beta), dtype=np.float32)\n","        # weights = np.array((total * probs) ** (-self._beta), dtype=np.float32)\n","        weights /= weights.max()\n","        return states, actions, rewards, dones, total_discounted_rewards, last_states, sampled_indices, weights\n","\n","    def update_priorities(self, sample_indices: np.ndarray, sample_priorities: np.ndarray) -> None:\n","        self._priorities[sample_indices] = sample_priorities\n","\n","    @property\n","    def gamma(self) -> float:\n","        return self._gamma\n","\n","    @property\n","    def n_step(self) -> float:\n","        return self._n_step"],"metadata":{"id":"JCXDGgJ7pr_Y","executionInfo":{"status":"ok","timestamp":1673899845112,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Agent:\n","\n","    def __init__(\n","        self,\n","        env,\n","        exp_buffer: NStepPriorityReplayBuffer,\n","        act_net: nn.Module,\n","        crt_net: nn.Module,\n","        twinq_net:  nn.Module,\n","        epsilon_start: float = 1.0,\n","        epsilon_final: float = 0.01,\n","        epsilon_decay_last_step: int = 200000,\n","        tgt_sync_steps: int = 10000,\n","        learning_rate_acts: float = 1e-4,\n","        learning_rate_vals: float = 1e-4,\n","        adam_eps: float = None,\n","        sac_entropy_alpha: float = 0.1,\n","        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    ):\n","        self._env = env\n","        self._exp_buffer = exp_buffer\n","        self._act_net = act_net\n","        self._crt_net = crt_net\n","        self._twinq_net = twinq_net\n","        self._tgt_act_net = deepcopy(act_net)\n","        self._tgt_crt_net = deepcopy(crt_net)\n","        for p in self._tgt_act_net.parameters():\n","            p.requires_grad = False\n","        for p in self._tgt_crt_net.parameters():\n","            p.requires_grad = False\n","        self._epsilon_start = epsilon_start\n","        self._epsilon_final = epsilon_final\n","        self._epsilon_decay_last_step = epsilon_decay_last_step\n","        self._epsilon = epsilon_start\n","        self._device = device\n","        self._total_step = 0\n","        self._total_trained_samples = 0\n","        self._tgt_sync_steps = tgt_sync_steps\n","        adam_kwargs = {}\n","        if adam_eps is not None:\n","            adam_kwargs['eps'] = adam_eps\n","        self._crt_optimizer = torch.optim.Adam(self._crt_net.parameters(), lr=learning_rate_vals, **adam_kwargs)\n","        self._act_optimizer = torch.optim.Adam(self._act_net.parameters(), lr=learning_rate_acts, **adam_kwargs)\n","        self._twinq_optimizer = torch.optim.Adam(self._twinq_net.parameters(), lr=learning_rate_vals, **adam_kwargs)\n","        self._sac_entropy_alpha = sac_entropy_alpha\n","        self._reset_episode()\n","\n","    def _reset_episode(self):\n","        self._state = self._env.reset()\n","        self._total_reward = 0.0\n","\n","    @torch.no_grad()\n","    def play_step(self, epsilon: Optional[float] = None, sync_target: bool = True):\n","        if epsilon is None:\n","            epsilon = self._epsilon\n","        done_reward = None\n","\n","        if np.random.random() < epsilon:\n","            action = self._env.action_space.sample()\n","        else:\n","            state_a = np.array([self._state], copy=False)\n","            state_v = torch.tensor(state_a).float().to(self._device)\n","            mu_v = self._act_net(state_v)\n","            action = 2.0*mu_v.squeeze(dim=0).data.cpu().numpy()\n","            action = np.clip(action, -2, 2)\n","\n","        next_state, reward, is_done, _ = self._env.step(action)\n","        self._total_reward += reward\n","\n","        self._exp_buffer.append(\n","            self._state, action, reward, is_done, next_state\n","        )\n","        self._state = next_state\n","        if is_done:\n","            done_reward = self._total_reward\n","            self._reset_episode()\n","\n","        self._total_step += 1\n","        self._update_epsilon(self._total_step)\n","        self._exp_buffer.update_bata(self._total_step)\n","\n","        # if self._total_step % self._tgt_sync_steps == 0 and sync_target:\n","        #     self._alpha_sync(self._act_net, self._tgt_act_net, alpha=1 - 1e-3)\n","        #     self._alpha_sync(self._crt_net, self._tgt_crt_net, alpha=1 - 1e-3)\n","        #     print(f'synced target net')\n","\n","        return done_reward\n","\n","    def train(self, n_iter: int = 1, batch_size: int = 32) -> None:\n","        n_step_gamma = self._exp_buffer.gamma ** self._exp_buffer.n_step\n","        for i in range(n_iter):\n","            states, actions, rewards, dones, total_discounted_rewards, \\\n","                last_states, sampled_indices, weights = self._exp_buffer.sample(batch_size)\n","            states_v = torch.tensor(states).float().to(self._device)\n","            actions_v = torch.tensor(actions).to(self._device)\n","            discounted_rewards_v = torch.tensor(total_discounted_rewards).to(self._device)\n","            done_mask = torch.BoolTensor(dones).to(self._device)\n","            weights_v = torch.tensor(weights).to(self._device)\n","            last_states_v = torch.tensor(last_states).float().to(self._device)\n","\n","            # last_act_v = self._tgt_act_net(last_states_v)\n","            q_last_v = self._tgt_crt_net(last_states_v)\n","            q_last_v[done_mask] = 0.0\n","            q_ref_v = discounted_rewards_v.unsqueeze(dim=-1) + \\\n","                        q_last_v * n_step_gamma\n","\n","            mu_v = self._act_net(states_v)\n","            act_dist = distr.Normal(mu_v, torch.exp(self._act_net.logstd))\n","            acts_v = act_dist.sample()\n","            q1_v, q2_v = self._twinq_net(states_v, acts_v)\n","            # element-wise minimum\n","            vals_ref_v = torch.min(q1_v, q2_v).squeeze() - \\\n","                        self._sac_entropy_alpha * act_dist.log_prob(acts_v).sum(dim=1)\n","\n","            # train TwinQ\n","            self._twinq_optimizer.zero_grad()\n","            q1_v, q2_v = self._twinq_net(states_v, actions_v)\n","            q1_loss_v = F.mse_loss(q1_v.squeeze().float(),\n","                                    q_ref_v.squeeze().detach().float())\n","            q2_loss_v = F.mse_loss(q2_v.squeeze().float(),\n","                                    q_ref_v.squeeze().detach().float())\n","            q_loss_v = q1_loss_v + q2_loss_v\n","            q_loss_v.backward()\n","            self._twinq_optimizer.step()\n","\n","            # Critic\n","            self._crt_optimizer.zero_grad()\n","            val_v = self._crt_net(states_v)\n","            v_loss_v = F.mse_loss(val_v.squeeze().float(),\n","                                    vals_ref_v.squeeze().detach().float())\n","            v_loss_v.backward()\n","            self._crt_optimizer.step()\n","\n","            # Actor\n","            self._act_optimizer.zero_grad()\n","            acts_v = self._act_net(states_v)\n","            q_out_v, _ = self._twinq_net(states_v, acts_v)\n","            act_loss = -q_out_v.mean()\n","            act_loss.backward()\n","            self._act_optimizer.step()\n","\n","            # self._alpha_sync(self._act_net, self._tgt_act_net, alpha=1 - 1e-3)\n","            self._alpha_sync(self._crt_net, self._tgt_crt_net, alpha=1 - 1e-3)\n","\n","            # self._exp_buffer.update_priorities(sampled_indices, prios)\n","            self._total_trained_samples += batch_size\n","\n","    def initial_exploration(self, n_steps: int = 10000, epsilon: float = 1.0) -> None:\n","        eps_bak = self._epsilon\n","        try:\n","            for i in pb(range(n_steps)):\n","                self._epsilon = epsilon\n","                self.play_step(sync_target=False)\n","        finally:\n","            self._total_step = 0\n","            self._epsilon = eps_bak\n","\n","    def _update_epsilon(self, step_index: int) -> None:\n","        self._epsilon = max(\n","            self._epsilon_final,\n","            self._epsilon_start - step_index / self._epsilon_decay_last_step\n","        )\n","\n","    def _sync(self, net, tgt_net):\n","        tgt_net.load_state_dict(net.state_dict())\n","\n","    def _alpha_sync(self, net, tgt_net, alpha):\n","        assert isinstance(alpha, float)\n","        assert 0.0 < alpha <= 1.0\n","        state = net.state_dict()\n","        tgt_state = tgt_net.state_dict()\n","        for k, v in state.items():\n","            tgt_state[k] = tgt_state[k] * alpha + (1 - alpha) * v\n","        tgt_net.load_state_dict(tgt_state)\n"],"metadata":{"id":"BJrQzs6DcAHI","executionInfo":{"status":"ok","timestamp":1673900172539,"user_tz":-540,"elapsed":241,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["class SkipFrame(gym.Wrapper):\n","    def __init__(self, env, skip):\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = False\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","        return obs, total_reward, done, info\n","\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def permute_orientation(self, observation):\n","        print(observation.shape)\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","        return observation\n","\n","    def observation(self, observation):\n","        observation = self.permute_orientation(observation)\n","        transform = T.Grayscale()\n","        observation = transform(observation)\n","        return observation\n","\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        if isinstance(shape, int):\n","            self.shape = (shape, shape)\n","        else:\n","            self.shape = tuple(shape)\n","\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        transforms = T.Compose(\n","            [T.Resize(self.shape), T.Normalize(0, 255)]\n","        )\n","        observation = transforms(observation).squeeze(0)\n","        return observation\n","\n","\n","class LazyFramesToNumpy(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super(LazyFramesToNumpy, self).__init__(env)\n","\n","    def observation(self, observation):\n","        return observation.__array__()"],"metadata":{"id":"LbLJwSKQk7n5","executionInfo":{"status":"ok","timestamp":1673900174302,"user_tz":-540,"elapsed":1,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["SEED = 77"],"metadata":{"id":"syE8tM5Dk9g1","executionInfo":{"status":"ok","timestamp":1673900174595,"user_tz":-540,"elapsed":1,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["random.seed(SEED)\n","torch.manual_seed(SEED)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","!rm -rf /content/video\n","\n","env = gym.make(\"Pendulum-v0\")\n","# env = SkipFrame(env, skip=4)\n","# env = GrayScaleObservation(env)\n","# env = ResizeObservation(env, shape=224)\n","# env = FrameStack(env, num_stack=4)\n","# env = LazyFramesToNumpy(env)\n","directory = './video'\n","# env = Recorder(env, directory)"],"metadata":{"id":"ISoAyX4Jk38h","executionInfo":{"status":"ok","timestamp":1673900175122,"user_tz":-540,"elapsed":295,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["env.observation_space.shape, env.action_space"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I726ZCwYlFaZ","executionInfo":{"status":"ok","timestamp":1673900176331,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"b2ea70f9-0aac-40dc-e9d0-5e35fc0d0550"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3,), Box(-2.0, 2.0, (1,), float32))"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["env.action_space.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f58M9n4aNloz","executionInfo":{"status":"ok","timestamp":1673900177027,"user_tz":-540,"elapsed":1,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"884e0cd6-576a-404d-f323-4f89a338eb3b"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["env.action_space.sample()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3g5fUDxtL_y","executionInfo":{"status":"ok","timestamp":1673900177670,"user_tz":-540,"elapsed":1,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"44fafe29-aacf-48bf-beb1-b3da2d9f1a94"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.2922716], dtype=float32)"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["state = env.reset()"],"metadata":{"id":"xJ0Ik8N_uEMn","executionInfo":{"status":"ok","timestamp":1673900178208,"user_tz":-540,"elapsed":1,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c--TBT5svHXx","executionInfo":{"status":"ok","timestamp":1673900179606,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"565c7e47-654f-4de3-8330-f4569b93b920"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.14746339, -0.98906751,  0.77418339])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["act_net = SACActor(\n","    env.observation_space.shape[0],\n","    env.action_space.shape[0]\n",").to(device)\n","crt_net = SACCritic(\n","    env.observation_space.shape[0],\n","    env.action_space.shape[0]\n",").to(device)\n","twinq_net = SACTwinQ(\n","    env.observation_space.shape[0],\n","    env.action_space.shape[0]\n",").to(device)"],"metadata":{"id":"TY1MgoxStSWJ","executionInfo":{"status":"ok","timestamp":1673903390527,"user_tz":-540,"elapsed":331,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["act_net"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F78dc9emvT2V","executionInfo":{"status":"ok","timestamp":1673903390758,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"234ef7c2-7aed-4db5-c133-baa3962794cc"},"execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SACActor(\n","  (mu): Sequential(\n","    (0): Linear(in_features=3, out_features=64, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=64, out_features=64, bias=True)\n","    (3): Tanh()\n","    (4): Linear(in_features=64, out_features=1, bias=True)\n","    (5): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["exp_buffer = NStepPriorityReplayBuffer(\n","    max_size=30000,\n","    prob_alpha=0.6,\n","    beta_start=0.4,\n","    beta_frames=30000, #100000,\n","    n_step=8,\n","    gamma=0.99,\n",")"],"metadata":{"id":"AN9q3Q6ztd5Y","executionInfo":{"status":"ok","timestamp":1673903390758,"user_tz":-540,"elapsed":1,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["agent = Agent(\n","    env=env,\n","    exp_buffer=exp_buffer,\n","    crt_net=crt_net,\n","    act_net=act_net,\n","    twinq_net=twinq_net,\n","    epsilon_start=0.1,\n","    epsilon_final=0.002,\n","    epsilon_decay_last_step=50000, #200000,\n","    # tgt_sync_steps=1000,\n","    learning_rate_acts=1e-4,\n","    learning_rate_vals=1e-4,\n","    device=device\n",")"],"metadata":{"id":"-ZXpf8fNs_ZM","executionInfo":{"status":"ok","timestamp":1673903392361,"user_tz":-540,"elapsed":369,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["act_net(torch.tensor(state).unsqueeze(0).float().to(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HrnLKSUt6D5","executionInfo":{"status":"ok","timestamp":1673903392684,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"cda5c22c-2b82-47a9-a52d-b1ad0e792956"},"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0463]], grad_fn=<TanhBackward0>)"]},"metadata":{},"execution_count":116}]},{"cell_type":"code","source":["crt_net(\n","    torch.tensor(state).unsqueeze(0).float().to(device),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asPrgS3NvQ9p","executionInfo":{"status":"ok","timestamp":1673903392967,"user_tz":-540,"elapsed":2,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"4f497f1a-baf3-4574-c819-cc80f2f39afa"},"execution_count":117,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0571]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":117}]},{"cell_type":"code","source":["agent.initial_exploration(n_steps=10000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"HYnpo5e7vy96","executionInfo":{"status":"ok","timestamp":1673903395351,"user_tz":-540,"elapsed":1540,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"c68eb4af-a996-483d-c3aa-3c07b502c137"},"execution_count":118,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [10000/10000 00:01&lt;00:00]\n","    </div>\n","    "]},"metadata":{}}]},{"cell_type":"code","source":["!rm -rf /content/video/*"],"metadata":{"id":"rj1Bcn59v9wO","executionInfo":{"status":"ok","timestamp":1673903396778,"user_tz":-540,"elapsed":243,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["# exp_buffer = NStepPriorityReplayBuffer(\n","#     max_size=30000,\n","#     prob_alpha=0.6,\n","#     beta_start=0.4,\n","#     beta_frames=30000, #100000,\n","#     n_step=8,\n","#     gamma=0.99,\n","# )\n","# agent = Agent(\n","#     env=env,\n","#     exp_buffer=exp_buffer,\n","#     crt_net=crt_net,\n","#     act_net=act_net,\n","#     twinq_net=twinq_net,\n","#     epsilon_start=0.1,\n","#     epsilon_final=0.002,\n","#     epsilon_decay_last_step=50000, #200000,\n","#     # tgt_sync_steps=1000,\n","#     learning_rate_acts=1e-4,\n","#     learning_rate_vals=1e-4,\n","#     device=device\n","# )\n","\n","episode = 0\n","\n","while True:\n","\n","    for stp in range(1):\n","        done_reward = agent.play_step()\n","        if done_reward is not None:\n","            print(f'episode : {episode}, done reward : {done_reward}, total_step : {agent._total_step}, cur_epsilon : {agent._epsilon}')\n","            episode += 1\n","\n","    agent.train(n_iter=1, batch_size=64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"W6UA58dgaPTg","executionInfo":{"status":"error","timestamp":1673904138072,"user_tz":-540,"elapsed":740305,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}},"outputId":"da4c71c3-fa87-428a-c26c-36d08fb47ffa"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["episode : 0, done reward : -1758.5510323260667, total_step : 200, cur_epsilon : 0.096\n","episode : 1, done reward : -1854.8954235927026, total_step : 400, cur_epsilon : 0.092\n","episode : 2, done reward : -1709.3123307951425, total_step : 600, cur_epsilon : 0.08800000000000001\n","episode : 3, done reward : -1822.8614335163486, total_step : 800, cur_epsilon : 0.084\n","episode : 4, done reward : -1657.1650496023972, total_step : 1000, cur_epsilon : 0.08\n","episode : 5, done reward : -1598.0071886688934, total_step : 1200, cur_epsilon : 0.07600000000000001\n","episode : 6, done reward : -1579.2583430600257, total_step : 1400, cur_epsilon : 0.07200000000000001\n","episode : 7, done reward : -1559.2730771077092, total_step : 1600, cur_epsilon : 0.068\n","episode : 8, done reward : -1588.7631826822753, total_step : 1800, cur_epsilon : 0.064\n","episode : 9, done reward : -1612.0594843419426, total_step : 2000, cur_epsilon : 0.060000000000000005\n","episode : 10, done reward : -1595.2397542739136, total_step : 2200, cur_epsilon : 0.05600000000000001\n","episode : 11, done reward : -1469.0159635281477, total_step : 2400, cur_epsilon : 0.052000000000000005\n","episode : 12, done reward : -1566.1620411346619, total_step : 2600, cur_epsilon : 0.04800000000000001\n","episode : 13, done reward : -1444.6643182905414, total_step : 2800, cur_epsilon : 0.044000000000000004\n","episode : 14, done reward : -1482.486341627605, total_step : 3000, cur_epsilon : 0.04000000000000001\n","episode : 15, done reward : -1426.375074323014, total_step : 3200, cur_epsilon : 0.036000000000000004\n","episode : 16, done reward : -1348.9640887668795, total_step : 3400, cur_epsilon : 0.032\n","episode : 17, done reward : -1445.5117183181953, total_step : 3600, cur_epsilon : 0.02800000000000001\n","episode : 18, done reward : -1544.9555267661565, total_step : 3800, cur_epsilon : 0.024000000000000007\n","episode : 19, done reward : -1467.5131230252794, total_step : 4000, cur_epsilon : 0.020000000000000004\n","episode : 20, done reward : -1287.5117971591983, total_step : 4200, cur_epsilon : 0.016\n","episode : 21, done reward : -1476.2689955332812, total_step : 4400, cur_epsilon : 0.01200000000000001\n","episode : 22, done reward : -1494.812644763206, total_step : 4600, cur_epsilon : 0.008000000000000007\n","episode : 23, done reward : -1333.8457035786078, total_step : 4800, cur_epsilon : 0.0040000000000000036\n","episode : 24, done reward : -1260.8262869437754, total_step : 5000, cur_epsilon : 0.002\n","episode : 25, done reward : -1411.8071796315387, total_step : 5200, cur_epsilon : 0.002\n","episode : 26, done reward : -1297.041996609365, total_step : 5400, cur_epsilon : 0.002\n","episode : 27, done reward : -1387.4108084839127, total_step : 5600, cur_epsilon : 0.002\n","episode : 28, done reward : -1306.8451752839885, total_step : 5800, cur_epsilon : 0.002\n","episode : 29, done reward : -1298.375023765078, total_step : 6000, cur_epsilon : 0.002\n","episode : 30, done reward : -1288.9223791752158, total_step : 6200, cur_epsilon : 0.002\n","episode : 31, done reward : -1179.217679881091, total_step : 6400, cur_epsilon : 0.002\n","episode : 32, done reward : -1344.00377564284, total_step : 6600, cur_epsilon : 0.002\n","episode : 33, done reward : -1259.1402960106457, total_step : 6800, cur_epsilon : 0.002\n","episode : 34, done reward : -1225.483735501697, total_step : 7000, cur_epsilon : 0.002\n","episode : 35, done reward : -1322.9131169936816, total_step : 7200, cur_epsilon : 0.002\n","episode : 36, done reward : -1293.4455456310081, total_step : 7400, cur_epsilon : 0.002\n","episode : 37, done reward : -1137.82111850233, total_step : 7600, cur_epsilon : 0.002\n","episode : 38, done reward : -1272.4901087948172, total_step : 7800, cur_epsilon : 0.002\n","episode : 39, done reward : -1300.7747954502288, total_step : 8000, cur_epsilon : 0.002\n","episode : 40, done reward : -1262.8129206610217, total_step : 8200, cur_epsilon : 0.002\n","episode : 41, done reward : -1219.3368867340869, total_step : 8400, cur_epsilon : 0.002\n","episode : 42, done reward : -1001.3738294187153, total_step : 8600, cur_epsilon : 0.002\n","episode : 43, done reward : -1256.216705315557, total_step : 8800, cur_epsilon : 0.002\n","episode : 44, done reward : -1189.8074091122626, total_step : 9000, cur_epsilon : 0.002\n","episode : 45, done reward : -1208.3831934788373, total_step : 9200, cur_epsilon : 0.002\n","episode : 46, done reward : -1146.0629742780977, total_step : 9400, cur_epsilon : 0.002\n","episode : 47, done reward : -1501.7355862092545, total_step : 9600, cur_epsilon : 0.002\n","episode : 48, done reward : -1234.344617673315, total_step : 9800, cur_epsilon : 0.002\n","episode : 49, done reward : -1202.384728233073, total_step : 10000, cur_epsilon : 0.002\n","episode : 50, done reward : -1093.091392529479, total_step : 10200, cur_epsilon : 0.002\n","episode : 51, done reward : -1144.8124362291182, total_step : 10400, cur_epsilon : 0.002\n","episode : 52, done reward : -1233.7946448009293, total_step : 10600, cur_epsilon : 0.002\n","episode : 53, done reward : -1205.963499557066, total_step : 10800, cur_epsilon : 0.002\n","episode : 54, done reward : -1115.767506114595, total_step : 11000, cur_epsilon : 0.002\n","episode : 55, done reward : -1114.6131137440846, total_step : 11200, cur_epsilon : 0.002\n","episode : 56, done reward : -1363.329724943168, total_step : 11400, cur_epsilon : 0.002\n","episode : 57, done reward : -1332.1144928119986, total_step : 11600, cur_epsilon : 0.002\n","episode : 58, done reward : -1290.2812405663235, total_step : 11800, cur_epsilon : 0.002\n","episode : 59, done reward : -1500.7697715749875, total_step : 12000, cur_epsilon : 0.002\n","episode : 60, done reward : -971.9625054692747, total_step : 12200, cur_epsilon : 0.002\n","episode : 61, done reward : -1083.2380442571355, total_step : 12400, cur_epsilon : 0.002\n","episode : 62, done reward : -1448.8811592691145, total_step : 12600, cur_epsilon : 0.002\n","episode : 63, done reward : -1425.5024859159214, total_step : 12800, cur_epsilon : 0.002\n","episode : 64, done reward : -1073.023940080052, total_step : 13000, cur_epsilon : 0.002\n","episode : 65, done reward : -1515.3642196821552, total_step : 13200, cur_epsilon : 0.002\n","episode : 66, done reward : -1021.6615569090854, total_step : 13400, cur_epsilon : 0.002\n","episode : 67, done reward : -1500.1167922447073, total_step : 13600, cur_epsilon : 0.002\n","episode : 68, done reward : -1099.7336216579517, total_step : 13800, cur_epsilon : 0.002\n","episode : 69, done reward : -1536.6951051685965, total_step : 14000, cur_epsilon : 0.002\n","episode : 70, done reward : -1102.7779334559928, total_step : 14200, cur_epsilon : 0.002\n","episode : 71, done reward : -1513.656011785204, total_step : 14400, cur_epsilon : 0.002\n","episode : 72, done reward : -1501.6815110256002, total_step : 14600, cur_epsilon : 0.002\n","episode : 73, done reward : -955.8391987195035, total_step : 14800, cur_epsilon : 0.002\n","episode : 74, done reward : -911.3326206990866, total_step : 15000, cur_epsilon : 0.002\n","episode : 75, done reward : -1497.0244919708873, total_step : 15200, cur_epsilon : 0.002\n","episode : 76, done reward : -1394.3709679949977, total_step : 15400, cur_epsilon : 0.002\n","episode : 77, done reward : -1050.4922270962306, total_step : 15600, cur_epsilon : 0.002\n","episode : 78, done reward : -1059.5515834614569, total_step : 15800, cur_epsilon : 0.002\n","episode : 79, done reward : -1076.6223037883453, total_step : 16000, cur_epsilon : 0.002\n","episode : 80, done reward : -928.5971285920807, total_step : 16200, cur_epsilon : 0.002\n","episode : 81, done reward : -1029.5449599253272, total_step : 16400, cur_epsilon : 0.002\n","episode : 82, done reward : -1082.171502602907, total_step : 16600, cur_epsilon : 0.002\n","episode : 83, done reward : -978.3471830765067, total_step : 16800, cur_epsilon : 0.002\n","episode : 84, done reward : -1101.3642127308547, total_step : 17000, cur_epsilon : 0.002\n","episode : 85, done reward : -1512.3926281604024, total_step : 17200, cur_epsilon : 0.002\n","episode : 86, done reward : -1055.5399813838862, total_step : 17400, cur_epsilon : 0.002\n","episode : 87, done reward : -1027.562793576894, total_step : 17600, cur_epsilon : 0.002\n","episode : 88, done reward : -1492.1649582031253, total_step : 17800, cur_epsilon : 0.002\n","episode : 89, done reward : -788.7448645363722, total_step : 18000, cur_epsilon : 0.002\n","episode : 90, done reward : -805.0414200996591, total_step : 18200, cur_epsilon : 0.002\n","episode : 91, done reward : -814.0773936557949, total_step : 18400, cur_epsilon : 0.002\n","episode : 92, done reward : -1046.366632914738, total_step : 18600, cur_epsilon : 0.002\n","episode : 93, done reward : -932.7875343478287, total_step : 18800, cur_epsilon : 0.002\n","episode : 94, done reward : -1499.7680555878408, total_step : 19000, cur_epsilon : 0.002\n","episode : 95, done reward : -788.3034749764985, total_step : 19200, cur_epsilon : 0.002\n","episode : 96, done reward : -996.9104361118318, total_step : 19400, cur_epsilon : 0.002\n","episode : 97, done reward : -868.7954271489774, total_step : 19600, cur_epsilon : 0.002\n","episode : 98, done reward : -1105.9498474379209, total_step : 19800, cur_epsilon : 0.002\n","episode : 99, done reward : -802.5782587547548, total_step : 20000, cur_epsilon : 0.002\n","episode : 100, done reward : -955.8355924809359, total_step : 20200, cur_epsilon : 0.002\n","episode : 101, done reward : -849.2356885995487, total_step : 20400, cur_epsilon : 0.002\n","episode : 102, done reward : -1389.0293931788472, total_step : 20600, cur_epsilon : 0.002\n","episode : 103, done reward : -882.5997775315624, total_step : 20800, cur_epsilon : 0.002\n","episode : 104, done reward : -861.3150686698865, total_step : 21000, cur_epsilon : 0.002\n","episode : 105, done reward : -875.9912369175122, total_step : 21200, cur_epsilon : 0.002\n","episode : 106, done reward : -1496.4271184512393, total_step : 21400, cur_epsilon : 0.002\n","episode : 107, done reward : -797.5015411936386, total_step : 21600, cur_epsilon : 0.002\n","episode : 108, done reward : -838.814865789425, total_step : 21800, cur_epsilon : 0.002\n","episode : 109, done reward : -809.1401960146313, total_step : 22000, cur_epsilon : 0.002\n","episode : 110, done reward : -1515.110566883164, total_step : 22200, cur_epsilon : 0.002\n","episode : 111, done reward : -959.8995053926034, total_step : 22400, cur_epsilon : 0.002\n","episode : 112, done reward : -1443.7413482616566, total_step : 22600, cur_epsilon : 0.002\n","episode : 113, done reward : -1465.0115375982925, total_step : 22800, cur_epsilon : 0.002\n","episode : 114, done reward : -836.5979633108278, total_step : 23000, cur_epsilon : 0.002\n","episode : 115, done reward : -843.6795429224999, total_step : 23200, cur_epsilon : 0.002\n","episode : 116, done reward : -906.6561611598486, total_step : 23400, cur_epsilon : 0.002\n","episode : 117, done reward : -1203.9673242739045, total_step : 23600, cur_epsilon : 0.002\n","episode : 118, done reward : -911.7426578667286, total_step : 23800, cur_epsilon : 0.002\n","episode : 119, done reward : -1456.0771029697098, total_step : 24000, cur_epsilon : 0.002\n","episode : 120, done reward : -956.0720848542024, total_step : 24200, cur_epsilon : 0.002\n","episode : 121, done reward : -1486.8310914455437, total_step : 24400, cur_epsilon : 0.002\n","episode : 122, done reward : -1479.570528530571, total_step : 24600, cur_epsilon : 0.002\n","episode : 123, done reward : -855.3426116371754, total_step : 24800, cur_epsilon : 0.002\n","episode : 124, done reward : -1401.3235468927408, total_step : 25000, cur_epsilon : 0.002\n","episode : 125, done reward : -899.903954716835, total_step : 25200, cur_epsilon : 0.002\n","episode : 126, done reward : -1497.6954083996868, total_step : 25400, cur_epsilon : 0.002\n","episode : 127, done reward : -829.3420512978433, total_step : 25600, cur_epsilon : 0.002\n","episode : 128, done reward : -1446.0444164147882, total_step : 25800, cur_epsilon : 0.002\n","episode : 129, done reward : -826.3175331964355, total_step : 26000, cur_epsilon : 0.002\n","episode : 130, done reward : -1510.62816913813, total_step : 26200, cur_epsilon : 0.002\n","episode : 131, done reward : -803.3353142841069, total_step : 26400, cur_epsilon : 0.002\n","episode : 132, done reward : -1053.458215799662, total_step : 26600, cur_epsilon : 0.002\n","episode : 133, done reward : -1464.7902223495448, total_step : 26800, cur_epsilon : 0.002\n","episode : 134, done reward : -831.7856581493526, total_step : 27000, cur_epsilon : 0.002\n","episode : 135, done reward : -857.1284667609258, total_step : 27200, cur_epsilon : 0.002\n","episode : 136, done reward : -788.345059926783, total_step : 27400, cur_epsilon : 0.002\n","episode : 137, done reward : -1261.6404461166157, total_step : 27600, cur_epsilon : 0.002\n","episode : 138, done reward : -1495.0203402245934, total_step : 27800, cur_epsilon : 0.002\n","episode : 139, done reward : -1517.661804176526, total_step : 28000, cur_epsilon : 0.002\n","episode : 140, done reward : -782.5418939663994, total_step : 28200, cur_epsilon : 0.002\n","episode : 141, done reward : -904.6918802857472, total_step : 28400, cur_epsilon : 0.002\n","episode : 142, done reward : -806.028424821027, total_step : 28600, cur_epsilon : 0.002\n","episode : 143, done reward : -1494.4214560646744, total_step : 28800, cur_epsilon : 0.002\n","episode : 144, done reward : -867.0878795620282, total_step : 29000, cur_epsilon : 0.002\n","episode : 145, done reward : -778.0789202075318, total_step : 29200, cur_epsilon : 0.002\n","episode : 146, done reward : -861.6095083255361, total_step : 29400, cur_epsilon : 0.002\n","episode : 147, done reward : -918.815191001583, total_step : 29600, cur_epsilon : 0.002\n","episode : 148, done reward : -813.0996077733134, total_step : 29800, cur_epsilon : 0.002\n","episode : 149, done reward : -792.5131464779763, total_step : 30000, cur_epsilon : 0.002\n","episode : 150, done reward : -881.5520451532508, total_step : 30200, cur_epsilon : 0.002\n","episode : 151, done reward : -777.1352439660276, total_step : 30400, cur_epsilon : 0.002\n","episode : 152, done reward : -801.260314507881, total_step : 30600, cur_epsilon : 0.002\n","episode : 153, done reward : -795.4439864010286, total_step : 30800, cur_epsilon : 0.002\n","episode : 154, done reward : -838.0355285646937, total_step : 31000, cur_epsilon : 0.002\n","episode : 155, done reward : -778.3058821856656, total_step : 31200, cur_epsilon : 0.002\n","episode : 156, done reward : -808.5569367101324, total_step : 31400, cur_epsilon : 0.002\n","episode : 157, done reward : -782.8692274722239, total_step : 31600, cur_epsilon : 0.002\n","episode : 158, done reward : -781.9260916174571, total_step : 31800, cur_epsilon : 0.002\n","episode : 159, done reward : -780.1518093928538, total_step : 32000, cur_epsilon : 0.002\n","episode : 160, done reward : -873.1373299963535, total_step : 32200, cur_epsilon : 0.002\n","episode : 161, done reward : -899.037565873468, total_step : 32400, cur_epsilon : 0.002\n","episode : 162, done reward : -815.6594590334739, total_step : 32600, cur_epsilon : 0.002\n","episode : 163, done reward : -784.6114126994266, total_step : 32800, cur_epsilon : 0.002\n","episode : 164, done reward : -782.621835809778, total_step : 33000, cur_epsilon : 0.002\n","episode : 165, done reward : -800.4512268948134, total_step : 33200, cur_epsilon : 0.002\n","episode : 166, done reward : -821.3580443435893, total_step : 33400, cur_epsilon : 0.002\n","episode : 167, done reward : -774.8098194468444, total_step : 33600, cur_epsilon : 0.002\n","episode : 168, done reward : -877.8335525103876, total_step : 33800, cur_epsilon : 0.002\n","episode : 169, done reward : -881.29325301496, total_step : 34000, cur_epsilon : 0.002\n","episode : 170, done reward : -767.4723727417196, total_step : 34200, cur_epsilon : 0.002\n","episode : 171, done reward : -800.5980628582363, total_step : 34400, cur_epsilon : 0.002\n","episode : 172, done reward : -876.7340316597691, total_step : 34600, cur_epsilon : 0.002\n","episode : 173, done reward : -771.5919203454388, total_step : 34800, cur_epsilon : 0.002\n","episode : 174, done reward : -735.2807697183594, total_step : 35000, cur_epsilon : 0.002\n","episode : 175, done reward : -767.7111287279812, total_step : 35200, cur_epsilon : 0.002\n","episode : 176, done reward : -711.4095166630709, total_step : 35400, cur_epsilon : 0.002\n","episode : 177, done reward : -659.9965011970093, total_step : 35600, cur_epsilon : 0.002\n","episode : 178, done reward : -796.5656277596423, total_step : 35800, cur_epsilon : 0.002\n","episode : 179, done reward : -660.9815523659278, total_step : 36000, cur_epsilon : 0.002\n","episode : 180, done reward : -1050.8156986716926, total_step : 36200, cur_epsilon : 0.002\n","episode : 181, done reward : -1176.8219020068586, total_step : 36400, cur_epsilon : 0.002\n","episode : 182, done reward : -1170.8079681211834, total_step : 36600, cur_epsilon : 0.002\n","episode : 183, done reward : -549.0972417366424, total_step : 36800, cur_epsilon : 0.002\n","episode : 184, done reward : -644.387760834203, total_step : 37000, cur_epsilon : 0.002\n","episode : 185, done reward : -644.6642564366667, total_step : 37200, cur_epsilon : 0.002\n","episode : 186, done reward : -651.227057612778, total_step : 37400, cur_epsilon : 0.002\n","episode : 187, done reward : -621.7744155094942, total_step : 37600, cur_epsilon : 0.002\n","episode : 188, done reward : -835.2928374529163, total_step : 37800, cur_epsilon : 0.002\n","episode : 189, done reward : -723.6739735550245, total_step : 38000, cur_epsilon : 0.002\n","episode : 190, done reward : -755.041864706355, total_step : 38200, cur_epsilon : 0.002\n","episode : 191, done reward : -646.677037459668, total_step : 38400, cur_epsilon : 0.002\n","episode : 192, done reward : -677.4278267591037, total_step : 38600, cur_epsilon : 0.002\n","episode : 193, done reward : -639.3649463767687, total_step : 38800, cur_epsilon : 0.002\n","episode : 194, done reward : -1370.3708586117837, total_step : 39000, cur_epsilon : 0.002\n","episode : 195, done reward : -1420.362533535814, total_step : 39200, cur_epsilon : 0.002\n","episode : 196, done reward : -642.9172763448487, total_step : 39400, cur_epsilon : 0.002\n","episode : 197, done reward : -645.3138843200919, total_step : 39600, cur_epsilon : 0.002\n","episode : 198, done reward : -642.6899741977829, total_step : 39800, cur_epsilon : 0.002\n","episode : 199, done reward : -645.3101663381001, total_step : 40000, cur_epsilon : 0.002\n","episode : 200, done reward : -1200.985234823489, total_step : 40200, cur_epsilon : 0.002\n","episode : 201, done reward : -1112.8210722826182, total_step : 40400, cur_epsilon : 0.002\n","episode : 202, done reward : -781.5343198199448, total_step : 40600, cur_epsilon : 0.002\n","episode : 203, done reward : -774.332015848837, total_step : 40800, cur_epsilon : 0.002\n","episode : 204, done reward : -1203.9244838722857, total_step : 41000, cur_epsilon : 0.002\n","episode : 205, done reward : -644.6859725759267, total_step : 41200, cur_epsilon : 0.002\n","episode : 206, done reward : -1193.3615607834795, total_step : 41400, cur_epsilon : 0.002\n","episode : 207, done reward : -995.6571293100014, total_step : 41600, cur_epsilon : 0.002\n","episode : 208, done reward : -624.6368132676279, total_step : 41800, cur_epsilon : 0.002\n","episode : 209, done reward : -651.8543832517107, total_step : 42000, cur_epsilon : 0.002\n","episode : 210, done reward : -666.4817844016268, total_step : 42200, cur_epsilon : 0.002\n","episode : 211, done reward : -1502.0874182381228, total_step : 42400, cur_epsilon : 0.002\n","episode : 212, done reward : -645.2263820234961, total_step : 42600, cur_epsilon : 0.002\n","episode : 213, done reward : -647.5007027560602, total_step : 42800, cur_epsilon : 0.002\n","episode : 214, done reward : -643.4652796183175, total_step : 43000, cur_epsilon : 0.002\n","episode : 215, done reward : -1495.0728131449175, total_step : 43200, cur_epsilon : 0.002\n","episode : 216, done reward : -635.1061639143724, total_step : 43400, cur_epsilon : 0.002\n","episode : 217, done reward : -646.2830860066746, total_step : 43600, cur_epsilon : 0.002\n","episode : 218, done reward : -526.6604678320055, total_step : 43800, cur_epsilon : 0.002\n","episode : 219, done reward : -741.9412566385765, total_step : 44000, cur_epsilon : 0.002\n","episode : 220, done reward : -758.643476344425, total_step : 44200, cur_epsilon : 0.002\n","episode : 221, done reward : -761.0973290992268, total_step : 44400, cur_epsilon : 0.002\n","episode : 222, done reward : -1503.1611001484594, total_step : 44600, cur_epsilon : 0.002\n","episode : 223, done reward : -767.8224663074633, total_step : 44800, cur_epsilon : 0.002\n","episode : 224, done reward : -653.3970169680008, total_step : 45000, cur_epsilon : 0.002\n","episode : 225, done reward : -645.1197461068203, total_step : 45200, cur_epsilon : 0.002\n","episode : 226, done reward : -637.5442174251065, total_step : 45400, cur_epsilon : 0.002\n","episode : 227, done reward : -635.399292911409, total_step : 45600, cur_epsilon : 0.002\n","episode : 228, done reward : -644.0422403048204, total_step : 45800, cur_epsilon : 0.002\n","episode : 229, done reward : -523.6936753733693, total_step : 46000, cur_epsilon : 0.002\n","episode : 230, done reward : -1495.8120630446047, total_step : 46200, cur_epsilon : 0.002\n","episode : 231, done reward : -519.6589659517947, total_step : 46400, cur_epsilon : 0.002\n","episode : 232, done reward : -516.2060540051046, total_step : 46600, cur_epsilon : 0.002\n","episode : 233, done reward : -285.91272226220246, total_step : 46800, cur_epsilon : 0.002\n","episode : 234, done reward : -140.29421810865728, total_step : 47000, cur_epsilon : 0.002\n","episode : 235, done reward : -136.68294416992688, total_step : 47200, cur_epsilon : 0.002\n","episode : 236, done reward : -264.5079631181863, total_step : 47400, cur_epsilon : 0.002\n","episode : 237, done reward : -133.0675093182427, total_step : 47600, cur_epsilon : 0.002\n","episode : 238, done reward : -259.17390057266374, total_step : 47800, cur_epsilon : 0.002\n","episode : 239, done reward : -140.12444932127156, total_step : 48000, cur_epsilon : 0.002\n","episode : 240, done reward : -131.17556464398854, total_step : 48200, cur_epsilon : 0.002\n","episode : 241, done reward : -1495.7182281581358, total_step : 48400, cur_epsilon : 0.002\n","episode : 242, done reward : -128.1669348668735, total_step : 48600, cur_epsilon : 0.002\n","episode : 243, done reward : -138.46221597077022, total_step : 48800, cur_epsilon : 0.002\n","episode : 244, done reward : -1472.669083137389, total_step : 49000, cur_epsilon : 0.002\n","episode : 245, done reward : -142.6050657442156, total_step : 49200, cur_epsilon : 0.002\n","episode : 246, done reward : -16.03786586441909, total_step : 49400, cur_epsilon : 0.002\n","episode : 247, done reward : -152.19532717626535, total_step : 49600, cur_epsilon : 0.002\n","episode : 248, done reward : -15.32969057955656, total_step : 49800, cur_epsilon : 0.002\n","episode : 249, done reward : -1508.0270869828514, total_step : 50000, cur_epsilon : 0.002\n","episode : 250, done reward : -269.9440347835525, total_step : 50200, cur_epsilon : 0.002\n","episode : 251, done reward : -130.91409819532552, total_step : 50400, cur_epsilon : 0.002\n","episode : 252, done reward : -16.040407832228773, total_step : 50600, cur_epsilon : 0.002\n","episode : 253, done reward : -16.406075623848334, total_step : 50800, cur_epsilon : 0.002\n","episode : 254, done reward : -780.9267995771444, total_step : 51000, cur_epsilon : 0.002\n","episode : 255, done reward : -252.2507743759963, total_step : 51200, cur_epsilon : 0.002\n","episode : 256, done reward : -262.216448800658, total_step : 51400, cur_epsilon : 0.002\n","episode : 257, done reward : -138.83480733967198, total_step : 51600, cur_epsilon : 0.002\n","episode : 258, done reward : -138.550693996972, total_step : 51800, cur_epsilon : 0.002\n","episode : 259, done reward : -131.13114688955764, total_step : 52000, cur_epsilon : 0.002\n","episode : 260, done reward : -1493.1314180117631, total_step : 52200, cur_epsilon : 0.002\n","episode : 261, done reward : -131.97504655974953, total_step : 52400, cur_epsilon : 0.002\n","episode : 262, done reward : -135.14649345286003, total_step : 52600, cur_epsilon : 0.002\n","episode : 263, done reward : -139.72927791626577, total_step : 52800, cur_epsilon : 0.002\n","episode : 264, done reward : -128.85588995568148, total_step : 53000, cur_epsilon : 0.002\n","episode : 265, done reward : -130.2994041672662, total_step : 53200, cur_epsilon : 0.002\n","episode : 266, done reward : -1519.9937693313543, total_step : 53400, cur_epsilon : 0.002\n","episode : 267, done reward : -140.6828985619002, total_step : 53600, cur_epsilon : 0.002\n","episode : 268, done reward : -265.54746926255825, total_step : 53800, cur_epsilon : 0.002\n","episode : 269, done reward : -1491.0885740627596, total_step : 54000, cur_epsilon : 0.002\n","episode : 270, done reward : -254.27018961465447, total_step : 54200, cur_epsilon : 0.002\n","episode : 271, done reward : -139.7319350730885, total_step : 54400, cur_epsilon : 0.002\n","episode : 272, done reward : -19.581308816220993, total_step : 54600, cur_epsilon : 0.002\n","episode : 273, done reward : -20.474456322347876, total_step : 54800, cur_epsilon : 0.002\n","episode : 274, done reward : -20.623230669198424, total_step : 55000, cur_epsilon : 0.002\n","episode : 275, done reward : -258.1181295289047, total_step : 55200, cur_epsilon : 0.002\n","episode : 276, done reward : -1497.7201710601673, total_step : 55400, cur_epsilon : 0.002\n","episode : 277, done reward : -144.2053916863813, total_step : 55600, cur_epsilon : 0.002\n","episode : 278, done reward : -143.86309656959327, total_step : 55800, cur_epsilon : 0.002\n","episode : 279, done reward : -144.1559541842538, total_step : 56000, cur_epsilon : 0.002\n","episode : 280, done reward : -141.14422129367233, total_step : 56200, cur_epsilon : 0.002\n","episode : 281, done reward : -1497.5465715146408, total_step : 56400, cur_epsilon : 0.002\n","episode : 282, done reward : -145.19967335167487, total_step : 56600, cur_epsilon : 0.002\n","episode : 283, done reward : -265.62793185475914, total_step : 56800, cur_epsilon : 0.002\n","episode : 284, done reward : -295.1641729109229, total_step : 57000, cur_epsilon : 0.002\n","episode : 285, done reward : -380.3726839790149, total_step : 57200, cur_epsilon : 0.002\n","episode : 286, done reward : -378.20184905820685, total_step : 57400, cur_epsilon : 0.002\n","episode : 287, done reward : -377.97052601068935, total_step : 57600, cur_epsilon : 0.002\n","episode : 288, done reward : -378.03326969416236, total_step : 57800, cur_epsilon : 0.002\n","episode : 289, done reward : -389.116520580692, total_step : 58000, cur_epsilon : 0.002\n","episode : 290, done reward : -1505.317105745681, total_step : 58200, cur_epsilon : 0.002\n","episode : 291, done reward : -510.6314377296486, total_step : 58400, cur_epsilon : 0.002\n","episode : 292, done reward : -1500.923548555996, total_step : 58600, cur_epsilon : 0.002\n","episode : 293, done reward : -386.2749510858814, total_step : 58800, cur_epsilon : 0.002\n","episode : 294, done reward : -389.3201034577846, total_step : 59000, cur_epsilon : 0.002\n","episode : 295, done reward : -496.4164065992012, total_step : 59200, cur_epsilon : 0.002\n","episode : 296, done reward : -494.0041856342071, total_step : 59400, cur_epsilon : 0.002\n","episode : 297, done reward : -417.7768985240053, total_step : 59600, cur_epsilon : 0.002\n","episode : 298, done reward : -503.4269232675231, total_step : 59800, cur_epsilon : 0.002\n","episode : 299, done reward : -393.8792644620784, total_step : 60000, cur_epsilon : 0.002\n","episode : 300, done reward : -499.4750428320362, total_step : 60200, cur_epsilon : 0.002\n","episode : 301, done reward : -473.44679247748024, total_step : 60400, cur_epsilon : 0.002\n","episode : 302, done reward : -487.9293589329668, total_step : 60600, cur_epsilon : 0.002\n","episode : 303, done reward : -396.08526782511257, total_step : 60800, cur_epsilon : 0.002\n","episode : 304, done reward : -637.1844343802467, total_step : 61000, cur_epsilon : 0.002\n","episode : 305, done reward : -502.554055196387, total_step : 61200, cur_epsilon : 0.002\n","episode : 306, done reward : -493.6627674984815, total_step : 61400, cur_epsilon : 0.002\n","episode : 307, done reward : -385.0325684500227, total_step : 61600, cur_epsilon : 0.002\n","episode : 308, done reward : -386.151763980655, total_step : 61800, cur_epsilon : 0.002\n","episode : 309, done reward : -378.14244363049437, total_step : 62000, cur_epsilon : 0.002\n","episode : 310, done reward : -498.7587378103261, total_step : 62200, cur_epsilon : 0.002\n","episode : 311, done reward : -389.41825470567915, total_step : 62400, cur_epsilon : 0.002\n","episode : 312, done reward : -393.8391060899997, total_step : 62600, cur_epsilon : 0.002\n","episode : 313, done reward : -386.7173185871204, total_step : 62800, cur_epsilon : 0.002\n","episode : 314, done reward : -389.9571297140292, total_step : 63000, cur_epsilon : 0.002\n","episode : 315, done reward : -508.86100692327017, total_step : 63200, cur_epsilon : 0.002\n","episode : 316, done reward : -388.2265038884901, total_step : 63400, cur_epsilon : 0.002\n","episode : 317, done reward : -383.1363435815804, total_step : 63600, cur_epsilon : 0.002\n","episode : 318, done reward : -1521.7850615444268, total_step : 63800, cur_epsilon : 0.002\n","episode : 319, done reward : -393.0011778898748, total_step : 64000, cur_epsilon : 0.002\n","episode : 320, done reward : -646.9740961434106, total_step : 64200, cur_epsilon : 0.002\n","episode : 321, done reward : -315.6778185863198, total_step : 64400, cur_epsilon : 0.002\n","episode : 322, done reward : -396.8538907688992, total_step : 64600, cur_epsilon : 0.002\n","episode : 323, done reward : -1491.1577741173, total_step : 64800, cur_epsilon : 0.002\n","episode : 324, done reward : -390.45683345242037, total_step : 65000, cur_epsilon : 0.002\n","episode : 325, done reward : -513.957769772435, total_step : 65200, cur_epsilon : 0.002\n","episode : 326, done reward : -394.0741318125378, total_step : 65400, cur_epsilon : 0.002\n","episode : 327, done reward : -394.2967475175187, total_step : 65600, cur_epsilon : 0.002\n","episode : 328, done reward : -372.3275367764503, total_step : 65800, cur_epsilon : 0.002\n","episode : 329, done reward : -392.9755846699478, total_step : 66000, cur_epsilon : 0.002\n","episode : 330, done reward : -395.22683239332906, total_step : 66200, cur_epsilon : 0.002\n","episode : 331, done reward : -1491.1508564168, total_step : 66400, cur_epsilon : 0.002\n","episode : 332, done reward : -393.92325309790454, total_step : 66600, cur_epsilon : 0.002\n","episode : 333, done reward : -390.3018994363285, total_step : 66800, cur_epsilon : 0.002\n","episode : 334, done reward : -1491.8819962600346, total_step : 67000, cur_epsilon : 0.002\n","episode : 335, done reward : -387.08074345348075, total_step : 67200, cur_epsilon : 0.002\n","episode : 336, done reward : -494.02390170484773, total_step : 67400, cur_epsilon : 0.002\n","episode : 337, done reward : -395.5212025239026, total_step : 67600, cur_epsilon : 0.002\n","episode : 338, done reward : -501.8648039660566, total_step : 67800, cur_epsilon : 0.002\n","episode : 339, done reward : -261.55532371716816, total_step : 68000, cur_epsilon : 0.002\n","episode : 340, done reward : -392.33507233485005, total_step : 68200, cur_epsilon : 0.002\n","episode : 341, done reward : -388.9594167373725, total_step : 68400, cur_epsilon : 0.002\n","episode : 342, done reward : -1494.6687317422404, total_step : 68600, cur_epsilon : 0.002\n","episode : 343, done reward : -517.0808182242414, total_step : 68800, cur_epsilon : 0.002\n","episode : 344, done reward : -263.82663274382685, total_step : 69000, cur_epsilon : 0.002\n","episode : 345, done reward : -352.82683073870425, total_step : 69200, cur_epsilon : 0.002\n","episode : 346, done reward : -626.49360079295, total_step : 69400, cur_epsilon : 0.002\n","episode : 347, done reward : -392.1476938522648, total_step : 69600, cur_epsilon : 0.002\n","episode : 348, done reward : -403.1890814515391, total_step : 69800, cur_epsilon : 0.002\n","episode : 349, done reward : -261.5390352869332, total_step : 70000, cur_epsilon : 0.002\n","episode : 350, done reward : -147.4351250655994, total_step : 70200, cur_epsilon : 0.002\n","episode : 351, done reward : -1500.3778753663596, total_step : 70400, cur_epsilon : 0.002\n","episode : 352, done reward : -20.07075138913687, total_step : 70600, cur_epsilon : 0.002\n","episode : 353, done reward : -140.0594460936843, total_step : 70800, cur_epsilon : 0.002\n","episode : 354, done reward : -143.16728316934362, total_step : 71000, cur_epsilon : 0.002\n","episode : 355, done reward : -142.93580758106796, total_step : 71200, cur_epsilon : 0.002\n","episode : 356, done reward : -143.20180201622776, total_step : 71400, cur_epsilon : 0.002\n","episode : 357, done reward : -1511.3299955041805, total_step : 71600, cur_epsilon : 0.002\n","episode : 358, done reward : -279.30678070515256, total_step : 71800, cur_epsilon : 0.002\n","episode : 359, done reward : -284.56517183504764, total_step : 72000, cur_epsilon : 0.002\n","episode : 360, done reward : -285.33205265385595, total_step : 72200, cur_epsilon : 0.002\n","episode : 361, done reward : -1503.2397112824312, total_step : 72400, cur_epsilon : 0.002\n","episode : 362, done reward : -904.4714528146761, total_step : 72600, cur_epsilon : 0.002\n","episode : 363, done reward : -394.81096722187146, total_step : 72800, cur_epsilon : 0.002\n","episode : 364, done reward : -655.6311782933295, total_step : 73000, cur_epsilon : 0.002\n","episode : 365, done reward : -640.8828909203843, total_step : 73200, cur_epsilon : 0.002\n","episode : 366, done reward : -415.1071313589174, total_step : 73400, cur_epsilon : 0.002\n","episode : 367, done reward : -1014.5634110037086, total_step : 73600, cur_epsilon : 0.002\n","episode : 368, done reward : -139.27668805164615, total_step : 73800, cur_epsilon : 0.002\n","episode : 369, done reward : -401.77866428286046, total_step : 74000, cur_epsilon : 0.002\n","episode : 370, done reward : -1492.6821041098524, total_step : 74200, cur_epsilon : 0.002\n","episode : 371, done reward : -408.58280748986044, total_step : 74400, cur_epsilon : 0.002\n","episode : 372, done reward : -128.56690400001366, total_step : 74600, cur_epsilon : 0.002\n","episode : 373, done reward : -140.85988809227018, total_step : 74800, cur_epsilon : 0.002\n","episode : 374, done reward : -1498.3957796535394, total_step : 75000, cur_epsilon : 0.002\n","episode : 375, done reward : -855.4085524055807, total_step : 75200, cur_epsilon : 0.002\n","episode : 376, done reward : -137.10940727828105, total_step : 75400, cur_epsilon : 0.002\n","episode : 377, done reward : -129.05896145482615, total_step : 75600, cur_epsilon : 0.002\n","episode : 378, done reward : -262.419447618638, total_step : 75800, cur_epsilon : 0.002\n","episode : 379, done reward : -1324.4240920332788, total_step : 76000, cur_epsilon : 0.002\n","episode : 380, done reward : -127.59329931383283, total_step : 76200, cur_epsilon : 0.002\n","episode : 381, done reward : -145.55251764295951, total_step : 76400, cur_epsilon : 0.002\n","episode : 382, done reward : -13.618059410882319, total_step : 76600, cur_epsilon : 0.002\n","episode : 383, done reward : -139.293971656884, total_step : 76800, cur_epsilon : 0.002\n","episode : 384, done reward : -133.50250202731718, total_step : 77000, cur_epsilon : 0.002\n","episode : 385, done reward : -1187.7330228903427, total_step : 77200, cur_epsilon : 0.002\n","episode : 386, done reward : -1268.912765618048, total_step : 77400, cur_epsilon : 0.002\n","episode : 387, done reward : -1260.6411053827055, total_step : 77600, cur_epsilon : 0.002\n","episode : 388, done reward : -1234.9628444633324, total_step : 77800, cur_epsilon : 0.002\n","episode : 389, done reward : -1204.5490622500197, total_step : 78000, cur_epsilon : 0.002\n","episode : 390, done reward : -138.350270709743, total_step : 78200, cur_epsilon : 0.002\n","episode : 391, done reward : -1270.7210918443964, total_step : 78400, cur_epsilon : 0.002\n","episode : 392, done reward : -12.39244452756018, total_step : 78600, cur_epsilon : 0.002\n","episode : 393, done reward : -132.1565689337786, total_step : 78800, cur_epsilon : 0.002\n","episode : 394, done reward : -380.6861656986236, total_step : 79000, cur_epsilon : 0.002\n","episode : 395, done reward : -378.4631075419453, total_step : 79200, cur_epsilon : 0.002\n","episode : 396, done reward : -13.378231850937844, total_step : 79400, cur_epsilon : 0.002\n","episode : 397, done reward : -273.25406339203766, total_step : 79600, cur_epsilon : 0.002\n","episode : 398, done reward : -136.9848060310856, total_step : 79800, cur_epsilon : 0.002\n","episode : 399, done reward : -139.05563297222298, total_step : 80000, cur_epsilon : 0.002\n","episode : 400, done reward : -1173.2494526089715, total_step : 80200, cur_epsilon : 0.002\n","episode : 401, done reward : -133.37746423111008, total_step : 80400, cur_epsilon : 0.002\n","episode : 402, done reward : -327.72105256871464, total_step : 80600, cur_epsilon : 0.002\n","episode : 403, done reward : -262.5351435623744, total_step : 80800, cur_epsilon : 0.002\n","episode : 404, done reward : -139.82377532023952, total_step : 81000, cur_epsilon : 0.002\n","episode : 405, done reward : -137.908560639596, total_step : 81200, cur_epsilon : 0.002\n","episode : 406, done reward : -138.13040023891742, total_step : 81400, cur_epsilon : 0.002\n","episode : 407, done reward : -136.36799866556817, total_step : 81600, cur_epsilon : 0.002\n","episode : 408, done reward : -1085.4023744404888, total_step : 81800, cur_epsilon : 0.002\n","episode : 409, done reward : -272.04679288666455, total_step : 82000, cur_epsilon : 0.002\n","episode : 410, done reward : -1013.3998414356192, total_step : 82200, cur_epsilon : 0.002\n","episode : 411, done reward : -1031.2494046124662, total_step : 82400, cur_epsilon : 0.002\n","episode : 412, done reward : -1000.2200193815271, total_step : 82600, cur_epsilon : 0.002\n","episode : 413, done reward : -270.11967109272734, total_step : 82800, cur_epsilon : 0.002\n","episode : 414, done reward : -1016.4572756633926, total_step : 83000, cur_epsilon : 0.002\n","episode : 415, done reward : -1000.467299157038, total_step : 83200, cur_epsilon : 0.002\n","episode : 416, done reward : -889.1471655616183, total_step : 83400, cur_epsilon : 0.002\n","episode : 417, done reward : -12.524383161965831, total_step : 83600, cur_epsilon : 0.002\n","episode : 418, done reward : -271.0462554098818, total_step : 83800, cur_epsilon : 0.002\n","episode : 419, done reward : -130.6929159510138, total_step : 84000, cur_epsilon : 0.002\n","episode : 420, done reward : -134.66711736046295, total_step : 84200, cur_epsilon : 0.002\n","episode : 421, done reward : -138.95254466589645, total_step : 84400, cur_epsilon : 0.002\n","episode : 422, done reward : -133.64792198628152, total_step : 84600, cur_epsilon : 0.002\n","episode : 423, done reward : -257.840925219444, total_step : 84800, cur_epsilon : 0.002\n","episode : 424, done reward : -139.89537560092074, total_step : 85000, cur_epsilon : 0.002\n","episode : 425, done reward : -381.19594732746856, total_step : 85200, cur_epsilon : 0.002\n","episode : 426, done reward : -11.87468643719602, total_step : 85400, cur_epsilon : 0.002\n","episode : 427, done reward : -427.94840012247084, total_step : 85600, cur_epsilon : 0.002\n","episode : 428, done reward : -133.39476602972212, total_step : 85800, cur_epsilon : 0.002\n","episode : 429, done reward : -130.16948644194377, total_step : 86000, cur_epsilon : 0.002\n","episode : 430, done reward : -259.46748543314845, total_step : 86200, cur_epsilon : 0.002\n","episode : 431, done reward : -138.12392722670734, total_step : 86400, cur_epsilon : 0.002\n","episode : 432, done reward : -137.47464042710408, total_step : 86600, cur_epsilon : 0.002\n","episode : 433, done reward : -1501.435559776341, total_step : 86800, cur_epsilon : 0.002\n","episode : 434, done reward : -271.70156409887755, total_step : 87000, cur_epsilon : 0.002\n","episode : 435, done reward : -139.83215277609784, total_step : 87200, cur_epsilon : 0.002\n","episode : 436, done reward : -1493.0311757535371, total_step : 87400, cur_epsilon : 0.002\n","episode : 437, done reward : -137.25001446971257, total_step : 87600, cur_epsilon : 0.002\n","episode : 438, done reward : -388.2594781027831, total_step : 87800, cur_epsilon : 0.002\n","episode : 439, done reward : -410.5102292335647, total_step : 88000, cur_epsilon : 0.002\n","episode : 440, done reward : -133.42485779566562, total_step : 88200, cur_epsilon : 0.002\n","episode : 441, done reward : -137.4206456057508, total_step : 88400, cur_epsilon : 0.002\n","episode : 442, done reward : -135.2237947074256, total_step : 88600, cur_epsilon : 0.002\n","episode : 443, done reward : -133.28011198369524, total_step : 88800, cur_epsilon : 0.002\n","episode : 444, done reward : -1493.526474990043, total_step : 89000, cur_epsilon : 0.002\n","episode : 445, done reward : -126.69926762293812, total_step : 89200, cur_epsilon : 0.002\n","episode : 446, done reward : -126.08907055408264, total_step : 89400, cur_epsilon : 0.002\n","episode : 447, done reward : -403.4222587657114, total_step : 89600, cur_epsilon : 0.002\n","episode : 448, done reward : -12.33230626971577, total_step : 89800, cur_epsilon : 0.002\n","episode : 449, done reward : -11.865175379226296, total_step : 90000, cur_epsilon : 0.002\n","episode : 450, done reward : -133.29685057676718, total_step : 90200, cur_epsilon : 0.002\n","episode : 451, done reward : -133.87857035763616, total_step : 90400, cur_epsilon : 0.002\n","episode : 452, done reward : -137.05043228187094, total_step : 90600, cur_epsilon : 0.002\n","episode : 453, done reward : -133.72635931581925, total_step : 90800, cur_epsilon : 0.002\n","episode : 454, done reward : -252.24156942785945, total_step : 91000, cur_epsilon : 0.002\n","episode : 455, done reward : -132.37225527062364, total_step : 91200, cur_epsilon : 0.002\n","episode : 456, done reward : -1495.0869188858376, total_step : 91400, cur_epsilon : 0.002\n","episode : 457, done reward : -133.08657111887882, total_step : 91600, cur_epsilon : 0.002\n","episode : 458, done reward : -423.1797539821354, total_step : 91800, cur_epsilon : 0.002\n","episode : 459, done reward : -11.276071819200958, total_step : 92000, cur_epsilon : 0.002\n","episode : 460, done reward : -134.09572079034223, total_step : 92200, cur_epsilon : 0.002\n","episode : 461, done reward : -280.0618718042742, total_step : 92400, cur_epsilon : 0.002\n","episode : 462, done reward : -131.0946284930452, total_step : 92600, cur_epsilon : 0.002\n","episode : 463, done reward : -265.7147344913511, total_step : 92800, cur_epsilon : 0.002\n","episode : 464, done reward : -398.7265339336855, total_step : 93000, cur_epsilon : 0.002\n","episode : 465, done reward : -132.0991398209194, total_step : 93200, cur_epsilon : 0.002\n","episode : 466, done reward : -138.50548645922572, total_step : 93400, cur_epsilon : 0.002\n","episode : 467, done reward : -248.07825127270888, total_step : 93600, cur_epsilon : 0.002\n","episode : 468, done reward : -13.281498449803248, total_step : 93800, cur_epsilon : 0.002\n","episode : 469, done reward : -133.48563092308248, total_step : 94000, cur_epsilon : 0.002\n","episode : 470, done reward : -134.54374167280963, total_step : 94200, cur_epsilon : 0.002\n","episode : 471, done reward : -261.5458764992034, total_step : 94400, cur_epsilon : 0.002\n","episode : 472, done reward : -258.20120533052807, total_step : 94600, cur_epsilon : 0.002\n","episode : 473, done reward : -126.3494160472633, total_step : 94800, cur_epsilon : 0.002\n","episode : 474, done reward : -131.6845044504486, total_step : 95000, cur_epsilon : 0.002\n","episode : 475, done reward : -133.8448247869968, total_step : 95200, cur_epsilon : 0.002\n","episode : 476, done reward : -141.7750035311749, total_step : 95400, cur_epsilon : 0.002\n","episode : 477, done reward : -390.5414384445001, total_step : 95600, cur_epsilon : 0.002\n","episode : 478, done reward : -131.540417607338, total_step : 95800, cur_epsilon : 0.002\n","episode : 479, done reward : -131.27648086116005, total_step : 96000, cur_epsilon : 0.002\n","episode : 480, done reward : -248.52696123376964, total_step : 96200, cur_epsilon : 0.002\n","episode : 481, done reward : -254.11736131569447, total_step : 96400, cur_epsilon : 0.002\n","episode : 482, done reward : -283.5585313084511, total_step : 96600, cur_epsilon : 0.002\n","episode : 483, done reward : -134.5806224770149, total_step : 96800, cur_epsilon : 0.002\n","episode : 484, done reward : -137.21193869137718, total_step : 97000, cur_epsilon : 0.002\n","episode : 485, done reward : -890.261444926082, total_step : 97200, cur_epsilon : 0.002\n","episode : 486, done reward : -137.64682638472885, total_step : 97400, cur_epsilon : 0.002\n","episode : 487, done reward : -127.8677266078787, total_step : 97600, cur_epsilon : 0.002\n","episode : 488, done reward : -789.1566959423151, total_step : 97800, cur_epsilon : 0.002\n","episode : 489, done reward : -386.22953038366836, total_step : 98000, cur_epsilon : 0.002\n","episode : 490, done reward : -253.19056151410626, total_step : 98200, cur_epsilon : 0.002\n","episode : 491, done reward : -123.02356043361765, total_step : 98400, cur_epsilon : 0.002\n","episode : 492, done reward : -10.800822928294183, total_step : 98600, cur_epsilon : 0.002\n","episode : 493, done reward : -139.6585201616938, total_step : 98800, cur_epsilon : 0.002\n","episode : 494, done reward : -281.79707856034895, total_step : 99000, cur_epsilon : 0.002\n","episode : 495, done reward : -264.9836232853713, total_step : 99200, cur_epsilon : 0.002\n","episode : 496, done reward : -270.84157553954907, total_step : 99400, cur_epsilon : 0.002\n","episode : 497, done reward : -136.95697397087736, total_step : 99600, cur_epsilon : 0.002\n","episode : 498, done reward : -10.084392712277094, total_step : 99800, cur_epsilon : 0.002\n","episode : 499, done reward : -361.43113531043866, total_step : 100000, cur_epsilon : 0.002\n","episode : 500, done reward : -9.303379800133355, total_step : 100200, cur_epsilon : 0.002\n","episode : 501, done reward : -256.5844024197262, total_step : 100400, cur_epsilon : 0.002\n","episode : 502, done reward : -126.28272932730376, total_step : 100600, cur_epsilon : 0.002\n","episode : 503, done reward : -10.101510061870723, total_step : 100800, cur_epsilon : 0.002\n","episode : 504, done reward : -254.18221717003308, total_step : 101000, cur_epsilon : 0.002\n","episode : 505, done reward : -321.41671284299576, total_step : 101200, cur_epsilon : 0.002\n","episode : 506, done reward : -579.9092609284846, total_step : 101400, cur_epsilon : 0.002\n","episode : 507, done reward : -0.44811456514689996, total_step : 101600, cur_epsilon : 0.002\n","episode : 508, done reward : -380.80205964307885, total_step : 101800, cur_epsilon : 0.002\n","episode : 509, done reward : -611.1757972764365, total_step : 102000, cur_epsilon : 0.002\n","episode : 510, done reward : -121.84708068866368, total_step : 102200, cur_epsilon : 0.002\n","episode : 511, done reward : -365.38680500415035, total_step : 102400, cur_epsilon : 0.002\n","episode : 512, done reward : -384.73843846572873, total_step : 102600, cur_epsilon : 0.002\n","episode : 513, done reward : -291.04077466178364, total_step : 102800, cur_epsilon : 0.002\n","episode : 514, done reward : -360.6589782288787, total_step : 103000, cur_epsilon : 0.002\n","episode : 515, done reward : -118.57012666067503, total_step : 103200, cur_epsilon : 0.002\n","episode : 516, done reward : -244.77083000979127, total_step : 103400, cur_epsilon : 0.002\n","episode : 517, done reward : -122.94839490947122, total_step : 103600, cur_epsilon : 0.002\n","episode : 518, done reward : -1051.7548579446868, total_step : 103800, cur_epsilon : 0.002\n","episode : 519, done reward : -287.4577400265492, total_step : 104000, cur_epsilon : 0.002\n","episode : 520, done reward : -1.161933961343877, total_step : 104200, cur_epsilon : 0.002\n","episode : 521, done reward : -1043.3766507509429, total_step : 104400, cur_epsilon : 0.002\n","episode : 522, done reward : -1063.4384880464902, total_step : 104600, cur_epsilon : 0.002\n","episode : 523, done reward : -0.3737755904002269, total_step : 104800, cur_epsilon : 0.002\n","episode : 524, done reward : -0.21627624338275248, total_step : 105000, cur_epsilon : 0.002\n","episode : 525, done reward : -1056.981474496664, total_step : 105200, cur_epsilon : 0.002\n","episode : 526, done reward : -1.2259607083413286, total_step : 105400, cur_epsilon : 0.002\n","episode : 527, done reward : -238.73280371475425, total_step : 105600, cur_epsilon : 0.002\n","episode : 528, done reward : -421.0169659696438, total_step : 105800, cur_epsilon : 0.002\n","episode : 529, done reward : -123.91520404688045, total_step : 106000, cur_epsilon : 0.002\n","episode : 530, done reward : -128.9592397015357, total_step : 106200, cur_epsilon : 0.002\n","episode : 531, done reward : -132.09270523769916, total_step : 106400, cur_epsilon : 0.002\n","episode : 532, done reward : -0.3531799391600819, total_step : 106600, cur_epsilon : 0.002\n","episode : 533, done reward : -249.95392660786126, total_step : 106800, cur_epsilon : 0.002\n","episode : 534, done reward : -240.39203640628637, total_step : 107000, cur_epsilon : 0.002\n","episode : 535, done reward : -0.6143913778736619, total_step : 107200, cur_epsilon : 0.002\n","episode : 536, done reward : -1061.5650759129078, total_step : 107400, cur_epsilon : 0.002\n","episode : 537, done reward : -130.42164030490127, total_step : 107600, cur_epsilon : 0.002\n","episode : 538, done reward : -308.93411992814157, total_step : 107800, cur_epsilon : 0.002\n","episode : 539, done reward : -125.96879361935582, total_step : 108000, cur_epsilon : 0.002\n","episode : 540, done reward : -254.91108200008756, total_step : 108200, cur_epsilon : 0.002\n","episode : 541, done reward : -398.5085457899943, total_step : 108400, cur_epsilon : 0.002\n","episode : 542, done reward : -1057.526695490709, total_step : 108600, cur_epsilon : 0.002\n","episode : 543, done reward : -715.777913030178, total_step : 108800, cur_epsilon : 0.002\n","episode : 544, done reward : -118.92140814550862, total_step : 109000, cur_epsilon : 0.002\n","episode : 545, done reward : -236.51737562451976, total_step : 109200, cur_epsilon : 0.002\n","episode : 546, done reward : -121.97306996678311, total_step : 109400, cur_epsilon : 0.002\n","episode : 547, done reward : -0.22153046105901142, total_step : 109600, cur_epsilon : 0.002\n","episode : 548, done reward : -385.909157848753, total_step : 109800, cur_epsilon : 0.002\n","episode : 549, done reward : -120.69537067315609, total_step : 110000, cur_epsilon : 0.002\n","episode : 550, done reward : -125.73100848579264, total_step : 110200, cur_epsilon : 0.002\n","episode : 551, done reward : -292.08297626789124, total_step : 110400, cur_epsilon : 0.002\n","episode : 552, done reward : -1059.8875288395996, total_step : 110600, cur_epsilon : 0.002\n","episode : 553, done reward : -123.90387568800851, total_step : 110800, cur_epsilon : 0.002\n","episode : 554, done reward : -132.59704765220332, total_step : 111000, cur_epsilon : 0.002\n","episode : 555, done reward : -291.4942296421967, total_step : 111200, cur_epsilon : 0.002\n","episode : 556, done reward : -1.0205431106387137, total_step : 111400, cur_epsilon : 0.002\n","episode : 557, done reward : -124.05646698311672, total_step : 111600, cur_epsilon : 0.002\n","episode : 558, done reward : -1133.189523878974, total_step : 111800, cur_epsilon : 0.002\n","episode : 559, done reward : -125.77940914585702, total_step : 112000, cur_epsilon : 0.002\n","episode : 560, done reward : -245.80120912213235, total_step : 112200, cur_epsilon : 0.002\n","episode : 561, done reward : -118.29894505106128, total_step : 112400, cur_epsilon : 0.002\n","episode : 562, done reward : -132.453547319694, total_step : 112600, cur_epsilon : 0.002\n","episode : 563, done reward : -391.1984475920403, total_step : 112800, cur_epsilon : 0.002\n","episode : 564, done reward : -120.49773905961844, total_step : 113000, cur_epsilon : 0.002\n","episode : 565, done reward : -1072.0053992530961, total_step : 113200, cur_epsilon : 0.002\n","episode : 566, done reward : -1150.8370314439198, total_step : 113400, cur_epsilon : 0.002\n","episode : 567, done reward : -118.9304546083508, total_step : 113600, cur_epsilon : 0.002\n","episode : 568, done reward : -258.66736164014384, total_step : 113800, cur_epsilon : 0.002\n","episode : 569, done reward : -122.93484567248288, total_step : 114000, cur_epsilon : 0.002\n","episode : 570, done reward : -1153.8120070366842, total_step : 114200, cur_epsilon : 0.002\n","episode : 571, done reward : -1165.8949034407947, total_step : 114400, cur_epsilon : 0.002\n","episode : 572, done reward : -1094.4602987125875, total_step : 114600, cur_epsilon : 0.002\n","episode : 573, done reward : -1042.0681565287798, total_step : 114800, cur_epsilon : 0.002\n","episode : 574, done reward : -380.82816962237837, total_step : 115000, cur_epsilon : 0.002\n","episode : 575, done reward : -0.41697765486767613, total_step : 115200, cur_epsilon : 0.002\n","episode : 576, done reward : -245.14352733783213, total_step : 115400, cur_epsilon : 0.002\n","episode : 577, done reward : -116.67562376021424, total_step : 115600, cur_epsilon : 0.002\n","episode : 578, done reward : -265.3056518807032, total_step : 115800, cur_epsilon : 0.002\n","episode : 579, done reward : -1083.7062813459727, total_step : 116000, cur_epsilon : 0.002\n","episode : 580, done reward : -1188.114778901255, total_step : 116200, cur_epsilon : 0.002\n","episode : 581, done reward : -125.81376756337548, total_step : 116400, cur_epsilon : 0.002\n","episode : 582, done reward : -396.4238599537804, total_step : 116600, cur_epsilon : 0.002\n","episode : 583, done reward : -118.45584532279577, total_step : 116800, cur_epsilon : 0.002\n","episode : 584, done reward : -123.16838121960234, total_step : 117000, cur_epsilon : 0.002\n","episode : 585, done reward : -122.86432009606114, total_step : 117200, cur_epsilon : 0.002\n","episode : 586, done reward : -131.47635700213095, total_step : 117400, cur_epsilon : 0.002\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-120-debae065ee03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mepisode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-4958b560386c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_iter, batch_size)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mq_loss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1_loss_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq2_loss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mq_loss_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_twinq_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmaximize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mexp_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# exp_buffer = NStepPriorityReplayBuffer(\n","#     max_size=30000,\n","#     prob_alpha=0.6,\n","#     beta_start=0.4,\n","#     beta_frames=30000, #100000,\n","#     n_step=10,\n","#     gamma=0.99,\n","# )\n","# agent = Agent(\n","#     env=env,\n","#     exp_buffer=exp_buffer,\n","#     crt_net=crt_net,\n","#     act_net=act_net,\n","#     twinq_net=twinq_net,\n","#     epsilon_start=0.1,\n","#     epsilon_final=0.002,\n","#     epsilon_decay_last_step=50000, #200000,\n","#     # tgt_sync_steps=1000,\n","#     learning_rate_acts=5e-3,\n","#     learning_rate_vals=5e-3,\n","#     device=device\n","# )\n","\n","episode = 0\n","\n","while True:\n","\n","    for stp in range(1):\n","        done_reward = agent.play_step()\n","        if done_reward is not None:\n","            print(f'episode : {episode}, done reward : {done_reward}, total_step : {agent._total_step}, cur_epsilon : {agent._epsilon}')\n","            episode += 1\n","\n","    agent.train(n_iter=1, batch_size=64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"iWH3aKTkyRR6","outputId":"fbc0dc55-fdc6-4ea1-f818-4ddb33757d89","executionInfo":{"status":"error","timestamp":1673903267599,"user_tz":-540,"elapsed":345256,"user":{"displayName":"Yoshihiro Matsumoto","userId":"05765613951479945662"}}},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["episode : 0, done reward : -1791.596726363171, total_step : 200, cur_epsilon : 0.096\n","episode : 1, done reward : -820.7458328369454, total_step : 400, cur_epsilon : 0.092\n","episode : 2, done reward : -419.2005364950901, total_step : 600, cur_epsilon : 0.08800000000000001\n","episode : 3, done reward : -1.4888059746269202, total_step : 800, cur_epsilon : 0.084\n","episode : 4, done reward : -829.1961982065752, total_step : 1000, cur_epsilon : 0.08\n","episode : 5, done reward : -267.61810227681985, total_step : 1200, cur_epsilon : 0.07600000000000001\n","episode : 6, done reward : -269.37881113718737, total_step : 1400, cur_epsilon : 0.07200000000000001\n","episode : 7, done reward : -331.449687215132, total_step : 1600, cur_epsilon : 0.068\n","episode : 8, done reward : -265.31014021348545, total_step : 1800, cur_epsilon : 0.064\n","episode : 9, done reward : -268.2124962824307, total_step : 2000, cur_epsilon : 0.060000000000000005\n","episode : 10, done reward : -556.403191665792, total_step : 2200, cur_epsilon : 0.05600000000000001\n","episode : 11, done reward : -635.0801674134527, total_step : 2400, cur_epsilon : 0.052000000000000005\n","episode : 12, done reward : -1501.4677635199535, total_step : 2600, cur_epsilon : 0.04800000000000001\n","episode : 13, done reward : -373.27095689469456, total_step : 2800, cur_epsilon : 0.044000000000000004\n","episode : 14, done reward : -138.8853302234601, total_step : 3000, cur_epsilon : 0.04000000000000001\n","episode : 15, done reward : -142.95988483730136, total_step : 3200, cur_epsilon : 0.036000000000000004\n","episode : 16, done reward : -278.82505001858163, total_step : 3400, cur_epsilon : 0.032\n","episode : 17, done reward : -722.85499366099, total_step : 3600, cur_epsilon : 0.02800000000000001\n","episode : 18, done reward : -539.2611260261954, total_step : 3800, cur_epsilon : 0.024000000000000007\n","episode : 19, done reward : -385.55159799956465, total_step : 4000, cur_epsilon : 0.020000000000000004\n","episode : 20, done reward : -139.21356280871643, total_step : 4200, cur_epsilon : 0.016\n","episode : 21, done reward : -140.4116934626994, total_step : 4400, cur_epsilon : 0.01200000000000001\n","episode : 22, done reward : -537.7863560268011, total_step : 4600, cur_epsilon : 0.008000000000000007\n","episode : 23, done reward : -548.893523360375, total_step : 4800, cur_epsilon : 0.0040000000000000036\n","episode : 24, done reward : -266.43236253149524, total_step : 5000, cur_epsilon : 0.002\n","episode : 25, done reward : -410.65398598414106, total_step : 5200, cur_epsilon : 0.002\n","episode : 26, done reward : -539.6211044166782, total_step : 5400, cur_epsilon : 0.002\n","episode : 27, done reward : -1492.7175491546075, total_step : 5600, cur_epsilon : 0.002\n","episode : 28, done reward : -376.2113738639207, total_step : 5800, cur_epsilon : 0.002\n","episode : 29, done reward : -330.8143746074831, total_step : 6000, cur_epsilon : 0.002\n","episode : 30, done reward : -272.56281583644216, total_step : 6200, cur_epsilon : 0.002\n","episode : 31, done reward : -401.4036543271305, total_step : 6400, cur_epsilon : 0.002\n","episode : 32, done reward : -269.31483883544894, total_step : 6600, cur_epsilon : 0.002\n","episode : 33, done reward : -271.98406091649497, total_step : 6800, cur_epsilon : 0.002\n","episode : 34, done reward : -561.4384917069137, total_step : 7000, cur_epsilon : 0.002\n","episode : 35, done reward : -288.70128397792536, total_step : 7200, cur_epsilon : 0.002\n","episode : 36, done reward : -1496.755703601805, total_step : 7400, cur_epsilon : 0.002\n","episode : 37, done reward : -142.29399776583193, total_step : 7600, cur_epsilon : 0.002\n","episode : 38, done reward : -734.9246755941901, total_step : 7800, cur_epsilon : 0.002\n","episode : 39, done reward : -408.812671783263, total_step : 8000, cur_epsilon : 0.002\n","episode : 40, done reward : -404.9976128670288, total_step : 8200, cur_epsilon : 0.002\n","episode : 41, done reward : -1494.69248655614, total_step : 8400, cur_epsilon : 0.002\n","episode : 42, done reward : -414.5190066572203, total_step : 8600, cur_epsilon : 0.002\n","episode : 43, done reward : -401.85874399973875, total_step : 8800, cur_epsilon : 0.002\n","episode : 44, done reward : -280.4818558502479, total_step : 9000, cur_epsilon : 0.002\n","episode : 45, done reward : -656.1418568623325, total_step : 9200, cur_epsilon : 0.002\n","episode : 46, done reward : -270.3503571469961, total_step : 9400, cur_epsilon : 0.002\n","episode : 47, done reward : -277.62045351930425, total_step : 9600, cur_epsilon : 0.002\n","episode : 48, done reward : -410.05945855151015, total_step : 9800, cur_epsilon : 0.002\n","episode : 49, done reward : -670.7369424936501, total_step : 10000, cur_epsilon : 0.002\n","episode : 50, done reward : -508.6510737746935, total_step : 10200, cur_epsilon : 0.002\n","episode : 51, done reward : -723.608314100748, total_step : 10400, cur_epsilon : 0.002\n","episode : 52, done reward : -540.1657607612148, total_step : 10600, cur_epsilon : 0.002\n","episode : 53, done reward : -406.406312915824, total_step : 10800, cur_epsilon : 0.002\n","episode : 54, done reward : -1492.4654116797365, total_step : 11000, cur_epsilon : 0.002\n","episode : 55, done reward : -281.51638573265376, total_step : 11200, cur_epsilon : 0.002\n","episode : 56, done reward : -539.3542201757642, total_step : 11400, cur_epsilon : 0.002\n","episode : 57, done reward : -406.607128548325, total_step : 11600, cur_epsilon : 0.002\n","episode : 58, done reward : -545.9878799191781, total_step : 11800, cur_epsilon : 0.002\n","episode : 59, done reward : -534.3169476059359, total_step : 12000, cur_epsilon : 0.002\n","episode : 60, done reward : -277.39387320946093, total_step : 12200, cur_epsilon : 0.002\n","episode : 61, done reward : -270.1353901499524, total_step : 12400, cur_epsilon : 0.002\n","episode : 62, done reward : -543.3971653207611, total_step : 12600, cur_epsilon : 0.002\n","episode : 63, done reward : -143.2002011191007, total_step : 12800, cur_epsilon : 0.002\n","episode : 64, done reward : -397.97119263659243, total_step : 13000, cur_epsilon : 0.002\n","episode : 65, done reward : -522.869661993273, total_step : 13200, cur_epsilon : 0.002\n","episode : 66, done reward : -266.03322361693404, total_step : 13400, cur_epsilon : 0.002\n","episode : 67, done reward : -140.26248493929768, total_step : 13600, cur_epsilon : 0.002\n","episode : 68, done reward : -650.034598825036, total_step : 13800, cur_epsilon : 0.002\n","episode : 69, done reward : -12.31756656804183, total_step : 14000, cur_epsilon : 0.002\n","episode : 70, done reward : -272.6300648796497, total_step : 14200, cur_epsilon : 0.002\n","episode : 71, done reward : -713.0624886254684, total_step : 14400, cur_epsilon : 0.002\n","episode : 72, done reward : -266.60025595623136, total_step : 14600, cur_epsilon : 0.002\n","episode : 73, done reward : -138.34571405415596, total_step : 14800, cur_epsilon : 0.002\n","episode : 74, done reward : -6.214209272240744, total_step : 15000, cur_epsilon : 0.002\n","episode : 75, done reward : -6.650380981281894, total_step : 15200, cur_epsilon : 0.002\n","episode : 76, done reward : -683.8340995162199, total_step : 15400, cur_epsilon : 0.002\n","episode : 77, done reward : -296.52850037223357, total_step : 15600, cur_epsilon : 0.002\n","episode : 78, done reward : -278.4687184976947, total_step : 15800, cur_epsilon : 0.002\n","episode : 79, done reward : -140.02841264915594, total_step : 16000, cur_epsilon : 0.002\n","episode : 80, done reward : -286.00951467977643, total_step : 16200, cur_epsilon : 0.002\n","episode : 81, done reward : -268.0493419517066, total_step : 16400, cur_epsilon : 0.002\n","episode : 82, done reward : -9.082872091643528, total_step : 16600, cur_epsilon : 0.002\n","episode : 83, done reward : -279.2689733376869, total_step : 16800, cur_epsilon : 0.002\n","episode : 84, done reward : -524.7505196010297, total_step : 17000, cur_epsilon : 0.002\n","episode : 85, done reward : -279.7002286932593, total_step : 17200, cur_epsilon : 0.002\n","episode : 86, done reward : -271.90426869141254, total_step : 17400, cur_epsilon : 0.002\n","episode : 87, done reward : -407.245557081554, total_step : 17600, cur_epsilon : 0.002\n","episode : 88, done reward : -144.92724336670818, total_step : 17800, cur_epsilon : 0.002\n","episode : 89, done reward : -1491.935990696387, total_step : 18000, cur_epsilon : 0.002\n","episode : 90, done reward : -796.2303550456575, total_step : 18200, cur_epsilon : 0.002\n","episode : 91, done reward : -144.04155654109886, total_step : 18400, cur_epsilon : 0.002\n","episode : 92, done reward : -132.90634425376675, total_step : 18600, cur_epsilon : 0.002\n","episode : 93, done reward : -273.1345626034898, total_step : 18800, cur_epsilon : 0.002\n","episode : 94, done reward : -143.36051442032274, total_step : 19000, cur_epsilon : 0.002\n","episode : 95, done reward : -143.3524425570242, total_step : 19200, cur_epsilon : 0.002\n","episode : 96, done reward : -329.46254975799894, total_step : 19400, cur_epsilon : 0.002\n","episode : 97, done reward : -545.9017523284156, total_step : 19600, cur_epsilon : 0.002\n","episode : 98, done reward : -672.1074773863219, total_step : 19800, cur_epsilon : 0.002\n","episode : 99, done reward : -737.0163071186478, total_step : 20000, cur_epsilon : 0.002\n","episode : 100, done reward : -143.56782087982148, total_step : 20200, cur_epsilon : 0.002\n","episode : 101, done reward : -301.17494490926885, total_step : 20400, cur_epsilon : 0.002\n","episode : 102, done reward : -597.8953935908838, total_step : 20600, cur_epsilon : 0.002\n","episode : 103, done reward : -418.7835244911093, total_step : 20800, cur_epsilon : 0.002\n","episode : 104, done reward : -673.4533528298647, total_step : 21000, cur_epsilon : 0.002\n","episode : 105, done reward : -866.5913334749562, total_step : 21200, cur_epsilon : 0.002\n","episode : 106, done reward : -768.9278465899121, total_step : 21400, cur_epsilon : 0.002\n","episode : 107, done reward : -531.4315099797873, total_step : 21600, cur_epsilon : 0.002\n","episode : 108, done reward : -678.4381716434666, total_step : 21800, cur_epsilon : 0.002\n","episode : 109, done reward : -402.3417739814567, total_step : 22000, cur_epsilon : 0.002\n","episode : 110, done reward : -877.1135589422039, total_step : 22200, cur_epsilon : 0.002\n","episode : 111, done reward : -827.5534312966822, total_step : 22400, cur_epsilon : 0.002\n","episode : 112, done reward : -8.655585702329123, total_step : 22600, cur_epsilon : 0.002\n","episode : 113, done reward : -146.02767242463386, total_step : 22800, cur_epsilon : 0.002\n","episode : 114, done reward : -680.9301506083905, total_step : 23000, cur_epsilon : 0.002\n","episode : 115, done reward : -818.1344207199194, total_step : 23200, cur_epsilon : 0.002\n","episode : 116, done reward : -1098.1661251021221, total_step : 23400, cur_epsilon : 0.002\n","episode : 117, done reward : -904.4070487450001, total_step : 23600, cur_epsilon : 0.002\n","episode : 118, done reward : -845.386108027661, total_step : 23800, cur_epsilon : 0.002\n","episode : 119, done reward : -848.110464920162, total_step : 24000, cur_epsilon : 0.002\n","episode : 120, done reward : -792.8370898183877, total_step : 24200, cur_epsilon : 0.002\n","episode : 121, done reward : -552.1449359686122, total_step : 24400, cur_epsilon : 0.002\n","episode : 122, done reward : -973.9783401877706, total_step : 24600, cur_epsilon : 0.002\n","episode : 123, done reward : -4.230193778704566, total_step : 24800, cur_epsilon : 0.002\n","episode : 124, done reward : -266.5136137246324, total_step : 25000, cur_epsilon : 0.002\n","episode : 125, done reward : -7.25687835955886, total_step : 25200, cur_epsilon : 0.002\n","episode : 126, done reward : -271.9081857491367, total_step : 25400, cur_epsilon : 0.002\n","episode : 127, done reward : -689.9005047994934, total_step : 25600, cur_epsilon : 0.002\n","episode : 128, done reward : -133.2957962077575, total_step : 25800, cur_epsilon : 0.002\n","episode : 129, done reward : -4.680190777893631, total_step : 26000, cur_epsilon : 0.002\n","episode : 130, done reward : -4.096151702704336, total_step : 26200, cur_epsilon : 0.002\n","episode : 131, done reward : -922.3657311340614, total_step : 26400, cur_epsilon : 0.002\n","episode : 132, done reward : -139.70081851827783, total_step : 26600, cur_epsilon : 0.002\n","episode : 133, done reward : -273.6515308935485, total_step : 26800, cur_epsilon : 0.002\n","episode : 134, done reward : -547.1093792729345, total_step : 27000, cur_epsilon : 0.002\n","episode : 135, done reward : -135.43034476085083, total_step : 27200, cur_epsilon : 0.002\n","episode : 136, done reward : -134.7630531377021, total_step : 27400, cur_epsilon : 0.002\n","episode : 137, done reward : -401.0505274759017, total_step : 27600, cur_epsilon : 0.002\n","episode : 138, done reward : -274.37907572710685, total_step : 27800, cur_epsilon : 0.002\n","episode : 139, done reward : -2.175197419896818, total_step : 28000, cur_epsilon : 0.002\n","episode : 140, done reward : -1077.8153324810976, total_step : 28200, cur_epsilon : 0.002\n","episode : 141, done reward : -1096.1507398023962, total_step : 28400, cur_epsilon : 0.002\n","episode : 142, done reward : -939.3272723577461, total_step : 28600, cur_epsilon : 0.002\n","episode : 143, done reward : -273.40532323170703, total_step : 28800, cur_epsilon : 0.002\n","episode : 144, done reward : -1025.3442337538356, total_step : 29000, cur_epsilon : 0.002\n","episode : 145, done reward : -130.49519175370907, total_step : 29200, cur_epsilon : 0.002\n","episode : 146, done reward : -1085.1094283454077, total_step : 29400, cur_epsilon : 0.002\n","episode : 147, done reward : -839.2015321220298, total_step : 29600, cur_epsilon : 0.002\n","episode : 148, done reward : -1078.7882416280365, total_step : 29800, cur_epsilon : 0.002\n","episode : 149, done reward : -1019.5368709029794, total_step : 30000, cur_epsilon : 0.002\n","episode : 150, done reward : -1211.6072480774906, total_step : 30200, cur_epsilon : 0.002\n","episode : 151, done reward : -1215.417937916857, total_step : 30400, cur_epsilon : 0.002\n","episode : 152, done reward : -1160.1732205446876, total_step : 30600, cur_epsilon : 0.002\n","episode : 153, done reward : -1177.480937461675, total_step : 30800, cur_epsilon : 0.002\n","episode : 154, done reward : -1184.214871605933, total_step : 31000, cur_epsilon : 0.002\n","episode : 155, done reward : -5.143141664742172, total_step : 31200, cur_epsilon : 0.002\n","episode : 156, done reward : -1137.8467151472537, total_step : 31400, cur_epsilon : 0.002\n","episode : 157, done reward : -1158.8572777764991, total_step : 31600, cur_epsilon : 0.002\n","episode : 158, done reward : -1221.9740787640858, total_step : 31800, cur_epsilon : 0.002\n","episode : 159, done reward : -1162.4395652554654, total_step : 32000, cur_epsilon : 0.002\n","episode : 160, done reward : -1102.488815990439, total_step : 32200, cur_epsilon : 0.002\n","episode : 161, done reward : -1086.9172672235145, total_step : 32400, cur_epsilon : 0.002\n","episode : 162, done reward : -968.1770814673056, total_step : 32600, cur_epsilon : 0.002\n","episode : 163, done reward : -974.326997930128, total_step : 32800, cur_epsilon : 0.002\n","episode : 164, done reward : -843.8594720162869, total_step : 33000, cur_epsilon : 0.002\n","episode : 165, done reward : -957.5854423478395, total_step : 33200, cur_epsilon : 0.002\n","episode : 166, done reward : -539.2793318688618, total_step : 33400, cur_epsilon : 0.002\n","episode : 167, done reward : -690.6398582155585, total_step : 33600, cur_epsilon : 0.002\n","episode : 168, done reward : -805.3197600989508, total_step : 33800, cur_epsilon : 0.002\n","episode : 169, done reward : -2.4156269665009917, total_step : 34000, cur_epsilon : 0.002\n","episode : 170, done reward : -1491.5173650654162, total_step : 34200, cur_epsilon : 0.002\n","episode : 171, done reward : -792.9732352536672, total_step : 34400, cur_epsilon : 0.002\n","episode : 172, done reward : -1494.967669063513, total_step : 34600, cur_epsilon : 0.002\n","episode : 173, done reward : -843.5795647429323, total_step : 34800, cur_epsilon : 0.002\n","episode : 174, done reward : -542.5442789758356, total_step : 35000, cur_epsilon : 0.002\n","episode : 175, done reward : -965.7223649468627, total_step : 35200, cur_epsilon : 0.002\n","episode : 176, done reward : -933.0932165153831, total_step : 35400, cur_epsilon : 0.002\n","episode : 177, done reward : -520.8800561702876, total_step : 35600, cur_epsilon : 0.002\n","episode : 178, done reward : -500.204520409515, total_step : 35800, cur_epsilon : 0.002\n","episode : 179, done reward : -1492.473303418754, total_step : 36000, cur_epsilon : 0.002\n","episode : 180, done reward : -655.5760693887296, total_step : 36200, cur_epsilon : 0.002\n","episode : 181, done reward : -780.4144362123218, total_step : 36400, cur_epsilon : 0.002\n","episode : 182, done reward : -439.1307239755552, total_step : 36600, cur_epsilon : 0.002\n","episode : 183, done reward : -659.361124787301, total_step : 36800, cur_epsilon : 0.002\n","episode : 184, done reward : -266.22763802273596, total_step : 37000, cur_epsilon : 0.002\n","episode : 185, done reward : -529.4663749658495, total_step : 37200, cur_epsilon : 0.002\n","episode : 186, done reward : -272.69911074115845, total_step : 37400, cur_epsilon : 0.002\n","episode : 187, done reward : -395.21758998080105, total_step : 37600, cur_epsilon : 0.002\n","episode : 188, done reward : -131.85272779182768, total_step : 37800, cur_epsilon : 0.002\n","episode : 189, done reward : -391.3708880756329, total_step : 38000, cur_epsilon : 0.002\n","episode : 190, done reward : -394.62864308277494, total_step : 38200, cur_epsilon : 0.002\n","episode : 191, done reward : -701.076052654064, total_step : 38400, cur_epsilon : 0.002\n","episode : 192, done reward : -131.67461419797255, total_step : 38600, cur_epsilon : 0.002\n","episode : 193, done reward : -338.317243486978, total_step : 38800, cur_epsilon : 0.002\n","episode : 194, done reward : -260.15662441405055, total_step : 39000, cur_epsilon : 0.002\n","episode : 195, done reward : -387.63515442992605, total_step : 39200, cur_epsilon : 0.002\n","episode : 196, done reward : -670.0411896820998, total_step : 39400, cur_epsilon : 0.002\n","episode : 197, done reward : -535.1343391948838, total_step : 39600, cur_epsilon : 0.002\n","episode : 198, done reward : -406.9772028912375, total_step : 39800, cur_epsilon : 0.002\n","episode : 199, done reward : -505.9557900488625, total_step : 40000, cur_epsilon : 0.002\n","episode : 200, done reward : -260.4713246941934, total_step : 40200, cur_epsilon : 0.002\n","episode : 201, done reward : -524.1027825053708, total_step : 40400, cur_epsilon : 0.002\n","episode : 202, done reward : -3.530830964817934, total_step : 40600, cur_epsilon : 0.002\n","episode : 203, done reward : -561.022403539238, total_step : 40800, cur_epsilon : 0.002\n","episode : 204, done reward : -642.2100067967864, total_step : 41000, cur_epsilon : 0.002\n","episode : 205, done reward : -421.36447599577, total_step : 41200, cur_epsilon : 0.002\n","episode : 206, done reward : -263.3611685848767, total_step : 41400, cur_epsilon : 0.002\n","episode : 207, done reward : -131.2395788563212, total_step : 41600, cur_epsilon : 0.002\n","episode : 208, done reward : -2.9358491331728027, total_step : 41800, cur_epsilon : 0.002\n","episode : 209, done reward : -265.65166035750127, total_step : 42000, cur_epsilon : 0.002\n","episode : 210, done reward : -408.5997100848031, total_step : 42200, cur_epsilon : 0.002\n","episode : 211, done reward : -132.60533232343386, total_step : 42400, cur_epsilon : 0.002\n","episode : 212, done reward : -289.32993223155, total_step : 42600, cur_epsilon : 0.002\n","episode : 213, done reward : -267.7740016854025, total_step : 42800, cur_epsilon : 0.002\n","episode : 214, done reward : -536.5856748283179, total_step : 43000, cur_epsilon : 0.002\n","episode : 215, done reward : -268.2017927654254, total_step : 43200, cur_epsilon : 0.002\n","episode : 216, done reward : -536.6743397621757, total_step : 43400, cur_epsilon : 0.002\n","episode : 217, done reward : -263.86019835099137, total_step : 43600, cur_epsilon : 0.002\n","episode : 218, done reward : -132.84458718313667, total_step : 43800, cur_epsilon : 0.002\n","episode : 219, done reward : -1492.1403422624928, total_step : 44000, cur_epsilon : 0.002\n","episode : 220, done reward : -277.60758423516626, total_step : 44200, cur_epsilon : 0.002\n","episode : 221, done reward : -139.80389189991385, total_step : 44400, cur_epsilon : 0.002\n","episode : 222, done reward : -2.3723517250778556, total_step : 44600, cur_epsilon : 0.002\n","episode : 223, done reward : -133.62166113807723, total_step : 44800, cur_epsilon : 0.002\n","episode : 224, done reward : -516.1591092240043, total_step : 45000, cur_epsilon : 0.002\n","episode : 225, done reward : -135.95968459772038, total_step : 45200, cur_epsilon : 0.002\n","episode : 226, done reward : -274.0494606411393, total_step : 45400, cur_epsilon : 0.002\n","episode : 227, done reward : -788.4293830019351, total_step : 45600, cur_epsilon : 0.002\n","episode : 228, done reward : -134.39076072836704, total_step : 45800, cur_epsilon : 0.002\n","episode : 229, done reward : -403.6569274549907, total_step : 46000, cur_epsilon : 0.002\n","episode : 230, done reward : -404.76153918394266, total_step : 46200, cur_epsilon : 0.002\n","episode : 231, done reward : -145.92672970849455, total_step : 46400, cur_epsilon : 0.002\n","episode : 232, done reward : -139.14796906039595, total_step : 46600, cur_epsilon : 0.002\n","episode : 233, done reward : -8.329786678721781, total_step : 46800, cur_epsilon : 0.002\n","episode : 234, done reward : -274.06585574342324, total_step : 47000, cur_epsilon : 0.002\n","episode : 235, done reward : -3.742550226158673, total_step : 47200, cur_epsilon : 0.002\n","episode : 236, done reward : -1492.6691739043954, total_step : 47400, cur_epsilon : 0.002\n","episode : 237, done reward : -141.70848598740025, total_step : 47600, cur_epsilon : 0.002\n","episode : 238, done reward : -133.39916564182514, total_step : 47800, cur_epsilon : 0.002\n","episode : 239, done reward : -141.90682649974872, total_step : 48000, cur_epsilon : 0.002\n","episode : 240, done reward : -419.65382574453315, total_step : 48200, cur_epsilon : 0.002\n","episode : 241, done reward : -138.54678067773284, total_step : 48400, cur_epsilon : 0.002\n","episode : 242, done reward : -246.80809622561648, total_step : 48600, cur_epsilon : 0.002\n","episode : 243, done reward : -134.22902918216897, total_step : 48800, cur_epsilon : 0.002\n","episode : 244, done reward : -132.36643067736483, total_step : 49000, cur_epsilon : 0.002\n","episode : 245, done reward : -271.0798800123383, total_step : 49200, cur_epsilon : 0.002\n","episode : 246, done reward : -270.7011390429493, total_step : 49400, cur_epsilon : 0.002\n","episode : 247, done reward : -136.46331245128152, total_step : 49600, cur_epsilon : 0.002\n","episode : 248, done reward : -565.2634466370765, total_step : 49800, cur_epsilon : 0.002\n","episode : 249, done reward : -16.638494154340034, total_step : 50000, cur_epsilon : 0.002\n","episode : 250, done reward : -550.6474652314114, total_step : 50200, cur_epsilon : 0.002\n","episode : 251, done reward : -396.44923428785984, total_step : 50400, cur_epsilon : 0.002\n","episode : 252, done reward : -145.9986082115474, total_step : 50600, cur_epsilon : 0.002\n","episode : 253, done reward : -275.8651683377333, total_step : 50800, cur_epsilon : 0.002\n","episode : 254, done reward : -435.5537046250423, total_step : 51000, cur_epsilon : 0.002\n","episode : 255, done reward : -192.85402835032968, total_step : 51200, cur_epsilon : 0.002\n","episode : 256, done reward : -428.8028341546625, total_step : 51400, cur_epsilon : 0.002\n","episode : 257, done reward : -284.2243878628663, total_step : 51600, cur_epsilon : 0.002\n","episode : 258, done reward : -1188.3154470047273, total_step : 51800, cur_epsilon : 0.002\n","episode : 259, done reward : -376.0450009285931, total_step : 52000, cur_epsilon : 0.002\n","episode : 260, done reward : -154.27760458355218, total_step : 52200, cur_epsilon : 0.002\n","episode : 261, done reward : -146.42130423807066, total_step : 52400, cur_epsilon : 0.002\n","episode : 262, done reward : -1495.9392195009953, total_step : 52600, cur_epsilon : 0.002\n","episode : 263, done reward : -151.73932339163517, total_step : 52800, cur_epsilon : 0.002\n","episode : 264, done reward : -150.0329750666866, total_step : 53000, cur_epsilon : 0.002\n","episode : 265, done reward : -281.1529974464184, total_step : 53200, cur_epsilon : 0.002\n","episode : 266, done reward : -364.1307117006445, total_step : 53400, cur_epsilon : 0.002\n","episode : 267, done reward : -276.8164873731851, total_step : 53600, cur_epsilon : 0.002\n","episode : 268, done reward : -1493.19207881377, total_step : 53800, cur_epsilon : 0.002\n","episode : 269, done reward : -302.1244042431321, total_step : 54000, cur_epsilon : 0.002\n","episode : 270, done reward : -145.79105887850105, total_step : 54200, cur_epsilon : 0.002\n","episode : 271, done reward : -142.65438221590023, total_step : 54400, cur_epsilon : 0.002\n","episode : 272, done reward : -137.52944552893177, total_step : 54600, cur_epsilon : 0.002\n","episode : 273, done reward : -1494.4972011979796, total_step : 54800, cur_epsilon : 0.002\n","episode : 274, done reward : -405.14857136737663, total_step : 55000, cur_epsilon : 0.002\n","episode : 275, done reward : -951.569946971284, total_step : 55200, cur_epsilon : 0.002\n","episode : 276, done reward : -1382.382495899994, total_step : 55400, cur_epsilon : 0.002\n","episode : 277, done reward : -1358.2189008777807, total_step : 55600, cur_epsilon : 0.002\n","episode : 278, done reward : -9.743403126446013, total_step : 55800, cur_epsilon : 0.002\n","episode : 279, done reward : -1505.5464351907558, total_step : 56000, cur_epsilon : 0.002\n","episode : 280, done reward : -134.64419525326213, total_step : 56200, cur_epsilon : 0.002\n","episode : 281, done reward : -944.3468775658723, total_step : 56400, cur_epsilon : 0.002\n","episode : 282, done reward : -1141.4188031454566, total_step : 56600, cur_epsilon : 0.002\n","episode : 283, done reward : -1173.5382640722491, total_step : 56800, cur_epsilon : 0.002\n","episode : 284, done reward : -1492.387320880812, total_step : 57000, cur_epsilon : 0.002\n","episode : 285, done reward : -1182.4735427677788, total_step : 57200, cur_epsilon : 0.002\n","episode : 286, done reward : -1491.6008000949798, total_step : 57400, cur_epsilon : 0.002\n","episode : 287, done reward : -1492.4801672663907, total_step : 57600, cur_epsilon : 0.002\n","episode : 288, done reward : -259.8336183434609, total_step : 57800, cur_epsilon : 0.002\n","episode : 289, done reward : -523.8472292435332, total_step : 58000, cur_epsilon : 0.002\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-102-debae065ee03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mepisode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-4958b560386c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_iter, batch_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# train TwinQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_twinq_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mq1_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_twinq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             q1_loss_v = F.mse_loss(q1_v.squeeze().float(),\n\u001b[1;32m    119\u001b[0m                                     q_ref_v.squeeze().detach().float())\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-3d337cfce897>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, act)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0m\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m   1194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"-3QYYbvmyV9Z"},"execution_count":null,"outputs":[]}]}