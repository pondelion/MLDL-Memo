{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72143cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n",
    "from mlagents_envs.environment import ActionTuple, UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import \\\n",
    "    EngineConfigurationChannel\n",
    "from mlagents_envs.exception import (\n",
    "    UnityEnvironmentException,\n",
    "    UnityCommunicationException,\n",
    "    UnityCommunicatorStoppedException,\n",
    ")\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Union, Tuple\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from fastprogress import progress_bar as pb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4a7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepPriorityReplayBuffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int,\n",
    "        prob_alpha: float = 0.6,\n",
    "        beta_start: float = 0.4,\n",
    "        beta_frames: float = 100000,\n",
    "        n_step: int = 4,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        self._prob_alpha = prob_alpha\n",
    "        self._max_size = max_size\n",
    "        self._pos = 0\n",
    "        self._buf = []\n",
    "        self._priorities = np.zeros((max_size,), dtype=np.float32)\n",
    "        self._beta_start = beta_start\n",
    "        self._beta = beta_start\n",
    "        self._beta_frames = beta_frames\n",
    "        self._n_step = n_step\n",
    "        self._gamma = gamma\n",
    "        self._total_discounted_rewards = np.array([np.nan]*max_size)\n",
    "        self._last_states = [np.nan]*max_size\n",
    "\n",
    "    def update_bata(self, idx) -> None:\n",
    "        beta = self._beta_start + idx * (1.0 - self._beta_start) / self._beta_frames\n",
    "        self._beta = min(1.0, beta)\n",
    "        return self._beta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buf)\n",
    "\n",
    "    def append(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: Union[int, float],\n",
    "        done: bool,\n",
    "        next_state: np.ndarray,\n",
    "    ) -> None:\n",
    "        max_prio = self._priorities.max() if self._buf else 1.0\n",
    "        if len(self._buf) < self._max_size:\n",
    "            self._buf.append(\n",
    "                (state, action, reward, done, next_state)\n",
    "            )\n",
    "        else:\n",
    "            self._buf[self._pos] = (state, action, reward, done, next_state)\n",
    "        self._priorities[self._pos] = max_prio\n",
    "\n",
    "        if len(self._buf) >= self._n_step:\n",
    "            dis_r = 0.0\n",
    "            last_state = self._buf[self._pos][0]\n",
    "            for i in range(self._n_step):\n",
    "                state, _, r, done, _ = self._buf[self._pos - i]\n",
    "                dis_r = r + self._gamma * dis_r\n",
    "                if done:\n",
    "                    last_state = state\n",
    "                    dis_r = r  # ※\n",
    "                self._total_discounted_rewards[self._pos - i] = dis_r\n",
    "                self._last_states[self._pos - i] = last_state\n",
    "            \n",
    "            for i in range(self._n_step-1):\n",
    "                done = self._buf[self._pos - i][3]\n",
    "                if done:\n",
    "                    break\n",
    "                self._total_discounted_rewards[self._pos - i] = np.nan\n",
    "                self._last_states[self._pos - i] = np.nan\n",
    "\n",
    "        self._pos = (self._pos + 1) % self._max_size\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        sample_target_indices = np.where(~np.isnan(self._total_discounted_rewards[:len(self._buf)]))[0]\n",
    "        # prios = self._priorities[sample_target_indices]  #self._priorities if len(self._buf) == self._max_size else self._priorities[:self._pos]\n",
    "        prios = self._priorities\n",
    "        probs = prios * self._prob_alpha\n",
    "        # probs /= np.nan_to_num(probs, 0.0).sum()\n",
    "        probs /= probs[sample_target_indices].sum()\n",
    "        sampled_indices = np.random.choice(\n",
    "            sample_target_indices,\n",
    "            # np.where(~np.isnan(self._total_discounted_rewards[:len(self._buf)]))[0],\n",
    "            size, p=probs[sample_target_indices]\n",
    "        )\n",
    "        states, actions, rewards, dones, next_states = zip(*[self._buf[idx] for idx in sampled_indices])\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        next_states = np.array(next_states)\n",
    "        total_discounted_rewards = self._total_discounted_rewards[sampled_indices]\n",
    "        last_states = np.stack([self._last_states[idx] for idx in sampled_indices])\n",
    "        total = len(self._buf)\n",
    "        weights = np.array((total * probs[sampled_indices]) ** (-self._beta), dtype=np.float32)\n",
    "        # weights = np.array((total * probs) ** (-self._beta), dtype=np.float32)\n",
    "        weights /= weights.max()\n",
    "        return states, actions, rewards, dones, total_discounted_rewards, last_states, sampled_indices, weights\n",
    "\n",
    "    def update_priorities(self, sample_indices: np.ndarray, sample_priorities: np.ndarray) -> None:\n",
    "        self._priorities[sample_indices] = sample_priorities\n",
    "\n",
    "    @property\n",
    "    def gamma(self) -> float:\n",
    "        return self._gamma\n",
    "\n",
    "    @property\n",
    "    def n_step(self) -> float:\n",
    "        return self._n_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfec9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        w = torch.full((out_features, in_features), sigma_init)\n",
    "        self._sigma_weight = nn.Parameter(w)\n",
    "        z = torch.zeros(out_features, in_features)\n",
    "        self.register_buffer(\"epsilon_weight\", z)\n",
    "        if bias:\n",
    "            w = torch.full((out_features,), sigma_init)\n",
    "            self._sigma_bias = nn.Parameter(w)\n",
    "            z = torch.zeros(out_features)\n",
    "            self.register_buffer(\"epsilon_bias\", z)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.epsilon_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.epsilon_bias.normal_()\n",
    "            bias = bias + self._sigma_bias * \\\n",
    "                   self.epsilon_bias.data\n",
    "        v = self._sigma_weight * self.epsilon_weight.data + \\\n",
    "            self.weight\n",
    "        return F.linear(input, v, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4bb1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape: np.ndarray, n_actions: int):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "\n",
    "        self._conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_dim = int(np.prod(\n",
    "            self._conv(torch.zeros(1, *input_shape)).size()\n",
    "        ))\n",
    "        self._fc_adv = nn.Sequential(\n",
    "            NoisyLinear(conv_out_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        self._fc_val = nn.Sequential(\n",
    "            nn.Linear(conv_out_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # norm = 256\n",
    "        # conv_out = self._conv(x / norm).view(x.size()[0], -1)\n",
    "#         conv_out = self._conv(x).view(x.size()[0], -1)\n",
    "        conv_out = self._conv(x).reshape(x.size()[0], -1)\n",
    "        adv = self._fc_adv(conv_out)\n",
    "        val = self._fc_val(conv_out)\n",
    "        return val - (adv - adv.mean(dim=1, keepdim=True))\n",
    "        # return val, adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e4108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_q = RainbowDQN(input_shape=(3, 84, 64), n_actions=5)(torch.rand(16, 3, 84, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569dc2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe0d4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_q = RainbowDQN(input_shape=(3, 64, 64), n_actions=5)(torch.rand(16, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fba139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7097bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rainbow_loss(\n",
    "    states: np.ndarray,\n",
    "    actions: np.ndarray,\n",
    "    total_discounted_rewards: np.ndarray,\n",
    "    dones: np.ndarray,\n",
    "    last_states: np.ndarray,\n",
    "    weights: np.ndarray,\n",
    "    net: nn.Module,\n",
    "    tgt_net: nn.Module,\n",
    "    n_step_gamma: float,\n",
    "    double: bool = True,\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "):\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    discounted_rewards_v = torch.tensor(total_discounted_rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    weights_v = torch.tensor(weights).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_values = net(states_v).gather(1, actions_v)\n",
    "    state_action_values = state_action_values.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        last_states_v = torch.tensor(last_states).to(device)\n",
    "        if double:\n",
    "            last_state_actions = net(last_states_v).max(1)[1]\n",
    "            last_state_actions = last_state_actions.unsqueeze(-1)\n",
    "            last_state_values = tgt_net(last_states_v).gather(1, last_state_actions).squeeze(-1)\n",
    "        else:\n",
    "            last_state_values = tgt_net(last_states_v).max(1)[0]\n",
    "        last_state_values[done_mask] = 0.0\n",
    "        expected_state_action_values = last_state_values.detach() * n_step_gamma + discounted_rewards_v\n",
    "    losses_v = (state_action_values - expected_state_action_values) ** 2\n",
    "    losses_v *= weights_v\n",
    "\n",
    "    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1baff8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        exp_buffer: NStepPriorityReplayBuffer,\n",
    "        net: nn.Module,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_final: float = 0.01,\n",
    "        epsilon_decay_last_step: int = 200000,\n",
    "        tgt_sync_steps: int = 10000,\n",
    "        learning_rate: float = 1e-4,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        self._env = env\n",
    "        self._exp_buffer = exp_buffer\n",
    "        self._net = net\n",
    "        self._tgt_net = deepcopy(net)\n",
    "        for p in self._tgt_net.parameters():\n",
    "            p.requires_grad = False\n",
    "        self._epsilon_start = epsilon_start\n",
    "        self._epsilon_final = epsilon_final\n",
    "        self._epsilon_decay_last_step = epsilon_decay_last_step\n",
    "        self._epsilon = epsilon_start\n",
    "        self._device = device\n",
    "        self._total_step = 0\n",
    "        self._total_trained_samples = 0\n",
    "        self._tgt_sync_steps = tgt_sync_steps\n",
    "        self._optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        self._reset_episode()\n",
    "\n",
    "    def _reset_episode(self):\n",
    "        self._state = self._env.reset()\n",
    "        self._total_reward = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(\n",
    "        self,\n",
    "        epsilon: Optional[float] = None,\n",
    "        sync_target: bool = True,\n",
    "        probabilistic_decision: bool = False,\n",
    "    ):\n",
    "        if epsilon is None:\n",
    "            epsilon = self._epsilon\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self._env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self._state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(self._device)\n",
    "            q_vals_v = self._net(state_v)\n",
    "            if probabilistic_decision:\n",
    "                prob = F.softmax(q_vals_v.squeeze(dim=0), dim=0).data.cpu().numpy()\n",
    "                action = np.random.choice(range(len(prob)), size=1, p=prob).astype(int)[0]\n",
    "            else:\n",
    "                _, act_v = torch.max(q_vals_v, dim=1)\n",
    "                action = int(act_v.item())\n",
    "\n",
    "        next_state, reward, is_done, _ = self._env.step(action)\n",
    "        self._total_reward += reward\n",
    "\n",
    "        self._exp_buffer.append(\n",
    "            self._state, action, reward, is_done, next_state\n",
    "        )\n",
    "        self._state = next_state\n",
    "        if is_done:\n",
    "            done_reward = self._total_reward\n",
    "            self._reset_episode()\n",
    "\n",
    "        self._total_step += 1\n",
    "        self._update_epsilon(self._total_step)\n",
    "        self._exp_buffer.update_bata(self._total_step)\n",
    "\n",
    "        if (self._total_step % self._tgt_sync_steps == 0) and sync_target:\n",
    "            self._tgt_net.load_state_dict(self._net.state_dict())\n",
    "            print(f'synced target net')\n",
    "\n",
    "        return done_reward\n",
    "\n",
    "    def train(self, n_iter: int = 1, batch_size: int = 32) -> None:\n",
    "        for i in range(n_iter):\n",
    "            states, actions, rewards, dones, total_discounted_rewards, \\\n",
    "                last_states, sampled_indices, weights = self._exp_buffer.sample(batch_size)\n",
    "            self._optimizer.zero_grad()\n",
    "            loss_v, prios = rainbow_loss(\n",
    "                states=states,\n",
    "                actions=actions,\n",
    "                total_discounted_rewards=total_discounted_rewards,\n",
    "                dones=dones,\n",
    "                last_states=last_states,\n",
    "                weights=weights,\n",
    "                net=self._net,\n",
    "                tgt_net=self._tgt_net,\n",
    "                n_step_gamma=self._exp_buffer.gamma ** self._exp_buffer.n_step\n",
    "            )\n",
    "            loss_v.backward()\n",
    "            self._optimizer.step()\n",
    "            self._exp_buffer.update_priorities(sampled_indices, prios)\n",
    "            self._total_trained_samples += batch_size\n",
    "\n",
    "    def simulate_episode(self, epsilon: float = 0.0) -> Tuple[float, int]:\n",
    "        eps_bak = self._epsilon\n",
    "        total_step_bak = self._total_step\n",
    "        beta_bak = self._exp_buffer._beta\n",
    "        done_reward = None\n",
    "        episode_steps = 0\n",
    "        while done_reward is None:\n",
    "            episode_steps += 1\n",
    "            self._epsilon = epsilon\n",
    "            done_reward = self.play_step(sync_target=False)\n",
    "        self._total_step = total_step_bak\n",
    "        self._epsilon = eps_bak\n",
    "        self._exp_buffer._beta = beta_bak\n",
    "        return done_reward, episode_steps\n",
    "            \n",
    "    def initial_exploration(self, n_steps: int = 10000, epsilon: float = 1.0) -> None:\n",
    "        eps_bak = self._epsilon\n",
    "        beta_bak = self._exp_buffer._beta\n",
    "        for i in pb(range(n_steps)):\n",
    "            self._epsilon = epsilon\n",
    "            self.play_step(sync_target=False)\n",
    "        self._total_step = 0\n",
    "        self._epsilon = eps_bak\n",
    "        self._exp_buffer._beta = beta_bak\n",
    "\n",
    "    def _update_epsilon(self, step_index: int) -> None:\n",
    "        self._epsilon = max(\n",
    "            self._epsilon_final,\n",
    "            self._epsilon_start - (self._epsilon_start - self._epsilon_final) * (step_index / self._epsilon_decay_last_step)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10cb10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityToGymNumpyImgWrapper(UnityToGymWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnityToGymNumpyImgWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        state = super().reset(*args, **kwargs)\n",
    "        return np.array(state[0]).transpose(2, 0, 1)\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        next_state, reward, is_done, info = super().step(*args, **kwargs)\n",
    "        return np.array(next_state[0]).transpose(2, 0, 1), reward, is_done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c97690a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityToGymNumpyImgResizeWrapper(UnityToGymNumpyImgWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnityToGymNumpyImgResizeWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        state = super().reset(*args, **kwargs)\n",
    "#         return cv2.resize(state[:, :, 10:-10].transpose(1, 2, 0), (224, 224)).transpose(2, 0, 1)\n",
    "        return state[:, :, 10:-10]\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        next_state, reward, is_done, info = super().step(*args, **kwargs)\n",
    "        next_state = cv2.resize(next_state[:, :, 10:-10].transpose(1, 2, 0), (224, 224)).transpose(2, 0, 1)\n",
    "#         return next_state, reward, is_done, info\n",
    "        return next_state[:, :, 10:-10], reward, is_done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7db1c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityToGymNumpyImgResizeGrayWrapper(UnityToGymNumpyImgWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnityToGymNumpyImgResizeGrayWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        state = super().reset(*args, **kwargs)\n",
    "        return cv2.cvtColor(state[:, :, 10:-10].transpose(1, 2, 0), cv2.COLOR_RGB2GRAY).reshape(1, 64, 64)\n",
    "#         return state[:, :, 10:-10]\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        next_state, reward, is_done, info = super().step(*args, **kwargs)\n",
    "        next_state = cv2.cvtColor(next_state[:, :, 10:-10].transpose(1, 2, 0), cv2.COLOR_RGB2GRAY).reshape(1, 64, 64)\n",
    "#         return next_state, reward, is_done, info\n",
    "        return next_state, reward, is_done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1cf24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e2ed681",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    unity_env.close()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced14eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_env = UnityEnvironment(file_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74eb9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5a8df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UnityToGymWrapper(unity_env, allow_multiple_obs=True)\n",
    "env = UnityToGymNumpyImgResizeGrayWrapper(unity_env, allow_multiple_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b546b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior_names: ['GridWorld?team=0']\n",
      "\n",
      "== BehaviorSpecの情報の確認 ==\n",
      "observation_specs: [ObservationSpec(shape=(64, 84, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='RenderTextureSensor'), ObservationSpec(shape=(2,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.GOAL_SIGNAL: 1>, name='VectorlSensor')]\n",
      "action_spec: Continuous: 0, Discrete: (5,)\n"
     ]
    }
   ],
   "source": [
    "# Unity環境のリセット\n",
    "unity_env.reset()\n",
    "\n",
    "# BehaviorNameのリストの取得\n",
    "behavior_names = list(unity_env.behavior_specs.keys())\n",
    "print('behavior_names:', behavior_names)\n",
    "\n",
    "# BehaviorSpecの取得\n",
    "behavior_spec = unity_env.behavior_specs[behavior_names[0]]\n",
    "\n",
    "# BehaviorSpecの情報の確認\n",
    "print('\\n== BehaviorSpecの情報の確認 ==')\n",
    "print('observation_specs:', behavior_spec.observation_specs)\n",
    "print('action_spec:', behavior_spec.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "509d354c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BehaviorSpec(observation_specs=[ObservationSpec(shape=(64, 84, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='RenderTextureSensor'), ObservationSpec(shape=(2,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.GOAL_SIGNAL: 1>, name='VectorlSensor')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(5,)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17a817cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== DecisionStepsの情報の確認 ==\n",
      "obj: [array([[[[0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]]]], dtype=float32), array([[1., 0.]], dtype=float32)]\n",
      "reward: [-0.01]\n",
      "agent_id: [0]\n",
      "action_mask: [array([[False, False, False, False, False]])]\n",
      "\n",
      "== TerminalStepsの情報の確認 ==\n",
      "obs: [array([], shape=(0, 64, 84, 3), dtype=float32), array([], shape=(0, 2), dtype=float32)]\n",
      "reward: []\n",
      "agent_id: []\n",
      "interrupted: []\n"
     ]
    }
   ],
   "source": [
    "# 現在のステップの情報の取得\n",
    "decision_steps, terminal_steps = unity_env.get_steps(behavior_names[0])\n",
    "\n",
    "# DecisionStepsの情報の確認\n",
    "print('\\n== DecisionStepsの情報の確認 ==')\n",
    "print('obj:', decision_steps.obs)\n",
    "print('reward:', decision_steps.reward)\n",
    "print('agent_id:', decision_steps.agent_id)\n",
    "print('action_mask:', decision_steps.action_mask)\n",
    "\n",
    "# TerminalStepsの情報の確認\n",
    "print('\\n== TerminalStepsの情報の確認 ==')\n",
    "print('obs:', terminal_steps.obs)\n",
    "print('reward:', terminal_steps.reward)\n",
    "print('agent_id:', terminal_steps.agent_id)\n",
    "print('interrupted:', terminal_steps.interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde7cf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decision_steps.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4213ef4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_steps.obs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6b8b6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 84, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_steps.obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571deb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AGENTS = decision_steps.obs[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "facfbef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43d59827",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = behavior_spec.action_spec.discrete_branches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cc340d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3e113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e8d0b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57aa05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "141c2141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Box(0.0, 1.0, (64, 84, 3), float32), Box(-inf, inf, (2,), float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0cbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "143a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "260b3271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b1e1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cc7d45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9b7c352b50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(state.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6e688ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array(state)).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02833629",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, is_done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff06fbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bcceb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f4128ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df86aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "554cb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19358419",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6bbce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RainbowDQN(input_shape=OBS_DIM, n_actions=N_ACTIONS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57226a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0545,  0.0035, -0.0700, -0.0649, -0.0612]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(state).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5eac5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_buffer = NStepPriorityReplayBuffer(\n",
    "    max_size=150000,\n",
    "    prob_alpha=0.6,\n",
    "    beta_start=0.4,\n",
    "    beta_frames=200000,\n",
    "    n_step=4,\n",
    "    gamma=0.99,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "998b3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env=env,\n",
    "    exp_buffer=exp_buffer,\n",
    "    net=net,\n",
    "    epsilon_start=0.9,\n",
    "    epsilon_final=0.4,\n",
    "    epsilon_decay_last_step=200000,\n",
    "    tgt_sync_steps=5000,\n",
    "    learning_rate=1e-4,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64f11bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10000/10000 03:26&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.initial_exploration(n_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ff6e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, dones, next_states = zip(*agent._exp_buffer._buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e49e2312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9b7c0ff970>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABSAAAAH5CAYAAACRa5DlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB04UlEQVR4nO39fZwcZZ3v/7+rqm+SIDMh5GYyEhPuFsgBAgYZ4i2a+ZEAR2GXdYnGDeRgWJGoEBTIHgEFJQosh4PLMavL7U8QZI+wip5IjNzsrjGw4WQRjTnAAkHIJEBMhiRk+qbq+0dX1XQnM317Vd++no/HPJL01FRfTRfTVe/6XNfH8jzPEwAAAAAAAABEwG70AAAAAAAAAAC0LwJIAAAAAAAAAJEhgAQAAAAAAAAQGQJIAAAAAAAAAJEhgAQAAAAAAAAQGQJIAAAAAAAAAJEhgAQAAAAAAAAQmVijB9AIruvq9ddf14EHHijLsho9HAAAAAAAAKCleJ6nt99+W729vbLt4jWOHRlAvv7665o2bVqjhwEAAAAAAAC0tFdffVWHHHJI0W06MoA88MADJeX+A3V1dTV4NAAAAAAAAEBrGRwc1LRp08KcrZiODCCDadddXV0EkAAAAAAAAECVylnekCY0AAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMpEGkE8++aQ+/vGPq7e3V5Zl6eGHHy75M48//rje+973KplM6ogjjtBdd9213za33XabZsyYoTFjxqivr09PPfWU+cEDAAAAAAAAqFmkAeTu3bs1a9Ys3XbbbWVt/9JLL+nMM8/URz/6UW3YsEGXXHKJPvvZz+oXv/hFuM0DDzygZcuW6ZprrtEzzzyjWbNmad68edq2bVtULwMAAAAAAABAlSzP87y6PJFl6aGHHtLZZ5896jZXXHGFfvazn+m5554LH1uwYIF27NihVatWSZL6+vr0vve9T3//938vSXJdV9OmTdMXvvAFXXnllWWNZXBwUN3d3dq5c6e6urqqf1EAAAAAAABAB6okX2uqNSDXrl2r/v7+gsfmzZuntWvXSpJSqZTWr19fsI1t2+rv7w+3GcnQ0JAGBwcLvtrZb37wNf3hm3P09I//Z6OHAgAAAAAAgA7XVAHkwMCApkyZUvDYlClTNDg4qHfeeUdvvvmmstnsiNsMDAyMut8VK1aou7s7/Jo2bVok428W1o5XdHT698puf6XRQwEAAAAAAECHa6oAMirLly/Xzp07w69XX3210UOKlOckcn9mhxo8EgAAAAAAAHS6WKMHkK+np0dbt24teGzr1q3q6urS2LFj5TiOHMcZcZuenp5R95tMJpVMJiMZczMKAkgrm2rwSAAAAAAAANDpmqoCcs6cOVqzZk3BY6tXr9acOXMkSYlEQrNnzy7YxnVdrVmzJtwGkkUACQAAAAAAgCYRaQC5a9cubdiwQRs2bJAkvfTSS9qwYYM2b94sKTc1etGiReH2n/vc5/Sf//mfuvzyy/WHP/xB/+t//S/96Ec/0qWXXhpus2zZMn3/+9/X3XffrY0bN+qiiy7S7t27tXjx4ihfSmtxctWeBJAAAAAAAABotEinYP/7v/+7PvrRj4b/XrZsmSTpvPPO01133aUtW7aEYaQkHXroofrZz36mSy+9VP/zf/5PHXLIIfrHf/xHzZs3L9zm3HPP1RtvvKGrr75aAwMDOuGEE7Rq1ar9GtN0MiuWq4C0XQJIAAAAAAAANJbleZ7X6EHU2+DgoLq7u7Vz5051dXU1ejjGPXX/9Tr5D9/W+nd9RLO//JNGDwcAAAAAAABtppJ8ranWgIQhsdwUbCogAQAAAAAA0GgEkG3I8gNIx003eCQAAAAAAADodASQbcgOKyAJIAEAAAAAANBYBJBtyI7nAsiYxxRsAAAAAAAANBYBZBty4kEFZKbBIwEAAAAAAECnI4BsQw4VkAAAAAAAAGgSBJBtaHgKNmtAAgAAAAAAoLEIINtQLAwgmYINAAAAAACAxiKAbENOYowkKS4qIAEAAAAAANBYBJBtKEYACQAAAAAAgCZBANmG4v4U7DhTsAEAAAAAANBgBJBtKJ6kAhIAAAAAAADNgQCyDcXiuQAyYWWVyVAFCQAAAAAAgMYhgGxDQQWkJKVSQw0cCQAAAAAAADodAWQbSuQFkOkhAkgAAAAAAAA0DgFkG4r5TWgkKZXa08CRAAAAAAAAoNMRQLYhy4kp4+Xe2jRTsAEAAAAAANBABJBtKmXFJUmZob0NHgkAAAAAAAA6GQFkm0rLDyDTVEACAAAAAACgcQgg21RGsdyfKSogAQAAAAAA0DgEkG0q7U/BzqYJIAEAAAAAANA4BJBtKhOsAZlONXgkAAAAAAAA6GQEkG0qY+WmYGeZgg0AAAAAAIAGIoBsU1krIUlyaUIDAAAAAACABiKAbFPZoAIyQwAJAAAAAACAxiGAbFNZmwpIAAAAAAAANB4BZJvK+k1oRAUkAAAAAAAAGogAsk25di6AdDN0wQYAAAAAAEDjEEC2KddhCjYAAAAAAAAajwCyTbn+GpDKEkACAAAAAACgcQgg25TnB5AeU7ABAAAAAADQQASQbcpzggpIAkgAAAAAAAA0DgFkmwoDSCogAQAAAAAA0EAEkO3KDyAtlwASAAAAAAAAjUMA2a6CAJImNAAAAAAAAGggAsh2FUtKkqxsusEDAQAAAAAAQCcjgGxTFlOwAQAAAAAA0AQIINtVLBdA2nTBBgAAAAAAQAMRQLYpy5+CbXtMwQYAAAAAAEDjEEC2KdsPIB2mYAMAAAAAAKCBCCDblB0PAkgqIAEAAAAAANA4BJBtKgggbQJIAAAAAAAANFBdAsjbbrtNM2bM0JgxY9TX16ennnpq1G1PPfVUWZa139eZZ54ZbnP++efv9/358+fX46W0DNtvQuOwBiQAAAAAAAAaKBb1EzzwwANatmyZVq5cqb6+Pt1yyy2aN2+eNm3apMmTJ++3/Y9//GOlUsPrFr711luaNWuWPvnJTxZsN3/+fN15553hv5PJZHQvogU58TGSpBgBJAAAAAAAABoo8grIm2++WUuWLNHixYs1c+ZMrVy5UuPGjdMdd9wx4vYTJkxQT09P+LV69WqNGzduvwAymUwWbHfQQQdF/VJaSjAFmwASAAAAAAAAjRRpAJlKpbR+/Xr19/cPP6Ftq7+/X2vXri1rH7fffrsWLFigAw44oODxxx9/XJMnT9ZRRx2liy66SG+99dao+xgaGtLg4GDBV7uLEUACAAAAAACgCUQaQL755pvKZrOaMmVKweNTpkzRwMBAyZ9/6qmn9Nxzz+mzn/1swePz58/XPffcozVr1ujb3/62nnjiCZ1++unKZrMj7mfFihXq7u4Ov6ZNm1b9i2oRsYQ/BVsEkAAAAAAAAGicyNeArMXtt9+u4447TieffHLB4wsWLAj/ftxxx+n444/X4Ycfrscff1xz587dbz/Lly/XsmXLwn8PDg62fQgZS+QqIONepsEjAQAAAAAAQCeLtAJy4sSJchxHW7duLXh869at6unpKfqzu3fv1v33368LLrig5PMcdthhmjhxol544YURv59MJtXV1VXw1e6CCsgEFZAAAAAAAABooEgDyEQiodmzZ2vNmjXhY67ras2aNZozZ07Rn33wwQc1NDSkz3zmMyWf549//KPeeustTZ06teYxt4u4H0DGlZHreg0eDQAAAAAAADpV5F2wly1bpu9///u6++67tXHjRl100UXavXu3Fi9eLElatGiRli9fvt/P3X777Tr77LN18MEHFzy+a9cufeUrX9FvfvMbvfzyy1qzZo3OOussHXHEEZo3b17UL6dlBFOwE0orlXUbPBoAAAAAAAB0qsjXgDz33HP1xhtv6Oqrr9bAwIBOOOEErVq1KmxMs3nzZtl2YQ66adMm/eu//qseffTR/fbnOI6effZZ3X333dqxY4d6e3t12mmn6brrrlMymYz65bSMeGKsJMmxPO1JpzQmPrbBIwIAAAAAAEAnsjzP67j5uYODg+ru7tbOnTvbdj1Id+/bsr91iCTprS+9rIMPOqjBIwIAAAAAAEC7qCRfi3wKNhrDjg1Xg6ZTexs4EgAAAAAAAHQyAsh25cTDv6aHCCABAAAAAADQGASQ7cqyNKRcCJlJDTV4MAAAAAAAAOhUBJBtLO33GGIKNgAAAAAAABqFALKNpYMKyDQVkAAAAAAAAGgMAsg2lrFyAWSWCkgAAAAAAAA0CAFkG8tYuSnY2TQBJAAAAAAAABqDALKNhRWQ6VSDRwIAAAAAAIBORQDZxrJ+AOlSAQkAAAAAAIAGIYBsY9mwApImNAAAAAAAAGgMAsg2lrETkqRshinYAAAAAAAAaAwCyDbm+hWQyjAFGwAAAAAAAI1BANnGXL8C0mUKNgAAAAAAABqEALKNuU6uAtJjCjYAAAAAAAAahACyjXl+BSQBJAAAAAAAABqFALKNBVOwlWUKNgAAAAAAABqDALKNeU4QQFIBCQAAAAAAgMYggGxjnr8GJAEkAAAAAAAAGoUAsp05ydyfrAEJAAAAAACABiGAbGex3BRsiwpIAAAAAAAANAgBZDvz14C0XQJIAAAAAAAANAYBZBuzYrkp2JabbvBIAAAAAAAA0KkIINuZvwakzRRsAAAAAAAANAgBZBuzYkzBBgAAAAAAQGMRQLYxO+5XQDIFGwAAAAAAAA1CANnGbL8C0vGogAQAAAAAAEBjEEC2MdtvQuNQAQkAAAAAAIAGIYBsY3ZijCTJ8QggAQAAAAAA0BgEkG3M8adgxwggAQAAAAAA0CAEkG3MiecqIAkgAQAAAAAA0CgEkG3MSeTWgCSABAAAAAAAQKMQQLaxWNwPIEUACQAAAAAAgMYggGxjMb8JTdzLNHgkAAAAAAAA6FQEkG0slvQDSKXleV6DRwMAAAAAAIBORADZxhJ+E5qEMsq4BJAAAAAAAACoPwLINjZcAZlRKuM2eDQAAAAAAADoRASQbSzurwGZtDJKZ7INHg0AAAAAAAA6EQFkGwu6YEtSamhvA0cCAAAAAACATkUA2c5ieQFkaqiBAwEAAAAAAECnIoBsZ04i/Gs6RQUkAAAAAAAA6o8Asp3ZjjL+W5xJvdPgwQAAAAAAAKATEUC2ubTikqQMU7ABAAAAAADQAHUJIG+77TbNmDFDY8aMUV9fn5566qlRt73rrrtkWVbB15gxYwq28TxPV199taZOnaqxY8eqv79fzz//fNQvoyVlFJMkZQkgAQAAAAAA0ACRB5APPPCAli1bpmuuuUbPPPOMZs2apXnz5mnbtm2j/kxXV5e2bNkSfr3yyisF37/hhht06623auXKlVq3bp0OOOAAzZs3T3v3ss7hvtKWXwGZ5r8NAAAAAAAA6i/yAPLmm2/WkiVLtHjxYs2cOVMrV67UuHHjdMcdd4z6M5ZlqaenJ/yaMmVK+D3P83TLLbfoq1/9qs466ywdf/zxuueee/T666/r4YcfHnF/Q0NDGhwcLPjqFJkwgKQCEgAAAAAAAPUXaQCZSqW0fv169ff3Dz+hbau/v19r164d9ed27dql6dOna9q0aTrrrLP0u9/9LvzeSy+9pIGBgYJ9dnd3q6+vb9R9rlixQt3d3eHXtGnTDLy61hAEkFkqIAEAAAAAANAAkQaQb775prLZbEEFoyRNmTJFAwMDI/7MUUcdpTvuuEP//M//rB/84AdyXVfvf//79cc//lGSwp+rZJ/Lly/Xzp07w69XX3211pfWMrJhAJlq8EgAAAAAAADQiWKNHsC+5syZozlz5oT/fv/7369jjjlG//AP/6Drrruuqn0mk0klk0lTQ2wpQQWkSxMaAAAAAAAANECkFZATJ06U4zjaunVrweNbt25VT09PWfuIx+M68cQT9cILL0hS+HO17LOTuLYfQGaYgg0AAAAAAID6izSATCQSmj17ttasWRM+5rqu1qxZU1DlWEw2m9Vvf/tbTZ06VZJ06KGHqqenp2Cfg4ODWrduXdn77CRZOyFJ8rJMwQYAAAAAAED9RT4Fe9myZTrvvPN00kkn6eSTT9Ytt9yi3bt3a/HixZKkRYsW6d3vfrdWrFghSbr22mt1yimn6IgjjtCOHTt044036pVXXtFnP/tZSbkO2Zdccom+8Y1v6Mgjj9Shhx6qq666Sr29vTr77LOjfjktxw0CSLpgAwAAAAAAoAEiDyDPPfdcvfHGG7r66qs1MDCgE044QatWrQqbyGzevFm2PVyI+ac//UlLlizRwMCADjroIM2ePVu//vWvNXPmzHCbyy+/XLt379aFF16oHTt26IMf/KBWrVqlMWPGRP1yWs7wFGwCSAAAAAAAANSf5Xme1+hB1Nvg4KC6u7u1c+dOdXV1NXo4kXruf5ylY3c+rjWHXa65i/57o4cDAAAAAACANlBJvhbpGpBoPM/JTcEWa0ACAAAAAACgAQgg25znT8EWU7ABAAAAAADQAASQbS6ogLSogAQAAAAAAEADEEC2OyeZ+5MAEgAAAAAAAA1AANnuYrkp2FRAAgAAAAAAoBEIINudXwFpuQSQAAAAAAAAqD8CyDZnxfwAkgpIAAAAAAAANAABZLvzm9DYbrrBAwEAAAAAAEAnIoBsc0EFpM0UbAAAAAAAADQAAWSbs+O5CkiHCkgAAAAAAAA0AAFkm7PDCkgCSAAAAAAAANQfAWSbs+NjJEmOxxRsAAAAAAAA1B8BZJtz/CnYMSogAQAAAAAA0AAEkG0umILteASQAAAAAAAAqD8CyDbnJHJTsGMEkAAAAAAAAGgAAsg2F4vnKiDjIoAEAAAAAABA/RFAtjknHlRAZho8EgAAAAAAAHQiAsg2F/OnYFMBCQAAAAAAgEYggGxz8cTwFGzX9Ro8GgAAAAAAAHQaAsg25/gBZEIZpbJug0cDAAAAAACATkMA2ebiybGScgHkUIYAEgAAAAAAAPVFANnmEuEU7IzSVEACAAAAAACgzggg25wV87tgW65SKRrRAAAAAAAAoL4IINudkwj/mh56p4EDAQAAAAAAQCcigGx3+QFkeqiBAwEAAAAAAEAnIoBsd048/Gt6aG8DBwIAAAAAAIBORADZ7ixLKcUkSekUASQAAAAAAADqiwCyA6SVq4LMMgUbAAAAAAAAdUYA2QEylh9AUgEJAAAAAACAOiOA7ABhAEkFJAAAAAAAAOqMALIDEEACAAAAAACgUQggOwABJAAAAAAAABqFALIDZP0A0k2zBiQAAAAAAADqiwCyA2RtP4DMpBo8EgAAAAAAAHQaAsgO4NqJ3J9MwQYAAAAAAECdEUB2ANefgu1lqYAEAAAAAABAfRFAdgDXyVVAehkqIAEAAAAAAFBfBJAdIJiCTQAJAAAAAACAeiOA7ACe34RGNKEBAAAAAABAnRFAdgDPn4It1oAEAAAAAABAnRFAdgDPSeb+kmUKNgAAAAAAAOqrLgHkbbfdphkzZmjMmDHq6+vTU089Neq23//+9/WhD31IBx10kA466CD19/fvt/35558vy7IKvubPnx/1y2hdjj8FO5tu7DgAAAAAAADQcSIPIB944AEtW7ZM11xzjZ555hnNmjVL8+bN07Zt20bc/vHHH9enPvUpPfbYY1q7dq2mTZum0047Ta+99lrBdvPnz9eWLVvCrx/+8IdRv5TWFctVQFpUQAIAAAAAAKDOIg8gb775Zi1ZskSLFy/WzJkztXLlSo0bN0533HHHiNvfe++9+vznP68TTjhBRx99tP7xH/9RrutqzZo1Bdslk0n19PSEXwcddFDUL6V1+WtA2qwBCQAAAAAAgDqLNIBMpVJav369+vv7h5/QttXf36+1a9eWtY89e/YonU5rwoQJBY8//vjjmjx5so466ihddNFFeuutt0bdx9DQkAYHBwu+OkosF0BaLlOwAQAAAAAAUF+RBpBvvvmmstmspkyZUvD4lClTNDAwUNY+rrjiCvX29haEmPPnz9c999yjNWvW6Nvf/raeeOIJnX766cpmsyPuY8WKFeru7g6/pk2bVv2LakGWPwWbCkgAAAAAAADUW6zRAyjmW9/6lu6//349/vjjGjNmTPj4ggULwr8fd9xxOv7443X44Yfr8ccf19y5c/fbz/Lly7Vs2bLw34ODgx0VQtr+FGzLowISAAAAAAAA9RVpBeTEiRPlOI62bt1a8PjWrVvV09NT9Gdvuukmfetb39Kjjz6q448/vui2hx12mCZOnKgXXnhhxO8nk0l1dXUVfHUSK56rgHRcKiABAAAAAABQX5EGkIlEQrNnzy5oIBM0lJkzZ86oP3fDDTfouuuu06pVq3TSSSeVfJ4//vGPeuuttzR16lQj4243djxXPeqwBiQAAAAAAADqLPIu2MuWLdP3v/993X333dq4caMuuugi7d69W4sXL5YkLVq0SMuXLw+3//a3v62rrrpKd9xxh2bMmKGBgQENDAxo165dkqRdu3bpK1/5in7zm9/o5Zdf1po1a3TWWWfpiCOO0Lx586J+OS3J9pvQ2EzBBgAAAAAAQJ1FvgbkueeeqzfeeENXX321BgYGdMIJJ2jVqlVhY5rNmzfLtodz0O9+97tKpVL6y7/8y4L9XHPNNfra174mx3H07LPP6u6779aOHTvU29ur0047Tdddd52SyWTUL6cl2f4U7BhTsAEAAAAAAFBnlud5XqMHUW+Dg4Pq7u7Wzp07O2I9yFf+5X5NX/M3+g/raM26Zl2jhwMAAAAAAIAWV0m+FvkUbDSek/Cb0HiZBo8EAAAAAAAAnYYAsgM4/hTsOGtAAgAAAAAAoM4IIDtALJHrgh0XASQAAAAAAADqiwCyA8SCJjReWh245CcAAAAAAAAaiACyA8SSuQrIhJVROksACQAAAAAAgPohgOwA8bgfQCqtVNZt8GgAAAAAAADQSQggO0A8GawBmVU6QwAJAAAAAACA+iGA7ABBF2wqIAEAAAAAAFBvBJCdwElIkpJWRql0tsGDAQAAAAAAQCchgOwEfgApSUOpoQYOBAAAAAAAAJ2GALITxJLhXzNDexs4EAAAAAAAAHQaAshOkFcBmU5TAQkAAAAAAID6IYDsBLajrP9WZ9NUQAIAAAAAAKB+CCA7RFpxSVImRQAJAAAAAACA+iGA7BAZK5b7kyY0AAAAAAAAqCMCyA6RtnLrQFIBCQAAAAAAgHoigOwQGSs3BdvNUAEJAAAAAACA+iGA7BBZfwp2li7YAAAAAAAAqCMCyA6R9adguwSQAAAAAAAAqCMCyA6RtXNTsD0CSAAAAAAAANQRAWSHcG3WgAQAAAAAAED9EUB2iGAKtkcACQAAAAAAgDoigOwQruOvAZlJNXgkAAAAAAAA6CQEkB3CC9aAJIAEAAAAAABAHRFAdgjXzlVAiinYAAAAAAAAqCMCyA7h+VOwlaUCEgAAAAAAAPVDANkpCCABAAAAAADQAASQncIPIK0sU7ABAAAAAABQPwSQnSIWVECmGzsOAAAAAAAAdBQCyE7hJCVJlssUbAAAAAAAANQPAWSn8CsgbdaABAAAAAAAQB0RQHYIKwggqYAEAAAAAABAHRFAdgjLn4Jtu6wBCQAAAAAAgPohgOwQdjwXQDpUQAIAAAAAAKCOCCA7hB2jAhIAAAAAAAD1RwDZIcIKSI8AEgAAAAAAAPVDANkhCCABAAAAAADQCASQHcLxp2DHCCABAAAAAABQRwSQHcJJjJEkxTya0AAAAAAAAKB+CCA7RCweVEBmGjwSAAAAAAAAdBICyA7hJHIBZJwp2AAAAAAAAKijugSQt912m2bMmKExY8aor69PTz31VNHtH3zwQR199NEaM2aMjjvuOP385z8v+L7nebr66qs1depUjR07Vv39/Xr++eejfAktLxbPTcGOK62s6zV4NAAAAAAAAOgUkQeQDzzwgJYtW6ZrrrlGzzzzjGbNmqV58+Zp27ZtI27/61//Wp/61Kd0wQUX6P/+3/+rs88+W2effbaee+65cJsbbrhBt956q1auXKl169bpgAMO0Lx587R3796oX07Liif9ANLKKpVxGzwaAAAAAAAAdArL87xIy+H6+vr0vve9T3//938vSXJdV9OmTdMXvvAFXXnllfttf+6552r37t165JFHwsdOOeUUnXDCCVq5cqU8z1Nvb68uu+wyffnLX5Yk7dy5U1OmTNFdd92lBQsWlBzT4OCguru7tXPnTnV1dRl6pc0t/dqzin//Q3rD61biihfVPS7e6CEBAAAAAACgRVWSr0VaAZlKpbR+/Xr19/cPP6Ftq7+/X2vXrh3xZ9auXVuwvSTNmzcv3P6ll17SwMBAwTbd3d3q6+sbdZ9DQ0MaHBws+Oo0Mb8LdkJppbJUQAIAAAAAAKA+Ig0g33zzTWWzWU2ZMqXg8SlTpmhgYGDEnxkYGCi6ffBnJftcsWKFuru7w69p06ZV9XpamRVLSJLiyhJAAgAAAAAAoG5ijR5APSxfvlzLli0L/z04ONh5IaST64KdUFr//aHf6oBER7z1aBK2bWlh33t0ymEHl9z2x8/8UWs2jrxGLAAAQNPxPB2+97c6YdeTei15hH7TNb/RI0IZDkg6+sLHjtS0CeOKbue6nm5e/f/00pu76zQyROWongP1hY8dIcuyim73n2/s0m2Pvai96WydRja6rrExXdr/Z5rcNabodumsqxt/sUmv/emdOo0MlTh04gH68ryjGj2Mhos0hZo4caIcx9HWrVsLHt+6dat6enpG/Jmenp6i2wd/bt26VVOnTi3Y5oQTThhxn8lkUslkstqX0R5iudcfs1w9uWmr3Po0QAdCW3fu1Y8+N6fkdv/9oef0ThN82AMAABRzqLVFf+78i/7c/jdNs98IH//9H9/UD7NzGzgylGvSgUl9Zd7RRbf5/ZZB/f1jL9RpRIjSz367RR+f1atDJx5QdLt71r6i//3MH+s0qtKmH3yAPveRw4tu8/TL2/W9J/+zTiNCpd77nvGSCCAjDSATiYRmz56tNWvW6Oyzz5aUa0KzZs0aLV26dMSfmTNnjtasWaNLLrkkfGz16tWaMycXXBx66KHq6enRmjVrwsBxcHBQ69at00UXXRTly2ltznDTmev+65HKOsXvoACmvLBtl+5Z+4p2DWVKbpt1vTB8vPL0ozUu4UQ9PAAAgLIlU3/SjIFf6LDXf6pJO58LH0874/RW1zHq+dN6fTN+p+adNFObe/qL7AmNtGbjNj3x/97Q7qHSN72Dc9iJ70rqi3OPiHpoiMhNv9ikwb0Z7S7jmuTtvbltTps5RR88cmLUQxvVz57donUvbS9rzLv8MU+bMFZLPnRY1ENDhSa9q8ML4nyRz8NdtmyZzjvvPJ100kk6+eSTdcstt2j37t1avHixJGnRokV697vfrRUrVkiSvvSlL+kjH/mI/u7v/k5nnnmm7r//fv37v/+7vve970mSLMvSJZdcom984xs68sgjdeihh+qqq65Sb29vGHJiBM7wAb9wdo80dnzjxoKOsvbFt3TP2lfKWns0lRne5q9Pma4DkiwVAAAAGiy9V/p/q6RnH5Cef1Ry/TDAcqTDPybNWqD4UWeoJz5WeuQS2evv0qnPLZdmHSkd9pHGjh0j2rEnrSf+3xsaypR/fjrpwKQWzZkR8cgQlX/8l5c0uDdT3nvuX7ecctjBDX3PX92+R+te2l5wjTSaYMy93WM5TtG0Ir+6P/fcc/XGG2/o6quv1sDAgE444QStWrUqbCKzefNm2fbwdOD3v//9uu+++/TVr35Vf/u3f6sjjzxSDz/8sI499thwm8svv1y7d+/WhRdeqB07duiDH/ygVq1apTFjqOobVV4FpLLpxo0DHScRy/3/XdYHZ942wc8BAADUnetKr/5G+o/7pd89LA3tHP7e1FnS8Quk4/5Setfkwp8782Zpz3Zp40+k+z8tnfdT6d3vrevQUVo156ecm7a2yt7zbMHPNErw/JUE5Y0eM1BMXcqLli5dOuqU68cff3y/xz75yU/qk5/85Kj7syxL1157ra699lpTQ2x/liU5CSmbkrJDjR4NOkiygg/7oWzuw96ypJhdfHFoAAAA4958QXr2/ly1447Nw493HSId/8lc8Di5yJqBtiOd84/SvX8pvfRk7s//9qg0kam7zSTh+Oen5czQ8bdJOgQ7rayi97xJwryEk1uOqpIxJwkg0cSY39hJnGQugFx7mzT2oEaPBq3CsqWZZ1d94hyv8MN+trVJH4r9XtaTv6vq+QAAACqWTUsvrpFeWz/8WOJAaeZZ0qxzpekflOwyL+xjSWnBfdJd/1XaskH6/58tvXdRFKPuXIkDpBM+XfU1TTy8QV56DchUxtXH7V/rw3sGpSf+parngwGTjpZmfqLqH49XUBSRSP1Jn3V+puNeWCvt7qr6OZXskk5cKCUPrOrH47FcQUa5VZvn2E/qIzuHpCceq+r5EKGuXunEzzR6FA1HANlJxnRLqbel3/yvRo8Ereblf5EW/XNVP1rpFJfvJ/5OE6xdEp+bAACg3ixHOmKudPy50lFnSIlx1e0neaC08J+kO+dLb70gPfZNs+OENLRLOvWKqn40qGYs5/w0ueMFfSfx99KgOD9ttEuek8ZPq+pHK3nPTx/8kc6O/5P0B+W+auJJp1TXLDdRwZi7t/+H/i6xUtoujtNmdMjJBJAigOwsn7hV2vjTRo8CrWTwden5X0i736p6F5UFkJlc+ChJsz6dqyAAAACoh0lHS8f+xf7rOlbrXZNya0Cu+wdp787S26M8rz8jbfkPac+bVe8iPD8tY4aO/U7ueXbb79IBJ46+TBgi9OyPpPRuac9bVQeQw+956arXAzM7JEk7J8xS96FVruH66jpp2++l3dUfp5UsZRV7J3e99rZzkA484eyqnxMROWhGo0fQFAggO8kRc3NfQLle/tdcAFnDuqH56614nifLGn1tx/TQ3uF/nHFD1dMVAAAAmkJXr/T/+3qjR9FenrgxF0Bmajg/rSDYcdO55/lTfIoO+PgtVT8navCfj0t/eim3nFiVKnnPbTf3PG/O+K/q/vjl1T3ho1/NBZC1XEdVEJR7/v8Pb46ZrgM5TtGkWKEUwOgcvwLRwAmeVPrDM5MfQDpUPwIAAGAfsUTuz1rCqAqmtnp+AJm1ElU/H2oUM3BNUsF77nhpSZIdr+F6JLyOqk9oGgSQrh2v+vmAqBFAAhidgRO8/E5spT48M6n8AJIPTwAAAOzD4A3yoUqCHYcAsmGC//YGqgnLec8dvwLSjo+p+vnC0LSmmWR+F+wyxiyOU7QAAkgAowtP8PYW366I4G6jVPrDM51+J7ed4lKRqdoAAADoUMENchNTsMua2po7D3Ztgp2GMVEBWcF7HveCALKWCkhzx+lQGWMOnsezmUWG5kUACWB04Qle9RWQtm0pZufCxFIf+Fm/AjJtUf0IAACAETgGKssqmNoaVpYRQDaOwarXshq6+FOwY7UEkCZD0zLGbPn/P3hUQKKJEUACGF3Mn3ZQwwmeVP6HZzaVe540a+wAAABgJHVeDzC4Ee+xPnnjhNOZ67PuZxBAOomxVT+f2TGX7twdPI8X4zhF8yKABDC64ETLzUhuGSdooyg7gEznKiAzVEACAABgJAaCnWQF03GDG/GsrddABkLnZAXVhPEwgKxhDUiTVZtlHKdW8P8DxymaGAEkgNHF8j7AalpAubxFn126DAIAAKCYOk/HFcFO4zkGOp+XGea5rqeEginYjW1CU0loGgaQVECiiRFAAhhd/lSTWhrRlPuB71dAZm0qIAEAADACk01oKlpbj2CnYWLmGmOWes9TWVdJyw8gk7VUQNb3OLVd/3kIINHECCABjM7JCwJraERT7oenFwSQVEACAABgJCaa0PhhVMb15Lpe0W2DALJgZhDqq45h3lDGVUIZSVK8linYdV6r1PYrIC0CSDQxAkgAo7MsI41oyv3wdP3nyDLFBQAAACMJg53ab45LpWfoEOw0ARMNXfz3fKjE+53KuOEU7HiywU1oKlgD0naD47SG0BSIGAEkgOKc2k/yyl2/xEvnnsO1CSABAAAwAgNr6+UHkKXWKA+CHaa2NlAQqtWhAjKVHa6ArCl0NrhWaTpbulLX8Y9Tm+MUTYwAEkBxwXQTAyd5Je/eBV0GCSABAAAwEgM3x4PZOVLpQMp2c9VwVEA2kIkmNE4Q5pVfAala1v2MmWucI5W+jnKC4zTOcYrmRQAJoDjHwKLP5S6g7D+HyxRsAAAAjCRsQlP9uallWcNLBJUMdnI3yKksayATTWjKrYBMpRSz3MLnrYaJayin/AAy5vkVkASQaGIEkACKC0/yar/jWDqAzD2HRwUkAAAARpLfhMYrPi21mHIDqWBqK5VlDeTUfj1S7pJQ6aG8wLCWoggTa5VWUKkbBJAOa0CiiRFAAijORKfBMhd9DtZI8bjDDAAAgJHkd6POpqveTfkBZO45bIKdxjG47mepSsJM+p39n7caTu3LWNm2pbhjSargOE1wHYXmRQAJoDgjnQYdSaU/OC3/DrNXy3orAAAAaF/5QWAtgVSZM3QcLwh2CCAbJmagoYtT3vVIUAGZlS3Zsaqfr6BxTi2VumUep3F/3UonUUPnbiBiBJAAijNxx7HMRZ+tYJFm1oAEAADASPJvVNd0g7y8irhwaitTsBsnnJFl4P0uEeRl07lrnrTikmVV/XzDlbqe5Gaq3s1wJ+xSx2kugIxxnKKJEUACKC5cc8XAlIdSFZBByMkUbAAAAIzEzqtMq0NTkrgfQMaogGwcg01ohkoFkKncFOyMFa/6uSTtE5RHP+4ExylaAAEkgOIMTHkod9Fn27+raTEFGwAAAKMxsUZ5mV2w435lmRMn2GkYA01oyn2/s6ncMVVzABmrX6VuJusq4U/BJoBEMyOABFCcwSY0pT7wbdd/jjhTsAEAADCKmIFAquwKSIKdhjPZhKbE++1mDFVA2o5k5dadjHqt0lTWVcLKTfOOJTlO0bwIIAEUZ6IJTZmLJ9t+9zaLLoMAAAAYTXCuGHEg5bqe4vKDHQLIxjFQAVnujKxgDciMZaAgIr8RTZXKaeaZygxXQMZpQoMmRgAJoDiDdxxLrV3i+F2wbRZPBgAAwGhMBlLZ7Kjb5CrL/ArIJMFOw5gMnEvMyHKDANI2EUD6+4i4eU4ugCQoR/MjgARQnIk1V8q84xgGkFRAAgAAYDQmbpCXMUNnKC/YiRPsNI6BNemD9zvresq63qjbBQGka9c4BVsaXsqqlrX0y1i7ciivAlKspY8mRgAJoDgTXefKXPQ55lEBCQAAgBIcc12RS1WWJWnu0XhhQUTtFZBS8ffc84+prMkKSAPjLn6cZpSw/EreGNdRaF4EkACKCz7wjayxM/oUF0mKuUGXQT44AQAAMAqDTWiKLRGUyusubBHsNI7BJaGkEgFkWAFpIIA02cyzyJjTQ+/kPSfNPNG8CCABFBcunhz9FOyY32XQZvFkAAAAjMbEmoBlzNBJpbNK+t2FmdraQAaWhIrZliwr9/ehIut+en61opkKSANNaPzjdKjIcZoeyqsEZikrNDECSADFGbjjmCxz0eeYqIAEAABACXVaozydyg92OD9tGAOBs2VZZa37GYSFnolKwjo1ocmk845Tx8DalUBECCABFGfiBK+cD3tJcY81dgAAAFCCwSm5xae2EkA2heC/vZuR3OJLOhVT1qwsPyz0TFS8GmhCU1YAmcrtP6W4wjJPoAkRQAIozkQTmjI+OD3PU8JvQkMACQAAgFEZbEpSPNjZs/9zov7y/9vX0lG6jFlZVjaCCsiIj9OsvwZk2qL6Ec2NABJAcY65qQPFFvnOuJ7i/ho7cdaABAAAwGhitVeWJctYAzKYgp1SjMqyRsqvPjWx7meRaxIrOKZMBJAmmtCEx+nolZ/BFOyMCCDR3AggARRn4ASvrEW+M66SYgo2AAAASqhTd+GsP7WVYKfB7JgkPwCOujGma3AKtoEKyGQ5x6nfuTtDBSSaHAEkgOJMdBks44MzlXGV8APIeJIKSAAAAIwivEEecXMPvwIybTH9uqEsq27XJFYw68tEN+lwzNEep65fAZk20bkbiBABJIDi6tRlMJV1lVBuCjZdsAEAADAqE01o/Bk6Q0Vm6GSDYIfKssaLmbsmKfae235YaMUMTsGOeCaZ61dAZgnK0eQIIAEUZ6AJTTkLPqfS2XAKNl0GAQAAMCojTWgcSeVVlmWpLGs8x0BjzDLWgHRc/5gycT1isAlNsbX03VSuCU3WJihHcyOABFCciSY0TukTvKFUSrblFT4nAAAAsC8Ta5SXNbWVtfWahomq1zLec9vNFURYJgLIOq1V6vn7JyhHsyOABFBcnU7wgi6DBc8JAAAA7Cu8QV6fAJLKsiZgZFmo0kURjt+ExjaxJJTBaeNFA0j/OHUJINHkCCABFBfebaz+gzPu5LrWZVxPruuNuE0mP4A00XUOAAAA7Slo7lFLsOOfnxZdW8+f7svaek3AyLqfpd/zIIC0jDahqX7M8TLWgPQyQVDONRSaGwEkgOJMLJ4cG/5VM9qHZxBAZmVLTqzq5wIAAECbq9N0XC9NsNM06tT53PFyU7CduIEA0sBapcmypmDn/pt4LGOFJhdpALl9+3YtXLhQXV1dGj9+vC644ALt2rWr6PZf+MIXdNRRR2ns2LF6z3veoy9+8YvauXNnwXaWZe33df/990f5UoDOZXAKtjT6AsqZodziyWkxxQUAAABFmGhCU8Ya5UFlmetwftpwdWpCE/dMTsE22AW7yJiD/yZMwUazi7TMaOHChdqyZYtWr16tdDqtxYsX68ILL9R999034vavv/66Xn/9dd10002aOXOmXnnlFX3uc5/T66+/rn/6p38q2PbOO+/U/Pnzw3+PHz8+ypcCdC4Ta+w4eRWQo3x4ZtN+AGnFZeB+IwAAANqVyTXKi0xtDfZPsNMEYgYaY5bxnjtuUAHZZE1oih6nVECiNUQWQG7cuFGrVq3S008/rZNOOkmS9J3vfEdnnHGGbrrpJvX29u73M8cee6z+9//+3+G/Dz/8cH3zm9/UZz7zGWUyGcViw8MdP368enp6oho+gED+CZ7nSZZV8S4sy1LCsZXKuqN+eGbpMggAAIByOAbDqDK6C3usT954BpeFGm1GliTFlTumnISJNSDr04TGCo9TAkg0t8imYK9du1bjx48Pw0dJ6u/vl23bWrduXdn72blzp7q6ugrCR0m6+OKLNXHiRJ188sm644475HkjN7aQpKGhIQ0ODhZ8AShT2JHak9xM1bsp9eGZ9deATLPINwAAAIoJm9DUPkOnWBhFZVkTMdKEpvS0+5i/BmQsMbbq5xneWe1NaMqZgm0FQXyMoBzNLbIKyIGBAU2ePLnwyWIxTZgwQQMDA2Xt480339R1112nCy+8sODxa6+9Vh/72Mc0btw4Pfroo/r85z+vXbt26Ytf/OKI+1mxYoW+/vWvV/dCgE6Xf8c3MyRVuQZOImZLQ8WmYAcVkJzgAQAAoIiYgSWCwpvj2VG3sTJUQDaNOjWhSXhpyZJiJiogHXMVkMWCcgJItIqKKyCvvPLKEZvA5H/94Q9/qHlgg4ODOvPMMzVz5kx97WtfK/jeVVddpQ984AM68cQTdcUVV+jyyy/XjTfeOOq+li9frp07d4Zfr776as3jAzpG/gdZLdNcSty9czNBl0GmYAMAAKAIp/YwKlnG2nqW6++fCsjGM7qe4sihcybrKm7lZnwZCSBNdmsvcpzawTUaQTmaXMUVkJdddpnOP//8otscdthh6unp0bZt2woez2Qy2r59e8m1G99++23Nnz9fBx54oB566CHF48UDib6+Pl133XUaGhpSMrn//3TJZHLExwGUwXYky5G8bG1d50p84Hvp3L6zVEACAACgmJiBjsgVra3HtWTDxWrvfJ4s8Z6nsq6SCqZgm6iArP04LTVmSbLd3H8TK8Z1FJpbxQHkpEmTNGnSpJLbzZkzRzt27ND69es1e/ZsSdKvfvUrua6rvr6+UX9ucHBQ8+bNUzKZ1E9+8hONGVP6f/wNGzbooIMOImQEohJLSuk9kS767KaDLoNUQAIAAKAIE01o/Nk5rperfIs5+08ODKa2WkxtbTwTTWhKzMhKZVwl/AAynmySJjRlrFtpuxynaA2RrQF5zDHHaP78+VqyZIlWrlypdDqtpUuXasGCBWEH7Ndee01z587VPffco5NPPlmDg4M67bTTtGfPHv3gBz8oaBgzadIkOY6jn/70p9q6datOOeUUjRkzRqtXr9b111+vL3/5y1G9FABBABnhFOzgZCLLHWYAAAAUY6IJTWw4cEyNEkAGwQ5r6zUBg+t+prMjN7BNZVx1y+QUbANNaMqYgu24udDUihsYMxChyAJISbr33nu1dOlSzZ07V7Zt65xzztGtt94afj+dTmvTpk3as2ePJOmZZ54JO2QfccQRBft66aWXNGPGDMXjcd1222269NJL5XmejjjiCN18881asmRJlC8F6Gwm7jiWmD7gZYIKSKYOAAAAoIiYgQrI/AAy42rcCKegNhWQzcPAup+lZmQNpbNKWn6YF2uuJjRZ11PW9eTY1v5P4wflNscpmlykAeSECRN03333jfr9GTNmyPOG7z6ceuqpBf8eyfz58zV//nxjYwRQBoMneaPdvfOyBJAAAAAog4Gb4zHbkmVJnjf6DfIg2LHiBDsNZ6Ka0Cl+PZJK5+3bROMhg01opNxxOjbh7LeN4+VCU5sKSDS5irtgA+hAdVhA2coEi3wTQAIAAKCIINhx05I7+tTUYizLCgOp0SriHI/KsqZhoAnN8IysUbpgD72T93wG3nPHwJidwgByJDG/CY1NUI4mRwAJoLRY9Is+iwASAAAA5cg/X4xwfb1waiuVZY1XhyWh0kN5xRYm1qU3cA0Vd4anXA9lRw5OY34FpMNxiiZHAAmgNBOdBkuc4AVdBo182AMAAKB95Ven1RDulJqhE3ODqa2cnzZcHZaEyqRzAWRGjmQbiEocQ5W6pY7TIIA00TgHiBABJIDSDHYaHHUKtn+H2WOKCwAAAIopqICsIZAqMUNnuLKM89OGM1ABmSzxfmdTuQAyJUMzsvKva2o4TkuNO04AiRZBAAmgNBN3HEussTNcAckUbAAAABRhWWan5I5SERfz14B0EmOrfg4YYrChy2hBXsYPINNWvOrnKFAQQEZ4nCoXQMYIINHkCCABlGagCU2pD3zb/1C2qIAEAABAKSbWKC9xfhoXlWVNow7vd1ABmTEVQOYXVkQ0bs/zlAgCSNaARJMjgARQmsmuc6PcubP9KdhGOs4BAACgvYVrlEcYQHpBsMP5acPVoeI1mzYcQFqWmU7YRY7TdNZTMgggkwSQaG4EkABKCz7wTSz6PFoFZNBlkAASAAAApZioiCuyRJDneWEFJFNbm0AdloRy07ljKWsqgJTMXEcVWQMylXXDCsgExymaHAEkgNIMNKEptXiy4weQFlMHAAAAUErM4A3yESriMu7w1NY4wU7jmayALBFAZmyDBREGp44PjXCcpjKuEspIkuJJ1ipFcyOABFCaiTuOJSsgcyd4FlNcAAAAUIqRQMqRNEplWV6ww9TWJmDyeiTryvO8/b7v+lOws7bBCsiIm+fkjtPcdZRNIQeaHAEkgNLq0WUwnILNBycAAABKMDglt1SwE0+Mq/o5YIiRGVm5wNnzchWu+3L9kDBrJ/b7XtXCNSAjOk5TKTmWV/hcQJMigARQmokmNCWmYMe83IeyQwUkAAAASglvkO+tehfJsLIsu9/3Ull3uLkHU7Abz2BBhDTKNYlfAelaBoO8WO3HadEmNKk9+z8X0KQIIAGU5piYOuBPcRmtAtLvMuhwggcAAIBSDK6tl87uXw2XSqUVs9zC50LjxMx1PZdGDvM8v0rRNVlJ6NReqZsMj9MRAsihvGDT4ThFcyOABFCawRO80SsgCSABAABQJhNNaJzRlwhKp/KDHaa2Nlx+BeQI6zeWtQvbkmNbkkYpivCvdYwGkAamjhdbyio4TjOyJSdW9XMA9UAACaA0k10GRwkg434AGWPxZAAAAJRicEru0IhTW98Z/gcVkI0XVEDKk9xM1bspuiyUf63jmawkNNGEpsiYs34FZFoGG+cAESGABFCaiRO8IneYJSkuKiABAABQJpNdkUcIdjJ+sJOVLdlUljVcfqPKiELnICT0mq0JTZExZ/x1KzMWASSaHwEkgNIMNKFJFjnB8zwv7DLIIt8AAAAoyUATmqIBZCqoLItJllX1c8CQ/KrEiJaFsvz9erHWaUKT9St1qYBEKyCABFCakSY0o39wprJuGEDGkwSQAAAAKCFmoLIsnKGzfxfsbIqprU3FzqtENTGdeYRZWVawX5NTsA00oUk4ozfzzKZzY6YCEq2AABJAaSZO8IosnpzKuEoot5ZLnApIAAAAlBLxDXKmtjYhA8tCFZuVZbm5ax0rkgrIaI5TNwggTU4bByJCAAmgtGDNlYgWT84FkH4FZGJs1c8BAACADmEg2CkWRgVTsDMWwU7TiHjdTzvYr2OwIMJEE5piU7DTHKdoHQSQAEqLuMtgKpNV0spVQNp0wQYAAEApYbATzQydoLIsTbDTPIKiCBPVhCNMu7f9CkjFTU7BDq6jqj9OiwXlwXGatanURfMjgARQmoEmNMN37vb/sE8NvbP/cwEAAACjMdGEpsgMHdevLCPYaSKOgWuSIu+5kw2mYBsMIE00oSmybqXnH6cuU7DRAgggAZRmYo2dIh+cmaG8D2STiz4DAACgPRlco3ykGTrDlWUEO03D4HTmkd5z28sdS7bJANJEE5oiFZBehuMUrYMAEkBpBk7wik0dSKXyA0g+PAEAAFBCxE1owgpImtA0D4PLQo1YAelPwbZNTsE22IRmpNA0CCCpgEQrIIAEUJqJJjT+B6frSZl9qiCDRb5Tikk2v5YAAABQgolgp9jUVirLmo+JJjRF3vOYm2uKaZlck97EWqVlHKcus8jQArjSB1CaY26Ki7T/h2cQQKbFHWYAAACUwWQTGirLWkPUFZBeLoB0ImlCE81a+sHx73GcogUQQAIozcDiyXEnL4Dc5wM/GwSQTHEBAABAOSJuQhPs12V5oOZhsup1hPc87q8B6RitgAwKOWq/jhppzFZ4nHIdheZHAAmgtOAEz01L7ggnaGWI2ZYsK/f30QLIDAEkAAAAymGwCc2IU1v9yjKmtjYRg01oRgrzYkEFZCKCCsgaKnWTRY7TcL8cp2gBBJAASovl3fmt8sPTsqzwjuO+CyhnMwSQAAAAqEDETWiCKjumtjaRcFkoA+/5CGFePJyCHcEakBFNGw8CeI8AEi2AABJAabG8D2ETJ3n7fOC7qdw+MxYneAAAAChDGOxEswakFaytxxTs5mGyoctIU7CVCyBjibFV738/Ea9Vavudu8PnAZoYASSA0vJPvGo4yUuO8uHppukyCAAAgAqYmI5bdA1If78xzk+bhsGGLvvOyPI8Ly+ANFgBaWLMxdaADI5/Aki0AAJIAKVZ1nAIGcFJXtb/QM4yBRsAAADlcMxVQA6NMB3XCirLHINhFGpjsppwn/c8nfWU8APIuNEKyOAaKpq1SoMKSItKXbQAAkgA5TF4x3G/D890bg1IKiABAABQFgPdhfOntnqeV/A9O0sFZNOJGeh87r/n6X0KIlJZV8kggExGUQFZ+5j3rdqUJCcIIONUQKL5EUACKE/M4KLP+0558E/wXAJIAAAAlMNAE5qk44R/T2f3DSBZW6/pOAY6nzsjF0SkMq4SykgyHECaWKu0yBTsMIDkOEULIIAEUJ6gEU0EnQa9YA1IurcBAACgHAab0Ej7B1I2wU7zMbDu52hr0qfSGcWtrCTJiUfRhMbAmLMjVOq6uapNK8ZSAWh+BJAAymPwjuN+0wf8qkqPCkgAAACUw0QTmvwAMrNvABkEOwSQTcOJbkZWeuid4X+YnHZvcBkrz5MybmEAGVRA2nECSDQ/AkgA5TF4krfvHeZwCjaLJwMAAKAcwXmj50rZTHW7sC05tiVp/0AqDHYIIJtHlE1oUnkBpMlZWQab0Ej7H6cxL7dfhzUg0QIIIAGUx8gdx9w6O/t+cFpBBSQBJAAAAMqRHwzW0uBjlPX1YmFzDyrLmkYwzbim9zt3PbLvjKyCCkgnXvX+92OiCY1TOoC0EwSQaH4EkADKEzMwfWC0BZSDO4KsAQkAAIBy5J83GqmIyxbuPgh2qCxrHiaWhBptCnYqFxAOKS5ZVtX7309wDVVDpW7MseUX6u5XuRnzcksFOATlaAEEkADKY6LTYPiBX3iCZ2WpgAQAAEAFnJhk+ZezBtbX27ciLgh2YgmCnaZhckmofd7vrB9ApmWw+lEaDk2lSMYdD45TAki0AAJIAOUx2Glw3zt3lj/FRayxAwAAgHIFU3JrCXZGm4JNZVnzMdgUc9/rkUwqdwylLcMFEfndqQ3MJNs3KA8DyCTHKZofASSA8pi44zjKCZ6dJYAEAABAhQwEUslRKssIIJtQlBWQ6VwFZMYyXQGZV6lb01IBI6+lH5d/nFKpixYQaQC5fft2LVy4UF1dXRo/frwuuOAC7dq1q+jPnHrqqbIsq+Drc5/7XME2mzdv1plnnqlx48Zp8uTJ+spXvqJMprr1FACUyUgTmtECyNw+LQJIAAAAlCtmoMHHKDN0EkF3YSrLmoeB93u0wNlNRRRASkYa0SRHOE5d11PCDyDjBOVoAbEod75w4UJt2bJFq1evVjqd1uLFi3XhhRfqvvvuK/pzS5Ys0bXXXhv+e9y4ceHfs9mszjzzTPX09OjXv/61tmzZokWLFikej+v666+P7LUAHc9EE5pgjZ19TvCcoMsgASQAAADKFa5Rbr4pSVBZxhqQTcSJbkmobCbCADKWkDLvGG+ek8q6SihXiMUUbLSCyALIjRs3atWqVXr66ad10kknSZK+853v6IwzztBNN92k3t7eUX923Lhx6unpGfF7jz76qH7/+9/rl7/8paZMmaITTjhB1113na644gp97WtfUyKx/5oNQ0NDGhoaDk0GBwdrfHVABwoqICOY8mC7uRM8mwASAAAA5YoZmKEzwhJBnucNB5BUljWPCJeEyvprQGbtCJpiGmjmOdK4U1lXyaACMjm2+vEBdRLZFOy1a9dq/PjxYfgoSf39/bJtW+vWrSv6s/fee68mTpyoY489VsuXL9eePXsK9nvcccdpypQp4WPz5s3T4OCgfve73424vxUrVqi7uzv8mjZtWo2vDuhAwQLKJhZ9zoxSAckJHgAAAMplognNCBVxuWAnV1kWp7KseZhoQjNKQYTnh9gZ001oJDPXUeFxmg0fS6WzSlq54zRBAIkWEFkF5MDAgCZPnlz4ZLGYJkyYoIGBgVF/7tOf/rSmT5+u3t5ePfvss7riiiu0adMm/fjHPw73mx8+Sgr/Pdp+ly9frmXLloX/HhwcJIQEKhXhos+Ov8i3HacCEgAAAGUyGEjldxdOZdzhtfUIdpqHyeuRrCvP82RZliTJTUdYARmLZiZZamh4TUmWskIrqDiAvPLKK/Xtb3+76DYbN26sekAXXnhh+PfjjjtOU6dO1dy5c/Xiiy/q8MMPr2qfyWRSyST/QwI1MdllMLtvl0F/kW8CSAAAAJQroim5qXRWB/gVkAnWgGweBgNnya909btLBxWQrh1lE5raj9P8oDydymtq43AdheZXcQB52WWX6fzzzy+6zWGHHaaenh5t27at4PFMJqPt27ePur7jSPr6+iRJL7zwgg4//HD19PToqaeeKthm69atklTRfgFUyGSXwX0qIGP+FGybKdgAAAAol2NgDciRKsvSQ7ItTxLnp00lnMpcw/WIkxdAZoYDyGCfrhNhBaTh4zQz9M7wBlGMGzCs4gBy0qRJmjRpUsnt5syZox07dmj9+vWaPXu2JOlXv/qVXNcNQ8VybNiwQZI0derUcL/f/OY3tW3btnCK9+rVq9XV1aWZM2dW+GoAlC1sQmN+Dci4PwXb4Q4zAAAAyhUzUFk2wgyddN7UVjG1tXkE74WbllxXsitvabFvABnw/KpKr1mb0IxwnGb8Csi0YopX8d8CqLfIjtJjjjlG8+fP15IlS/TUU0/p3/7t37R06VItWLAg7ID92muv6eijjw4rGl988UVdd911Wr9+vV5++WX95Cc/0aJFi/ThD39Yxx9/vCTptNNO08yZM/XXf/3X+o//+A/94he/0Fe/+lVdfPHFTLMGohTecTT7wSlJsaDLIAEkAAAAymVgCnZyhMqyggCSqa3NI7/Kr8qiCNu2FHdy6z4WXJP4x5AbxfsdBuVmm+cEU7BTimDaOBCBSGPye++9V0cffbTmzp2rM844Qx/84Af1ve99L/x+Op3Wpk2bwi7XiURCv/zlL3Xaaafp6KOP1mWXXaZzzjlHP/3pT8OfcRxHjzzyiBzH0Zw5c/SZz3xGixYt0rXXXhvlSwFgcPHkoVEqIGNxFvkGAABAmRwDwc4IM3SCYCcjp6oqO0QkvxrV8Lqfll9k4UUyBdtAUD7CmMMKSIsAEq0hsi7YkjRhwgTdd999o35/xowZ8jwv/Pe0adP0xBNPlNzv9OnT9fOf/9zIGAGUKaITPEmKUwEJAACAShnuihzI5FWWRXrBjMrkh4M1VhPuTmULr0n8ikovigrIiNYqzaaDoJwAEq2B2zkAyhNRExrX9ZQMAsgkASQAAADKFFWwk8o196CyrMlYVl5RRO3XJPmzsqwgxI50CrbZoDwbVOpynKJFEEACKI+JJjQjfHCmsq4SykiS4lRAAgAAoFwmgh0n1wV5aKSprVSWNZ+w6tXsNYkV7C/WpE1oRphJ5gYVkFE0zgEiQAAJoDwGTvBGWuR7KOMq4VdAEkACAACgbCZvkBdMbc2d71JZ1oRMVL2OEObZrr+/KLqeB6GmgSY0QyNNweY4RYsggARQHhN3G/07zOn8CshUWjEr9+94kiY0AAAAKFPMv3ltYGpr/vmpFwY7VJY1HSPrfuauSQoCyGyuIMKKRVAQEezTwFql+cep6wflWSog0SIIIAGUxzG4dklBl8F3wr9bUdxxBAAAQHsKKstMNKHJryzLBMEOlWVNxzFXTZj/njtubn+RXI9EtFapFxynBOVoEQSQAMpjcvHk/AByaDiAjGTKAwAAANqTgRvkSWf/9QDdFBWQTSuseq2+Cc1I73kQQNrxJm1CM9Jx6lfqZh2OU7QGAkgA5XHM3WEeyuYHkLkPTleWZMeqHx8AAAA6S0Q3yN0MU1ubViyadT9tL8oKyNqnjY+0ln5QAelSqYsWQQAJoDwm79xlXHmel9tdfpdBy6ptjAAAAOgcETWh8Vhbr3lFtCxULNIKyGimjQf7czlO0SIIIAGUx0QTmtjwr5x01g8g07kp2Clx5w4AAAAVMNGExtl/hs5wZRnBTtMx0YRmhPfc8XJNaOx4czehyZ+CHRz3rsMyVmgNBJAAymNijZ28ADL48MykcvtLWwSQAAAAqEBE03HDAJK19ZpPRE1oYn4A6SQiCCBNjNnJde4eyq+A9ANNj+MULYIAEkB5gruNXlbKZqraRXC3URr+wM/6XbAzBJAAAACoRHiDvPqGJMNhVHb4QX9/HhWQzSdm8j0fDvPi/hqQTqRNaMyO2QoqKqmARIsggARQnvw7a1VOH7BtSzE7t87jcACZ2xcBJAAAACpisglN/tRWv6KSqa1NyOCyUIUBZK4CMhbFFOyI1ioNKyqpgESLIIAEUJ78jnAGF312/TuBGYsPTgAAAFTARLDjjFBZFpzrEuw0HxNNaIL3PDtc9RoGkFFMwTbZzDMvKLf9496LonM3EAECSADlsWOS5f/KqOEkL77PB76bDiogOcEDAABABUw0oRmpsiwIdgggm0+47qfZ9zyuYA3IsdWPbTRGmtAUziKTJMsNKiAJINEaCCABlMeyzNxx9D/wgwWUgwAyazMFGwAAABUw0YRmpArIMIAk2Gk64fWIuffcdT0l/AAynmzuJjQjHaeiAhItggASQPkiOMkLugxmWeQbAAAAlTB4czx/amtYWRbj/LTphGtAmnjPPf9PVwnlmmxGOgXbyJiHj1PHP04tAki0CAJIAOUz0Gkwue8akOncvlwCSAAAAFTCYBOadNaT6+YCKTubOz+1YhGEUaiNycZD/vVIKpNVMqiAjCKADCsgzU4bd9zc/ggg0SoIIAGUL2ZgysO+d+/8D2ICSAAAAFTEMbceoDR8fuq4uTCKYKcJGW1C4weQqZRsKxc+J5JRrAFpPjSVho9TO85xitZAAAmgfCanD+w7BZtFvgEAAFCJsLlHSvK8qnYRhFHScCBlM7W1eZlYEiq8Hsk1xUwPDc/usuNRTMHOO06rlB+aev6x7ni5/UUyZiACBJAAymfyjmNw945FvgEAAFCN/DUaqwx3CgLITFAB6QeQVJY1H5Prfvrvd3roneFvRhE6G5yCLe1fqUsAiVZBAAmgfCbvOO4zBdujAhIAAACVyL+BXWW4Y9uW4o4lKT+A9IMdKiCbT6z2affJfa5H0v6a9BnZku3UNr6R5M8iq7JSNxnbPyiP+RWQDkE5WgQBJIDyGWhCEwSQQ/4HpxWcPLAGJAAAACqRfwPb4AydoLkHa+s1oQhmZGX8Csi04rWNbTRONJW6MS8XlDtUQKJFEEACKF9wx7GWJjT7fOBbwRRs7jADAACgErYt2X5oZGKN8uw+wU4igoYkqE2wnqLBKdiZVG5fkQWQMTOVujHbr9T1j9M4FZBoMQSQAMrnmG9CE1ZAMgUbAAAAlYqgw3AQQFIB2YQMLgk1FAaQudldaSuqCsi848hI8xw/gFQQlFMBidZAAAmgfCZP8Pw7d5a/yHd4NxMAAAAoV7i+nrlAKgggY0xtbT4mp2D71yPZdBBARlQQkV+pazAoj3uZ3J9JKnXRGgggAZTPwAlecp8PTtvfl8UUbAAAAFQqgjUBg8qyGJVlzcdkU8xMEEDmjp1MVBWQUmEjmioFx+kQxylaFAEkgPKZaEKz3yLfBJAAAACoUrhGeS2VZbnOx0FFXMJfWy+W4Py06Rhsihlcj7ipXBOaSANIx8RxOly5mXU9Jf0AMk4AiRZBAAmgfCaa0OwzBdsOAkjW2AEAAEClIlijPKwsY2pr8wmb0BiYkeVfj7h+mJmNagq2ZHyt0lTGVSIIIJMEkGgNBJAAyhfBCZ7j+ot8UwEJAACAShm4QZ7Mm6Hjup4Syvi7JthpOuEU7FqmMvsVr0EFpD8FO2vXoQKylqnjecdpKpVWzPIDc4JytAgCSADlM3Hnzv/AH9pnCrbNIt8AAACoVFARZ+IGeTarVDavsixBsNN0TKz5uU9BhBcGkBEWRISVm9WPO38t/aHUnuFdE5SjRRBAAiifwS6DqX26DNpMwQYAAEClDAdSQ+mMElZWElNbm5LBJjQZ15PrenIzdaiANFG5mTd1PD00vAamFeM4RWsggARQPpOLPvtrrsT9Rb4d7twBAACgUiaa0ORPbd37zvDjBJDNx+D1iJS7JvH8fbl2hGtAGg7KM0O549SVJdmxmocH1AMBJIDymWxCk8ndWY4FASQVkAAAAKiUwTXKhzKu0ikqy5paMCPLc6VspqpdBIGz5C8L5V/beE6TN6HJC8oz/nGaUlyyrJqHB9QDASSA8hk4wctf5FsanoLtsAYkAAAAKmXyBnm2MIBUlIEUqpPfuLLKa5K4MxzYpTJuGApGWwFpbur4UHY4gEyL6ke0DgJIAOUL79yZOcGTpLi/yHeM7m0AAAColMkmNBlXGX9tvSEqy5qTkxdAVllNaFlW4TWJHwp6seZuQpOIDXfvDgNIi5AcrYMAEkD5YuamuAQVkHEvN3UiRgUkAAAAKuWYXQNyuLIswoYkqJ4Tkyw/xqihmjB/VpblX9t4TpQBpIEKyLwxZ1O5MXOcopUQQAIon+ETPElK+BWQdBkEAABAxcIb5DWEUfkVkOlccw+CnSZmuKGLFRw7UU65N92Exm+ck7E4TtE6CCABlM/E4sl5i3xnsq6SwRRsumADAACgUga7IqeyrjIpP4Ak2GleJjqfx/avgFQ9KiBrOE7DoDybVTZFAInWQwAJoHwGuwymsq5SmaySFhWQAAAAqJKJJjQjTW0l2GleJtf9zGaHKyBjdaiANNCEJpVx5ab9ADLKxjmAYQSQAMpnsstgxlUqNXzSkEjQhAYAAAAVMnmDPD/YoblH83IMNMZ0hmdl2W5uP1akTWgMzCRz8o/T3H6yBOVoIQSQAMpn4m5j3gdnemh4CgJdsAEAAFCxmIEwKlgiKOsqS7DT/MKGLmZCZ8cNKiAjnJFlYK3S/Jlknh9kZu0IQ1PAsEgDyO3bt2vhwoXq6urS+PHjdcEFF2jXrl2jbv/yyy/LsqwRvx588MFwu5G+f//990f5UgBIeU1oDCzynXWVGnonb998eAIAAKBCMdMVkEGwQwVk0zLc0CWogLTjEV6PGBzzUN5x6nKcooXEotz5woULtWXLFq1evVrpdFqLFy/WhRdeqPvuu2/E7adNm6YtW7YUPPa9731PN954o04//fSCx++8807Nnz8//Pf48eONjx/APmIGF/nOuEoHi3zLUdymIBsAAAAVcmpv7pE/Q8fzp2BnbSogm5aJJjTOcFFErB4BpMkxZ1zJ4jhF64ksgNy4caNWrVqlp59+WieddJIk6Tvf+Y7OOOMM3XTTTert7d3vZxzHUU9PT8FjDz30kP7qr/5K73rXuwoeHz9+/H7bAoiY4cWTM/4U7LTi4qMTAAAAFTM4BTuVceUytbX5mWxCk3HluLmmmHWpgDQ0Zs/K7cdzqIBE64is5Gjt2rUaP358GD5KUn9/v2zb1rp168rax/r167VhwwZdcMEF+33v4osv1sSJE3XyySfrjjvukOd5o+5naGhIg4ODBV8AqmDwzl3G9cI1INPRFmMDAACgXRkIdpIjrq3H7fGmZXJZqIwrx/MrIOuxBqSJaeNZV57/2pmCjVYS2VX/wMCAJk+eXPhksZgmTJiggYGBsvZx++2365hjjtH73//+gsevvfZafexjH9O4ceP06KOP6vOf/7x27dqlL37xiyPuZ8WKFfr6179e3QsBMCz/bqPnSZZV8S6CD05J2rs3NwU7RZdBAAAAVMNwBWQQQBLsNDGT635mXcW8XAWkk4iwKaaBJjT5oan8CkiXdfTRQiqugLzyyitHbRQTfP3hD3+oeWDvvPOO7rvvvhGrH6+66ip94AMf0IknnqgrrrhCl19+uW688cZR97V8+XLt3Lkz/Hr11VdrHh/QkfJL/LPpqnZRGEDukSRlmIANAACAapgIoxxHkh/sZJja2vQcs+spxoMAstmb0OSN2fIDdy9GAInWUXEF5GWXXabzzz+/6DaHHXaYenp6tG3btoLHM5mMtm/fXtbajf/0T/+kPXv2aNGiRSW37evr03XXXaehoSElk/v/D5hMJkd8HECF8j/gMnuHp2RXIPjglKShd/wAkikuAAAAqIaJMCqvGo4AsgUYnM48lHEV96dgO/Eop2CbPU4tvwmNqNRFC6k4gJw0aZImTZpUcrs5c+Zox44dWr9+vWbPni1J+tWvfiXXddXX11fy52+//XZ94hOfKOu5NmzYoIMOOoiQEYhafol/ldMHLMtSwrGVyroa8teAzDAFGwAAANUwubZexpWVDQJIri2bluGGLnHlKiBjiQgDSMNjtmz/WowKSLSQyNaAPOaYYzR//nwtWbJEK1euVDqd1tKlS7VgwYKwA/Zrr72muXPn6p577tHJJ58c/uwLL7ygJ598Uj//+c/32+9Pf/pTbd26VaeccorGjBmj1atX6/rrr9eXv/zlqF4KgIBtS3ZMcjM1n+Slsq7SqdwakFmLCkgAAABUwUSw4+RVlvk32Qkgm5iJdT+DafdZfwq2JcWT9aiArGXMecepFwSQFHKgdUTaevbee+/V0qVLNXfuXNm2rXPOOUe33npr+P10Oq1NmzZpz549BT93xx136JBDDtFpp5223z7j8bhuu+02XXrppfI8T0cccYRuvvlmLVmyJMqXAiAQGyOldtV+925IYRfsDFMHAAAAUA3DTWiCAFJMwW5eBpvQpDOu4srkdhtpE5q8Zp5Vyj9O7VjuOLWi7NwNGBZpADlhwgTdd999o35/xowZ8jxvv8evv/56XX/99SP+zPz58zV//nxjYwRQIcfc3btMKhdAZgkgAQAAUA0DYVRypACSqa3Ny+C6n++ks0rUcwq2oaUCbKZgowVV3AUbQIczeMcxCCBdAkgAAABUw8TN8fzmHm5QAUmw07TC65Hq3/MgdN69N62klauArMsUbANjTmVd2W4uNLUJINFCCCABVMbAHce4Y0mS3DQBJAAAAGoQTsHeW/Uu4v7snKzryQ5usrO2XvNyTLznueuRvXvfGX6sLhWQtR+nqYyrmBscpwSQaB0EkAAqY6TTYG7R5ywVkAAAAKhFEOx4WcnNVrWLoAJSkiz/HNeKs7Ze0zLShCb3nhcEkM3ehCZvCrbjV+racQJItA4CSACVMTgF2/VP8FymuAAAAKAa+RVgVd4gD8IoSeEUbKa2NjEj1yN+F+yh4QAy0oYuJprQBOvou56cYAo2QTlaCAEkgMo4td9xTPofnp5/kujRZRAAAADVyA8Kqwx3gum4ksImNFSWNTGT6376AWTKi0mWVexHahNcQ7kZyXWr2kV+pa7t5V67w3GKFkIACaAyBisgg5MGAkgAAABUxY5J8oOjKgMpy7LC89Phqa1UljUtg9cj6aHcklApK17zsIrKX1O0ynHnB5AxvwLS4ThFCyGABFAZA01owg/P7N7CfQIAAACVsCwjjWiCGToxL5iCTbDTtExcjwTTmf016dOKOIDMX3KqyuM0f6mA4Dh1omycAxhGAAmgMiaa0AQneP6dO7q3AQAAoGpBuJOtfUpuzPMryxKcnzYtA9cjSf/9tvxqxHTUFZBO3v5rqdT1r6Piyh2nMY5TtBACSACVCe44GjjBSyiTe4AAEgAAANWKmZuhE5yfOomxNQ8LETHR0GWf9zsTdQWkZeUF5SbG7QeQTMFGCyGABFCZ4APfxAmeRQUkAAAAamQwkEpawdp6nJ82LYNNaIL3O23XYUmo8DrKwLj94DSWJChH6yCABFCZ4A6zwTt3lsMJHgAAAKpkIpByCiviYqyt17xMNKHZ5/3ORD0FWzJzHeXsUwGZ5DhF6yCABFAZx9wakEn/g9PmDjMAAACqZaAJzXBlWS7EjBNANi+DTTGD9ztbjwDSMXWcehrjV24m4lRAonUQQAKojMFFn4M7jhYBJAAAAKoVwRrlVJY1MRNNMfd5v7N1mYJtZup4XNnw33GmYKOFEEACqIzBE7xgzRWbNSABAABQLROB1D5TW+M0oWle4ZqfJqbc597vrF3HCsgap2AHY5akxBiCcrQOAkgAlTHRhGafNVdspg4AAACgWuGagLXdIHeUlWN5kqgsa2oGpmCHM7KsoAKyDgURYVBe23GaH0A6dMFGCyGABFCZCJrQOAkqIAEAAFAlA2uUJ/cJdhJMwW5e+U1oPK+qXex7PeLWZQq2gQrImB0WcaQ9R7IdEyMD6oIAEkBlTDShiRU2oeHOHQAAAKpmqAlNMi+ApAt2E3PywsIqq173vR6pSwBpqHIzWMYqbcVMjAqoGwJIAJUxuuhz7sMzRgAJAACAaplYo9wZrizLepYspw5rAqI6+evHV3lNsu+SUJ5TxwrIGpeyCq6hUuIYRWshgARQGUMneNLwmisOd5gBAABQLUM3yBN+ZVnKIthpak5eAFnlNUnMsWVbCt/zugSQJprQxGwl/dA0QwCJFkMACaAyJprQ7FsBySLfAAAAqJahJjTJsLKsDmEUqmfbUtC1usZrkmRYAVmHggjDTWhSFscpWgsBJIDKxGqvgAy7zvkf+KyxAwAAgKqZWKPccYabe4i19ZqeiYYuedOZvViLNKHJWyogQ6UuWgwBJIDKGGxCE3zgxwkgAQAAUK3wBnmt3YVz56YEOy0gbOhSSzWhE77nVv607qgYGfPwUgEcp2g1BJAAKhOrvXtbwnFkyVXCykqS4kkCSAAAAFTJ0A3yYAp2mqmtzc9A5/NkfufzujahMdOtPctxihZDAAmgMoYWTw6mDkhSggASAAAA1TLQhCYZs8MGiVSWtQATjTHz3nMrXscKSFOVujbHKVoLASSAyhjqMhjebZSUSNCEBgAAAFUyEUY5+VOwqSxreiYaY+a95+E1TpQMNKFJ5q0BmbU5TtFaCCABVMZEl0GnsALSrscdRwAAALQnE2FULD/YobKs6Rlb99OvgIzVowu2/xy1jtlfA5IAEq2GABJAZQytsRPcbRzy4pJlmRgZAAAAOlHM0HRc1tZrHU7t1YT573ldCiJMNaHxQ1OXABIthgASQGUMNKFJxmwl/Tt3KStmYlQAAADoVI6B5h7O8PkplWUtwERDl7z3vC4BpKkxKxdgEkCi1RBAAqiMsSY0fpdBMcUFAAAANTC8RjnBTgsw1YSmERWQNY3ZUdKvgPTq0bkbMIgAEkBlghM8NyO5blW7yF8DMk2XQQAAANTCWBjlrwFJsNP8DK37GYR5Tl3WgDS0lJVftUkAiVZDAAmgMvkd4qqsgiysgOSDEwAAADUw1oTGD3aogGx+htf9dBIt1IQmWAPSoZEnWgsBJIDK5H/QVXmSl7tzl/vgzFABCQAAgFoYCKOSTn6wQwDZ9Aw0xsx/z+sSQJpoQuMMh6YigESLIYAEUBknLzCs8gM/ZlsaY+U+eAkgAQAAUBMTTWhitpL++alHsNP8wsaYtb7naX939ZyCXf2Yk3lrlRJAotUQQAKojGXV3IjGsiyNs7O5XTDFBQAAALWIGagsy5vaKiogm194PWJmCnYsXscKSENrlYbHPdAiCCABVC68e1f9h+c4JwggqYAEAABADWq8OS7tswYkAWTzM9HQJW8KdizZek1oCtbmB1oAASSAysVqP8kLKiAzFh+cAAAAqEHYhKbWtfWCyrI6hFGoTcxsBWQ8OdbEqIozMeb8NSA5TtFiCCABVM7Aos/jbH+RbyogAQAAUIuwCY2ZyjKLyrLmZ6QJjaeY5UqSEok6BJAGxpw/BdtmCjZaDAEkgMoZ6DQ4JgggWTwZAAAAtcgPdjyvql0kYraSfrBjEew0PwOh81j/ekSS4vWcgl1jUB40obHrsW4lYBABJIDKGeg0OFwByQkeAAAAahAGhp6UTVe1i6TjhMEOFZAtwEA14VhrOIBM1GMKdrC2aI3rVgadu604xylaCwEkgMoZ6DQ4xgoqIAkgAQAAUIP8GTVVVpflrwdIANkCDDR0CSogXc+SE6vDslCx2it1k3lTsB2OU7QYAkgAlQsWPK5h+kAQQHpMwQYAAEAt8oOYKm+Q56+tZyWY2tr0DDR0SfrXI0OKS5ZlYlTFhcepJ7mZopuOJj8oZwo2Wk1kAeQ3v/lNvf/979e4ceM0fvz4sn7G8zxdffXVmjp1qsaOHav+/n49//zzBdts375dCxcuVFdXl8aPH68LLrhAu3btiuAVABiVgekDwQe+RxMaAAAA1MJ2JDuW+3uVN8gd2wqb0Nh0F25+JprQ+NcjaatO1yP5hRdVjjue162dABKtJrIAMpVK6ZOf/KQuuuiisn/mhhtu0K233qqVK1dq3bp1OuCAAzRv3jzt3Tu8ztzChQv1u9/9TqtXr9YjjzyiJ598UhdeeGEULwHAaAzecfSYOgAAAIBaGQikghk6NmvrNT8DTTGD65GU6hRA5l/3VDnu/ApIJ8FxitYSi2rHX//61yVJd911V1nbe56nW265RV/96ld11llnSZLuueceTZkyRQ8//LAWLFigjRs3atWqVXr66ad10kknSZK+853v6IwzztBNN92k3t7eSF4LgH0YaEIzxr/DLKZgAwAAoFaxhJTeXWNFnB/sEEA2PwPXI0HToUy9AkjbkSxH8rJVjztmW+FxGmOpALSYyALISr300ksaGBhQf39/+Fh3d7f6+vq0du1aLViwQGvXrtX48ePD8FGS+vv7Zdu21q1bpz//8z8fcd9DQ0MaGhr+IBocHIzuhQCdILjj+K+3SP/xQFW7OHHPJn9fnOABAACgRkEg9ePPSvEDqtrFIdomSYoxtbX5Bdcj2zZKt8+rahfHDv5JUh2nYEu5a5/0HumHnxpeV78ClqSDlcszCCDRapomgBwYGJAkTZkypeDxKVOmhN8bGBjQ5MmTC74fi8U0YcKEcJuRrFixIqzIBGDA+Om5P3e8kvuqQpf/Z+Lg6WbGBAAAgM510HRp14A08NuqdzFGuY7I43sPMzcuRCO4HknvkV79TVW7eJf/55+SUzXNzKhKGz9demOjtGVD1buIW1LKi+mgyXUbNWBERQHklVdeqW9/+9tFt9m4caOOPvromgZl2vLly7Vs2bLw34ODg5o2jf9Zgaqdulya/oGapjyksq5e2ZPQiSdVd8cSAAAACC34obT515LnVb2Lt4cyeivxbs2YdoTBgSESE4+U/uZJ6U/VFUMEXt2xV9OP/qihQZVh0T9Lr66raRc796a1Y9yhmn7wlNIbA02kogDysssu0/nnn190m8MOq+5uUU9PjyRp69atmjp1avj41q1bdcIJJ4TbbNu2reDnMpmMtm/fHv78SJLJpJJJpnkCxsTHSH92Wk27SEg60sxoAAAA0OkOOFg65uM17eJA/wstYuqs3FcN6l6WdOAUaeYnatpFt/8FtJqKAshJkyZp0qRJkQzk0EMPVU9Pj9asWRMGjoODg1q3bl3YSXvOnDnasWOH1q9fr9mzZ0uSfvWrX8l1XfX19UUyLgAAAAAAAADVs6Pa8ebNm7VhwwZt3rxZ2WxWGzZs0IYNG7Rr165wm6OPPloPPfSQJMmyLF1yySX6xje+oZ/85Cf67W9/q0WLFqm3t1dnn322JOmYY47R/PnztWTJEj311FP6t3/7Ny1dulQLFiygAzYAAAAAAADQhCJrQnP11Vfr7rvvDv994oknSpIee+wxnXrqqZKkTZs2aefOneE2l19+uXbv3q0LL7xQO3bs0Ac/+EGtWrVKY8YMd3e69957tXTpUs2dO1e2beucc87RrbfeGtXLAAAAAAAAAFADy/NqWKW3RQ0ODqq7u1s7d+5UV1dX6R8AAAAAAAAAEKokX4tsCjYAAAAAAAAAEEACAAAAAAAAiAwBJAAAAAAAAIDIEEACAAAAAAAAiAwBJAAAAAAAAIDIEEACAAAAAAAAiAwBJAAAAAAAAIDIEEACAAAAAAAAiAwBJAAAAAAAAIDIEEACAAAAAAAAiAwBJAAAAAAAAIDIEEACAAAAAAAAiAwBJAAAAAAAAIDIxBo9gEbwPE+SNDg42OCRAAAAAAAAAK0nyNWCnK2Yjgwg3377bUnStGnTGjwSAAAAAAAAoHW9/fbb6u7uLrqN5ZUTU7YZ13X1+uuv68ADD5RlWY0ejnGDg4OaNm2aXn31VXV1dTV6OGgRHDeoBscNKsUxg2pw3KAaHDeoFMcMqsFxg0q10zHjeZ7efvtt9fb2yraLr/LYkRWQtm3rkEMOafQwItfV1dXyBzPqj+MG1eC4QaU4ZlANjhtUg+MGleKYQTU4blCpdjlmSlU+BmhCAwAAAAAAACAyBJAAAAAAAAAAIkMA2YaSyaSuueYaJZPJRg8FLYTjBtXguEGlOGZQDY4bVIPjBpXimEE1OG5QqU49ZjqyCQ0AAAAAAACA+qACEgAAAAAAAEBkCCABAAAAAAAARIYAEgAAAAAAAEBkCCABAAAAAAAARIYAEgAAAAAAAEBkCCDb0G233aYZM2ZozJgx6uvr01NPPdXoIaFJrFixQu973/t04IEHavLkyTr77LO1adOmgm1OPfVUWZZV8PW5z32uQSNGM/ja17623zFx9NFHh9/fu3evLr74Yh188MF617vepXPOOUdbt25t4IjRDGbMmLHfcWNZli6++GJJ/K6B9OSTT+rjH/+4ent7ZVmWHn744YLve56nq6++WlOnTtXYsWPV39+v559/vmCb7du3a+HCherq6tL48eN1wQUXaNeuXXV8Fai3YsdNOp3WFVdcoeOOO04HHHCAent7tWjRIr3++usF+xjp99O3vvWtOr8S1FOp3zfnn3/+fsfE/PnzC7bh901nKXXMjHSOY1mWbrzxxnAbftd0lnKutcu5btq8ebPOPPNMjRs3TpMnT9ZXvvIVZTKZer6UyBBAtpkHHnhAy5Yt0zXXXKNnnnlGs2bN0rx587Rt27ZGDw1N4IknntDFF1+s3/zmN1q9erXS6bROO+007d69u2C7JUuWaMuWLeHXDTfc0KARo1n8l//yXwqOiX/9138Nv3fppZfqpz/9qR588EE98cQTev311/UXf/EXDRwtmsHTTz9dcMysXr1akvTJT34y3IbfNZ1t9+7dmjVrlm677bYRv3/DDTfo1ltv1cqVK7Vu3TodcMABmjdvnvbu3Rtus3DhQv3ud7/T6tWr9cgjj+jJJ5/UhRdeWK+XgAYodtzs2bNHzzzzjK666io988wz+vGPf6xNmzbpE5/4xH7bXnvttQW/f77whS/UY/hokFK/byRp/vz5BcfED3/4w4Lv8/ums5Q6ZvKPlS1btuiOO+6QZVk655xzCrbjd03nKOdau9R1Uzab1ZlnnqlUKqVf//rXuvvuu3XXXXfp6quvbsRLMs9DWzn55JO9iy++OPx3Npv1ent7vRUrVjRwVGhW27Zt8yR5TzzxRPjYRz7yEe9LX/pS4waFpnPNNdd4s2bNGvF7O3bs8OLxuPfggw+Gj23cuNGT5K1du7ZOI0Qr+NKXvuQdfvjhnuu6nufxuwaFJHkPPfRQ+G/Xdb2enh7vxhtvDB/bsWOHl0wmvR/+8Iee53ne73//e0+S9/TTT4fb/J//8388y7K81157rW5jR+Pse9yM5KmnnvIkea+88kr42PTp073/8T/+R7SDQ9Ma6bg577zzvLPOOmvUn+H3TWcr53fNWWed5X3sYx8reIzfNZ1t32vtcq6bfv7zn3u2bXsDAwPhNt/97ne9rq4ub2hoqL4vIAJUQLaRVCql9evXq7+/P3zMtm319/dr7dq1DRwZmtXOnTslSRMmTCh4/N5779XEiRN17LHHavny5dqzZ08jhocm8vzzz6u3t1eHHXaYFi5cqM2bN0uS1q9fr3Q6XfB75+ijj9Z73vMefu8glEql9IMf/ED/7b/9N1mWFT7O7xqM5qWXXtLAwEDB75bu7m719fWFv1vWrl2r8ePH66STTgq36e/vl23bWrduXd3HjOa0c+dOWZal8ePHFzz+rW99SwcffLBOPPFE3XjjjW0zvQ3Ve/zxxzV58mQdddRRuuiii/TWW2+F3+P3DYrZunWrfvazn+mCCy7Y73v8rulc+15rl3PdtHbtWh133HGaMmVKuM28efM0ODio3/3ud3UcfTRijR4AzHnzzTeVzWYLDlZJmjJliv7whz80aFRoVq7r6pJLLtEHPvABHXvsseHjn/70pzV9+nT19vbq2Wef1RVXXKFNmzbpxz/+cQNHi0bq6+vTXXfdpaOOOkpbtmzR17/+dX3oQx/Sc889p4GBASUSif0u7KZMmaKBgYHGDBhN5+GHH9aOHTt0/vnnh4/xuwbFBL8/RjqnCb43MDCgyZMnF3w/FotpwoQJ/P6BpNxaW1dccYU+9alPqaurK3z8i1/8ot773vdqwoQJ+vWvf63ly5dry5Ytuvnmmxs4WjTS/Pnz9Rd/8Rc69NBD9eKLL+pv//Zvdfrpp2vt2rVyHIffNyjq7rvv1oEHHrjfEkT8rulcI11rl3PdNDAwMOK5T/C9VkcACXSoiy++WM8991zBWn6SCtayOe644zR16lTNnTtXL774og4//PB6DxNN4PTTTw//fvzxx6uvr0/Tp0/Xj370I40dO7aBI0OruP3223X66aert7c3fIzfNQCilE6n9Vd/9VfyPE/f/e53C763bNmy8O/HH3+8EomE/uZv/kYrVqxQMpms91DRBBYsWBD+/bjjjtPxxx+vww8/XI8//rjmzp3bwJGhFdxxxx1auHChxowZU/A4v2s612jX2p2OKdhtZOLEiXIcZ78uSlu3blVPT0+DRoVmtHTpUj3yyCN67LHHdMghhxTdtq+vT5L0wgsv1GNoaAHjx4/Xn/3Zn+mFF15QT0+PUqmUduzYUbANv3cQeOWVV/TLX/5Sn/3sZ4tux+8a5At+fxQ7p+np6dmvyV4mk9H27dv5/dPhgvDxlVde0erVqwuqH0fS19enTCajl19+uT4DRNM77LDDNHHixPAzid83GM2//Mu/aNOmTSXPcyR+13SK0a61y7lu6unpGfHcJ/heqyOAbCOJREKzZ8/WmjVrwsdc19WaNWs0Z86cBo4MzcLzPC1dulQPPfSQfvWrX+nQQw8t+TMbNmyQJE2dOjXi0aFV7Nq1Sy+++KKmTp2q2bNnKx6PF/ze2bRpkzZv3szvHUiS7rzzTk2ePFlnnnlm0e34XYN8hx56qHp6egp+twwODmrdunXh75Y5c+Zox44dWr9+fbjNr371K7muGwba6DxB+Pj888/rl7/8pQ4++OCSP7NhwwbZtr3fFFt0rj/+8Y966623ws8kft9gNLfffrtmz56tWbNmldyW3zXtrdS1djnXTXPmzNFvf/vbghsewY20mTNn1ueFRIgp2G1m2bJlOu+883TSSSfp5JNP1i233KLdu3dr8eLFjR4amsDFF1+s++67T//8z/+sAw88MFxHoru7W2PHjtWLL76o++67T2eccYYOPvhgPfvss7r00kv14Q9/WMcff3yDR49G+fKXv6yPf/zjmj59ul5//XVdc801chxHn/rUp9Td3a0LLrhAy5Yt04QJE9TV1aUvfOELmjNnjk455ZRGDx0N5rqu7rzzTp133nmKxYZPOfhdAyl3MyO/4vWll17Shg0bNGHCBL3nPe/RJZdcom984xs68sgjdeihh+qqq65Sb2+vzj77bEnSMccco/nz52vJkiVauXKl0um0li5dqgULFhRM90d7KXbcTJ06VX/5l3+pZ555Ro888oiy2Wx4rjNhwgQlEgmtXbtW69at00c/+lEdeOCBWrt2rS699FJ95jOf0UEHHdSol4WIFTtuJkyYoK9//es655xz1NPToxdffFGXX365jjjiCM2bN08Sv286UanPKCl3Y+zBBx/U3/3d3+338/yu6TylrrXLuW467bTTNHPmTP31X/+1brjhBg0MDOirX/2qLr744vaYtt/gLtyIwHe+8x3vPe95j5dIJLyTTz7Z+81vftPoIaFJSBrx68477/Q8z/M2b97sffjDH/YmTJjgJZNJ74gjjvC+8pWveDt37mzswNFQ5557rjd16lQvkUh47373u71zzz3Xe+GFF8Lvv/POO97nP/9576CDDvLGjRvn/fmf/7m3ZcuWBo4YzeIXv/iFJ8nbtGlTweP8roHned5jjz024mfSeeed53me57mu61111VXelClTvGQy6c2dO3e/Y+mtt97yPvWpT3nvete7vK6uLm/x4sXe22+/3YBXg3opdty89NJLo57rPPbYY57ned769eu9vr4+r7u72xszZox3zDHHeNdff723d+/exr4wRKrYcbNnzx7vtNNO8yZNmuTF43Fv+vTp3pIlS7yBgYGCffD7prOU+ozyPM/7h3/4B2/s2LHejh079vt5ftd0nlLX2p5X3nXTyy+/7J1++une2LFjvYkTJ3qXXXaZl06n6/xqomF5nudFmG8CAAAAAAAA6GCsAQkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMgSQAAAAAAAAACJDAAkAAAAAAAAgMv8fOFzfeQ+7X0AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(rewards[:200])\n",
    "plt.plot(agent._exp_buffer._total_discounted_rewards[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2bbd4fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 10, reward mean : -0.21699999067932368, total_step : 396, cur_epsilon : 0.89901, cur_beta : 0.40118800000000004\n",
      "episode : 20, reward mean : -0.15849999198690057, total_step : 706, cur_epsilon : 0.898235, cur_beta : 0.40211800000000003\n",
      "episode : 30, reward mean : -0.2139999929815531, total_step : 940, cur_epsilon : 0.8976500000000001, cur_beta : 0.40282\n",
      "episode : 40, reward mean : -0.2872499924618751, total_step : 1356, cur_epsilon : 0.89661, cur_beta : 0.40406800000000004\n",
      "episode : 50, reward mean : -0.10559999316930771, total_step : 1545, cur_epsilon : 0.8961375, cur_beta : 0.404635\n",
      "[EVAL] episode : 50, reward mean : -0.8004999821074307, step mean : 80.25, eval_episode_rewards : [-1.   -1.01 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    1.  ]\n",
      "episode : 60, reward mean : -0.14916665997977058, total_step : 1821, cur_epsilon : 0.8954475000000001, cur_beta : 0.405463\n",
      "episode : 70, reward mean : -0.18871427926101855, total_step : 2057, cur_epsilon : 0.8948575, cur_beta : 0.406171\n",
      "episode : 80, reward mean : -0.13462499335873873, total_step : 2423, cur_epsilon : 0.8939425, cur_beta : 0.40726900000000005\n",
      "episode : 90, reward mean : -0.11199999377131462, total_step : 2564, cur_epsilon : 0.89359, cur_beta : 0.407692\n",
      "episode : 100, reward mean : -0.0760999938286841, total_step : 2826, cur_epsilon : 0.892935, cur_beta : 0.408478\n",
      "[EVAL] episode : 100, reward mean : -0.6309999814257026, step mean : 83.3, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.68  0.98 -1.    0.72 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    1.   -1.   -1.   -1.  ]\n",
      "episode : 110, reward mean : -0.08109090280803767, total_step : 3167, cur_epsilon : 0.8920825, cur_beta : 0.409501\n",
      "episode : 120, reward mean : -0.06908332731885215, total_step : 3314, cur_epsilon : 0.891715, cur_beta : 0.40994200000000003\n",
      "episode : 130, reward mean : -0.06676922463453733, total_step : 3662, cur_epsilon : 0.890845, cur_beta : 0.410986\n",
      "episode : 140, reward mean : -0.10571427967931543, total_step : 3884, cur_epsilon : 0.89029, cur_beta : 0.411652\n",
      "episode : 150, reward mean : -0.13799999393522738, total_step : 4183, cur_epsilon : 0.8895425, cur_beta : 0.412549\n",
      "[EVAL] episode : 150, reward mean : -0.5039999831467867, step mean : 75.65, eval_episode_rewards : [ 1.    1.   -1.   -1.   -1.   -1.   -1.    0.99  0.93 -1.    1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 160, reward mean : -0.11581249405862763, total_step : 4376, cur_epsilon : 0.8890600000000001, cur_beta : 0.413128\n",
      "episode : 170, reward mean : -0.10770587639554459, total_step : 4663, cur_epsilon : 0.8883425, cur_beta : 0.413989\n",
      "episode : 180, reward mean : -0.09055554955783818, total_step : 4971, cur_epsilon : 0.8875725, cur_beta : 0.41491300000000003\n",
      "synced target net\n",
      "episode : 190, reward mean : -0.10247367815555711, total_step : 5298, cur_epsilon : 0.8867550000000001, cur_beta : 0.41589400000000004\n",
      "episode : 200, reward mean : -0.0880499937850982, total_step : 5720, cur_epsilon : 0.8857, cur_beta : 0.41716000000000003\n",
      "[EVAL] episode : 200, reward mean : -0.12349998829886318, step mean : 52.85, eval_episode_rewards : [ 0.96  0.83 -1.   -1.   -1.   -1.    0.98 -1.   -1.    0.99  0.92  0.99\n",
      "  0.99 -1.    1.   -1.04 -1.   -1.    0.91 -1.  ]\n",
      "episode : 210, reward mean : -0.0993809461150141, total_step : 6056, cur_epsilon : 0.88486, cur_beta : 0.41816800000000004\n",
      "episode : 220, reward mean : -0.07749999369579283, total_step : 6383, cur_epsilon : 0.8840425000000001, cur_beta : 0.419149\n",
      "episode : 230, reward mean : -0.06469564596595971, total_step : 6576, cur_epsilon : 0.88356, cur_beta : 0.41972800000000005\n",
      "episode : 240, reward mean : -0.07241666048454741, total_step : 6836, cur_epsilon : 0.88291, cur_beta : 0.420508\n",
      "episode : 250, reward mean : -0.0604799939095974, total_step : 7020, cur_epsilon : 0.8824500000000001, cur_beta : 0.42106000000000005\n",
      "[EVAL] episode : 250, reward mean : -0.3184999872930348, step mean : 57.3, eval_episode_rewards : [-1.   -1.    0.98  0.97 -1.   -1.   -1.   -1.    0.98 -1.    0.98 -1.01\n",
      " -1.   -1.   -1.17  0.98 -1.    0.94  0.98 -1.  ]\n",
      "episode : 260, reward mean : -0.03549999396245067, total_step : 7241, cur_epsilon : 0.8818975, cur_beta : 0.421723\n",
      "episode : 270, reward mean : -0.04037036441818431, total_step : 7418, cur_epsilon : 0.881455, cur_beta : 0.422254\n",
      "episode : 280, reward mean : -0.0325357083696872, total_step : 7649, cur_epsilon : 0.8808775, cur_beta : 0.422947\n",
      "episode : 290, reward mean : -0.04110344234725525, total_step : 7940, cur_epsilon : 0.88015, cur_beta : 0.42382000000000003\n",
      "episode : 300, reward mean : -0.02536666080976526, total_step : 8119, cur_epsilon : 0.8797025, cur_beta : 0.42435700000000004\n",
      "[EVAL] episode : 300, reward mean : 0.16250000977888704, step mean : 44.35, eval_episode_rewards : [ 0.66 -1.    0.97  0.96  0.93  0.96  0.99  0.99 -1.    0.99  0.98 -1.\n",
      "  0.95 -1.   -1.    0.97  0.9  -1.   -1.   -1.  ]\n",
      "episode : 310, reward mean : -0.028580639331090833, total_step : 8353, cur_epsilon : 0.8791175, cur_beta : 0.425059\n",
      "episode : 320, reward mean : -0.009031244210200384, total_step : 8566, cur_epsilon : 0.8785850000000001, cur_beta : 0.425698\n",
      "episode : 330, reward mean : -0.0047272669320756735, total_step : 8843, cur_epsilon : 0.8778925000000001, cur_beta : 0.42652900000000005\n",
      "episode : 340, reward mean : -0.00332352368380217, total_step : 9010, cur_epsilon : 0.877475, cur_beta : 0.42703\n",
      "episode : 350, reward mean : 0.0032285771039979798, total_step : 9194, cur_epsilon : 0.877015, cur_beta : 0.427582\n",
      "[EVAL] episode : 350, reward mean : 0.3170000085607171, step mean : 39.0, eval_episode_rewards : [ 0.18  0.99  0.98  0.58 -1.    0.99 -1.    0.99  0.95  0.98  0.94 -1.\n",
      "  0.93 -1.   -1.    0.96 -1.    0.95  0.99  0.93]\n",
      "episode : 360, reward mean : 0.011361116755546794, total_step : 9407, cur_epsilon : 0.8764825, cur_beta : 0.428221\n",
      "episode : 370, reward mean : -0.0048108051455504185, total_step : 9703, cur_epsilon : 0.8757425, cur_beta : 0.429109\n",
      "synced target net\n",
      "episode : 380, reward mean : 0.003394742530623549, total_step : 10006, cur_epsilon : 0.874985, cur_beta : 0.430018\n",
      "episode : 390, reward mean : 0.004000005641808877, total_step : 10189, cur_epsilon : 0.8745275, cur_beta : 0.43056700000000003\n",
      "episode : 400, reward mean : -0.002524994355626404, total_step : 10456, cur_epsilon : 0.87386, cur_beta : 0.43136800000000003\n",
      "[EVAL] episode : 400, reward mean : -0.0309999892488122, step mean : 48.65, eval_episode_rewards : [-1.   -1.   -1.    0.94 -1.   -1.   -1.    0.77  0.95  0.97  0.97  0.97\n",
      " -1.    0.94 -1.    0.98 -1.   -1.04  0.97  0.96]\n",
      "episode : 410, reward mean : 0.010000005555225582, total_step : 10555, cur_epsilon : 0.8736125, cur_beta : 0.431665\n",
      "episode : 420, reward mean : 0.0015238150776851745, total_step : 10810, cur_epsilon : 0.8729750000000001, cur_beta : 0.43243000000000004\n",
      "episode : 430, reward mean : 0.0065116334793179536, total_step : 11103, cur_epsilon : 0.8722425, cur_beta : 0.433309\n",
      "episode : 440, reward mean : 0.010727278329432011, total_step : 11420, cur_epsilon : 0.8714500000000001, cur_beta : 0.43426000000000003\n",
      "episode : 450, reward mean : 0.00735556125226948, total_step : 11870, cur_epsilon : 0.870325, cur_beta : 0.43561000000000005\n",
      "[EVAL] episode : 450, reward mean : -0.026499987114220858, step mean : 58.2, eval_episode_rewards : [-1.    0.99  0.97  0.55 -1.    0.72  0.99 -1.   -1.    0.77  0.94 -1.\n",
      " -1.    0.97 -1.   -1.    0.18 -1.    0.89  0.5 ]\n",
      "episode : 460, reward mean : 0.007695657881381719, total_step : 12156, cur_epsilon : 0.86961, cur_beta : 0.436468\n",
      "episode : 470, reward mean : 0.012872346177538659, total_step : 12515, cur_epsilon : 0.8687125, cur_beta : 0.437545\n",
      "episode : 480, reward mean : 0.020291672406407692, total_step : 12756, cur_epsilon : 0.86811, cur_beta : 0.438268\n",
      "episode : 490, reward mean : 0.024408169014721502, total_step : 13043, cur_epsilon : 0.8673925, cur_beta : 0.439129\n",
      "episode : 500, reward mean : 0.03224000571668148, total_step : 13237, cur_epsilon : 0.8669075, cur_beta : 0.439711\n",
      "[EVAL] episode : 500, reward mean : 0.4095000076107681, step mean : 34.8, eval_episode_rewards : [-1.    0.99  0.93 -1.    0.83  1.    0.85 -1.    0.12  0.98  1.    0.99\n",
      "  0.89 -1.    0.84  0.86 -1.    0.97  0.97  0.97]\n",
      "episode : 510, reward mean : 0.037686280241491746, total_step : 13537, cur_epsilon : 0.8661575, cur_beta : 0.44061100000000003\n",
      "episode : 520, reward mean : 0.043923082646842186, total_step : 13785, cur_epsilon : 0.8655375000000001, cur_beta : 0.44135500000000005\n",
      "episode : 530, reward mean : 0.044867930273121255, total_step : 14101, cur_epsilon : 0.8647475, cur_beta : 0.442303\n",
      "episode : 540, reward mean : 0.045314820590256544, total_step : 14442, cur_epsilon : 0.8638950000000001, cur_beta : 0.443326\n",
      "episode : 550, reward mean : 0.03665455121885647, total_step : 14683, cur_epsilon : 0.8632925, cur_beta : 0.444049\n",
      "[EVAL] episode : 550, reward mean : 0.26450000749900937, step mean : 34.25, eval_episode_rewards : [ 0.97  0.99  1.    0.98 -1.   -1.   -1.    0.99 -1.    0.9   0.92 -1.\n",
      "  0.97  0.97  0.96  0.64  1.   -1.   -1.    1.  ]\n",
      "episode : 560, reward mean : 0.03637500573282263, total_step : 14872, cur_epsilon : 0.86282, cur_beta : 0.444616\n",
      "synced target net\n",
      "episode : 570, reward mean : 0.03526316361599847, total_step : 15109, cur_epsilon : 0.8622275, cur_beta : 0.44532700000000003\n",
      "episode : 580, reward mean : 0.045413798793893435, total_step : 15295, cur_epsilon : 0.8617625, cur_beta : 0.44588500000000003\n",
      "episode : 590, reward mean : 0.04118644634924703, total_step : 15509, cur_epsilon : 0.8612275, cur_beta : 0.446527\n",
      "episode : 600, reward mean : 0.036066672342518966, total_step : 15785, cur_epsilon : 0.8605375000000001, cur_beta : 0.447355\n",
      "[EVAL] episode : 600, reward mean : -0.19149998566135765, step mean : 64.6, eval_episode_rewards : [ 0.68  0.97 -1.   -1.   -1.   -1.   -1.    0.17  0.97 -1.    1.   -1.\n",
      "  1.    0.99 -1.    0.4  -1.   -1.    0.99 -1.  ]\n",
      "episode : 610, reward mean : 0.03878689090125873, total_step : 15993, cur_epsilon : 0.8600175, cur_beta : 0.447979\n",
      "episode : 620, reward mean : 0.03816129603092709, total_step : 16403, cur_epsilon : 0.8589925, cur_beta : 0.449209\n",
      "episode : 630, reward mean : 0.02985714857599565, total_step : 16698, cur_epsilon : 0.858255, cur_beta : 0.450094\n",
      "episode : 640, reward mean : 0.025250005756970496, total_step : 17072, cur_epsilon : 0.85732, cur_beta : 0.451216\n",
      "episode : 650, reward mean : 0.025584621105629663, total_step : 17235, cur_epsilon : 0.8569125, cur_beta : 0.451705\n",
      "[EVAL] episode : 650, reward mean : 0.1480000089854002, step mean : 40.85, eval_episode_rewards : [-1.   -1.    0.81 -1.    0.98  0.95  0.95  0.8  -1.   -1.    0.98  0.97\n",
      "  0.99  0.95  0.8  -1.   -1.16 -1.    0.98  0.96]\n",
      "episode : 660, reward mean : 0.02119697542070891, total_step : 17509, cur_epsilon : 0.8562275, cur_beta : 0.452527\n",
      "episode : 670, reward mean : 0.023746274364417168, total_step : 17727, cur_epsilon : 0.8556825, cur_beta : 0.453181\n",
      "episode : 680, reward mean : 0.02133824106251054, total_step : 18176, cur_epsilon : 0.85456, cur_beta : 0.45452800000000004\n",
      "episode : 690, reward mean : 0.01705797678804484, total_step : 18460, cur_epsilon : 0.85385, cur_beta : 0.45538\n",
      "episode : 700, reward mean : 0.01191429148295096, total_step : 18712, cur_epsilon : 0.85322, cur_beta : 0.456136\n",
      "[EVAL] episode : 700, reward mean : -0.4439999844878912, step mean : 69.75, eval_episode_rewards : [-1.    0.68 -1.   -1.    0.82 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.95  0.98 -1.    0.76 -1.    0.93 -1.   -1.  ]\n",
      "episode : 710, reward mean : 0.0161971888124523, total_step : 18806, cur_epsilon : 0.852985, cur_beta : 0.45641800000000005\n",
      "episode : 720, reward mean : 0.01902778346815871, total_step : 18996, cur_epsilon : 0.85251, cur_beta : 0.456988\n",
      "episode : 730, reward mean : 0.030123293336021574, total_step : 19177, cur_epsilon : 0.8520575, cur_beta : 0.457531\n",
      "episode : 740, reward mean : 0.02920270839303329, total_step : 19524, cur_epsilon : 0.85119, cur_beta : 0.45857200000000004\n",
      "episode : 750, reward mean : 0.029200005695223807, total_step : 19804, cur_epsilon : 0.85049, cur_beta : 0.45941200000000004\n",
      "[EVAL] episode : 750, reward mean : -0.32649998599663377, step mean : 63.05, eval_episode_rewards : [-1.   -1.    0.94 -1.   -1.   -1.   -1.    0.99 -1.    0.77  0.94  0.99\n",
      " -1.06 -1.    0.94 -1.   -1.   -1.   -1.    0.96]\n",
      "episode : 760, reward mean : 0.037118426722621445, total_step : 19983, cur_epsilon : 0.8500425, cur_beta : 0.459949\n",
      "synced target net\n",
      "episode : 770, reward mean : 0.03900000568866343, total_step : 20310, cur_epsilon : 0.849225, cur_beta : 0.46093\n",
      "episode : 780, reward mean : 0.03975641595868346, total_step : 20622, cur_epsilon : 0.848445, cur_beta : 0.461866\n",
      "episode : 790, reward mean : 0.04191139808891318, total_step : 20822, cur_epsilon : 0.8479450000000001, cur_beta : 0.46246600000000004\n",
      "episode : 800, reward mean : 0.04708750570891425, total_step : 21176, cur_epsilon : 0.84706, cur_beta : 0.46352800000000005\n",
      "[EVAL] episode : 800, reward mean : -0.39699998330324887, step mean : 75.05, eval_episode_rewards : [-1.   -1.    0.01 -1.   -1.   -1.   -1.   -1.    0.96 -1.    0.35  0.95\n",
      " -1.    1.    0.89 -1.   -1.    0.9  -1.   -1.  ]\n",
      "episode : 810, reward mean : 0.051728400749004914, total_step : 21363, cur_epsilon : 0.8465925000000001, cur_beta : 0.46408900000000003\n",
      "episode : 820, reward mean : 0.05314634717183142, total_step : 21704, cur_epsilon : 0.84574, cur_beta : 0.465112\n",
      "episode : 830, reward mean : 0.056493981615696326, total_step : 21983, cur_epsilon : 0.8450425, cur_beta : 0.465949\n",
      "episode : 840, reward mean : 0.05710714863214109, total_step : 22483, cur_epsilon : 0.8437925000000001, cur_beta : 0.467449\n",
      "episode : 850, reward mean : 0.05438824104693006, total_step : 22667, cur_epsilon : 0.8433325, cur_beta : 0.468001\n",
      "[EVAL] episode : 850, reward mean : -0.28399998471140864, step mean : 68.8, eval_episode_rewards : [ 0.53 -1.   -1.    0.92  0.95 -1.   -1.    0.3  -1.   -1.    0.98  0.97\n",
      "  0.71 -1.   -1.   -1.   -1.   -1.    0.96 -1.  ]\n",
      "episode : 860, reward mean : 0.052941866247199994, total_step : 23046, cur_epsilon : 0.842385, cur_beta : 0.46913800000000005\n",
      "episode : 870, reward mean : 0.05649425865033235, total_step : 23294, cur_epsilon : 0.841765, cur_beta : 0.469882\n",
      "episode : 880, reward mean : 0.061102278498848056, total_step : 23541, cur_epsilon : 0.8411475, cur_beta : 0.470623\n",
      "episode : 890, reward mean : 0.05822472485682268, total_step : 23746, cur_epsilon : 0.840635, cur_beta : 0.47123800000000005\n",
      "episode : 900, reward mean : 0.0536888946675592, total_step : 24106, cur_epsilon : 0.839735, cur_beta : 0.472318\n",
      "[EVAL] episode : 900, reward mean : -0.4159999839961529, step mean : 71.9, eval_episode_rewards : [ 0.88 -1.   -1.   -1.   -1.   -1.    0.98  0.98 -1.   -1.    0.96 -1.\n",
      " -1.   -1.   -1.    0.99 -1.    0.89 -1.   -1.  ]\n",
      "episode : 910, reward mean : 0.050461544260218906, total_step : 24455, cur_epsilon : 0.8388625000000001, cur_beta : 0.47336500000000004\n",
      "episode : 920, reward mean : 0.04969565798730954, total_step : 24784, cur_epsilon : 0.83804, cur_beta : 0.474352\n",
      "synced target net\n",
      "episode : 930, reward mean : 0.053483876790250504, total_step : 25091, cur_epsilon : 0.8372725, cur_beta : 0.475273\n",
      "episode : 940, reward mean : 0.047127665393054484, total_step : 25344, cur_epsilon : 0.83664, cur_beta : 0.476032\n",
      "episode : 950, reward mean : 0.046831584747292494, total_step : 25535, cur_epsilon : 0.8361625, cur_beta : 0.47660500000000006\n",
      "[EVAL] episode : 950, reward mean : -0.2459999855607748, step mean : 65.0, eval_episode_rewards : [-1.    0.92 -1.    0.98  0.96  0.52 -1.   -1.    0.92 -1.   -1.   -1.\n",
      "  0.8  -1.    0.98 -1.    1.   -1.   -1.   -1.  ]\n",
      "episode : 960, reward mean : 0.048989589106834805, total_step : 25691, cur_epsilon : 0.8357725, cur_beta : 0.477073\n",
      "episode : 970, reward mean : 0.04202062432298955, total_step : 25928, cur_epsilon : 0.83518, cur_beta : 0.477784\n",
      "episode : 980, reward mean : 0.03729592414800914, total_step : 26258, cur_epsilon : 0.8343550000000001, cur_beta : 0.47877400000000003\n",
      "episode : 990, reward mean : 0.04034344012463334, total_step : 26529, cur_epsilon : 0.8336775000000001, cur_beta : 0.47958700000000004\n",
      "episode : 1000, reward mean : 0.04372000577300787, total_step : 26761, cur_epsilon : 0.8330975, cur_beta : 0.480283\n",
      "[EVAL] episode : 1000, reward mean : -0.4714999827556312, step mean : 77.45, eval_episode_rewards : [ 0.05 -1.    0.62 -1.   -1.    0.99  0.98  1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.93 -1.   -1.   -1.   -1.  ]\n",
      "episode : 1010, reward mean : 0.05112000574171543, total_step : 27048, cur_epsilon : 0.83238, cur_beta : 0.481144\n",
      "episode : 1020, reward mean : 0.04684000572562218, total_step : 27285, cur_epsilon : 0.8317875, cur_beta : 0.48185500000000003\n",
      "episode : 1030, reward mean : 0.05806000572070479, total_step : 27498, cur_epsilon : 0.8312550000000001, cur_beta : 0.48249400000000003\n",
      "episode : 1040, reward mean : 0.06548000571131707, total_step : 27873, cur_epsilon : 0.8303175, cur_beta : 0.483619\n",
      "episode : 1050, reward mean : 0.061440005734562875, total_step : 28165, cur_epsilon : 0.8295875, cur_beta : 0.484495\n",
      "[EVAL] episode : 1050, reward mean : -0.5229999827221036, step mean : 77.55, eval_episode_rewards : [ 0.92 -1.   -1.   -1.   -1.   -1.   -1.    0.69  0.99 -1.   -1.   -1.\n",
      "  0.96 -1.   -1.   -1.    0.98 -1.   -1.   -1.  ]\n",
      "episode : 1060, reward mean : 0.06617000574059785, total_step : 28469, cur_epsilon : 0.8288275, cur_beta : 0.48540700000000003\n",
      "episode : 1070, reward mean : 0.06833000575937331, total_step : 28788, cur_epsilon : 0.82803, cur_beta : 0.486364\n",
      "episode : 1080, reward mean : 0.06821000571735203, total_step : 28966, cur_epsilon : 0.827585, cur_beta : 0.486898\n",
      "episode : 1090, reward mean : 0.06727000576071442, total_step : 29300, cur_epsilon : 0.82675, cur_beta : 0.4879\n",
      "episode : 1100, reward mean : 0.0641000057645142, total_step : 29580, cur_epsilon : 0.8260500000000001, cur_beta : 0.48874\n",
      "[EVAL] episode : 1100, reward mean : -0.8184999794699251, step mean : 91.95, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.9  -1.   -1.\n",
      "  0.73 -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1110, reward mean : 0.06581000572629273, total_step : 29750, cur_epsilon : 0.825625, cur_beta : 0.48925\n",
      "synced target net\n",
      "episode : 1120, reward mean : 0.059230005783960225, total_step : 30153, cur_epsilon : 0.8246175, cur_beta : 0.49045900000000003\n",
      "episode : 1130, reward mean : 0.061270005783066156, total_step : 30497, cur_epsilon : 0.8237575, cur_beta : 0.491491\n",
      "episode : 1140, reward mean : 0.06996000583469868, total_step : 30949, cur_epsilon : 0.8226275000000001, cur_beta : 0.49284700000000004\n",
      "episode : 1150, reward mean : 0.07076000581681728, total_step : 31168, cur_epsilon : 0.82208, cur_beta : 0.493504\n",
      "[EVAL] episode : 1150, reward mean : -0.4709999827668071, step mean : 77.4, eval_episode_rewards : [-1.    0.98 -1.   -1.    0.87  0.43 -1.   -1.    0.98 -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.57 -1.    0.75 -1.   -1.  ]\n",
      "episode : 1160, reward mean : 0.06617000585235655, total_step : 31519, cur_epsilon : 0.8212025000000001, cur_beta : 0.494557\n",
      "episode : 1170, reward mean : 0.06852000584453344, total_step : 31771, cur_epsilon : 0.8205725, cur_beta : 0.495313\n",
      "episode : 1180, reward mean : 0.06647000586800277, total_step : 32185, cur_epsilon : 0.8195375, cur_beta : 0.496555\n",
      "episode : 1190, reward mean : 0.07156000586599112, total_step : 32502, cur_epsilon : 0.8187450000000001, cur_beta : 0.497506\n",
      "episode : 1200, reward mean : 0.07177000588364899, total_step : 33004, cur_epsilon : 0.81749, cur_beta : 0.499012\n",
      "[EVAL] episode : 1200, reward mean : -0.7219999805092812, step mean : 87.35, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.88 -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.75  0.93 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1210, reward mean : 0.07639000586979092, total_step : 33278, cur_epsilon : 0.816805, cur_beta : 0.499834\n",
      "episode : 1220, reward mean : 0.0720300058554858, total_step : 33542, cur_epsilon : 0.816145, cur_beta : 0.500626\n",
      "episode : 1230, reward mean : 0.07035000589303672, total_step : 33903, cur_epsilon : 0.8152425, cur_beta : 0.501709\n",
      "episode : 1240, reward mean : 0.07388000590354205, total_step : 34210, cur_epsilon : 0.8144750000000001, cur_beta : 0.50263\n",
      "episode : 1250, reward mean : 0.07555000591091812, total_step : 34427, cur_epsilon : 0.8139325000000001, cur_beta : 0.503281\n",
      "[EVAL] episode : 1250, reward mean : -0.7364999790675938, step mean : 93.85, eval_episode_rewards : [-1.   -1.   -1.    0.11 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.83\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.25  0.08]\n",
      "episode : 1260, reward mean : 0.06633000593818725, total_step : 34770, cur_epsilon : 0.813075, cur_beta : 0.50431\n",
      "synced target net\n",
      "episode : 1270, reward mean : 0.06941000595875084, total_step : 35039, cur_epsilon : 0.8124025, cur_beta : 0.505117\n",
      "episode : 1280, reward mean : 0.06549000595696271, total_step : 35262, cur_epsilon : 0.811845, cur_beta : 0.5057860000000001\n",
      "episode : 1290, reward mean : 0.07277000595070422, total_step : 35524, cur_epsilon : 0.8111900000000001, cur_beta : 0.506572\n",
      "episode : 1300, reward mean : 0.06686000601574779, total_step : 35993, cur_epsilon : 0.8100175000000001, cur_beta : 0.507979\n",
      "[EVAL] episode : 1300, reward mean : -0.7184999805875123, step mean : 87.0, eval_episode_rewards : [-1.   -1.   -1.    0.98 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.82\n",
      " -1.    0.83 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1310, reward mean : 0.06923000602982939, total_step : 36291, cur_epsilon : 0.8092725000000001, cur_beta : 0.508873\n",
      "episode : 1320, reward mean : 0.06170000606402755, total_step : 36655, cur_epsilon : 0.8083625, cur_beta : 0.509965\n",
      "episode : 1330, reward mean : 0.0632100060749799, total_step : 36981, cur_epsilon : 0.8075475, cur_beta : 0.510943\n",
      "episode : 1340, reward mean : 0.05827000609599054, total_step : 37242, cur_epsilon : 0.806895, cur_beta : 0.511726\n",
      "episode : 1350, reward mean : 0.05350000611320138, total_step : 37503, cur_epsilon : 0.8062425, cur_beta : 0.512509\n",
      "[EVAL] episode : 1350, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 1360, reward mean : 0.05243000615946949, total_step : 37922, cur_epsilon : 0.805195, cur_beta : 0.5137660000000001\n",
      "episode : 1370, reward mean : 0.05519000618718564, total_step : 38342, cur_epsilon : 0.804145, cur_beta : 0.515026\n",
      "episode : 1380, reward mean : 0.049760006174445155, total_step : 38588, cur_epsilon : 0.8035300000000001, cur_beta : 0.515764\n",
      "episode : 1390, reward mean : 0.04893000623770058, total_step : 39054, cur_epsilon : 0.802365, cur_beta : 0.517162\n",
      "episode : 1400, reward mean : 0.047900006260722876, total_step : 39422, cur_epsilon : 0.801445, cur_beta : 0.518266\n",
      "[EVAL] episode : 1400, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 1410, reward mean : 0.042720006287097934, total_step : 39639, cur_epsilon : 0.8009025000000001, cur_beta : 0.5189170000000001\n",
      "episode : 1420, reward mean : 0.05352000629156828, total_step : 39915, cur_epsilon : 0.8002125, cur_beta : 0.519745\n",
      "synced target net\n",
      "episode : 1430, reward mean : 0.04881000628508628, total_step : 40180, cur_epsilon : 0.79955, cur_beta : 0.52054\n",
      "episode : 1440, reward mean : 0.04700000632554293, total_step : 40678, cur_epsilon : 0.798305, cur_beta : 0.522034\n",
      "episode : 1450, reward mean : 0.050320006273686886, total_step : 40897, cur_epsilon : 0.7977575, cur_beta : 0.522691\n",
      "[EVAL] episode : 1450, reward mean : -1.005999979749322, step mean : 90.7, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.12 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1460, reward mean : 0.053160006299614905, total_step : 41299, cur_epsilon : 0.7967525, cur_beta : 0.5238970000000001\n",
      "episode : 1470, reward mean : 0.05056000629067421, total_step : 41617, cur_epsilon : 0.7959575, cur_beta : 0.5248510000000001\n",
      "episode : 1480, reward mean : 0.03957000629045069, total_step : 41856, cur_epsilon : 0.7953600000000001, cur_beta : 0.525568\n",
      "episode : 1490, reward mean : 0.041790006263181564, total_step : 42022, cur_epsilon : 0.794945, cur_beta : 0.526066\n",
      "episode : 1500, reward mean : 0.03683000630699098, total_step : 42411, cur_epsilon : 0.7939725, cur_beta : 0.5272330000000001\n",
      "[EVAL] episode : 1500, reward mean : -0.8864999779500067, step mean : 98.75, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.05 -1.   -1.    0.22 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1510, reward mean : 0.03555000633560121, total_step : 42837, cur_epsilon : 0.7929075, cur_beta : 0.5285110000000001\n",
      "episode : 1520, reward mean : 0.03188000632822514, total_step : 43052, cur_epsilon : 0.79237, cur_beta : 0.529156\n",
      "episode : 1530, reward mean : 0.026760006308555604, total_step : 43280, cur_epsilon : 0.7918000000000001, cur_beta : 0.5298400000000001\n",
      "episode : 1540, reward mean : 0.026920006304979323, total_step : 43605, cur_epsilon : 0.7909875000000001, cur_beta : 0.530815\n",
      "episode : 1550, reward mean : 0.03358000629022718, total_step : 43780, cur_epsilon : 0.79055, cur_beta : 0.53134\n",
      "[EVAL] episode : 1550, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 1560, reward mean : 0.03657000631280243, total_step : 44070, cur_epsilon : 0.789825, cur_beta : 0.5322100000000001\n",
      "episode : 1570, reward mean : 0.0324300063829869, total_step : 44618, cur_epsilon : 0.788455, cur_beta : 0.533854\n",
      "episode : 1580, reward mean : 0.02202000641450286, total_step : 44944, cur_epsilon : 0.78764, cur_beta : 0.534832\n",
      "synced target net\n",
      "episode : 1590, reward mean : 0.025730006443336607, total_step : 45286, cur_epsilon : 0.7867850000000001, cur_beta : 0.5358580000000001\n",
      "episode : 1600, reward mean : 0.02828000643104315, total_step : 45507, cur_epsilon : 0.7862325, cur_beta : 0.536521\n",
      "[EVAL] episode : 1600, reward mean : -1.006499979738146, step mean : 90.75, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.12 -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.01]\n",
      "episode : 1610, reward mean : 0.029770006442442536, total_step : 45766, cur_epsilon : 0.785585, cur_beta : 0.537298\n",
      "episode : 1620, reward mean : 0.028990006437525154, total_step : 46153, cur_epsilon : 0.7846175, cur_beta : 0.538459\n",
      "episode : 1630, reward mean : 0.033270006431266666, total_step : 46420, cur_epsilon : 0.78395, cur_beta : 0.5392600000000001\n",
      "episode : 1640, reward mean : 0.03009000643528998, total_step : 46813, cur_epsilon : 0.7829675, cur_beta : 0.540439\n",
      "episode : 1650, reward mean : 0.033640006445348265, total_step : 47021, cur_epsilon : 0.7824475, cur_beta : 0.5410630000000001\n",
      "[EVAL] episode : 1650, reward mean : -0.8124999796040356, step mean : 91.35, eval_episode_rewards : [ 0.75 -1.   -1.   -1.    1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1660, reward mean : 0.036580006424337626, total_step : 47201, cur_epsilon : 0.7819975, cur_beta : 0.5416030000000001\n",
      "episode : 1670, reward mean : 0.040960006415843964, total_step : 47381, cur_epsilon : 0.7815475000000001, cur_beta : 0.542143\n",
      "episode : 1680, reward mean : 0.045740006376057865, total_step : 47653, cur_epsilon : 0.7808675, cur_beta : 0.542959\n",
      "episode : 1690, reward mean : 0.044940006393939254, total_step : 48015, cur_epsilon : 0.7799625, cur_beta : 0.544045\n",
      "episode : 1700, reward mean : 0.04692000641673803, total_step : 48368, cur_epsilon : 0.77908, cur_beta : 0.545104\n",
      "[EVAL] episode : 1700, reward mean : -0.8274999792687595, step mean : 92.85, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.93  0.52 -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1710, reward mean : 0.042830006485804914, total_step : 48770, cur_epsilon : 0.7780750000000001, cur_beta : 0.5463100000000001\n",
      "episode : 1720, reward mean : 0.040620006512850526, total_step : 49080, cur_epsilon : 0.7773, cur_beta : 0.5472400000000001\n",
      "episode : 1730, reward mean : 0.03767000653408468, total_step : 49356, cur_epsilon : 0.77661, cur_beta : 0.548068\n",
      "episode : 1740, reward mean : 0.03795000650547445, total_step : 49576, cur_epsilon : 0.77606, cur_beta : 0.548728\n",
      "episode : 1750, reward mean : 0.03940000649541617, total_step : 49812, cur_epsilon : 0.77547, cur_beta : 0.549436\n",
      "[EVAL] episode : 1750, reward mean : -0.9019999787211418, step mean : 95.25, eval_episode_rewards : [-1.   -1.   -1.    0.96 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 1760, reward mean : 0.030170006522908806, total_step : 50114, cur_epsilon : 0.774715, cur_beta : 0.550342\n",
      "episode : 1770, reward mean : 0.032470006516203284, total_step : 50411, cur_epsilon : 0.7739725000000001, cur_beta : 0.5512330000000001\n",
      "episode : 1780, reward mean : 0.037310006519779566, total_step : 50738, cur_epsilon : 0.773155, cur_beta : 0.552214\n",
      "episode : 1790, reward mean : 0.041660006534308194, total_step : 51002, cur_epsilon : 0.772495, cur_beta : 0.553006\n",
      "episode : 1800, reward mean : 0.03293000650592148, total_step : 51229, cur_epsilon : 0.7719275, cur_beta : 0.553687\n",
      "[EVAL] episode : 1800, reward mean : -0.7279999803751707, step mean : 87.95, eval_episode_rewards : [-1.    1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.99 -1.   -1.   -1.    0.45 -1.   -1.   -1.  ]\n",
      "episode : 1810, reward mean : 0.02988000659644604, total_step : 51818, cur_epsilon : 0.770455, cur_beta : 0.555454\n",
      "episode : 1820, reward mean : 0.028970006572082638, total_step : 52050, cur_epsilon : 0.7698750000000001, cur_beta : 0.55615\n",
      "episode : 1830, reward mean : 0.02293000659532845, total_step : 52432, cur_epsilon : 0.76892, cur_beta : 0.557296\n",
      "episode : 1840, reward mean : 0.025140006568282844, total_step : 52812, cur_epsilon : 0.76797, cur_beta : 0.558436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# done_reward = agent.play_step(epsilon=epsilon)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m         done_reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done_reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m             episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 60\u001b[0m, in \u001b[0;36mAgent.play_step\u001b[0;34m(self, epsilon, sync_target, probabilistic_decision)\u001b[0m\n\u001b[1;32m     57\u001b[0m         _, act_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(q_vals_v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_v\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 60\u001b[0m next_state, reward, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_buffer\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state, action, reward, is_done, next_state\n\u001b[1;32m     65\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgResizeGrayWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 11\u001b[0m         next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         next_state \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(next_state[:, :, \u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         return next_state, reward, is_done, info\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(next_state[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), reward, is_done, info\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/envs/unity_gym_env.py:200\u001b[0m, in \u001b[0;36mUnityToGymWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    197\u001b[0m     action_tuple\u001b[38;5;241m.\u001b[39madd_discrete(action)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mset_actions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, action_tuple)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m decision_step, terminal_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_steps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_agents(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(decision_step), \u001b[38;5;28mlen\u001b[39m(terminal_step)))\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/environment.py:348\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m step_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_step_input(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicator.exchange\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_communicator.py:142\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    140\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll_for_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_communicator.py:106\u001b[0m, in \u001b[0;36mRpcCommunicator.poll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    104\u001b[0m callback_timeout_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout_wait \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m<\u001b[39m deadline:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_timeout_wait\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# Got an acknowledgment from the connection\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m poll_callback:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# Fire the callback - if it detects something wrong, it should raise an exception.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# random episode epsilon\n",
    "\n",
    "episode = 0\n",
    "reward_history = []\n",
    "spisode_steps_history = []\n",
    "spisode_start_steps = 0\n",
    "PRINT_EPISODE_INTERVAL = 10\n",
    "print_episode_rewards = []\n",
    "EVAL_EPISODE_INTERVAL = 50\n",
    "N_EVAL_EPISODES = 20\n",
    "epsilon = np.random.rand()\n",
    "\n",
    "while True:\n",
    "\n",
    "    for stp in range(20):\n",
    "        # done_reward = agent.play_step(epsilon=epsilon)\n",
    "        done_reward = agent.play_step()\n",
    "        if done_reward is not None:\n",
    "            episode += 1\n",
    "            reward_history.append(done_reward)\n",
    "            # print_episode_rewards.append(done_reward)\n",
    "            spisode_steps_history.append(agent._total_step-spisode_start_steps)\n",
    "            spisode_start_steps = agent._total_step\n",
    "            if episode % PRINT_EPISODE_INTERVAL == 0:\n",
    "                # rm = sum(print_episode_rewards) / len(print_episode_rewards)\n",
    "                rm = sum(reward_history[-1000:]) / len(reward_history[-1000:])\n",
    "                print(f'episode : {episode}, reward mean : {rm}, total_step : {agent._total_step}, cur_epsilon : {agent._epsilon}, cur_beta : {agent._exp_buffer._beta}')\n",
    "            if episode % EVAL_EPISODE_INTERVAL == 0:\n",
    "                eval_episode_rewards = []\n",
    "                eval_episode_steps = []\n",
    "                for i in range(N_EVAL_EPISODES):\n",
    "                    reward, steps = agent.simulate_episode()\n",
    "                    eval_episode_rewards.append(reward)\n",
    "                    eval_episode_steps.append(steps)\n",
    "                mr = sum(eval_episode_rewards) / len(eval_episode_rewards)\n",
    "                ms = sum(eval_episode_steps) / len(eval_episode_steps)\n",
    "                print(f'[EVAL] episode : {episode}, reward mean : {mr}, step mean : {ms}, eval_episode_rewards : {np.round(eval_episode_rewards, 3)}')\n",
    "            # epsilon = np.random.rand()\n",
    "\n",
    "    agent.train(n_iter=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42c8619e-9259-44e2-9ffe-66742d13b75d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 10, reward mean : -0.5319999925792217, total_step : 306, cur_epsilon : 0.8973072\n",
      "episode : 20, reward mean : -0.25649999314919114, total_step : 596, cur_epsilon : 0.8947552\n",
      "episode : 30, reward mean : -0.41933332694073516, total_step : 850, cur_epsilon : 0.89252\n",
      "episode : 40, reward mean : -0.4819999925792217, total_step : 1330, cur_epsilon : 0.888296\n",
      "episode : 50, reward mean : -0.39499999295920135, total_step : 1587, cur_epsilon : 0.8860344\n",
      "[EVAL] episode : 50, reward mean : -0.5494999832473695, step mean : 75.25, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.98 -1.   -1.    0.99\n",
      "  0.99 -1.   -1.   -1.03  1.    0.08 -1.   -1.  ]\n",
      "episode : 60, reward mean : -0.38599999360740184, total_step : 1738, cur_epsilon : 0.8847056\n",
      "episode : 70, reward mean : -0.39485713658588273, total_step : 1996, cur_epsilon : 0.8824352\n",
      "episode : 80, reward mean : -0.4401249937945977, total_step : 2262, cur_epsilon : 0.8800944\n",
      "episode : 90, reward mean : -0.4211111049271292, total_step : 2541, cur_epsilon : 0.8776392000000001\n",
      "episode : 100, reward mean : -0.42949999330565336, total_step : 3054, cur_epsilon : 0.8731248\n",
      "[EVAL] episode : 100, reward mean : -0.6494999843649566, step mean : 70.3, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.91 -1.57 -1.   -1.   -1.\n",
      " -1.01 -1.    1.    0.69 -1.   -1.   -1.    0.99]\n",
      "episode : 110, reward mean : -0.43081817523660987, total_step : 3308, cur_epsilon : 0.8708896\n",
      "episode : 120, reward mean : -0.4223333265011509, total_step : 3746, cur_epsilon : 0.8670352\n",
      "episode : 130, reward mean : -0.4070769163851555, total_step : 3980, cur_epsilon : 0.864976\n",
      "episode : 140, reward mean : -0.3882142789528838, total_step : 4331, cur_epsilon : 0.8618872\n",
      "episode : 150, reward mean : -0.3605999929830432, total_step : 4814, cur_epsilon : 0.8576368\n",
      "[EVAL] episode : 150, reward mean : -0.5054999842308462, step mean : 70.85, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.97 -1.   -1.   -1.   -1.02 -1.    0.98 -1.\n",
      " -1.   -1.   -1.    1.    1.   -1.    0.96 -1.  ]\n",
      "synced target net\n",
      "episode : 160, reward mean : -0.3548124930472113, total_step : 5092, cur_epsilon : 0.8551904\n",
      "episode : 170, reward mean : -0.36311764012583914, total_step : 5398, cur_epsilon : 0.8524976\n",
      "episode : 180, reward mean : -0.3462777709071007, total_step : 5668, cur_epsilon : 0.8501216\n",
      "episode : 190, reward mean : -0.3334736773450124, total_step : 5981, cur_epsilon : 0.8473672\n",
      "episode : 200, reward mean : -0.3503499931748956, total_step : 6262, cur_epsilon : 0.8448944\n",
      "[EVAL] episode : 200, reward mean : 0.11500001084059477, step mean : 49.1, eval_episode_rewards : [ 0.96 -1.   -1.   -1.    0.97  0.95  0.94 -1.   -1.    0.9   0.98  0.68\n",
      " -1.    0.58  0.98  0.95  0.97  0.44 -1.   -1.  ]\n",
      "episode : 210, reward mean : -0.3613333264277095, total_step : 6651, cur_epsilon : 0.8414712\n",
      "episode : 220, reward mean : -0.3805454475635832, total_step : 7045, cur_epsilon : 0.838004\n",
      "episode : 230, reward mean : -0.3731739061601136, total_step : 7266, cur_epsilon : 0.8360592\n",
      "episode : 240, reward mean : -0.3612083264704173, total_step : 7562, cur_epsilon : 0.8334544\n",
      "episode : 250, reward mean : -0.35599999311566355, total_step : 7902, cur_epsilon : 0.8304624\n",
      "[EVAL] episode : 250, reward mean : 0.26400000862777234, step mean : 39.25, eval_episode_rewards : [ 0.99  0.95  0.99  0.98  0.99 -1.   -1.   -1.    0.99  0.98 -1.    0.96\n",
      " -1.    0.98 -1.    0.63 -1.    0.98  0.88  0.98]\n",
      "episode : 260, reward mean : -0.36488460834639574, total_step : 8398, cur_epsilon : 0.8260976\n",
      "episode : 270, reward mean : -0.35837036343636336, total_step : 8597, cur_epsilon : 0.8243464\n",
      "episode : 280, reward mean : -0.3477499931052859, total_step : 8868, cur_epsilon : 0.8219616000000001\n",
      "episode : 290, reward mean : -0.35948275174174843, total_step : 9166, cur_epsilon : 0.8193392\n",
      "episode : 300, reward mean : -0.34626665989557903, total_step : 9339, cur_epsilon : 0.8178168\n",
      "[EVAL] episode : 300, reward mean : 0.4580000054091215, step mean : 25.0, eval_episode_rewards : [ 0.92 -1.    0.94  0.98  1.    1.    1.    0.98  0.98  0.8   0.98  0.66\n",
      " -1.    0.97  1.    0.99 -1.   -1.    0.96 -1.  ]\n",
      "episode : 310, reward mean : -0.33599999328294106, total_step : 9577, cur_epsilon : 0.8157224000000001\n",
      "episode : 320, reward mean : -0.3321874932735227, total_step : 9900, cur_epsilon : 0.81288\n",
      "synced target net\n",
      "episode : 330, reward mean : -0.3214545387436043, total_step : 10187, cur_epsilon : 0.8103544\n",
      "episode : 340, reward mean : -0.30367646386627767, total_step : 10514, cur_epsilon : 0.8074768\n",
      "episode : 350, reward mean : -0.30074285042073046, total_step : 10824, cur_epsilon : 0.8047488\n",
      "[EVAL] episode : 350, reward mean : -0.18499998692423106, step mean : 59.0, eval_episode_rewards : [ 1.    0.91 -1.01 -1.   -1.    1.    0.44 -1.   -1.   -1.   -1.    0.14\n",
      "  1.   -1.    1.   -1.   -1.    0.98 -1.    0.84]\n",
      "episode : 360, reward mean : -0.2899722154924853, total_step : 11147, cur_epsilon : 0.8019064\n",
      "episode : 370, reward mean : -0.2868108041584492, total_step : 11330, cur_epsilon : 0.800296\n",
      "episode : 380, reward mean : -0.2756842038937305, total_step : 11604, cur_epsilon : 0.7978848000000001\n",
      "episode : 390, reward mean : -0.26271794215799904, total_step : 11784, cur_epsilon : 0.7963008\n",
      "episode : 400, reward mean : -0.25559999339282513, total_step : 12172, cur_epsilon : 0.7928864\n",
      "[EVAL] episode : 400, reward mean : 0.004500011075288057, step mean : 50.15, eval_episode_rewards : [ 0.99  0.24  0.95 -1.    0.99 -1.    0.97 -1.   -1.    0.84  0.93  0.96\n",
      " -1.    0.77 -1.   -1.   -1.03  0.98 -1.    0.5 ]\n",
      "episode : 410, reward mean : -0.24753657874265095, total_step : 12507, cur_epsilon : 0.7899384\n",
      "episode : 420, reward mean : -0.25023808847403245, total_step : 13076, cur_epsilon : 0.7849312\n",
      "episode : 430, reward mean : -0.25444185368630085, total_step : 13416, cur_epsilon : 0.7819392000000001\n",
      "episode : 440, reward mean : -0.26118181148035957, total_step : 13577, cur_epsilon : 0.7805224000000001\n",
      "episode : 450, reward mean : -0.25526665986826025, total_step : 14081, cur_epsilon : 0.7760872000000001\n",
      "[EVAL] episode : 450, reward mean : 0.2775000072084367, step mean : 32.95, eval_episode_rewards : [ 0.99  0.99  0.95  0.99  0.95  0.96  0.85 -1.    0.97 -1.   -1.    0.97\n",
      "  0.97  0.97 -1.    1.    0.99 -1.   -1.   -1.  ]\n",
      "episode : 460, reward mean : -0.23469564537315266, total_step : 14400, cur_epsilon : 0.77328\n",
      "episode : 470, reward mean : -0.2267872272138583, total_step : 14772, cur_epsilon : 0.7700064\n",
      "synced target net\n",
      "episode : 480, reward mean : -0.21812499316874892, total_step : 15092, cur_epsilon : 0.7671904\n",
      "episode : 490, reward mean : -0.21346938088536263, total_step : 15492, cur_epsilon : 0.7636704000000001\n",
      "episode : 500, reward mean : -0.19677999318763614, total_step : 15681, cur_epsilon : 0.7620072\n",
      "[EVAL] episode : 500, reward mean : -0.32599998377263545, step mean : 73.0, eval_episode_rewards : [ 0.96 -1.   -1.   -1.   -1.    0.87 -1.    0.32 -1.    0.98 -1.   -1.\n",
      "  0.48 -1.   -1.    0.86  0.99 -1.   -1.    0.02]\n",
      "episode : 510, reward mean : -0.17615685600276088, total_step : 15836, cur_epsilon : 0.7606432000000001\n",
      "episode : 520, reward mean : -0.16428845485743995, total_step : 16005, cur_epsilon : 0.759156\n",
      "episode : 530, reward mean : -0.16449055932462214, total_step : 16390, cur_epsilon : 0.755768\n",
      "episode : 540, reward mean : -0.16862962296301567, total_step : 16588, cur_epsilon : 0.7540256000000001\n",
      "episode : 550, reward mean : -0.16490908425639977, total_step : 16861, cur_epsilon : 0.7516232\n",
      "[EVAL] episode : 550, reward mean : 0.21250000866129995, step mean : 39.45, eval_episode_rewards : [-1.    0.86  0.64  0.9   0.88  0.91  0.71 -1.    0.93 -1.   -1.    0.71\n",
      "  0.97  0.91 -1.   -1.    0.85  0.99  0.99 -1.  ]\n",
      "episode : 560, reward mean : -0.16526785045529582, total_step : 17256, cur_epsilon : 0.7481472\n",
      "episode : 570, reward mean : -0.15875437931860226, total_step : 17460, cur_epsilon : 0.746352\n",
      "episode : 580, reward mean : -0.1595689588675982, total_step : 17775, cur_epsilon : 0.74358\n",
      "episode : 590, reward mean : -0.1581016882748927, total_step : 18058, cur_epsilon : 0.7410896\n",
      "episode : 600, reward mean : -0.15876666002596418, total_step : 18365, cur_epsilon : 0.738388\n",
      "[EVAL] episode : 600, reward mean : -0.42799998484551904, step mean : 68.15, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.96  0.94 -1.22 -1.   -1.\n",
      " -1.   -1.    0.98  0.98 -1.    0.99 -1.    0.81]\n",
      "episode : 610, reward mean : -0.15716392779875485, total_step : 18636, cur_epsilon : 0.7360032000000001\n",
      "episode : 620, reward mean : -0.16345160632965064, total_step : 18793, cur_epsilon : 0.7346216\n",
      "episode : 630, reward mean : -0.15484126329362866, total_step : 19024, cur_epsilon : 0.7325888\n",
      "episode : 640, reward mean : -0.15226561841845979, total_step : 19422, cur_epsilon : 0.7290864\n",
      "episode : 650, reward mean : -0.15106153188703153, total_step : 19706, cur_epsilon : 0.7265872\n",
      "[EVAL] episode : 650, reward mean : -0.5534999820403754, step mean : 80.6, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.87 -1.   -1.   -1.    0.9  -1.   -1.   -1.\n",
      " -1.    0.83  0.98 -1.   -1.   -1.   -1.    0.35]\n",
      "episode : 660, reward mean : -0.15160605406783748, total_step : 19903, cur_epsilon : 0.7248536\n",
      "synced target net\n",
      "episode : 670, reward mean : -0.1499253665805975, total_step : 20251, cur_epsilon : 0.7217912\n",
      "episode : 680, reward mean : -0.14063234637929675, total_step : 20579, cur_epsilon : 0.7189048\n",
      "episode : 690, reward mean : -0.14850723985042694, total_step : 20772, cur_epsilon : 0.7172064\n",
      "episode : 700, reward mean : -0.14842856494975942, total_step : 20925, cur_epsilon : 0.71586\n",
      "[EVAL] episode : 700, reward mean : -0.674999980442226, step mean : 87.7, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.94\n",
      "  0.12  0.72 -1.   -1.   -1.   -1.    0.72 -1.  ]\n",
      "episode : 710, reward mean : -0.14674647241358607, total_step : 21164, cur_epsilon : 0.7137568000000001\n",
      "episode : 720, reward mean : -0.15036110467691388, total_step : 21381, cur_epsilon : 0.7118472\n",
      "episode : 730, reward mean : -0.14897259631742762, total_step : 21640, cur_epsilon : 0.709568\n",
      "episode : 740, reward mean : -0.15377026381273123, total_step : 22053, cur_epsilon : 0.7059336\n",
      "episode : 750, reward mean : -0.15079999358952045, total_step : 22194, cur_epsilon : 0.7046928\n",
      "[EVAL] episode : 750, reward mean : -0.46549998288974165, step mean : 76.85, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.31 -1.    0.98 -1.    0.94\n",
      " -1.   -1.    0.47  1.   -1.   -1.    0.99 -1.  ]\n",
      "episode : 760, reward mean : -0.1528157830581461, total_step : 22508, cur_epsilon : 0.7019296\n",
      "episode : 770, reward mean : -0.15554544810786264, total_step : 22881, cur_epsilon : 0.6986472\n",
      "episode : 780, reward mean : -0.15147435252244273, total_step : 23228, cur_epsilon : 0.6955936\n",
      "episode : 790, reward mean : -0.1503291074799586, total_step : 23499, cur_epsilon : 0.6932088000000001\n",
      "episode : 800, reward mean : -0.15157499353867024, total_step : 23858, cur_epsilon : 0.6900496\n",
      "[EVAL] episode : 800, reward mean : -0.6384999812580645, step mean : 84.05, eval_episode_rewards : [-1.   -1.   -1.    0.87 -1.   -1.   -1.    0.73 -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.77 -1.   -1.   -1.   -1.    0.86]\n",
      "episode : 810, reward mean : -0.14867900587664343, total_step : 24185, cur_epsilon : 0.687172\n",
      "episode : 820, reward mean : -0.15090243257127883, total_step : 24425, cur_epsilon : 0.68506\n",
      "episode : 830, reward mean : -0.14418071646422867, total_step : 24628, cur_epsilon : 0.6832736\n",
      "episode : 840, reward mean : -0.14558332688618628, total_step : 24999, cur_epsilon : 0.6800088\n",
      "synced target net\n",
      "episode : 850, reward mean : -0.14038822884287905, total_step : 25313, cur_epsilon : 0.6772456\n",
      "[EVAL] episode : 850, reward mean : -0.8224999793805182, step mean : 92.35, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.97 -1.    0.58]\n",
      "episode : 860, reward mean : -0.13826743544317607, total_step : 25481, cur_epsilon : 0.6757672\n",
      "episode : 870, reward mean : -0.1418390740522708, total_step : 25740, cur_epsilon : 0.673488\n",
      "episode : 880, reward mean : -0.1410113572333516, total_step : 26019, cur_epsilon : 0.6710328000000001\n",
      "episode : 890, reward mean : -0.14030336438563099, total_step : 26307, cur_epsilon : 0.6684984\n",
      "episode : 900, reward mean : -0.1360777713337706, total_step : 26776, cur_epsilon : 0.6643712\n",
      "[EVAL] episode : 900, reward mean : -0.6629999807104469, step mean : 86.5, eval_episode_rewards : [-1.   -1.    0.62 -1.   -1.   -1.   -1.   -1.    0.36  0.92 -1.   -1.\n",
      " -1.   -1.    0.84 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 910, reward mean : -0.13556043311626045, total_step : 27075, cur_epsilon : 0.66174\n",
      "episode : 920, reward mean : -0.13398912398994942, total_step : 27375, cur_epsilon : 0.6591\n",
      "episode : 930, reward mean : -0.13224730538705023, total_step : 27656, cur_epsilon : 0.6566272000000001\n",
      "episode : 940, reward mean : -0.1314680786486319, total_step : 28024, cur_epsilon : 0.6533888\n",
      "episode : 950, reward mean : -0.1364631514094378, total_step : 28438, cur_epsilon : 0.6497455999999999\n",
      "[EVAL] episode : 950, reward mean : -0.6669999806210398, step mean : 86.9, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.81 -1.   -1.   -1.    0.92 -1.   -1.   -1.\n",
      "  0.69 -1.   -1.   -1.   -1.    0.24 -1.   -1.  ]\n",
      "episode : 960, reward mean : -0.13092707684457613, total_step : 28752, cur_epsilon : 0.6469824\n",
      "episode : 970, reward mean : -0.12549483889195415, total_step : 28966, cur_epsilon : 0.6450992\n",
      "episode : 980, reward mean : -0.13015305475730982, total_step : 29257, cur_epsilon : 0.6425384000000001\n",
      "episode : 990, reward mean : -0.12909090263825473, total_step : 29492, cur_epsilon : 0.6404704\n",
      "episode : 1000, reward mean : -0.1257699935454875, total_step : 29798, cur_epsilon : 0.6377776\n",
      "[EVAL] episode : 1000, reward mean : -0.5329999824985862, step mean : 78.55, eval_episode_rewards : [ 1.   -1.   -1.   -1.   -1.   -1.    0.99  0.99 -1.    0.85 -1.   -1.\n",
      "  0.51 -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 1010, reward mean : -0.12048999355174601, total_step : 30111, cur_epsilon : 0.6350232\n",
      "episode : 1020, reward mean : -0.12003999353945255, total_step : 30457, cur_epsilon : 0.6319783999999999\n",
      "episode : 1030, reward mean : -0.1199299935195595, total_step : 30801, cur_epsilon : 0.6289511999999999\n",
      "episode : 1040, reward mean : -0.11359999357163907, total_step : 31048, cur_epsilon : 0.6267776\n",
      "episode : 1050, reward mean : -0.11077999354526401, total_step : 31421, cur_epsilon : 0.6234952\n",
      "[EVAL] episode : 1050, reward mean : -0.9049999786540865, step mean : 95.55, eval_episode_rewards : [-1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1.  -1.  -1.   0.9 -1. ]\n",
      "episode : 1060, reward mean : -0.10329999351128936, total_step : 31723, cur_epsilon : 0.6208376\n",
      "episode : 1070, reward mean : -0.10000999347306788, total_step : 32151, cur_epsilon : 0.6170712\n",
      "episode : 1080, reward mean : -0.09429999346658588, total_step : 32446, cur_epsilon : 0.6144752\n",
      "episode : 1090, reward mean : -0.08951999348402023, total_step : 32647, cur_epsilon : 0.6127064\n",
      "episode : 1100, reward mean : -0.08617999355867505, total_step : 32828, cur_epsilon : 0.6111135999999999\n",
      "[EVAL] episode : 1100, reward mean : -0.674999980442226, step mean : 87.7, eval_episode_rewards : [-1.    0.77 -1.   -1.    0.94  0.02 -1.   -1.   -1.    0.77 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1110, reward mean : -0.08305999356135726, total_step : 33069, cur_epsilon : 0.6089928\n",
      "episode : 1120, reward mean : -0.08318999358080327, total_step : 33419, cur_epsilon : 0.6059128\n",
      "episode : 1130, reward mean : -0.07977999356761575, total_step : 33712, cur_epsilon : 0.6033344\n",
      "episode : 1140, reward mean : -0.08102999353967608, total_step : 34188, cur_epsilon : 0.5991456\n",
      "episode : 1150, reward mean : -0.07945999357476831, total_step : 34514, cur_epsilon : 0.5962768\n",
      "[EVAL] episode : 1150, reward mean : -0.6289999825879932, step mean : 78.15, eval_episode_rewards : [ 0.49 -1.   -1.   -1.   -1.    0.96 -1.   -1.   -1.   -1.   -1.    0.97\n",
      " -1.   -1.   -1.   -1.    1.   -1.   -1.   -1.  ]\n",
      "episode : 1160, reward mean : -0.07934999359957874, total_step : 34680, cur_epsilon : 0.594816\n",
      "synced target net\n",
      "episode : 1170, reward mean : -0.07927999357879162, total_step : 35078, cur_epsilon : 0.5913136000000001\n",
      "episode : 1180, reward mean : -0.07740999355353415, total_step : 35460, cur_epsilon : 0.587952\n",
      "episode : 1190, reward mean : -0.0808699935209006, total_step : 35917, cur_epsilon : 0.5839304000000001\n",
      "episode : 1200, reward mean : -0.07524999351240695, total_step : 36236, cur_epsilon : 0.5811232\n",
      "[EVAL] episode : 1200, reward mean : -0.5349999824538827, step mean : 78.75, eval_episode_rewards : [-1.    0.94 -1.   -1.   -1.    0.69 -1.   -1.   -1.   -1.    0.99 -1.\n",
      "  0.68 -1.    1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1210, reward mean : -0.06872999356873334, total_step : 36375, cur_epsilon : 0.5799000000000001\n",
      "episode : 1220, reward mean : -0.05599999362975359, total_step : 36496, cur_epsilon : 0.5788352\n",
      "episode : 1230, reward mean : -0.04662999366037548, total_step : 36580, cur_epsilon : 0.5780959999999999\n",
      "episode : 1240, reward mean : -0.0441599936708808, total_step : 36829, cur_epsilon : 0.5759048\n",
      "episode : 1250, reward mean : -0.04077999367937445, total_step : 37132, cur_epsilon : 0.5732384\n",
      "[EVAL] episode : 1250, reward mean : -0.7224999804981053, step mean : 87.4, eval_episode_rewards : [-1.   -1.    0.84 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.75 -1.\n",
      " -1.   -1.   -1.    0.96 -1.   -1.   -1.   -1.  ]\n",
      "episode : 1260, reward mean : -0.041789993723854425, total_step : 37430, cur_epsilon : 0.570616\n",
      "episode : 1270, reward mean : -0.039819993700832126, total_step : 37731, cur_epsilon : 0.5679672\n",
      "episode : 1280, reward mean : -0.04324999366886914, total_step : 38143, cur_epsilon : 0.5643416000000001\n",
      "episode : 1290, reward mean : -0.03786999369971454, total_step : 38303, cur_epsilon : 0.5629336\n",
      "episode : 1300, reward mean : -0.04005999367311597, total_step : 38594, cur_epsilon : 0.5603728\n",
      "[EVAL] episode : 1300, reward mean : -0.8084999796934426, step mean : 90.95, eval_episode_rewards : [-1.   -1.   -1.    0.85 -1.   -1.   -1.   -1.   -1.    0.98 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1310, reward mean : -0.0365599936619401, total_step : 38882, cur_epsilon : 0.5578384000000001\n",
      "episode : 1320, reward mean : -0.03129999371245504, total_step : 38980, cur_epsilon : 0.556976\n",
      "episode : 1330, reward mean : -0.030179993648082017, total_step : 39555, cur_epsilon : 0.5519160000000001\n",
      "episode : 1340, reward mean : -0.033909993631765244, total_step : 39954, cur_epsilon : 0.5484048\n",
      "synced target net\n",
      "episode : 1350, reward mean : -0.03677999363467097, total_step : 40252, cur_epsilon : 0.5457824\n",
      "[EVAL] episode : 1350, reward mean : -0.8574999785982073, step mean : 95.85, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.62 -1.   -1.   -1.   -1.   -1.    0.23]\n",
      "episode : 1360, reward mean : -0.03824999362416565, total_step : 40621, cur_epsilon : 0.5425352\n",
      "episode : 1370, reward mean : -0.03508999358303845, total_step : 40987, cur_epsilon : 0.5393144000000001\n",
      "episode : 1380, reward mean : -0.037409993531182406, total_step : 41491, cur_epsilon : 0.5348792\n",
      "episode : 1390, reward mean : -0.04150999350659549, total_step : 41780, cur_epsilon : 0.532336\n",
      "episode : 1400, reward mean : -0.04123999351263046, total_step : 42141, cur_epsilon : 0.5291592\n",
      "[EVAL] episode : 1400, reward mean : -0.7459999799728394, step mean : 89.75, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.78 -1.    0.58 -1.   -1.   -1.    0.72\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1410, reward mean : -0.04138999348692596, total_step : 42590, cur_epsilon : 0.525208\n",
      "episode : 1420, reward mean : -0.03792999349720776, total_step : 43112, cur_epsilon : 0.5206144\n",
      "episode : 1430, reward mean : -0.031549993505701424, total_step : 43414, cur_epsilon : 0.5179568\n",
      "episode : 1440, reward mean : -0.028219993490725757, total_step : 43642, cur_epsilon : 0.5159504\n",
      "episode : 1450, reward mean : -0.034219993513077494, total_step : 44045, cur_epsilon : 0.5124040000000001\n",
      "[EVAL] episode : 1450, reward mean : -0.7124999807216227, step mean : 86.4, eval_episode_rewards : [ 1.   -1.   -1.   -1.   -1.    0.8  -1.    0.95 -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1460, reward mean : -0.040899993542581795, total_step : 44232, cur_epsilon : 0.5107584000000001\n",
      "episode : 1470, reward mean : -0.04104999351687729, total_step : 44718, cur_epsilon : 0.5064816\n",
      "synced target net\n",
      "episode : 1480, reward mean : -0.040089993515983226, total_step : 45041, cur_epsilon : 0.5036392000000001\n",
      "episode : 1490, reward mean : -0.04180999354459345, total_step : 45312, cur_epsilon : 0.5012544\n",
      "episode : 1500, reward mean : -0.04660999352671206, total_step : 45581, cur_epsilon : 0.49888720000000003\n",
      "[EVAL] episode : 1500, reward mean : -0.8069999797269702, step mean : 90.8, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.98 -1.    0.88 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1510, reward mean : -0.05259999348223209, total_step : 45935, cur_epsilon : 0.49577200000000005\n",
      "episode : 1520, reward mean : -0.05954999346099794, total_step : 46199, cur_epsilon : 0.4934488\n",
      "episode : 1530, reward mean : -0.056609993482008576, total_step : 46490, cur_epsilon : 0.49088800000000005\n",
      "episode : 1540, reward mean : -0.052409993441775445, total_step : 46868, cur_epsilon : 0.48756160000000004\n",
      "episode : 1550, reward mean : -0.05388999347575009, total_step : 46990, cur_epsilon : 0.48648800000000003\n",
      "[EVAL] episode : 1550, reward mean : -0.9269999781623482, step mean : 97.75, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.46 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1560, reward mean : -0.051449993485584856, total_step : 47339, cur_epsilon : 0.48341680000000004\n",
      "episode : 1570, reward mean : -0.05473999345675111, total_step : 47672, cur_epsilon : 0.48048640000000004\n",
      "episode : 1580, reward mean : -0.05125999342277646, total_step : 48138, cur_epsilon : 0.4763856\n",
      "episode : 1590, reward mean : -0.05446999339573085, total_step : 48542, cur_epsilon : 0.4728304\n",
      "episode : 1600, reward mean : -0.055299993377178905, total_step : 48932, cur_epsilon : 0.46939840000000005\n",
      "[EVAL] episode : 1600, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 1610, reward mean : -0.052209993379190564, total_step : 49193, cur_epsilon : 0.46710160000000006\n",
      "episode : 1620, reward mean : -0.043679993346333504, total_step : 49497, cur_epsilon : 0.4644264\n",
      "episode : 1630, reward mean : -0.0532299933116883, total_step : 49883, cur_epsilon : 0.46102960000000004\n",
      "synced target net\n",
      "episode : 1640, reward mean : -0.05094999329559505, total_step : 50354, cur_epsilon : 0.45688480000000004\n",
      "episode : 1650, reward mean : -0.048929993273690346, total_step : 50735, cur_epsilon : 0.45353200000000005\n",
      "[EVAL] episode : 1650, reward mean : -0.7429999800398945, step mean : 89.45, eval_episode_rewards : [-1.    0.69 -1.   -1.   -1.    0.79 -1.   -1.   -1.    0.66 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1660, reward mean : -0.05163999319076538, total_step : 51300, cur_epsilon : 0.44856\n",
      "episode : 1670, reward mean : -0.052779993232339624, total_step : 51463, cur_epsilon : 0.4471256\n",
      "episode : 1680, reward mean : -0.06333999321982264, total_step : 51847, cur_epsilon : 0.44374640000000004\n",
      "episode : 1690, reward mean : -0.056589993191882966, total_step : 52165, cur_epsilon : 0.44094800000000006\n",
      "episode : 1700, reward mean : -0.061059993136674165, total_step : 52563, cur_epsilon : 0.4374456\n",
      "[EVAL] episode : 1700, reward mean : -0.9394999778829515, step mean : 99.0, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.21 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1710, reward mean : -0.06220999311096966, total_step : 52917, cur_epsilon : 0.4343304\n",
      "episode : 1720, reward mean : -0.059289993042126295, total_step : 53440, cur_epsilon : 0.42972800000000005\n",
      "episode : 1730, reward mean : -0.05984999300725758, total_step : 53854, cur_epsilon : 0.4260848\n",
      "episode : 1740, reward mean : -0.048589993057772514, total_step : 54042, cur_epsilon : 0.4244304\n",
      "episode : 1750, reward mean : -0.04883999302983284, total_step : 54307, cur_epsilon : 0.4220984\n",
      "[EVAL] episode : 1750, reward mean : -0.9019999787211418, step mean : 95.25, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.96\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1760, reward mean : -0.04456999305821955, total_step : 54493, cur_epsilon : 0.4204616\n",
      "episode : 1770, reward mean : -0.04071999309957027, total_step : 54681, cur_epsilon : 0.4188072\n",
      "synced target net\n",
      "episode : 1780, reward mean : -0.04063999307900667, total_step : 55119, cur_epsilon : 0.41495280000000007\n",
      "episode : 1790, reward mean : -0.0414299930613488, total_step : 55467, cur_epsilon : 0.41189040000000005\n",
      "episode : 1800, reward mean : -0.0381999930664897, total_step : 55804, cur_epsilon : 0.40892480000000003\n",
      "[EVAL] episode : 1800, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 1810, reward mean : -0.040889993051067, total_step : 56198, cur_epsilon : 0.4054576\n",
      "episode : 1820, reward mean : -0.04112999302335083, total_step : 56563, cur_epsilon : 0.40224560000000004\n",
      "episode : 1830, reward mean : -0.0450699930023402, total_step : 56859, cur_epsilon : 0.3996408\n",
      "episode : 1840, reward mean : -0.04157999301329255, total_step : 57182, cur_epsilon : 0.3967984\n",
      "episode : 1850, reward mean : -0.04498999302648008, total_step : 57437, cur_epsilon : 0.39455439999999997\n",
      "[EVAL] episode : 1850, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 1860, reward mean : -0.04280999298579991, total_step : 57785, cur_epsilon : 0.39149200000000006\n",
      "episode : 1870, reward mean : -0.03769999294355512, total_step : 58232, cur_epsilon : 0.3875584000000001\n",
      "episode : 1880, reward mean : -0.04069999296590686, total_step : 58411, cur_epsilon : 0.38598319999999997\n",
      "episode : 1890, reward mean : -0.04133999295160174, total_step : 58761, cur_epsilon : 0.3829032\n",
      "episode : 1900, reward mean : -0.040679992988705636, total_step : 59065, cur_epsilon : 0.380228\n",
      "[EVAL] episode : 1900, reward mean : -0.8514999798499048, step mean : 90.3, eval_episode_rewards : [-1.   -1.    0.98 -1.   -1.   -1.   -1.02 -1.   -1.   -1.    0.01 -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 1910, reward mean : -0.036599993012845514, total_step : 59255, cur_epsilon : 0.378556\n",
      "episode : 1920, reward mean : -0.03500999298132956, total_step : 59695, cur_epsilon : 0.374684\n",
      "synced target net\n",
      "episode : 1930, reward mean : -0.040319992929697034, total_step : 60206, cur_epsilon : 0.37018719999999994\n",
      "episode : 1940, reward mean : -0.040739992897957565, total_step : 60715, cur_epsilon : 0.36570800000000003\n",
      "episode : 1950, reward mean : -0.039749992875382306, total_step : 61230, cur_epsilon : 0.36117600000000005\n",
      "[EVAL] episode : 1950, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 1960, reward mean : -0.04567999283224344, total_step : 61735, cur_epsilon : 0.35673200000000005\n",
      "episode : 1970, reward mean : -0.05434999277256429, total_step : 62214, cur_epsilon : 0.35251679999999996\n",
      "episode : 1980, reward mean : -0.04966999276541174, total_step : 62538, cur_epsilon : 0.3496656\n",
      "episode : 1990, reward mean : -0.05411999273300171, total_step : 62917, cur_epsilon : 0.34633040000000004\n",
      "episode : 2000, reward mean : -0.05722999270819128, total_step : 63334, cur_epsilon : 0.3426608\n",
      "[EVAL] episode : 2000, reward mean : -0.801999979838729, step mean : 90.3, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.98 -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.98]\n",
      "episode : 2010, reward mean : -0.05721999264135957, total_step : 63943, cur_epsilon : 0.3373016\n",
      "episode : 2020, reward mean : -0.05751999267935753, total_step : 64119, cur_epsilon : 0.33575279999999996\n",
      "episode : 2030, reward mean : -0.04793999264761806, total_step : 64604, cur_epsilon : 0.3314848\n",
      "synced target net\n",
      "episode : 2040, reward mean : -0.04579999260604382, total_step : 65037, cur_epsilon : 0.32767440000000003\n",
      "episode : 2050, reward mean : -0.045169992620125415, total_step : 65349, cur_epsilon : 0.3249288\n",
      "[EVAL] episode : 2050, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2060, reward mean : -0.05018999264203012, total_step : 65553, cur_epsilon : 0.3231336\n",
      "episode : 2070, reward mean : -0.04311999268829823, total_step : 65775, cur_epsilon : 0.32118\n",
      "episode : 2080, reward mean : -0.038329992661252615, total_step : 66191, cur_epsilon : 0.3175192\n",
      "episode : 2090, reward mean : -0.042319992616772655, total_step : 66589, cur_epsilon : 0.3140168\n",
      "episode : 2100, reward mean : -0.0435299925673753, total_step : 66990, cur_epsilon : 0.310488\n",
      "[EVAL] episode : 2100, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2110, reward mean : -0.04003999257832765, total_step : 67183, cur_epsilon : 0.3087896\n",
      "episode : 2120, reward mean : -0.03713999259844422, total_step : 67445, cur_epsilon : 0.306484\n",
      "episode : 2130, reward mean : -0.036969992535188796, total_step : 68018, cur_epsilon : 0.3014416\n",
      "episode : 2140, reward mean : -0.032939992513507606, total_step : 68590, cur_epsilon : 0.296408\n",
      "episode : 2150, reward mean : -0.033589992498978975, total_step : 68981, cur_epsilon : 0.2929672\n",
      "[EVAL] episode : 2150, reward mean : -0.9094999785535037, step mean : 96.0, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.81 -1.  ]\n",
      "episode : 2160, reward mean : -0.031779992427676915, total_step : 69465, cur_epsilon : 0.2887080000000001\n",
      "episode : 2170, reward mean : -0.021289992416277527, total_step : 69915, cur_epsilon : 0.284748\n",
      "synced target net\n",
      "episode : 2180, reward mean : -0.020249992417171597, total_step : 70292, cur_epsilon : 0.2814304000000001\n",
      "episode : 2190, reward mean : -0.013079992465674877, total_step : 70533, cur_epsilon : 0.27930960000000005\n",
      "episode : 2200, reward mean : -0.012919992446899413, total_step : 70935, cur_epsilon : 0.275772\n",
      "[EVAL] episode : 2200, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2210, reward mean : -0.016349992414936423, total_step : 71217, cur_epsilon : 0.27329040000000004\n",
      "episode : 2220, reward mean : -0.02128999232687056, total_step : 71731, cur_epsilon : 0.2687672\n",
      "episode : 2230, reward mean : -0.02918999223969877, total_step : 72203, cur_epsilon : 0.2646136000000001\n",
      "episode : 2240, reward mean : -0.03258999218605459, total_step : 72689, cur_epsilon : 0.26033680000000003\n",
      "episode : 2250, reward mean : -0.036139992173761126, total_step : 73046, cur_epsilon : 0.25719520000000007\n",
      "[EVAL] episode : 2250, reward mean : -0.8389999790117144, step mean : 94.0, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.89 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.33 -1.   -1.   -1.  ]\n",
      "episode : 2260, reward mean : -0.028459992188960313, total_step : 73275, cur_epsilon : 0.25517999999999996\n",
      "episode : 2270, reward mean : -0.02812999221868813, total_step : 73444, cur_epsilon : 0.25369280000000005\n",
      "episode : 2280, reward mean : -0.027919992223381995, total_step : 73837, cur_epsilon : 0.2502344000000001\n",
      "episode : 2290, reward mean : -0.029879992179572584, total_step : 74193, cur_epsilon : 0.24710160000000003\n",
      "episode : 2300, reward mean : -0.027889992134645582, total_step : 74683, cur_epsilon : 0.24278960000000005\n",
      "[EVAL] episode : 2300, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "synced target net\n",
      "episode : 2310, reward mean : -0.03475999209284782, total_step : 75157, cur_epsilon : 0.2386184\n",
      "episode : 2320, reward mean : -0.03663999202847481, total_step : 75542, cur_epsilon : 0.23523040000000006\n",
      "episode : 2330, reward mean : -0.03972999202646315, total_step : 76125, cur_epsilon : 0.23010000000000008\n",
      "episode : 2340, reward mean : -0.03635999205708504, total_step : 76387, cur_epsilon : 0.22779439999999995\n",
      "episode : 2350, reward mean : -0.03222999203763902, total_step : 76771, cur_epsilon : 0.22441520000000004\n",
      "[EVAL] episode : 2350, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2360, reward mean : -0.031019992042332887, total_step : 77120, cur_epsilon : 0.22134399999999999\n",
      "episode : 2370, reward mean : -0.031159992016851903, total_step : 77597, cur_epsilon : 0.21714639999999996\n",
      "episode : 2380, reward mean : -0.027429992033168673, total_step : 78029, cur_epsilon : 0.2133448\n",
      "episode : 2390, reward mean : -0.026989991975948216, total_step : 78573, cur_epsilon : 0.2085576\n",
      "episode : 2400, reward mean : -0.0319199919551611, total_step : 79025, cur_epsilon : 0.20457999999999998\n",
      "[EVAL] episode : 2400, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2410, reward mean : -0.03178999198041856, total_step : 79362, cur_epsilon : 0.20161439999999997\n",
      "episode : 2420, reward mean : -0.031259992014616725, total_step : 79734, cur_epsilon : 0.19834079999999998\n",
      "synced target net\n",
      "episode : 2430, reward mean : -0.030799992002546787, total_step : 80089, cur_epsilon : 0.19521680000000008\n",
      "episode : 2440, reward mean : -0.02712999197281897, total_step : 80449, cur_epsilon : 0.19204880000000002\n",
      "episode : 2450, reward mean : -0.022569991962984204, total_step : 80897, cur_epsilon : 0.1881064\n",
      "[EVAL] episode : 2450, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2460, reward mean : -0.02201999193057418, total_step : 81229, cur_epsilon : 0.18518480000000004\n",
      "episode : 2470, reward mean : -0.022689991960301994, total_step : 81584, cur_epsilon : 0.18206080000000002\n",
      "episode : 2480, reward mean : -0.031749991914257404, total_step : 82112, cur_epsilon : 0.17741440000000008\n",
      "episode : 2490, reward mean : -0.030719991892576216, total_step : 82480, cur_epsilon : 0.174176\n",
      "episode : 2500, reward mean : -0.02806999190710485, total_step : 82684, cur_epsilon : 0.1723808\n",
      "[EVAL] episode : 2500, reward mean : -0.9069999786093831, step mean : 95.75, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.86 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 2510, reward mean : -0.030369991878047585, total_step : 83165, cur_epsilon : 0.16814799999999996\n",
      "episode : 2520, reward mean : -0.032839991800487044, total_step : 83773, cur_epsilon : 0.1627976\n",
      "episode : 2530, reward mean : -0.03113999181613326, total_step : 83993, cur_epsilon : 0.16086160000000005\n",
      "episode : 2540, reward mean : -0.029309991834685207, total_step : 84287, cur_epsilon : 0.15827440000000004\n",
      "episode : 2550, reward mean : -0.032429991720244286, total_step : 84917, cur_epsilon : 0.15273040000000004\n",
      "[EVAL] episode : 2550, reward mean : -0.7384999801404775, step mean : 89.0, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.87 -1.   -1.   -1.\n",
      " -1.    0.36 -1.   -1.   -1.   -1.   -1.    1.  ]\n",
      "synced target net\n",
      "episode : 2560, reward mean : -0.030519991718232633, total_step : 85275, cur_epsilon : 0.14958000000000005\n",
      "episode : 2570, reward mean : -0.031609991716220975, total_step : 85616, cur_epsilon : 0.14657920000000002\n",
      "episode : 2580, reward mean : -0.030599991761147975, total_step : 85882, cur_epsilon : 0.1442384\n",
      "episode : 2590, reward mean : -0.027509991785511376, total_step : 86177, cur_epsilon : 0.14164239999999995\n",
      "episode : 2600, reward mean : -0.026169991770759224, total_step : 86633, cur_epsilon : 0.13762960000000002\n",
      "[EVAL] episode : 2600, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2610, reward mean : -0.032119991682469845, total_step : 87287, cur_epsilon : 0.13187439999999995\n",
      "episode : 2620, reward mean : -0.03744999167509377, total_step : 87623, cur_epsilon : 0.12891760000000008\n",
      "episode : 2630, reward mean : -0.033289991611614826, total_step : 88290, cur_epsilon : 0.12304800000000005\n",
      "episode : 2640, reward mean : -0.029749991646036505, total_step : 88607, cur_epsilon : 0.12025839999999999\n",
      "episode : 2650, reward mean : -0.031479991629719734, total_step : 89062, cur_epsilon : 0.11625440000000009\n",
      "[EVAL] episode : 2650, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2660, reward mean : -0.027659991648048162, total_step : 89548, cur_epsilon : 0.11197760000000001\n",
      "episode : 2670, reward mean : -0.02928999158926308, total_step : 89971, cur_epsilon : 0.1082552\n",
      "synced target net\n",
      "episode : 2680, reward mean : -0.02489999155327678, total_step : 90514, cur_epsilon : 0.10347680000000004\n",
      "episode : 2690, reward mean : -0.02218999152444303, total_step : 90959, cur_epsilon : 0.0995608\n",
      "episode : 2700, reward mean : -0.013229991545900703, total_step : 91261, cur_epsilon : 0.09690319999999997\n",
      "[EVAL] episode : 2700, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2710, reward mean : -0.01756999151594937, total_step : 91746, cur_epsilon : 0.09263519999999992\n",
      "episode : 2720, reward mean : -0.016989991528913378, total_step : 92211, cur_epsilon : 0.08854320000000004\n",
      "episode : 2730, reward mean : -0.020709991512820124, total_step : 92696, cur_epsilon : 0.0842752\n",
      "episode : 2740, reward mean : -0.028859991509467362, total_step : 92899, cur_epsilon : 0.08248880000000003\n",
      "episode : 2750, reward mean : -0.03397999143972993, total_step : 93474, cur_epsilon : 0.07742879999999996\n",
      "[EVAL] episode : 2750, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2760, reward mean : -0.03706999139301479, total_step : 93868, cur_epsilon : 0.07396160000000007\n",
      "episode : 2770, reward mean : -0.033639991357922554, total_step : 94212, cur_epsilon : 0.07093440000000006\n",
      "episode : 2780, reward mean : -0.03906999132595956, total_step : 94793, cur_epsilon : 0.06582159999999992\n",
      "synced target net\n",
      "episode : 2790, reward mean : -0.038679991312325, total_step : 95203, cur_epsilon : 0.06221359999999998\n",
      "episode : 2800, reward mean : -0.0379199912622571, total_step : 95763, cur_epsilon : 0.05728560000000005\n",
      "[EVAL] episode : 2800, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2810, reward mean : -0.032399991296231744, total_step : 96007, cur_epsilon : 0.05513840000000003\n",
      "episode : 2820, reward mean : -0.026919991306960583, total_step : 96323, cur_epsilon : 0.052357600000000004\n",
      "episode : 2830, reward mean : -0.03233999127522111, total_step : 96759, cur_epsilon : 0.04852080000000003\n",
      "episode : 2840, reward mean : -0.03347999122738838, total_step : 97293, cur_epsilon : 0.043821600000000016\n",
      "episode : 2850, reward mean : -0.030689991222694515, total_step : 97568, cur_epsilon : 0.04140160000000004\n",
      "[EVAL] episode : 2850, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 2860, reward mean : -0.034989991215988996, total_step : 97948, cur_epsilon : 0.038057600000000025\n",
      "episode : 2870, reward mean : -0.03861999120190739, total_step : 98457, cur_epsilon : 0.03357840000000012\n",
      "episode : 2880, reward mean : -0.03334999116323888, total_step : 98808, cur_epsilon : 0.030489600000000006\n",
      "episode : 2890, reward mean : -0.03262999113462865, total_step : 99286, cur_epsilon : 0.026283200000000062\n",
      "episode : 2900, reward mean : -0.0361299910787493, total_step : 99837, cur_epsilon : 0.021434400000000076\n",
      "[EVAL] episode : 2900, reward mean : -0.9094999785535037, step mean : 96.0, eval_episode_rewards : [-1.   -1.   -1.    0.81 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 2910, reward mean : -0.043839990995824335, total_step : 100396, cur_epsilon : 0.02\n",
      "episode : 2920, reward mean : -0.04422999098710716, total_step : 100875, cur_epsilon : 0.02\n",
      "episode : 2930, reward mean : -0.033239991053938864, total_step : 101089, cur_epsilon : 0.02\n",
      "episode : 2940, reward mean : -0.030619991090148686, total_step : 101437, cur_epsilon : 0.02\n",
      "episode : 2950, reward mean : -0.025279991120100022, total_step : 101818, cur_epsilon : 0.02\n",
      "[EVAL] episode : 2950, reward mean : -0.8454999788664281, step mean : 94.65, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.68  0.41 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 2960, reward mean : -0.019439991138875484, total_step : 102240, cur_epsilon : 0.02\n",
      "episode : 2970, reward mean : -0.014349991140887142, total_step : 102709, cur_epsilon : 0.02\n",
      "episode : 2980, reward mean : -0.010799991130828858, total_step : 103076, cur_epsilon : 0.02\n",
      "episode : 2990, reward mean : -0.008799991086125374, total_step : 103653, cur_epsilon : 0.02\n",
      "episode : 3000, reward mean : -0.011019991036504508, total_step : 104290, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3000, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3010, reward mean : -0.012499991070479154, total_step : 104750, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3020, reward mean : -0.010459991026669741, total_step : 105122, cur_epsilon : 0.02\n",
      "episode : 3030, reward mean : -0.015079991057515145, total_step : 105469, cur_epsilon : 0.02\n",
      "episode : 3040, reward mean : -0.021259991008788347, total_step : 106114, cur_epsilon : 0.02\n",
      "episode : 3050, reward mean : -0.026839990973472595, total_step : 106582, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3050, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3060, reward mean : -0.027579990912228822, total_step : 107058, cur_epsilon : 0.02\n",
      "episode : 3070, reward mean : -0.03661999082192779, total_step : 107679, cur_epsilon : 0.02\n",
      "episode : 3080, reward mean : -0.04050999082438648, total_step : 108084, cur_epsilon : 0.02\n",
      "episode : 3090, reward mean : -0.03660999079979956, total_step : 108591, cur_epsilon : 0.02\n",
      "episode : 3100, reward mean : -0.03383999077230692, total_step : 109113, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3100, reward mean : -0.8174999794922769, step mean : 91.85, eval_episode_rewards : [-1.    0.87 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.78 -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 3110, reward mean : -0.030919990748167037, total_step : 109414, cur_epsilon : 0.02\n",
      "episode : 3120, reward mean : -0.02947999071329832, total_step : 109831, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3130, reward mean : -0.035779990706592796, total_step : 110432, cur_epsilon : 0.02\n",
      "episode : 3140, reward mean : -0.034519990779459477, total_step : 110680, cur_epsilon : 0.02\n",
      "episode : 3150, reward mean : -0.032219990786165, total_step : 111041, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3150, reward mean : -0.8384999790228903, step mean : 93.95, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.33 -1.   -1.   -1.   -1.   -1.    0.9\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 3160, reward mean : -0.031009990790858866, total_step : 111505, cur_epsilon : 0.02\n",
      "episode : 3170, reward mean : -0.04164999075420201, total_step : 112116, cur_epsilon : 0.02\n",
      "episode : 3180, reward mean : -0.047839990727603435, total_step : 112611, cur_epsilon : 0.02\n",
      "episode : 3190, reward mean : -0.048009990679100153, total_step : 113067, cur_epsilon : 0.02\n",
      "episode : 3200, reward mean : -0.05123999065160751, total_step : 113590, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3200, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3210, reward mean : -0.050569990621879696, total_step : 114003, cur_epsilon : 0.02\n",
      "episode : 3220, reward mean : -0.04992999061383307, total_step : 114552, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3230, reward mean : -0.04893999059125781, total_step : 115123, cur_epsilon : 0.02\n",
      "episode : 3240, reward mean : -0.04925999056175351, total_step : 115740, cur_epsilon : 0.02\n",
      "episode : 3250, reward mean : -0.04749999055638909, total_step : 116121, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3250, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3260, reward mean : -0.05040999049134552, total_step : 116639, cur_epsilon : 0.02\n",
      "episode : 3270, reward mean : -0.053339990425854925, total_step : 117099, cur_epsilon : 0.02\n",
      "episode : 3280, reward mean : -0.04586999043636024, total_step : 117444, cur_epsilon : 0.02\n",
      "episode : 3290, reward mean : -0.0449599903896451, total_step : 118006, cur_epsilon : 0.02\n",
      "episode : 3300, reward mean : -0.045569990420714024, total_step : 118359, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3300, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3310, reward mean : -0.046849990414455533, total_step : 118860, cur_epsilon : 0.02\n",
      "episode : 3320, reward mean : -0.05040999040193856, total_step : 119300, cur_epsilon : 0.02\n",
      "episode : 3330, reward mean : -0.047959990434348586, total_step : 119739, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3340, reward mean : -0.050799990393221375, total_step : 120184, cur_epsilon : 0.02\n",
      "episode : 3350, reward mean : -0.04758999039791524, total_step : 120546, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3350, reward mean : -0.8029999798163772, step mean : 90.4, eval_episode_rewards : [-1.   -1.    0.98 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.96 -1.  ]\n",
      "episode : 3360, reward mean : -0.05096999036706984, total_step : 121029, cur_epsilon : 0.02\n",
      "episode : 3370, reward mean : -0.054189990339800716, total_step : 121628, cur_epsilon : 0.02\n",
      "episode : 3380, reward mean : -0.059489990288391706, total_step : 122287, cur_epsilon : 0.02\n",
      "episode : 3390, reward mean : -0.05757999033108353, total_step : 122640, cur_epsilon : 0.02\n",
      "episode : 3400, reward mean : -0.05103999036550522, total_step : 122939, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3400, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3410, reward mean : -0.04869999035075307, total_step : 123341, cur_epsilon : 0.02\n",
      "episode : 3420, reward mean : -0.05258999028615653, total_step : 123997, cur_epsilon : 0.02\n",
      "episode : 3430, reward mean : -0.059449990222230555, total_step : 124636, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3440, reward mean : -0.05889999021217227, total_step : 125042, cur_epsilon : 0.02\n",
      "episode : 3450, reward mean : -0.060309990203008056, total_step : 125530, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3450, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3460, reward mean : -0.06316999013908207, total_step : 126144, cur_epsilon : 0.02\n",
      "episode : 3470, reward mean : -0.06366999008320272, total_step : 126747, cur_epsilon : 0.02\n",
      "episode : 3480, reward mean : -0.05574999010376632, total_step : 127186, cur_epsilon : 0.02\n",
      "episode : 3490, reward mean : -0.05420999009348452, total_step : 127600, cur_epsilon : 0.02\n",
      "episode : 3500, reward mean : -0.06311998998373747, total_step : 128291, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3500, reward mean : -0.9209999782964587, step mean : 97.15, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.58]\n",
      "episode : 3510, reward mean : -0.06267998999357223, total_step : 128728, cur_epsilon : 0.02\n",
      "episode : 3520, reward mean : -0.06402999000810086, total_step : 129271, cur_epsilon : 0.02\n",
      "episode : 3530, reward mean : -0.06968998994864524, total_step : 129756, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3540, reward mean : -0.07378998992405832, total_step : 130161, cur_epsilon : 0.02\n",
      "episode : 3550, reward mean : -0.0690499899405986, total_step : 130719, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3550, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3560, reward mean : -0.07242998995445668, total_step : 131017, cur_epsilon : 0.02\n",
      "episode : 3570, reward mean : -0.06630998997949064, total_step : 131247, cur_epsilon : 0.02\n",
      "episode : 3580, reward mean : -0.07364998994953931, total_step : 131647, cur_epsilon : 0.02\n",
      "episode : 3590, reward mean : -0.07511998987197877, total_step : 132285, cur_epsilon : 0.02\n",
      "episode : 3600, reward mean : -0.07404998987354339, total_step : 132733, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3600, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3610, reward mean : -0.06884998994506895, total_step : 133069, cur_epsilon : 0.02\n",
      "episode : 3620, reward mean : -0.06772998990304768, total_step : 133592, cur_epsilon : 0.02\n",
      "episode : 3630, reward mean : -0.07058998992852866, total_step : 134145, cur_epsilon : 0.02\n",
      "episode : 3640, reward mean : -0.0774599898867309, total_step : 134648, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3650, reward mean : -0.07468998990394175, total_step : 135026, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3650, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3660, reward mean : -0.07664998992718756, total_step : 135407, cur_epsilon : 0.02\n",
      "episode : 3670, reward mean : -0.07650998995266854, total_step : 135717, cur_epsilon : 0.02\n",
      "episode : 3680, reward mean : -0.0824699899982661, total_step : 136056, cur_epsilon : 0.02\n",
      "episode : 3690, reward mean : -0.08747998993098736, total_step : 136800, cur_epsilon : 0.02\n",
      "episode : 3700, reward mean : -0.09233998988941312, total_step : 137289, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3700, reward mean : -0.9039999786764383, step mean : 95.45, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.92 -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 3710, reward mean : -0.0887499898802489, total_step : 137817, cur_epsilon : 0.02\n",
      "episode : 3720, reward mean : -0.08976998990215361, total_step : 138184, cur_epsilon : 0.02\n",
      "episode : 3730, reward mean : -0.08458998997323215, total_step : 138353, cur_epsilon : 0.02\n",
      "episode : 3740, reward mean : -0.08696998985297978, total_step : 139091, cur_epsilon : 0.02\n",
      "episode : 3750, reward mean : -0.0813499898891896, total_step : 139504, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3750, reward mean : -0.9169999783858657, step mean : 96.75, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.66 -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 3760, reward mean : -0.08148998988606036, total_step : 139912, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3770, reward mean : -0.08541998982056975, total_step : 140546, cur_epsilon : 0.02\n",
      "episode : 3780, reward mean : -0.0846199898608029, total_step : 140946, cur_epsilon : 0.02\n",
      "episode : 3790, reward mean : -0.08487998987734317, total_step : 141281, cur_epsilon : 0.02\n",
      "episode : 3800, reward mean : -0.09186998985521495, total_step : 141938, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3800, reward mean : -0.9019999787211418, step mean : 95.25, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.96 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 3810, reward mean : -0.09750998979620636, total_step : 142443, cur_epsilon : 0.02\n",
      "episode : 3820, reward mean : -0.10062998974882066, total_step : 142970, cur_epsilon : 0.02\n",
      "episode : 3830, reward mean : -0.1000899897608906, total_step : 143354, cur_epsilon : 0.02\n",
      "episode : 3840, reward mean : -0.10850998970679938, total_step : 144128, cur_epsilon : 0.02\n",
      "episode : 3850, reward mean : -0.11306998964957893, total_step : 144657, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3850, reward mean : -0.8199999794363976, step mean : 92.1, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.86 -1.   -1.   -1.    0.74 -1.  ]\n",
      "synced target net\n",
      "episode : 3860, reward mean : -0.10890998963080346, total_step : 145118, cur_epsilon : 0.02\n",
      "episode : 3870, reward mean : -0.10982998963259161, total_step : 145618, cur_epsilon : 0.02\n",
      "episode : 3880, reward mean : -0.11698998953960836, total_step : 146380, cur_epsilon : 0.02\n",
      "episode : 3890, reward mean : -0.11610998953692615, total_step : 146869, cur_epsilon : 0.02\n",
      "episode : 3900, reward mean : -0.11194998958520591, total_step : 147206, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3900, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3910, reward mean : -0.10625998962298036, total_step : 147598, cur_epsilon : 0.02\n",
      "episode : 3920, reward mean : -0.10903998960554599, total_step : 148155, cur_epsilon : 0.02\n",
      "episode : 3930, reward mean : -0.1129099895413965, total_step : 148653, cur_epsilon : 0.02\n",
      "episode : 3940, reward mean : -0.1189199894964695, total_step : 149200, cur_epsilon : 0.02\n",
      "episode : 3950, reward mean : -0.12339998946338893, total_step : 149728, cur_epsilon : 0.02\n",
      "[EVAL] episode : 3950, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 3960, reward mean : -0.12298998951725662, total_step : 149911, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 3970, reward mean : -0.13059998950362206, total_step : 150442, cur_epsilon : 0.02\n",
      "episode : 3980, reward mean : -0.13350998950563372, total_step : 150801, cur_epsilon : 0.02\n",
      "episode : 3990, reward mean : -0.13092998954094948, total_step : 151221, cur_epsilon : 0.02\n",
      "episode : 4000, reward mean : -0.1302999895773828, total_step : 151696, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4000, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 4010, reward mean : -0.12925998955592513, total_step : 152250, cur_epsilon : 0.02\n",
      "episode : 4020, reward mean : -0.1294599895514548, total_step : 152640, cur_epsilon : 0.02\n",
      "episode : 4030, reward mean : -0.12710998955927788, total_step : 152952, cur_epsilon : 0.02\n",
      "episode : 4040, reward mean : -0.12763998959213496, total_step : 153454, cur_epsilon : 0.02\n",
      "episode : 4050, reward mean : -0.1255199895724654, total_step : 154009, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4050, reward mean : -0.9014999787323177, step mean : 95.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.97 -1.   -1.  ]\n",
      "episode : 4060, reward mean : -0.12859998954832555, total_step : 154595, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 4070, reward mean : -0.13311998955905438, total_step : 155169, cur_epsilon : 0.02\n",
      "episode : 4080, reward mean : -0.13410998953692616, total_step : 155671, cur_epsilon : 0.02\n",
      "episode : 4090, reward mean : -0.14095998951792718, total_step : 156261, cur_epsilon : 0.02\n",
      "episode : 4100, reward mean : -0.14470998950116337, total_step : 156859, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4100, reward mean : -0.9014999787323177, step mean : 95.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.97 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 4110, reward mean : -0.15372998945601285, total_step : 157359, cur_epsilon : 0.02\n",
      "episode : 4120, reward mean : -0.1585299894157797, total_step : 157955, cur_epsilon : 0.02\n",
      "episode : 4130, reward mean : -0.1551399894244969, total_step : 158518, cur_epsilon : 0.02\n",
      "episode : 4140, reward mean : -0.15950998937152325, total_step : 159003, cur_epsilon : 0.02\n",
      "episode : 4150, reward mean : -0.16055998934805393, total_step : 159469, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4150, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "synced target net\n",
      "episode : 4160, reward mean : -0.16658998932503163, total_step : 160033, cur_epsilon : 0.02\n",
      "episode : 4170, reward mean : -0.1630499893594533, total_step : 160490, cur_epsilon : 0.02\n",
      "episode : 4180, reward mean : -0.16315998937934637, total_step : 160897, cur_epsilon : 0.02\n",
      "episode : 4190, reward mean : -0.16883998936414718, total_step : 161422, cur_epsilon : 0.02\n",
      "episode : 4200, reward mean : -0.16858998936973513, total_step : 161920, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4200, reward mean : -0.9059999786317349, step mean : 95.65, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.88 -1.   -1.   -1.  ]\n",
      "episode : 4210, reward mean : -0.1691199893578887, total_step : 162386, cur_epsilon : 0.02\n",
      "episode : 4220, reward mean : -0.1711699893567711, total_step : 162938, cur_epsilon : 0.02\n",
      "episode : 4230, reward mean : -0.17449998934939503, total_step : 163543, cur_epsilon : 0.02\n",
      "episode : 4240, reward mean : -0.17784998934157192, total_step : 164196, cur_epsilon : 0.02\n",
      "episode : 4250, reward mean : -0.17506998933665455, total_step : 164600, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4250, reward mean : -0.8219999793916941, step mean : 92.3, eval_episode_rewards : [-1.    0.58 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.98 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 4260, reward mean : -0.17665998934581875, total_step : 165077, cur_epsilon : 0.02\n",
      "episode : 4270, reward mean : -0.1704799893721938, total_step : 165420, cur_epsilon : 0.02\n",
      "episode : 4280, reward mean : -0.1708999893628061, total_step : 165807, cur_epsilon : 0.02\n",
      "episode : 4290, reward mean : -0.1711899894233793, total_step : 166101, cur_epsilon : 0.02\n",
      "episode : 4300, reward mean : -0.1733399893976748, total_step : 166568, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4300, reward mean : -0.8414999789558351, step mean : 94.25, eval_episode_rewards : [-1.   -1.   -1.    0.73 -1.   -1.   -1.   -1.    0.44 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 4310, reward mean : -0.16804998938180507, total_step : 167140, cur_epsilon : 0.02\n",
      "episode : 4320, reward mean : -0.17070998934470116, total_step : 167745, cur_epsilon : 0.02\n",
      "episode : 4330, reward mean : -0.17220998935587703, total_step : 168134, cur_epsilon : 0.02\n",
      "episode : 4340, reward mean : -0.17162998936884105, total_step : 168521, cur_epsilon : 0.02\n",
      "episode : 4350, reward mean : -0.1709599893838167, total_step : 168818, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4350, reward mean : -0.9019999787211418, step mean : 95.25, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.96 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 4360, reward mean : -0.17134998937509954, total_step : 169342, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 4370, reward mean : -0.17219998935610056, total_step : 170026, cur_epsilon : 0.02\n",
      "episode : 4380, reward mean : -0.17277998938784003, total_step : 170543, cur_epsilon : 0.02\n",
      "episode : 4390, reward mean : -0.17290998936258256, total_step : 171010, cur_epsilon : 0.02\n",
      "episode : 4400, reward mean : -0.18369998932257295, total_step : 171487, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4400, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 4410, reward mean : -0.18948998928256333, total_step : 172066, cur_epsilon : 0.02\n",
      "episode : 4420, reward mean : -0.18601998936012387, total_step : 172379, cur_epsilon : 0.02\n",
      "episode : 4430, reward mean : -0.1833099893759936, total_step : 172949, cur_epsilon : 0.02\n",
      "episode : 4440, reward mean : -0.19251998937129974, total_step : 173375, cur_epsilon : 0.02\n",
      "episode : 4450, reward mean : -0.19003998938202857, total_step : 173815, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4450, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 4460, reward mean : -0.19027998935431242, total_step : 174552, cur_epsilon : 0.02\n",
      "episode : 4470, reward mean : -0.18781998943164946, total_step : 174810, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 4480, reward mean : -0.19469998943433164, total_step : 175236, cur_epsilon : 0.02\n",
      "episode : 4490, reward mean : -0.19740998937375845, total_step : 175919, cur_epsilon : 0.02\n",
      "episode : 4500, reward mean : -0.1956099894363433, total_step : 176331, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4500, reward mean : -0.9014999787323177, step mean : 95.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.97 -1.   -1.  ]\n",
      "episode : 4510, reward mean : -0.19762998945824803, total_step : 176673, cur_epsilon : 0.02\n",
      "episode : 4520, reward mean : -0.19410998946987093, total_step : 177165, cur_epsilon : 0.02\n",
      "episode : 4530, reward mean : -0.1935999894812703, total_step : 177601, cur_epsilon : 0.02\n",
      "episode : 4540, reward mean : -0.19187998947501184, total_step : 178032, cur_epsilon : 0.02\n",
      "episode : 4550, reward mean : -0.1946899895016104, total_step : 178471, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4550, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 4560, reward mean : -0.19523998944461346, total_step : 179022, cur_epsilon : 0.02\n",
      "episode : 4570, reward mean : -0.20924998935498298, total_step : 179651, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 4580, reward mean : -0.21209998933598398, total_step : 180134, cur_epsilon : 0.02\n",
      "episode : 4590, reward mean : -0.21246998937241732, total_step : 180611, cur_epsilon : 0.02\n",
      "episode : 4600, reward mean : -0.21507998935878278, total_step : 181118, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4600, reward mean : -0.9349999779835343, step mean : 98.55, eval_episode_rewards : [-1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.   0.3 -1.  -1.  -1.  -1.\n",
      " -1.  -1.  -1.  -1.  -1.  -1. ]\n",
      "episode : 4610, reward mean : -0.21713998933508993, total_step : 181559, cur_epsilon : 0.02\n",
      "episode : 4620, reward mean : -0.21777998934313655, total_step : 182045, cur_epsilon : 0.02\n",
      "episode : 4630, reward mean : -0.21205998935922982, total_step : 182527, cur_epsilon : 0.02\n",
      "episode : 4640, reward mean : -0.2126499894130975, total_step : 182790, cur_epsilon : 0.02\n",
      "episode : 4650, reward mean : -0.21877998934313655, total_step : 183476, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4650, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "episode : 4660, reward mean : -0.2196699893232435, total_step : 183946, cur_epsilon : 0.02\n",
      "episode : 4670, reward mean : -0.21338998930715025, total_step : 184329, cur_epsilon : 0.02\n",
      "episode : 4680, reward mean : -0.2060399892926216, total_step : 184733, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 4690, reward mean : -0.20038998939655722, total_step : 185017, cur_epsilon : 0.02\n",
      "episode : 4700, reward mean : -0.20125998939946294, total_step : 185492, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4700, reward mean : -0.8209999794140458, step mean : 92.2, eval_episode_rewards : [-1.   -1.   -1.    0.6   0.98 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 4710, reward mean : -0.20437998935207724, total_step : 186229, cur_epsilon : 0.02\n",
      "episode : 4720, reward mean : -0.20630998930893837, total_step : 186789, cur_epsilon : 0.02\n",
      "episode : 4730, reward mean : -0.20848998923785986, total_step : 187273, cur_epsilon : 0.02\n",
      "episode : 4740, reward mean : -0.20840998926199972, total_step : 187902, cur_epsilon : 0.02\n",
      "episode : 4750, reward mean : -0.2098799892514944, total_step : 188363, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4750, reward mean : -0.8424999789334834, step mean : 94.35, eval_episode_rewards : [ 0.98 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.17]\n",
      "episode : 4760, reward mean : -0.20515998926758766, total_step : 188699, cur_epsilon : 0.02\n",
      "episode : 4770, reward mean : -0.20554998932592572, total_step : 189075, cur_epsilon : 0.02\n",
      "episode : 4780, reward mean : -0.21388998927362263, total_step : 189711, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 4790, reward mean : -0.21321998924389482, total_step : 190179, cur_epsilon : 0.02\n",
      "episode : 4800, reward mean : -0.21109998929128052, total_step : 190624, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4800, reward mean : -0.9029999786987901, step mean : 95.35, eval_episode_rewards : [-1.   -1.    0.94 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 4810, reward mean : -0.20874998932145536, total_step : 190995, cur_epsilon : 0.02\n",
      "episode : 4820, reward mean : -0.20781998934224247, total_step : 191429, cur_epsilon : 0.02\n",
      "episode : 4830, reward mean : -0.19956998932547867, total_step : 191889, cur_epsilon : 0.02\n",
      "episode : 4840, reward mean : -0.19095998938381673, total_step : 192404, cur_epsilon : 0.02\n",
      "episode : 4850, reward mean : -0.1895899894144386, total_step : 192798, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4850, reward mean : -0.9319999791681767, step mean : 93.3, eval_episode_rewards : [ 0.94 -1.   -1.   -1.   -1.58 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 4860, reward mean : -0.1863799894414842, total_step : 193140, cur_epsilon : 0.02\n",
      "episode : 4870, reward mean : -0.18412998944707215, total_step : 193615, cur_epsilon : 0.02\n",
      "episode : 4880, reward mean : -0.178189989535138, total_step : 193989, cur_epsilon : 0.02\n",
      "episode : 4890, reward mean : -0.1811399895362556, total_step : 194474, cur_epsilon : 0.02\n",
      "episode : 4900, reward mean : -0.1868099894989282, total_step : 194978, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4900, reward mean : -0.9999999776482582, step mean : 100.0, eval_episode_rewards : [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.]\n",
      "synced target net\n",
      "episode : 4910, reward mean : -0.18429998948797582, total_step : 195418, cur_epsilon : 0.02\n",
      "episode : 4920, reward mean : -0.18096998951770366, total_step : 195842, cur_epsilon : 0.02\n",
      "episode : 4930, reward mean : -0.18718998953513802, total_step : 196265, cur_epsilon : 0.02\n",
      "episode : 4940, reward mean : -0.18360998952575028, total_step : 196856, cur_epsilon : 0.02\n",
      "episode : 4950, reward mean : -0.18145998952910303, total_step : 197369, cur_epsilon : 0.02\n",
      "[EVAL] episode : 4950, reward mean : -0.8659999784082174, step mean : 96.7, eval_episode_rewards : [-1.   -1.    0.14 -1.   -1.   -1.   -1.   -1.   -1.    0.54 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 4960, reward mean : -0.1868099894989282, total_step : 197687, cur_epsilon : 0.02\n",
      "episode : 4970, reward mean : -0.1779399895183742, total_step : 198131, cur_epsilon : 0.02\n",
      "episode : 4980, reward mean : -0.17937998950853945, total_step : 198533, cur_epsilon : 0.02\n",
      "episode : 4990, reward mean : -0.1828599894531071, total_step : 199198, cur_epsilon : 0.02\n",
      "episode : 5000, reward mean : -0.18080998943187296, total_step : 199767, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5000, reward mean : -0.8609999785199761, step mean : 96.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.71  0.07\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 5010, reward mean : -0.1832799894437194, total_step : 200269, cur_epsilon : 0.02\n",
      "episode : 5020, reward mean : -0.1821899894680828, total_step : 200552, cur_epsilon : 0.02\n",
      "episode : 5030, reward mean : -0.18076998945511877, total_step : 200922, cur_epsilon : 0.02\n",
      "episode : 5040, reward mean : -0.1786999894566834, total_step : 201417, cur_epsilon : 0.02\n",
      "episode : 5050, reward mean : -0.17463998952507973, total_step : 201669, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5050, reward mean : -0.7564999797381461, step mean : 90.8, eval_episode_rewards : [-1.   -1.    0.09 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.96\n",
      " -1.   -1.    0.82 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5060, reward mean : -0.17497998949512839, total_step : 202386, cur_epsilon : 0.02\n",
      "episode : 5070, reward mean : -0.16587998949736357, total_step : 202951, cur_epsilon : 0.02\n",
      "episode : 5080, reward mean : -0.16142998952977358, total_step : 203309, cur_epsilon : 0.02\n",
      "episode : 5090, reward mean : -0.15837998957559465, total_step : 203697, cur_epsilon : 0.02\n",
      "episode : 5100, reward mean : -0.15623998960107566, total_step : 204182, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5100, reward mean : -0.9194999783299863, step mean : 97.0, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.61 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5110, reward mean : -0.1507199896350503, total_step : 204532, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5120, reward mean : -0.1455699896607548, total_step : 205013, cur_epsilon : 0.02\n",
      "episode : 5130, reward mean : -0.14800998967327178, total_step : 205523, cur_epsilon : 0.02\n",
      "episode : 5140, reward mean : -0.1468599896989763, total_step : 205893, cur_epsilon : 0.02\n",
      "episode : 5150, reward mean : -0.15569998965784906, total_step : 206542, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5150, reward mean : -0.7739999793469906, step mean : 92.55, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.88 -1.   -1.   -1.   -1.\n",
      " -1.    0.17 -1.    0.47 -1.   -1.   -1.   -1.  ]\n",
      "episode : 5160, reward mean : -0.14962998968176544, total_step : 207000, cur_epsilon : 0.02\n",
      "episode : 5170, reward mean : -0.15358998970501125, total_step : 207354, cur_epsilon : 0.02\n",
      "episode : 5180, reward mean : -0.15050998970679938, total_step : 207752, cur_epsilon : 0.02\n",
      "episode : 5190, reward mean : -0.14459998972713947, total_step : 208187, cur_epsilon : 0.02\n",
      "episode : 5200, reward mean : -0.14179998972266913, total_step : 208706, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5200, reward mean : -0.640499981213361, step mean : 84.25, eval_episode_rewards : [-1.    0.81  0.98 -1.   -1.   -1.   -1.   -1.    0.72 -1.   -1.   -1.\n",
      " -1.    0.68 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5210, reward mean : -0.14206998976133764, total_step : 209001, cur_epsilon : 0.02\n",
      "episode : 5220, reward mean : -0.1415399897955358, total_step : 209403, cur_epsilon : 0.02\n",
      "episode : 5230, reward mean : -0.13810998984985054, total_step : 209768, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5240, reward mean : -0.1377799899019301, total_step : 210190, cur_epsilon : 0.02\n",
      "episode : 5250, reward mean : -0.1394799899533391, total_step : 210364, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5250, reward mean : -0.914499978441745, step mean : 96.5, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.71 -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5260, reward mean : -0.1380699899625033, total_step : 210801, cur_epsilon : 0.02\n",
      "episode : 5270, reward mean : -0.14665998994931578, total_step : 211203, cur_epsilon : 0.02\n",
      "episode : 5280, reward mean : -0.1563499899338931, total_step : 211658, cur_epsilon : 0.02\n",
      "episode : 5290, reward mean : -0.15766998988203704, total_step : 212183, cur_epsilon : 0.02\n",
      "episode : 5300, reward mean : -0.15431998988986015, total_step : 212616, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5300, reward mean : -0.9464999777264893, step mean : 99.7, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.07 -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5310, reward mean : -0.1537799899019301, total_step : 213134, cur_epsilon : 0.02\n",
      "episode : 5320, reward mean : -0.14888998996652664, total_step : 213452, cur_epsilon : 0.02\n",
      "episode : 5330, reward mean : -0.15070998992584644, total_step : 214021, cur_epsilon : 0.02\n",
      "episode : 5340, reward mean : -0.15630998986773192, total_step : 214667, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5350, reward mean : -0.1638099898342043, total_step : 215112, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5350, reward mean : -0.8154999795369804, step mean : 91.65, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.87 -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.82]\n",
      "episode : 5360, reward mean : -0.15827998991310596, total_step : 215285, cur_epsilon : 0.02\n",
      "episode : 5370, reward mean : -0.15262998999468982, total_step : 215608, cur_epsilon : 0.02\n",
      "episode : 5380, reward mean : -0.1431199900507927, total_step : 215877, cur_epsilon : 0.02\n",
      "episode : 5390, reward mean : -0.14327999006956815, total_step : 216259, cur_epsilon : 0.02\n",
      "episode : 5400, reward mean : -0.13596999009884894, total_step : 216607, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5400, reward mean : -0.9024999787099659, step mean : 95.3, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.95 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5410, reward mean : -0.14047999013215304, total_step : 217039, cur_epsilon : 0.02\n",
      "episode : 5420, reward mean : -0.14107999014109374, total_step : 217313, cur_epsilon : 0.02\n",
      "episode : 5430, reward mean : -0.14256999010778965, total_step : 218030, cur_epsilon : 0.02\n",
      "episode : 5440, reward mean : -0.13647999013215303, total_step : 218348, cur_epsilon : 0.02\n",
      "episode : 5450, reward mean : -0.13564999017305673, total_step : 218606, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5450, reward mean : -0.5424999822862446, step mean : 79.5, eval_episode_rewards : [ 0.98 -1.   -1.    0.69  0.82 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.96 -1.   -1.    0.7  -1.   -1.  ]\n",
      "episode : 5460, reward mean : -0.13214999025128782, total_step : 218997, cur_epsilon : 0.02\n",
      "episode : 5470, reward mean : -0.13708999020792545, total_step : 219450, cur_epsilon : 0.02\n",
      "episode : 5480, reward mean : -0.12848999022133648, total_step : 219816, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5490, reward mean : -0.12661999030783772, total_step : 220114, cur_epsilon : 0.02\n",
      "episode : 5500, reward mean : -0.12583999034762383, total_step : 220351, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5500, reward mean : -0.5874999812804163, step mean : 84.0, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.89 -1.    0.4  -1.   -1.    0.97 -1.   -1.\n",
      " -1.    0.81 -1.   -1.   -1.   -1.    0.18 -1.  ]\n",
      "episode : 5510, reward mean : -0.12276999030448496, total_step : 220883, cur_epsilon : 0.02\n",
      "episode : 5520, reward mean : -0.1218399902805686, total_step : 221482, cur_epsilon : 0.02\n",
      "episode : 5530, reward mean : -0.1236699902396649, total_step : 222099, cur_epsilon : 0.02\n",
      "episode : 5540, reward mean : -0.12450999024324119, total_step : 222515, cur_epsilon : 0.02\n",
      "episode : 5550, reward mean : -0.1245899902638048, total_step : 222863, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5550, reward mean : -0.7454999799840152, step mean : 89.7, eval_episode_rewards : [-1.   -1.   -1.    0.96 -1.   -1.   -1.    0.54 -1.   -1.   -1.   -1.\n",
      "  0.59 -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5560, reward mean : -0.1278399902805686, total_step : 223339, cur_epsilon : 0.02\n",
      "episode : 5570, reward mean : -0.11351999033242464, total_step : 223736, cur_epsilon : 0.02\n",
      "episode : 5580, reward mean : -0.10840999033488333, total_step : 224209, cur_epsilon : 0.02\n",
      "episode : 5590, reward mean : -0.110329990291968, total_step : 224876, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5600, reward mean : -0.10425999033823609, total_step : 225180, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5600, reward mean : -0.8214999794028699, step mean : 92.25, eval_episode_rewards : [-1.    0.64 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.93\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5610, reward mean : -0.10815999034047127, total_step : 225613, cur_epsilon : 0.02\n",
      "episode : 5620, reward mean : -0.10797999034449458, total_step : 226081, cur_epsilon : 0.02\n",
      "episode : 5630, reward mean : -0.10812999038584531, total_step : 226380, cur_epsilon : 0.02\n",
      "episode : 5640, reward mean : -0.10542999035678804, total_step : 226773, cur_epsilon : 0.02\n",
      "episode : 5650, reward mean : -0.09666999041847885, total_step : 227187, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5650, reward mean : -0.6249999815598131, step mean : 82.7, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.96 -1.   -1.   -1.   -1.   -1.    0.73 -1.\n",
      "  0.83 -1.   -1.   -1.   -1.   -1.    0.98 -1.  ]\n",
      "episode : 5660, reward mean : -0.09227999042719602, total_step : 227618, cur_epsilon : 0.02\n",
      "episode : 5670, reward mean : -0.09014999043010176, total_step : 227988, cur_epsilon : 0.02\n",
      "episode : 5680, reward mean : -0.09165999041870236, total_step : 228444, cur_epsilon : 0.02\n",
      "episode : 5690, reward mean : -0.09359999039769172, total_step : 228821, cur_epsilon : 0.02\n",
      "episode : 5700, reward mean : -0.09296999038942158, total_step : 229332, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5700, reward mean : -0.38549998244270683, step mean : 78.95, eval_episode_rewards : [-1.   -1.    0.59 -1.   -1.   -1.    0.93  0.49 -1.    0.08 -1.   -1.\n",
      " -1.    0.03  0.91  0.87 -1.   -1.    0.39 -1.  ]\n",
      "episode : 5710, reward mean : -0.09457999048754573, total_step : 229634, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5720, reward mean : -0.09380999052710831, total_step : 230018, cur_epsilon : 0.02\n",
      "episode : 5730, reward mean : -0.08977999052777887, total_step : 230499, cur_epsilon : 0.02\n",
      "episode : 5740, reward mean : -0.07997999061271548, total_step : 230752, cur_epsilon : 0.02\n",
      "episode : 5750, reward mean : -0.0817999906167388, total_step : 231195, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5750, reward mean : -0.4684999817050993, step mean : 82.2, eval_episode_rewards : [ 0.23  0.79 -1.    0.3   0.95  0.09 -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.49  0.78 -1.   -1.  ]\n",
      "episode : 5760, reward mean : -0.08536999062635005, total_step : 231490, cur_epsilon : 0.02\n",
      "episode : 5770, reward mean : -0.08375999061763287, total_step : 231903, cur_epsilon : 0.02\n",
      "episode : 5780, reward mean : -0.07361999059841037, total_step : 232622, cur_epsilon : 0.02\n",
      "episode : 5790, reward mean : -0.07059999059885741, total_step : 233089, cur_epsilon : 0.02\n",
      "episode : 5800, reward mean : -0.07223999058455229, total_step : 233599, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5800, reward mean : -0.5799999814480543, step mean : 83.25, eval_episode_rewards : [-1.    0.73  0.08  0.95 -1.   -1.   -1.   -1.    0.92 -1.   -1.   -1.\n",
      "  0.72 -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5810, reward mean : -0.07083999059349298, total_step : 233931, cur_epsilon : 0.02\n",
      "episode : 5820, reward mean : -0.0702099905628711, total_step : 234502, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5830, reward mean : -0.07773999055102468, total_step : 235014, cur_epsilon : 0.02\n",
      "episode : 5840, reward mean : -0.07755999059975147, total_step : 235313, cur_epsilon : 0.02\n",
      "episode : 5850, reward mean : -0.0741499905642122, total_step : 235865, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5850, reward mean : -0.5249999826774001, step mean : 77.75, eval_episode_rewards : [ 0.92 -1.    0.85 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.86  0.99 -1.    0.88 -1.   -1.   -1.  ]\n",
      "episode : 5860, reward mean : -0.07404999058879912, total_step : 236098, cur_epsilon : 0.02\n",
      "episode : 5870, reward mean : -0.07354999062232673, total_step : 236426, cur_epsilon : 0.02\n",
      "episode : 5880, reward mean : -0.07489999061450363, total_step : 236834, cur_epsilon : 0.02\n",
      "episode : 5890, reward mean : -0.07205999061092734, total_step : 237336, cur_epsilon : 0.02\n",
      "episode : 5900, reward mean : -0.07211999063193798, total_step : 237745, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5900, reward mean : -0.2594999852590263, step mean : 66.35, eval_episode_rewards : [-1.    0.7  -1.    0.97 -1.   -1.    0.97 -1.    0.78  0.82 -1.   -1.\n",
      "  0.99  0.88 -1.    0.7  -1.   -1.   -1.   -1.  ]\n",
      "episode : 5910, reward mean : -0.07551999062299729, total_step : 238226, cur_epsilon : 0.02\n",
      "episode : 5920, reward mean : -0.07640999060310423, total_step : 238739, cur_epsilon : 0.02\n",
      "episode : 5930, reward mean : -0.07540999060310423, total_step : 239161, cur_epsilon : 0.02\n",
      "episode : 5940, reward mean : -0.07228999062813818, total_step : 239638, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 5950, reward mean : -0.07093999063596129, total_step : 240117, cur_epsilon : 0.02\n",
      "[EVAL] episode : 5950, reward mean : -0.7344999802298844, step mean : 88.6, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.77 -1.   -1.   -1.   -1.   -1.    0.7  -1.\n",
      " -1.   -1.    0.84 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 5960, reward mean : -0.07270999057404696, total_step : 240709, cur_epsilon : 0.02\n",
      "episode : 5970, reward mean : -0.07655999057739973, total_step : 241138, cur_epsilon : 0.02\n",
      "episode : 5980, reward mean : -0.07638999058119952, total_step : 241525, cur_epsilon : 0.02\n",
      "episode : 5990, reward mean : -0.07172999064065516, total_step : 241928, cur_epsilon : 0.02\n",
      "episode : 6000, reward mean : -0.0748799906373024, total_step : 242513, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6000, reward mean : -0.48299998249858617, step mean : 78.6, eval_episode_rewards : [ 0.96 -1.   -1.    0.81 -1.   -1.   -1.   -1.    0.21 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.48 -1.    0.88  1.  ]\n",
      "episode : 6010, reward mean : -0.07102999063394964, total_step : 243030, cur_epsilon : 0.02\n",
      "episode : 6020, reward mean : -0.07441999062523245, total_step : 243351, cur_epsilon : 0.02\n",
      "episode : 6030, reward mean : -0.07574999061785638, total_step : 243755, cur_epsilon : 0.02\n",
      "episode : 6040, reward mean : -0.07125999067351221, total_step : 244003, cur_epsilon : 0.02\n",
      "episode : 6050, reward mean : -0.07454999064467847, total_step : 244384, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6050, reward mean : -0.45699998307973144, step mean : 76.0, eval_episode_rewards : [ 0.51 -1.   -1.   -1.   -1.   -1.    0.98 -1.   -1.    0.57 -1.    0.94\n",
      " -1.   -1.   -1.    0.92  0.94 -1.   -1.   -1.  ]\n",
      "episode : 6060, reward mean : -0.07513999067619442, total_step : 244962, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 6070, reward mean : -0.08238999064825475, total_step : 245650, cur_epsilon : 0.02\n",
      "episode : 6080, reward mean : -0.08212999063171446, total_step : 246083, cur_epsilon : 0.02\n",
      "episode : 6090, reward mean : -0.08360999062098563, total_step : 246520, cur_epsilon : 0.02\n",
      "episode : 6100, reward mean : -0.07920999062992633, total_step : 246965, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6100, reward mean : -0.4889999823644757, step mean : 79.2, eval_episode_rewards : [-1.    0.42  0.92  0.88 -1.   -1.   -1.    0.23 -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.93 -1.   -1.   -1.    0.84 -1.  ]\n",
      "episode : 6110, reward mean : -0.07778999059461057, total_step : 247474, cur_epsilon : 0.02\n",
      "episode : 6120, reward mean : -0.07683999061584473, total_step : 247862, cur_epsilon : 0.02\n",
      "episode : 6130, reward mean : -0.06479999068379402, total_step : 248069, cur_epsilon : 0.02\n",
      "episode : 6140, reward mean : -0.06397999065741897, total_step : 248557, cur_epsilon : 0.02\n",
      "episode : 6150, reward mean : -0.05932999069429934, total_step : 249040, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6150, reward mean : -0.6504999809898436, step mean : 85.25, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.74 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.    1.    0.33  0.92]\n",
      "episode : 6160, reward mean : -0.05975999068468809, total_step : 249541, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 6170, reward mean : -0.05384999066032469, total_step : 250003, cur_epsilon : 0.02\n",
      "episode : 6180, reward mean : -0.05156999068893492, total_step : 250276, cur_epsilon : 0.02\n",
      "episode : 6190, reward mean : -0.05317999067530036, total_step : 250771, cur_epsilon : 0.02\n",
      "episode : 6200, reward mean : -0.052999990701675416, total_step : 251173, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6200, reward mean : -0.06799998730421067, step mean : 57.3, eval_episode_rewards : [-1.    0.88 -1.   -1.   -1.    0.98  0.63 -1.    0.98  0.98 -1.    0.91\n",
      "  0.55 -1.   -1.   -1.    0.92  0.88 -1.    0.93]\n",
      "episode : 6210, reward mean : -0.0438599907271564, total_step : 251354, cur_epsilon : 0.02\n",
      "episode : 6220, reward mean : -0.03941999075934291, total_step : 251613, cur_epsilon : 0.02\n",
      "episode : 6230, reward mean : -0.0335899908002466, total_step : 251795, cur_epsilon : 0.02\n",
      "episode : 6240, reward mean : -0.030479990758001806, total_step : 252403, cur_epsilon : 0.02\n",
      "episode : 6250, reward mean : -0.028349990716204046, total_step : 252762, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6250, reward mean : -0.3679999839514494, step mean : 72.15, eval_episode_rewards : [-1.    0.86 -1.   -1.   -1.    0.95  0.95  0.55 -1.   -1.    0.88 -1.\n",
      " -1.    0.47  0.98 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 6260, reward mean : -0.020509990757331252, total_step : 253017, cur_epsilon : 0.02\n",
      "episode : 6270, reward mean : -0.017659990776330233, total_step : 253334, cur_epsilon : 0.02\n",
      "episode : 6280, reward mean : -0.008639990776777268, total_step : 253788, cur_epsilon : 0.02\n",
      "episode : 6290, reward mean : -0.005939990792423487, total_step : 254241, cur_epsilon : 0.02\n",
      "episode : 6300, reward mean : -0.0068299908172339205, total_step : 254563, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6300, reward mean : -0.2164999851025641, step mean : 67.1, eval_episode_rewards : [ 0.69  0.32  0.91 -1.   -1.   -1.    0.99 -1.    0.96 -1.    0.06 -1.\n",
      " -1.   -1.   -1.    0.8   0.97  0.97 -1.   -1.  ]\n",
      "episode : 6310, reward mean : -0.014389990849420428, total_step : 254938, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 6320, reward mean : -0.01528999080695212, total_step : 255445, cur_epsilon : 0.02\n",
      "episode : 6330, reward mean : -0.013189990853890776, total_step : 255806, cur_epsilon : 0.02\n",
      "episode : 6340, reward mean : -0.007589990912005305, total_step : 256195, cur_epsilon : 0.02\n",
      "episode : 6350, reward mean : -0.0029399909265339376, total_step : 256577, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6350, reward mean : -0.5194999816827476, step mean : 82.25, eval_episode_rewards : [ 0.8   0.3  -1.   -1.   -1.    0.66 -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.84 -1.   -1.    0.96 -1.   -1.    0.05 -1.  ]\n",
      "episode : 6360, reward mean : -0.0023799908719956877, total_step : 256991, cur_epsilon : 0.02\n",
      "episode : 6370, reward mean : -0.009999990791082382, total_step : 257674, cur_epsilon : 0.02\n",
      "episode : 6380, reward mean : -0.015439990781247616, total_step : 257988, cur_epsilon : 0.02\n",
      "episode : 6390, reward mean : -0.013619990799576044, total_step : 258289, cur_epsilon : 0.02\n",
      "episode : 6400, reward mean : -0.010439990803599357, total_step : 258618, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6400, reward mean : -0.3349999846890569, step mean : 68.85, eval_episode_rewards : [-1.   -1.   -1.    0.93 -1.   -1.    0.98 -1.   -1.    0.89 -1.   -1.\n",
      " -1.   -1.    0.92 -1.    1.    0.64  0.94 -1.  ]\n",
      "episode : 6410, reward mean : -0.0018299908172339202, total_step : 258989, cur_epsilon : 0.02\n",
      "episode : 6420, reward mean : -0.0057699907515197995, total_step : 259554, cur_epsilon : 0.02\n",
      "episode : 6430, reward mean : 0.002920009143650532, total_step : 259806, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 6440, reward mean : 0.001860009167343378, total_step : 260228, cur_epsilon : 0.02\n",
      "episode : 6450, reward mean : -0.0001599907875061035, total_step : 260688, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6450, reward mean : -0.38849998349323867, step mean : 74.2, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.98  1.   -1.    0.52  0.97  0.96 -1.    0.69\n",
      " -1.    0.11 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 6460, reward mean : 0.0008600091673433781, total_step : 260878, cur_epsilon : 0.02\n",
      "episode : 6470, reward mean : 0.005100009139627219, total_step : 261206, cur_epsilon : 0.02\n",
      "episode : 6480, reward mean : 0.00323000911436975, total_step : 261460, cur_epsilon : 0.02\n",
      "episode : 6490, reward mean : 0.009100009117275477, total_step : 261771, cur_epsilon : 0.02\n",
      "episode : 6500, reward mean : 0.012750009125098586, total_step : 262043, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6500, reward mean : -0.281499984767288, step mean : 68.55, eval_episode_rewards : [ 0.44 -1.   -1.   -1.   -1.   -1.   -1.    0.98 -1.   -1.    0.89  0.83\n",
      "  0.9  -1.    0.77  0.94  0.62 -1.   -1.   -1.  ]\n",
      "episode : 6510, reward mean : 0.011310009045526386, total_step : 262222, cur_epsilon : 0.02\n",
      "episode : 6520, reward mean : 0.013170009003952146, total_step : 262635, cur_epsilon : 0.02\n",
      "episode : 6530, reward mean : 0.02220000895857811, total_step : 263050, cur_epsilon : 0.02\n",
      "episode : 6540, reward mean : 0.019540008973330258, total_step : 263530, cur_epsilon : 0.02\n",
      "episode : 6550, reward mean : 0.024860008988529445, total_step : 263946, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6550, reward mean : -0.16399998627603055, step mean : 61.85, eval_episode_rewards : [ 0.99 -1.   -1.   -1.    0.99  0.99  0.95  0.67  0.68  0.75 -1.   -1.\n",
      " -1.   -1.   -1.    0.86  0.84 -1.   -1.   -1.  ]\n",
      "episode : 6560, reward mean : 0.032910008942708374, total_step : 264219, cur_epsilon : 0.02\n",
      "episode : 6570, reward mean : 0.030910008942708372, total_step : 264618, cur_epsilon : 0.02\n",
      "episode : 6580, reward mean : 0.03517000891454518, total_step : 264966, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 6590, reward mean : 0.037540008883923295, total_step : 265497, cur_epsilon : 0.02\n",
      "episode : 6600, reward mean : 0.03304000896215439, total_step : 266146, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6600, reward mean : -0.39999998435378076, step mean : 70.4, eval_episode_rewards : [ 0.86 -1.   -1.    0.96 -1.   -1.    1.    0.38 -1.   -1.   -1.   -1.11\n",
      " -1.   -1.    0.85 -1.   -1.    0.93  0.13 -1.  ]\n",
      "episode : 6610, reward mean : 0.0370600089840591, total_step : 266674, cur_epsilon : 0.02\n",
      "episode : 6620, reward mean : 0.03620000898092985, total_step : 267129, cur_epsilon : 0.02\n",
      "episode : 6630, reward mean : 0.039160008981823924, total_step : 267431, cur_epsilon : 0.02\n",
      "episode : 6640, reward mean : 0.037720008991658685, total_step : 267867, cur_epsilon : 0.02\n",
      "episode : 6650, reward mean : 0.036280008979141715, total_step : 268225, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6650, reward mean : -0.16199998632073404, step mean : 61.65, eval_episode_rewards : [ 0.92  0.96  0.82  0.78 -1.   -1.   -1.    0.8  -1.   -1.   -1.   -1.\n",
      " -1.    0.99  0.92 -1.   -1.   -1.    0.77  0.8 ]\n",
      "episode : 6660, reward mean : 0.03536000897735357, total_step : 268647, cur_epsilon : 0.02\n",
      "episode : 6670, reward mean : 0.028680008947849275, total_step : 268885, cur_epsilon : 0.02\n",
      "episode : 6680, reward mean : 0.0337900089006871, total_step : 269131, cur_epsilon : 0.02\n",
      "episode : 6690, reward mean : 0.032640008881688116, total_step : 269423, cur_epsilon : 0.02\n",
      "episode : 6700, reward mean : 0.038390008820220825, total_step : 269662, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6700, reward mean : -0.40999998301267626, step mean : 76.35, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.95  0.77 -1.    0.95  0.6\n",
      " -1.   -1.   -1.    0.3   0.95  0.28 -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 6710, reward mean : 0.0451400088481605, total_step : 270087, cur_epsilon : 0.02\n",
      "episode : 6720, reward mean : 0.051490008840337394, total_step : 270436, cur_epsilon : 0.02\n",
      "episode : 6730, reward mean : 0.05036000882089138, total_step : 270832, cur_epsilon : 0.02\n",
      "episode : 6740, reward mean : 0.04556000883877277, total_step : 271163, cur_epsilon : 0.02\n",
      "episode : 6750, reward mean : 0.04490000885352492, total_step : 271672, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6750, reward mean : 0.061500012036412956, step mean : 54.45, eval_episode_rewards : [-1.   -1.    0.71  0.95  0.97  0.97 -1.   -1.    0.93 -1.    0.46  0.91\n",
      " -1.    0.32  0.97  0.08  0.97  0.99 -1.   -1.  ]\n",
      "episode : 6760, reward mean : 0.045450008841231467, total_step : 271912, cur_epsilon : 0.02\n",
      "episode : 6770, reward mean : 0.0498100088108331, total_step : 272192, cur_epsilon : 0.02\n",
      "episode : 6780, reward mean : 0.05679000869952142, total_step : 272417, cur_epsilon : 0.02\n",
      "episode : 6790, reward mean : 0.05442000870779157, total_step : 272919, cur_epsilon : 0.02\n",
      "episode : 6800, reward mean : 0.06657000865973532, total_step : 273216, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6800, reward mean : -0.5844999813474715, step mean : 83.7, eval_episode_rewards : [-1.    0.46 -1.   -1.   -1.   -1.   -1.    0.23  0.98  0.87 -1.   -1.\n",
      " -1.   -1.    0.77 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 6810, reward mean : 0.07083000863157213, total_step : 273423, cur_epsilon : 0.02\n",
      "episode : 6820, reward mean : 0.07351000857166946, total_step : 273728, cur_epsilon : 0.02\n",
      "episode : 6830, reward mean : 0.0816900085452944, total_step : 274123, cur_epsilon : 0.02\n",
      "episode : 6840, reward mean : 0.08326000857725739, total_step : 274566, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 6850, reward mean : 0.07477000858820974, total_step : 275167, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6850, reward mean : -0.339499984588474, step mean : 69.3, eval_episode_rewards : [ 0.66 -1.   -1.    0.98 -1.   -1.    0.92  0.93 -1.   -1.    0.99 -1.\n",
      "  0.76 -1.   -1.   -1.   -1.    0.97 -1.   -1.  ]\n",
      "episode : 6860, reward mean : 0.07060000861436129, total_step : 275516, cur_epsilon : 0.02\n",
      "episode : 6870, reward mean : 0.0699000086300075, total_step : 275914, cur_epsilon : 0.02\n",
      "episode : 6880, reward mean : 0.07223000860027969, total_step : 276190, cur_epsilon : 0.02\n",
      "episode : 6890, reward mean : 0.07244000857323409, total_step : 276570, cur_epsilon : 0.02\n",
      "episode : 6900, reward mean : 0.07248000854998828, total_step : 276876, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6900, reward mean : 0.15800001099705696, step mean : 49.85, eval_episode_rewards : [ 0.98  0.86 -1.   -1.   -1.    0.9   0.69  0.92 -1.    0.99 -1.    0.2\n",
      " -1.   -1.    0.94  0.49  0.97  0.95  0.61  0.66]\n",
      "episode : 6910, reward mean : 0.06752000852674246, total_step : 277253, cur_epsilon : 0.02\n",
      "episode : 6920, reward mean : 0.06550000850483775, total_step : 277669, cur_epsilon : 0.02\n",
      "episode : 6930, reward mean : 0.06890000851824879, total_step : 278151, cur_epsilon : 0.02\n",
      "episode : 6940, reward mean : 0.06777000849880278, total_step : 278543, cur_epsilon : 0.02\n",
      "episode : 6950, reward mean : 0.06984000845253467, total_step : 278817, cur_epsilon : 0.02\n",
      "[EVAL] episode : 6950, reward mean : -0.0004999876953661441, step mean : 55.6, eval_episode_rewards : [ 1.    0.83  0.91  0.97  0.03 -1.    0.86 -1.   -1.   -1.   -1.    0.49\n",
      "  0.93  0.99  0.99 -1.    0.99 -1.   -1.   -1.  ]\n",
      "episode : 6960, reward mean : 0.07252000841498375, total_step : 279242, cur_epsilon : 0.02\n",
      "episode : 6970, reward mean : 0.07988000838458538, total_step : 279537, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 6980, reward mean : 0.08388000840693713, total_step : 280023, cur_epsilon : 0.02\n",
      "episode : 6990, reward mean : 0.07910000842437148, total_step : 280504, cur_epsilon : 0.02\n",
      "episode : 7000, reward mean : 0.0886500083450228, total_step : 280736, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7000, reward mean : -0.05699998643249273, step mean : 61.25, eval_episode_rewards : [ 0.96  0.88 -1.   -1.   -1.   -1.    0.9   0.95 -1.   -1.    0.81 -1.\n",
      " -1.   -1.    0.17  0.94  0.14  0.65  0.67  0.79]\n",
      "episode : 7010, reward mean : 0.08918000833317638, total_step : 281202, cur_epsilon : 0.02\n",
      "episode : 7020, reward mean : 0.09289000833965838, total_step : 281552, cur_epsilon : 0.02\n",
      "episode : 7030, reward mean : 0.09023000833205878, total_step : 281921, cur_epsilon : 0.02\n",
      "episode : 7040, reward mean : 0.08295000838302076, total_step : 282396, cur_epsilon : 0.02\n",
      "episode : 7050, reward mean : 0.08564000836759805, total_step : 282708, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7050, reward mean : -0.17149998610839248, step mean : 62.6, eval_episode_rewards : [ 0.94  0.81  0.56  0.54 -1.   -1.   -1.   -1.   -1.   -1.    0.95  0.92\n",
      " -1.    0.99 -1.   -1.   -1.   -1.    0.91  0.95]\n",
      "episode : 7060, reward mean : 0.09099000833742321, total_step : 283153, cur_epsilon : 0.02\n",
      "episode : 7070, reward mean : 0.09410000831261277, total_step : 283732, cur_epsilon : 0.02\n",
      "episode : 7080, reward mean : 0.08610000833496452, total_step : 284264, cur_epsilon : 0.02\n",
      "episode : 7090, reward mean : 0.08682000831887125, total_step : 284629, cur_epsilon : 0.02\n",
      "episode : 7100, reward mean : 0.08904000829160214, total_step : 284953, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7100, reward mean : -0.15299998540431262, step mean : 65.8, eval_episode_rewards : [ 0.87  0.42 -1.   -1.   -1.   -1.   -1.    0.75  0.9   0.86  0.78 -1.\n",
      " -1.   -1.    0.94 -1.   -1.    0.33  0.91  0.18]\n",
      "synced target net\n",
      "episode : 7110, reward mean : 0.08570000825449825, total_step : 285295, cur_epsilon : 0.02\n",
      "episode : 7120, reward mean : 0.08532000821828842, total_step : 285521, cur_epsilon : 0.02\n",
      "episode : 7130, reward mean : 0.07703000822477042, total_step : 285757, cur_epsilon : 0.02\n",
      "episode : 7140, reward mean : 0.0793100081961602, total_step : 286116, cur_epsilon : 0.02\n",
      "episode : 7150, reward mean : 0.08333000817336142, total_step : 286500, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7150, reward mean : -0.3834999836049974, step mean : 73.7, eval_episode_rewards : [-1.    0.94  0.78 -1.   -1.   -1.    0.92 -1.    1.   -1.   -1.    0.49\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.86  0.34]\n",
      "episode : 7160, reward mean : 0.08862000814452767, total_step : 286874, cur_epsilon : 0.02\n",
      "episode : 7170, reward mean : 0.09314000811055302, total_step : 287185, cur_epsilon : 0.02\n",
      "episode : 7180, reward mean : 0.0928300081398338, total_step : 287588, cur_epsilon : 0.02\n",
      "episode : 7190, reward mean : 0.08974000814184546, total_step : 288091, cur_epsilon : 0.02\n",
      "episode : 7200, reward mean : 0.09164000816643238, total_step : 288602, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7200, reward mean : -0.053499987628310916, step mean : 55.85, eval_episode_rewards : [-1.    0.96  0.94  0.85  0.55  0.95 -1.   -1.    0.88  0.98 -1.    0.98\n",
      "  0.87 -1.   -1.   -1.   -1.    0.97 -1.   -1.  ]\n",
      "episode : 7210, reward mean : 0.08537000821717083, total_step : 289010, cur_epsilon : 0.02\n",
      "episode : 7220, reward mean : 0.0861900081988424, total_step : 289187, cur_epsilon : 0.02\n",
      "episode : 7230, reward mean : 0.07828000826388598, total_step : 289659, cur_epsilon : 0.02\n",
      "episode : 7240, reward mean : 0.08074000818654894, total_step : 289924, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 7250, reward mean : 0.07241000826098025, total_step : 290613, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7250, reward mean : 0.19450001018121837, step mean : 46.2, eval_episode_rewards : [-1.   -1.    0.94 -1.    0.36  0.93  0.91  0.92  0.87  0.79 -1.    0.57\n",
      " -1.   -1.    0.79  0.96  0.92  0.95 -1.    0.98]\n",
      "episode : 7260, reward mean : 0.07010000829026103, total_step : 290998, cur_epsilon : 0.02\n",
      "episode : 7270, reward mean : 0.07141000830568373, total_step : 291384, cur_epsilon : 0.02\n",
      "episode : 7280, reward mean : 0.06698000833764672, total_step : 291980, cur_epsilon : 0.02\n",
      "episode : 7290, reward mean : 0.0679100083168596, total_step : 292342, cur_epsilon : 0.02\n",
      "episode : 7300, reward mean : 0.07046000834926963, total_step : 292809, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7300, reward mean : -0.12749998485669495, step mean : 68.3, eval_episode_rewards : [-1.    0.03  0.97  0.94 -1.   -1.   -1.    0.95 -1.    0.96  0.96  0.01\n",
      " -1.    0.37 -1.    0.67 -1.   -1.    0.02  0.57]\n",
      "episode : 7310, reward mean : 0.080110008334741, total_step : 293120, cur_epsilon : 0.02\n",
      "episode : 7320, reward mean : 0.0826000083014369, total_step : 293479, cur_epsilon : 0.02\n",
      "episode : 7330, reward mean : 0.0819100083168596, total_step : 293909, cur_epsilon : 0.02\n",
      "episode : 7340, reward mean : 0.08187000829540193, total_step : 294201, cur_epsilon : 0.02\n",
      "episode : 7350, reward mean : 0.08700000827014447, total_step : 294470, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7350, reward mean : -0.12099998611956835, step mean : 62.6, eval_episode_rewards : [-1.   -1.    0.47 -1.    0.95  0.07  0.97 -1.   -1.    0.9  -1.    0.94\n",
      "  0.91 -1.    0.96 -1.    0.61 -1.    0.8  -1.  ]\n",
      "episode : 7360, reward mean : 0.08526000824198127, total_step : 294761, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 7370, reward mean : 0.09187000816129148, total_step : 295084, cur_epsilon : 0.02\n",
      "episode : 7380, reward mean : 0.08558000816777349, total_step : 295427, cur_epsilon : 0.02\n",
      "episode : 7390, reward mean : 0.08821000819839538, total_step : 295865, cur_epsilon : 0.02\n",
      "episode : 7400, reward mean : 0.08558000819012522, total_step : 296158, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7400, reward mean : -0.2874999835155904, step mean : 74.2, eval_episode_rewards : [-1.   -1.    0.59 -1.    0.78  0.94 -1.   -1.    0.72 -1.    0.86  0.27\n",
      " -1.    0.19  0.75 -1.   -1.    0.15 -1.   -1.  ]\n",
      "episode : 7410, reward mean : 0.08787000816129148, total_step : 296401, cur_epsilon : 0.02\n",
      "episode : 7420, reward mean : 0.09792000809311867, total_step : 296664, cur_epsilon : 0.02\n",
      "episode : 7430, reward mean : 0.0903200081512332, total_step : 297175, cur_epsilon : 0.02\n",
      "episode : 7440, reward mean : 0.09621000813134015, total_step : 297509, cur_epsilon : 0.02\n",
      "episode : 7450, reward mean : 0.10163000814430416, total_step : 298027, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7450, reward mean : -0.4919999822974205, step mean : 79.5, eval_episode_rewards : [ 0.35 -1.    0.81 -1.    0.42 -1.   -1.   -1.    0.95 -1.   -1.   -1.\n",
      " -1.    0.95 -1.   -1.   -1.   -1.   -1.    0.68]\n",
      "episode : 7460, reward mean : 0.0963400081731379, total_step : 298346, cur_epsilon : 0.02\n",
      "episode : 7470, reward mean : 0.09321000815369189, total_step : 298587, cur_epsilon : 0.02\n",
      "episode : 7480, reward mean : 0.0916500081885606, total_step : 298995, cur_epsilon : 0.02\n",
      "episode : 7490, reward mean : 0.08471000823192298, total_step : 299499, cur_epsilon : 0.02\n",
      "episode : 7500, reward mean : 0.08751000821404159, total_step : 299691, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7500, reward mean : -0.1909999856725335, step mean : 64.55, eval_episode_rewards : [-1.   -1.    0.75 -1.    0.98 -1.   -1.    0.98  0.94  0.01 -1.    0.97\n",
      " -1.    0.59 -1.   -1.   -1.   -1.    0.98  0.98]\n",
      "synced target net\n",
      "episode : 7510, reward mean : 0.0898800082951784, total_step : 300231, cur_epsilon : 0.02\n",
      "episode : 7520, reward mean : 0.08793000833876431, total_step : 300839, cur_epsilon : 0.02\n",
      "episode : 7530, reward mean : 0.07713000837899744, total_step : 301431, cur_epsilon : 0.02\n",
      "episode : 7540, reward mean : 0.08370000834390522, total_step : 301757, cur_epsilon : 0.02\n",
      "episode : 7550, reward mean : 0.08426000830903649, total_step : 302018, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7550, reward mean : 0.010000012069940566, step mean : 54.55, eval_episode_rewards : [-1.    0.99 -1.    0.7   0.96 -1.    0.46  0.99 -1.    0.87 -1.   -1.\n",
      " -1.    0.85  0.94  0.62 -1.    0.85  0.97 -1.  ]\n",
      "episode : 7560, reward mean : 0.083110008334741, total_step : 302404, cur_epsilon : 0.02\n",
      "episode : 7570, reward mean : 0.0809100083168596, total_step : 302722, cur_epsilon : 0.02\n",
      "episode : 7580, reward mean : 0.08390000833943487, total_step : 303171, cur_epsilon : 0.02\n",
      "episode : 7590, reward mean : 0.08824000833183528, total_step : 303670, cur_epsilon : 0.02\n",
      "episode : 7600, reward mean : 0.09287000827305018, total_step : 304060, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7600, reward mean : 0.21100000981241465, step mean : 44.55, eval_episode_rewards : [ 0.93  0.89  0.98  0.97 -1.    0.25 -1.    0.98  0.96  0.94 -1.    0.59\n",
      "  0.97 -1.    0.98 -1.   -1.   -1.    0.85  0.93]\n",
      "episode : 7610, reward mean : 0.08772000829875469, total_step : 304703, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 7620, reward mean : 0.0890200083144009, total_step : 305230, cur_epsilon : 0.02\n",
      "episode : 7630, reward mean : 0.0834200083501637, total_step : 305692, cur_epsilon : 0.02\n",
      "episode : 7640, reward mean : 0.08506000833585858, total_step : 306065, cur_epsilon : 0.02\n",
      "episode : 7650, reward mean : 0.07931000833027065, total_step : 306398, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7650, reward mean : -0.18499998468905687, step mean : 69.0, eval_episode_rewards : [ 0.15 -1.    0.98 -1.    0.86 -1.    1.    0.14  0.89  0.1   0.64 -1.\n",
      "  0.98 -1.   -1.   -1.   -1.   -1.    0.56 -1.  ]\n",
      "episode : 7660, reward mean : 0.08250000830367207, total_step : 306703, cur_epsilon : 0.02\n",
      "episode : 7670, reward mean : 0.08377000838704407, total_step : 307313, cur_epsilon : 0.02\n",
      "episode : 7680, reward mean : 0.0816900084335357, total_step : 307765, cur_epsilon : 0.02\n",
      "episode : 7690, reward mean : 0.08581000845320523, total_step : 308146, cur_epsilon : 0.02\n",
      "episode : 7700, reward mean : 0.08318000846728682, total_step : 308448, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7700, reward mean : 0.08100001160055399, step mean : 52.5, eval_episode_rewards : [ 0.95  0.99 -1.   -1.    0.71 -1.   -1.   -1.    0.98  0.95 -1.   -1.\n",
      "  0.96  0.77  0.52  0.42  0.51  0.96 -1.    0.9 ]\n",
      "episode : 7710, reward mean : 0.08101000844873488, total_step : 308791, cur_epsilon : 0.02\n",
      "episode : 7720, reward mean : 0.08325000844337047, total_step : 309116, cur_epsilon : 0.02\n",
      "episode : 7730, reward mean : 0.07796000840514898, total_step : 309342, cur_epsilon : 0.02\n",
      "episode : 7740, reward mean : 0.07858000843599439, total_step : 309813, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 7750, reward mean : 0.08534000837430358, total_step : 310048, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7750, reward mean : -0.331499984767288, step mean : 68.6, eval_episode_rewards : [-1.    0.72 -1.    0.92 -1.   -1.   -1.    0.63 -1.    0.81 -1.   -1.\n",
      "  0.64  0.53 -1.   -1.34  0.89  0.57 -1.   -1.  ]\n",
      "episode : 7760, reward mean : 0.08379000840894878, total_step : 310441, cur_epsilon : 0.02\n",
      "episode : 7770, reward mean : 0.0768800084516406, total_step : 310909, cur_epsilon : 0.02\n",
      "episode : 7780, reward mean : 0.07253000848181546, total_step : 311268, cur_epsilon : 0.02\n",
      "episode : 7790, reward mean : 0.07252000845968723, total_step : 311672, cur_epsilon : 0.02\n",
      "episode : 7800, reward mean : 0.06524000846594571, total_step : 311996, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7800, reward mean : 0.30800000876188277, step mean : 39.9, eval_episode_rewards : [-1.    0.99  0.52 -1.    0.98 -1.    0.98 -1.    0.61  0.74  0.99 -1.\n",
      "  0.93  0.9   0.95  0.98  0.88 -1.    0.79  0.92]\n",
      "episode : 7810, reward mean : 0.058050008514896036, total_step : 312419, cur_epsilon : 0.02\n",
      "episode : 7820, reward mean : 0.054220008555799726, total_step : 312905, cur_epsilon : 0.02\n",
      "episode : 7830, reward mean : 0.05050000854954124, total_step : 313272, cur_epsilon : 0.02\n",
      "episode : 7840, reward mean : 0.05011000851355493, total_step : 313554, cur_epsilon : 0.02\n",
      "episode : 7850, reward mean : 0.05294000847265124, total_step : 313973, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7850, reward mean : 0.14350001132115722, step mean : 51.3, eval_episode_rewards : [-1.   -1.   -1.    0.86  0.81 -1.    0.27  0.9   0.94 -1.    0.98  0.78\n",
      "  0.19  0.6  -1.    0.96  0.84  0.74  1.   -1.  ]\n",
      "episode : 7860, reward mean : 0.04975000852160156, total_step : 314538, cur_epsilon : 0.02\n",
      "episode : 7870, reward mean : 0.04762000852450728, total_step : 314949, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 7880, reward mean : 0.047280008554458616, total_step : 315358, cur_epsilon : 0.02\n",
      "episode : 7890, reward mean : 0.049590008547529575, total_step : 315709, cur_epsilon : 0.02\n",
      "episode : 7900, reward mean : 0.050140008557587865, total_step : 316059, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7900, reward mean : 0.017000011913478375, step mean : 53.85, eval_episode_rewards : [-1.    0.89  0.88  0.75 -1.    0.78  0.96  0.82 -1.    0.94 -1.   -1.\n",
      " -1.   -1.   -1.    0.7   0.81  0.91 -1.    0.9 ]\n",
      "episode : 7910, reward mean : 0.058110008580610155, total_step : 316540, cur_epsilon : 0.02\n",
      "episode : 7920, reward mean : 0.054000008583068845, total_step : 316967, cur_epsilon : 0.02\n",
      "episode : 7930, reward mean : 0.04973000852204859, total_step : 317177, cur_epsilon : 0.02\n",
      "episode : 7940, reward mean : 0.05057000850327313, total_step : 317485, cur_epsilon : 0.02\n",
      "episode : 7950, reward mean : 0.05573000849969685, total_step : 317742, cur_epsilon : 0.02\n",
      "[EVAL] episode : 7950, reward mean : 0.2910000102594495, step mean : 46.65, eval_episode_rewards : [ 0.93 -1.   -1.    0.56 -1.    0.83 -1.    0.49  0.91  0.04  0.99  0.7\n",
      "  0.88 -1.    0.85  0.95  0.65  0.83  0.29  0.92]\n",
      "episode : 7960, reward mean : 0.054490008549764754, total_step : 318390, cur_epsilon : 0.02\n",
      "episode : 7970, reward mean : 0.0529200085401535, total_step : 318642, cur_epsilon : 0.02\n",
      "episode : 7980, reward mean : 0.05018000851199031, total_step : 319002, cur_epsilon : 0.02\n",
      "episode : 7990, reward mean : 0.05222000851109624, total_step : 319479, cur_epsilon : 0.02\n",
      "episode : 8000, reward mean : 0.049390008551999924, total_step : 319893, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8000, reward mean : 0.02350001288577914, step mean : 58.25, eval_episode_rewards : [ 0.29  0.29  0.85  0.97  0.51 -1.    0.82  0.97 -1.   -1.   -1.   -1.\n",
      "  0.65  0.96 -1.   -1.    0.98  0.97 -1.    0.21]\n",
      "synced target net\n",
      "episode : 8010, reward mean : 0.04595000856183469, total_step : 320402, cur_epsilon : 0.02\n",
      "episode : 8020, reward mean : 0.04135000857524574, total_step : 320812, cur_epsilon : 0.02\n",
      "episode : 8030, reward mean : 0.04435000859759748, total_step : 321281, cur_epsilon : 0.02\n",
      "episode : 8040, reward mean : 0.04645000861771405, total_step : 321845, cur_epsilon : 0.02\n",
      "episode : 8050, reward mean : 0.04156000865995884, total_step : 322343, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8050, reward mean : 0.48350000819191336, step mean : 37.5, eval_episode_rewards : [ 0.89  0.71  0.48 -1.    0.96  0.07  0.96  0.27 -1.    0.99  0.75  0.65\n",
      "  0.83  0.56  0.83 -1.    1.    0.97  0.83  0.92]\n",
      "episode : 8060, reward mean : 0.042090008670464155, total_step : 322832, cur_epsilon : 0.02\n",
      "episode : 8070, reward mean : 0.04338000864163041, total_step : 323282, cur_epsilon : 0.02\n",
      "episode : 8080, reward mean : 0.05130000862106681, total_step : 323723, cur_epsilon : 0.02\n",
      "episode : 8090, reward mean : 0.0544200086183846, total_step : 324077, cur_epsilon : 0.02\n",
      "episode : 8100, reward mean : 0.050980008628219366, total_step : 324444, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8100, reward mean : -0.34349998449906705, step mean : 69.7, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.49 -1.    0.91 -1.   -1.    0.91  0.97 -1.\n",
      " -1.    0.93  0.97  0.95 -1.   -1.   -1.   -1.  ]\n",
      "episode : 8110, reward mean : 0.05543000861816108, total_step : 324741, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 8120, reward mean : 0.053920008674263956, total_step : 325217, cur_epsilon : 0.02\n",
      "episode : 8130, reward mean : 0.06047000870667398, total_step : 325598, cur_epsilon : 0.02\n",
      "episode : 8140, reward mean : 0.058000008694827555, total_step : 325905, cur_epsilon : 0.02\n",
      "episode : 8150, reward mean : 0.05850000868365168, total_step : 326239, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8150, reward mean : -0.07599998712539673, step mean : 58.1, eval_episode_rewards : [ 1.    0.96  0.95 -1.   -1.   -1.   -1.    0.55 -1.   -1.    0.95 -1.\n",
      "  0.7   0.65  0.77 -1.   -1.   -1.    0.98  0.97]\n",
      "episode : 8160, reward mean : 0.05519000864587724, total_step : 326445, cur_epsilon : 0.02\n",
      "episode : 8170, reward mean : 0.05602000867202878, total_step : 326875, cur_epsilon : 0.02\n",
      "episode : 8180, reward mean : 0.05796000867336988, total_step : 327284, cur_epsilon : 0.02\n",
      "episode : 8190, reward mean : 0.06203000862710178, total_step : 327582, cur_epsilon : 0.02\n",
      "episode : 8200, reward mean : 0.06461000859178603, total_step : 327936, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8200, reward mean : 0.044500012416392565, step mean : 56.15, eval_episode_rewards : [ 0.91 -1.    0.99  0.91 -1.    0.21  0.99 -1.   -1.    0.95  0.73  0.85\n",
      "  0.25 -1.   -1.    0.82 -1.    0.47  0.81 -1.  ]\n",
      "episode : 8210, reward mean : 0.0680900085810572, total_step : 328295, cur_epsilon : 0.02\n",
      "episode : 8220, reward mean : 0.07077000861056149, total_step : 328604, cur_epsilon : 0.02\n",
      "episode : 8230, reward mean : 0.07101000860519707, total_step : 329050, cur_epsilon : 0.02\n",
      "episode : 8240, reward mean : 0.06799000865034759, total_step : 329516, cur_epsilon : 0.02\n",
      "episode : 8250, reward mean : 0.07321000857837498, total_step : 329887, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8250, reward mean : 0.30250001000240445, step mean : 45.5, eval_episode_rewards : [ 0.99  0.84 -1.    0.93  0.76  0.71 -1.    0.35 -1.    0.99  0.39  0.5\n",
      "  0.98  0.79  0.6   0.65  0.62  0.95 -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 8260, reward mean : 0.07132000862061977, total_step : 330459, cur_epsilon : 0.02\n",
      "episode : 8270, reward mean : 0.07634000864252448, total_step : 330943, cur_epsilon : 0.02\n",
      "episode : 8280, reward mean : 0.07842000861838459, total_step : 331432, cur_epsilon : 0.02\n",
      "episode : 8290, reward mean : 0.08281000860966742, total_step : 331755, cur_epsilon : 0.02\n",
      "episode : 8300, reward mean : 0.08232000857591629, total_step : 332071, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8300, reward mean : 0.048000012338161466, step mean : 55.8, eval_episode_rewards : [ 0.51  0.99  1.   -1.   -1.   -1.    0.97  0.77 -1.    0.92  1.    0.16\n",
      "  0.46 -1.    0.4  -1.    0.99 -1.    0.79 -1.  ]\n",
      "episode : 8310, reward mean : 0.07608000858128071, total_step : 332406, cur_epsilon : 0.02\n",
      "episode : 8320, reward mean : 0.07533000859804452, total_step : 332840, cur_epsilon : 0.02\n",
      "episode : 8330, reward mean : 0.08111000858061015, total_step : 333193, cur_epsilon : 0.02\n",
      "episode : 8340, reward mean : 0.08927000855468213, total_step : 333370, cur_epsilon : 0.02\n",
      "episode : 8350, reward mean : 0.08087000858597457, total_step : 333778, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8350, reward mean : -0.19049998568370938, step mean : 64.5, eval_episode_rewards : [ 0.75 -1.    0.62 -1.    0.96  0.4  -1.    0.99 -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.74 -1.    0.95 -1.    0.82  0.96]\n",
      "episode : 8360, reward mean : 0.07919000864587725, total_step : 334334, cur_epsilon : 0.02\n",
      "episode : 8370, reward mean : 0.07862000863626599, total_step : 334615, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 8380, reward mean : 0.0865500086825341, total_step : 335163, cur_epsilon : 0.02\n",
      "episode : 8390, reward mean : 0.08331000866554678, total_step : 335525, cur_epsilon : 0.02\n",
      "episode : 8400, reward mean : 0.08788000867515802, total_step : 335860, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8400, reward mean : -0.1064999864436686, step mean : 61.15, eval_episode_rewards : [ 0.97  0.42  0.97  0.28 -1.    0.98  0.86  0.88  0.99 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.56  0.96 -1.  ]\n",
      "episode : 8410, reward mean : 0.08645000870712101, total_step : 336246, cur_epsilon : 0.02\n",
      "episode : 8420, reward mean : 0.08442000875249506, total_step : 336710, cur_epsilon : 0.02\n",
      "episode : 8430, reward mean : 0.08263000868074596, total_step : 336901, cur_epsilon : 0.02\n",
      "episode : 8440, reward mean : 0.07976000870019198, total_step : 337322, cur_epsilon : 0.02\n",
      "episode : 8450, reward mean : 0.07359000868164003, total_step : 337756, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8450, reward mean : -0.33299998473376036, step mean : 68.65, eval_episode_rewards : [-1.   -1.   -1.    0.97  0.84 -1.   -1.   -1.   -1.    0.67 -1.   -1.\n",
      "  0.98 -1.   -1.   -1.    0.97 -1.    0.99  0.92]\n",
      "episode : 8460, reward mean : 0.07912000871449709, total_step : 338221, cur_epsilon : 0.02\n",
      "episode : 8470, reward mean : 0.08364000872522592, total_step : 338511, cur_epsilon : 0.02\n",
      "episode : 8480, reward mean : 0.08495000874064863, total_step : 338988, cur_epsilon : 0.02\n",
      "episode : 8490, reward mean : 0.08939000870846212, total_step : 339349, cur_epsilon : 0.02\n",
      "episode : 8500, reward mean : 0.08815000875853002, total_step : 339764, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8500, reward mean : -0.06249998742714524, step mean : 56.75, eval_episode_rewards : [-1.    0.9   0.98  0.98 -1.   -1.    0.98 -1.    0.46 -1.   -1.   -1.\n",
      " -1.   -1.    0.82  0.87  1.   -1.    0.89  0.87]\n",
      "synced target net\n",
      "episode : 8510, reward mean : 0.08823000873439014, total_step : 340197, cur_epsilon : 0.02\n",
      "episode : 8520, reward mean : 0.09010000873729587, total_step : 340816, cur_epsilon : 0.02\n",
      "episode : 8530, reward mean : 0.09563000868074596, total_step : 341158, cur_epsilon : 0.02\n",
      "episode : 8540, reward mean : 0.0902300087120384, total_step : 341622, cur_epsilon : 0.02\n",
      "episode : 8550, reward mean : 0.08644000870734453, total_step : 341862, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8550, reward mean : 0.04200001247227192, step mean : 56.4, eval_episode_rewards : [-1.    0.58 -1.    0.73  0.95 -1.   -1.   -1.    0.85 -1.    0.79  1.\n",
      "  0.78  0.93  0.98  0.38 -1.   -1.    0.02  0.85]\n",
      "episode : 8560, reward mean : 0.08484000872075557, total_step : 342309, cur_epsilon : 0.02\n",
      "episode : 8570, reward mean : 0.0855400087274611, total_step : 342658, cur_epsilon : 0.02\n",
      "episode : 8580, reward mean : 0.08363000872544944, total_step : 343098, cur_epsilon : 0.02\n",
      "episode : 8590, reward mean : 0.08383000869862735, total_step : 343476, cur_epsilon : 0.02\n",
      "episode : 8600, reward mean : 0.08562000868096947, total_step : 343788, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8600, reward mean : 0.13300001043826343, step mean : 47.3, eval_episode_rewards : [ 0.97  0.58  0.9   0.95  0.98 -1.    0.86  0.96 -1.    0.97  0.57 -1.\n",
      "  0.96 -1.    0.96  1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 8610, reward mean : 0.09144000861793757, total_step : 344150, cur_epsilon : 0.02\n",
      "episode : 8620, reward mean : 0.08672000863403082, total_step : 344745, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 8630, reward mean : 0.09122000860050321, total_step : 345056, cur_epsilon : 0.02\n",
      "episode : 8640, reward mean : 0.09132000862061977, total_step : 345518, cur_epsilon : 0.02\n",
      "episode : 8650, reward mean : 0.09370000863447786, total_step : 345912, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8650, reward mean : -0.40999998301267626, step mean : 76.35, eval_episode_rewards : [-1.    0.77 -1.   -1.    0.94 -1.   -1.   -1.    0.87 -1.   -1.   -1.\n",
      " -1.    0.62 -1.   -1.    0.93  0.62  0.05 -1.  ]\n",
      "episode : 8660, reward mean : 0.09368000865727663, total_step : 346318, cur_epsilon : 0.02\n",
      "episode : 8670, reward mean : 0.0958200086094439, total_step : 346714, cur_epsilon : 0.02\n",
      "episode : 8680, reward mean : 0.09318000862374902, total_step : 347230, cur_epsilon : 0.02\n",
      "episode : 8690, reward mean : 0.09138000864163041, total_step : 347690, cur_epsilon : 0.02\n",
      "episode : 8700, reward mean : 0.09030000868812203, total_step : 348197, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8700, reward mean : -0.2804999847896397, step mean : 68.45, eval_episode_rewards : [-1.   -1.   -1.    0.57  0.49 -1.   -1.   -1.   -1.    0.96 -1.    0.88\n",
      " -1.    0.9  -1.    0.95  0.96 -1.   -1.    0.68]\n",
      "episode : 8710, reward mean : 0.09170000870153308, total_step : 348600, cur_epsilon : 0.02\n",
      "episode : 8720, reward mean : 0.0857900087442249, total_step : 349116, cur_epsilon : 0.02\n",
      "episode : 8730, reward mean : 0.0918100088108331, total_step : 349637, cur_epsilon : 0.02\n",
      "episode : 8740, reward mean : 0.09063000877015293, total_step : 349925, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 8750, reward mean : 0.0907500088121742, total_step : 350348, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8750, reward mean : -0.2874999846331775, step mean : 69.15, eval_episode_rewards : [ 0.63 -1.    0.99 -1.   -1.   -1.   -1.    0.13 -1.   -1.   -1.    0.97\n",
      "  0.71 -1.    0.95  0.91 -1.    0.96 -1.   -1.  ]\n",
      "episode : 8760, reward mean : 0.09299000882916153, total_step : 350818, cur_epsilon : 0.02\n",
      "episode : 8770, reward mean : 0.09607000882737339, total_step : 351279, cur_epsilon : 0.02\n",
      "episode : 8780, reward mean : 0.09747000884078443, total_step : 351698, cur_epsilon : 0.02\n",
      "episode : 8790, reward mean : 0.09839000882022082, total_step : 352012, cur_epsilon : 0.02\n",
      "episode : 8800, reward mean : 0.09628000888973474, total_step : 352644, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8800, reward mean : -0.31599998511373995, step mean : 66.95, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.96 -1.   -1.    0.95  0.98  0.9\n",
      " -1.   -1.   -1.    1.    0.89 -1.   -1.    1.  ]\n",
      "episode : 8810, reward mean : 0.10216000887006521, total_step : 352982, cur_epsilon : 0.02\n",
      "episode : 8820, reward mean : 0.10607000882737339, total_step : 353279, cur_epsilon : 0.02\n",
      "episode : 8830, reward mean : 0.10541000881977379, total_step : 353611, cur_epsilon : 0.02\n",
      "episode : 8840, reward mean : 0.10582000885531306, total_step : 354050, cur_epsilon : 0.02\n",
      "episode : 8850, reward mean : 0.1149200088083744, total_step : 354260, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8850, reward mean : 0.11450001085177064, step mean : 49.15, eval_episode_rewards : [-1.   -1.    0.98 -1.    0.88  0.89  0.96 -1.    0.51 -1.   -1.    0.8\n",
      "  0.96  0.93  0.98  0.95  0.83  0.62 -1.   -1.  ]\n",
      "episode : 8860, reward mean : 0.11334000877663493, total_step : 354686, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 8870, reward mean : 0.11404000880569219, total_step : 355227, cur_epsilon : 0.02\n",
      "episode : 8880, reward mean : 0.10938000882044435, total_step : 355702, cur_epsilon : 0.02\n",
      "episode : 8890, reward mean : 0.1057200088351965, total_step : 356118, cur_epsilon : 0.02\n",
      "episode : 8900, reward mean : 0.1044300088416785, total_step : 356498, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8900, reward mean : -0.37049998389557004, step mean : 72.4, eval_episode_rewards : [ 0.83  0.9  -1.    0.62 -1.   -1.   -1.   -1.    0.52  0.9  -1.   -1.\n",
      " -1.   -1.    0.83 -1.    0.99 -1.   -1.   -1.  ]\n",
      "episode : 8910, reward mean : 0.1048400087878108, total_step : 356738, cur_epsilon : 0.02\n",
      "episode : 8920, reward mean : 0.11379000878892839, total_step : 357169, cur_epsilon : 0.02\n",
      "episode : 8930, reward mean : 0.11863000881485641, total_step : 357495, cur_epsilon : 0.02\n",
      "episode : 8940, reward mean : 0.1171200088262558, total_step : 357855, cur_epsilon : 0.02\n",
      "episode : 8950, reward mean : 0.1069100088980049, total_step : 358432, cur_epsilon : 0.02\n",
      "[EVAL] episode : 8950, reward mean : -0.2819999847561121, step mean : 68.6, eval_episode_rewards : [-1.   -1.    0.37 -1.   -1.    0.78  0.87 -1.    0.98 -1.    0.44 -1.\n",
      "  0.97 -1.    0.97 -1.    0.98 -1.   -1.   -1.  ]\n",
      "episode : 8960, reward mean : 0.10765000883676111, total_step : 358808, cur_epsilon : 0.02\n",
      "episode : 8970, reward mean : 0.10263000883720816, total_step : 359061, cur_epsilon : 0.02\n",
      "episode : 8980, reward mean : 0.1017400088571012, total_step : 359510, cur_epsilon : 0.02\n",
      "episode : 8990, reward mean : 0.1073800087980926, total_step : 359724, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 9000, reward mean : 0.10570000879094005, total_step : 360106, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9000, reward mean : -0.012499987427145243, step mean : 56.8, eval_episode_rewards : [ 0.76 -1.    0.98 -1.   -1.    0.96  0.98  0.66  0.67  0.94  0.17 -1.\n",
      " -1.    0.71  0.96 -1.   -1.    0.96 -1.   -1.  ]\n",
      "episode : 9010, reward mean : 0.10968000879138708, total_step : 360617, cur_epsilon : 0.02\n",
      "episode : 9020, reward mean : 0.10695000874064862, total_step : 360801, cur_epsilon : 0.02\n",
      "episode : 9030, reward mean : 0.11001000869460403, total_step : 361065, cur_epsilon : 0.02\n",
      "episode : 9040, reward mean : 0.11403000867180527, total_step : 361528, cur_epsilon : 0.02\n",
      "episode : 9050, reward mean : 0.11588000863045454, total_step : 361843, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9050, reward mean : -0.6414999811910093, step mean : 84.35, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.69 -1.   -1.   -1.\n",
      " -1.   -1.    0.58  0.97 -1.   -1.   -1.    0.93]\n",
      "episode : 9060, reward mean : 0.11511000862531363, total_step : 362310, cur_epsilon : 0.02\n",
      "episode : 9070, reward mean : 0.12227000859938562, total_step : 362646, cur_epsilon : 0.02\n",
      "episode : 9080, reward mean : 0.12170000854507089, total_step : 362845, cur_epsilon : 0.02\n",
      "episode : 9090, reward mean : 0.12511000858061017, total_step : 363357, cur_epsilon : 0.02\n",
      "episode : 9100, reward mean : 0.127190008578822, total_step : 363716, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9100, reward mean : -0.5724999816156924, step mean : 82.5, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.87 -1.   -1.   -1.   -1.   -1.    0.25  0.81\n",
      "  0.98 -1.   -1.   -1.   -1.    0.64 -1.   -1.  ]\n",
      "episode : 9110, reward mean : 0.12541000859625637, total_step : 364092, cur_epsilon : 0.02\n",
      "episode : 9120, reward mean : 0.12707000858150422, total_step : 364501, cur_epsilon : 0.02\n",
      "episode : 9130, reward mean : 0.12585000856406986, total_step : 364804, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 9140, reward mean : 0.1285200085490942, total_step : 365044, cur_epsilon : 0.02\n",
      "episode : 9150, reward mean : 0.1268900085631758, total_step : 365440, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9150, reward mean : -0.49699998218566177, step mean : 80.0, eval_episode_rewards : [-1.    0.5  -1.   -1.   -1.   -1.   -1.   -1.    0.97  0.88 -1.   -1.\n",
      " -1.    0.45 -1.   -1.    0.32 -1.    0.94 -1.  ]\n",
      "episode : 9160, reward mean : 0.12448000861704349, total_step : 365885, cur_epsilon : 0.02\n",
      "episode : 9170, reward mean : 0.11893000867404044, total_step : 366567, cur_epsilon : 0.02\n",
      "episode : 9180, reward mean : 0.12105000864900649, total_step : 366865, cur_epsilon : 0.02\n",
      "episode : 9190, reward mean : 0.11930000868812203, total_step : 367338, cur_epsilon : 0.02\n",
      "episode : 9200, reward mean : 0.111320008687675, total_step : 367690, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9200, reward mean : -0.29649998443201186, step mean : 70.05, eval_episode_rewards : [-1.   -1.   -1.    0.79  0.96  0.89  0.82 -1.   -1.   -1.    0.55 -1.\n",
      "  0.99 -1.   -1.   -1.    0.91  0.16 -1.   -1.  ]\n",
      "episode : 9210, reward mean : 0.1061500086914748, total_step : 368067, cur_epsilon : 0.02\n",
      "episode : 9220, reward mean : 0.10177000867761672, total_step : 368313, cur_epsilon : 0.02\n",
      "episode : 9230, reward mean : 0.10259000872634351, total_step : 368976, cur_epsilon : 0.02\n",
      "episode : 9240, reward mean : 0.10382000872120262, total_step : 369420, cur_epsilon : 0.02\n",
      "episode : 9250, reward mean : 0.10382000874355435, total_step : 369890, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9250, reward mean : -0.32899998482316734, step mean : 68.25, eval_episode_rewards : [ 0.93 -1.    0.98 -1.   -1.   -1.    0.99 -1.   -1.    0.92 -1.    0.97\n",
      " -1.    0.98 -1.   -1.   -1.   -1.    0.65 -1.  ]\n",
      "synced target net\n",
      "episode : 9260, reward mean : 0.10499000871740281, total_step : 370347, cur_epsilon : 0.02\n",
      "episode : 9270, reward mean : 0.1045700087044388, total_step : 370772, cur_epsilon : 0.02\n",
      "episode : 9280, reward mean : 0.10277000867761671, total_step : 371142, cur_epsilon : 0.02\n",
      "episode : 9290, reward mean : 0.09959000868164003, total_step : 371482, cur_epsilon : 0.02\n",
      "episode : 9300, reward mean : 0.09327000868879258, total_step : 371830, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9300, reward mean : -0.6399999812245369, step mean : 84.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.86  0.96 -1.   -1.   -1.    0.78\n",
      " -1.   -1.   -1.   -1.   -1.    0.6  -1.   -1.  ]\n",
      "episode : 9310, reward mean : 0.09092000874131918, total_step : 372398, cur_epsilon : 0.02\n",
      "episode : 9320, reward mean : 0.09298000873997808, total_step : 372826, cur_epsilon : 0.02\n",
      "episode : 9330, reward mean : 0.08865000876970588, total_step : 373309, cur_epsilon : 0.02\n",
      "episode : 9340, reward mean : 0.08322000880166888, total_step : 373629, cur_epsilon : 0.02\n",
      "episode : 9350, reward mean : 0.0806900088582188, total_step : 374288, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9350, reward mean : -0.28799998462200166, step mean : 69.2, eval_episode_rewards : [-1.    0.38  0.95 -1.   -1.   -1.    0.92 -1.   -1.    0.58 -1.   -1.\n",
      " -1.    1.    0.62  0.87 -1.   -1.   -1.    0.92]\n",
      "episode : 9360, reward mean : 0.08031000884436071, total_step : 374783, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 9370, reward mean : 0.08158000888302923, total_step : 375236, cur_epsilon : 0.02\n",
      "episode : 9380, reward mean : 0.080290008822456, total_step : 375515, cur_epsilon : 0.02\n",
      "episode : 9390, reward mean : 0.08096000885218382, total_step : 376010, cur_epsilon : 0.02\n",
      "episode : 9400, reward mean : 0.07615000889264047, total_step : 376525, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9400, reward mean : -0.4989999821409583, step mean : 80.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.18  0.47 -1.   -1.    0.93\n",
      " -1.    0.98 -1.   -1.   -1.   -1.    0.54  0.92]\n",
      "episode : 9410, reward mean : 0.07133000888861715, total_step : 376892, cur_epsilon : 0.02\n",
      "episode : 9420, reward mean : 0.06906000887230039, total_step : 377284, cur_epsilon : 0.02\n",
      "episode : 9430, reward mean : 0.07136000891029835, total_step : 377645, cur_epsilon : 0.02\n",
      "episode : 9440, reward mean : 0.07332000891119242, total_step : 378070, cur_epsilon : 0.02\n",
      "episode : 9450, reward mean : 0.07614000893756748, total_step : 378622, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9450, reward mean : -0.512999981828034, step mean : 81.6, eval_episode_rewards : [ 0.99 -1.   -1.    0.89  0.9  -1.   -1.    0.04 -1.    0.09 -1.   -1.\n",
      " -1.    0.83 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 9460, reward mean : 0.07494000891968608, total_step : 379007, cur_epsilon : 0.02\n",
      "episode : 9470, reward mean : 0.07576000890135764, total_step : 379215, cur_epsilon : 0.02\n",
      "episode : 9480, reward mean : 0.07572000890225172, total_step : 379696, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 9490, reward mean : 0.07115000891499221, total_step : 380114, cur_epsilon : 0.02\n",
      "episode : 9500, reward mean : 0.07277000890113414, total_step : 380468, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9500, reward mean : -0.5784999814815819, step mean : 83.1, eval_episode_rewards : [-1.   -1.   -1.    0.91 -1.   -1.   -1.    0.95 -1.   -1.   -1.    0.46\n",
      "  0.19 -1.   -1.   -1.   -1.   -1.   -1.    0.92]\n",
      "episode : 9510, reward mean : 0.06791000889800489, total_step : 380888, cur_epsilon : 0.02\n",
      "episode : 9520, reward mean : 0.06921000882424413, total_step : 381181, cur_epsilon : 0.02\n",
      "episode : 9530, reward mean : 0.07140000881999731, total_step : 381504, cur_epsilon : 0.02\n",
      "episode : 9540, reward mean : 0.07158000879362225, total_step : 381851, cur_epsilon : 0.02\n",
      "episode : 9550, reward mean : 0.073000008828938, total_step : 382248, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9550, reward mean : -0.3724999838508666, step mean : 72.6, eval_episode_rewards : [ 0.63 -1.    0.46 -1.   -1.   -1.    0.96 -1.   -1.   -1.    0.97  0.69\n",
      " -1.   -1.   -1.    0.89 -1.   -1.    0.95 -1.  ]\n",
      "episode : 9560, reward mean : 0.07812000878155231, total_step : 382484, cur_epsilon : 0.02\n",
      "episode : 9570, reward mean : 0.07728000880032777, total_step : 382915, cur_epsilon : 0.02\n",
      "episode : 9580, reward mean : 0.0730400088056922, total_step : 383379, cur_epsilon : 0.02\n",
      "episode : 9590, reward mean : 0.07182000878825784, total_step : 383681, cur_epsilon : 0.02\n",
      "episode : 9600, reward mean : 0.06803000882826746, total_step : 384170, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9600, reward mean : -0.34649998443201185, step mean : 70.0, eval_episode_rewards : [ 0.96  0.98  0.98 -1.   -1.   -1.   -1.    0.7  -1.   -1.    0.75 -1.\n",
      " -1.   -1.   -1.    0.92 -1.    0.78 -1.   -1.  ]\n",
      "episode : 9610, reward mean : 0.06543000886403025, total_step : 384691, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 9620, reward mean : 0.07453000881709158, total_step : 385079, cur_epsilon : 0.02\n",
      "episode : 9630, reward mean : 0.07313000880368054, total_step : 385332, cur_epsilon : 0.02\n",
      "episode : 9640, reward mean : 0.07580000876635314, total_step : 385628, cur_epsilon : 0.02\n",
      "episode : 9650, reward mean : 0.07639000877551734, total_step : 386064, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9650, reward mean : -0.3874999823980033, step mean : 79.15, eval_episode_rewards : [-1.    0.97 -1.    0.99 -1.    0.2  -1.   -1.   -1.    0.31  0.03 -1.\n",
      " -1.    0.38  1.   -1.   -1.   -1.    0.37 -1.  ]\n",
      "episode : 9660, reward mean : 0.07580000876635314, total_step : 386430, cur_epsilon : 0.02\n",
      "episode : 9670, reward mean : 0.07825000875629484, total_step : 386783, cur_epsilon : 0.02\n",
      "episode : 9680, reward mean : 0.08222000871226191, total_step : 387104, cur_epsilon : 0.02\n",
      "episode : 9690, reward mean : 0.07739000870846212, total_step : 387546, cur_epsilon : 0.02\n",
      "episode : 9700, reward mean : 0.07906000869348645, total_step : 387987, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9700, reward mean : -0.09899998661130667, step mean : 60.4, eval_episode_rewards : [-1.    0.86 -1.    0.91  0.93 -1.   -1.   -1.    0.65  0.8  -1.    0.89\n",
      "  0.95  0.51 -1.    0.9  -1.   -1.    0.62 -1.  ]\n",
      "episode : 9710, reward mean : 0.08108000869303941, total_step : 388388, cur_epsilon : 0.02\n",
      "episode : 9720, reward mean : 0.08138000866398215, total_step : 388775, cur_epsilon : 0.02\n",
      "episode : 9730, reward mean : 0.08400000862777234, total_step : 389135, cur_epsilon : 0.02\n",
      "episode : 9740, reward mean : 0.0887300086338073, total_step : 389449, cur_epsilon : 0.02\n",
      "episode : 9750, reward mean : 0.08902000860497356, total_step : 389742, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9750, reward mean : -0.1664999862201512, step mean : 62.1, eval_episode_rewards : [ 0.92  0.94 -1.    0.97 -1.   -1.   -1.   -1.    0.98  0.86  0.98  0.9\n",
      " -1.    0.86 -1.   -1.    0.26 -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 9760, reward mean : 0.09500000856071711, total_step : 390014, cur_epsilon : 0.02\n",
      "episode : 9770, reward mean : 0.09226000853255391, total_step : 390351, cur_epsilon : 0.02\n",
      "episode : 9780, reward mean : 0.09430000853165985, total_step : 390766, cur_epsilon : 0.02\n",
      "episode : 9790, reward mean : 0.09678000856563448, total_step : 391232, cur_epsilon : 0.02\n",
      "episode : 9800, reward mean : 0.10085000851936639, total_step : 391659, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9800, reward mean : -0.7469999810680747, step mean : 84.9, eval_episode_rewards : [-1.   -1.    0.48 -1.   -1.    0.88 -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.22 -1.   -1.   -1.   -1.   -1.    0.92]\n",
      "episode : 9810, reward mean : 0.09306000853702426, total_step : 392075, cur_epsilon : 0.02\n",
      "episode : 9820, reward mean : 0.09142000852897764, total_step : 392336, cur_epsilon : 0.02\n",
      "episode : 9830, reward mean : 0.08948000852763653, total_step : 392662, cur_epsilon : 0.02\n",
      "episode : 9840, reward mean : 0.08717000853456557, total_step : 393132, cur_epsilon : 0.02\n",
      "episode : 9850, reward mean : 0.08563000852428376, total_step : 393296, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9850, reward mean : -0.667499980609864, step mean : 86.95, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.36  0.83 -1.   -1.   -1.    0.74 -1.\n",
      "  0.72 -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 9860, reward mean : 0.09496000849455595, total_step : 393589, cur_epsilon : 0.02\n",
      "episode : 9870, reward mean : 0.09916000844538211, total_step : 393910, cur_epsilon : 0.02\n",
      "episode : 9880, reward mean : 0.10752000841498376, total_step : 394250, cur_epsilon : 0.02\n",
      "episode : 9890, reward mean : 0.10710000842437148, total_step : 394708, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 9900, reward mean : 0.11126000846549869, total_step : 395272, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9900, reward mean : -0.21799998506903648, step mean : 67.25, eval_episode_rewards : [ 0.92 -1.   -1.   -1.   -1.    0.97 -1.    0.91  0.64 -1.   -1.   -1.\n",
      " -1.    0.98  0.55 -1.   -1.    0.73  0.05  0.89]\n",
      "episode : 9910, reward mean : 0.11197000847198069, total_step : 395540, cur_epsilon : 0.02\n",
      "episode : 9920, reward mean : 0.10406000849232078, total_step : 396063, cur_epsilon : 0.02\n",
      "episode : 9930, reward mean : 0.10917000851221383, total_step : 396478, cur_epsilon : 0.02\n",
      "episode : 9940, reward mean : 0.10668000854551792, total_step : 396984, cur_epsilon : 0.02\n",
      "episode : 9950, reward mean : 0.11565000850148499, total_step : 397365, cur_epsilon : 0.02\n",
      "[EVAL] episode : 9950, reward mean : -0.51649998286739, step mean : 76.9, eval_episode_rewards : [-1.   -1.   -1.    0.73  1.    0.99 -1.    0.95  1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 9960, reward mean : 0.11785000849701464, total_step : 397721, cur_epsilon : 0.02\n",
      "episode : 9970, reward mean : 0.12284000849723815, total_step : 397976, cur_epsilon : 0.02\n",
      "episode : 9980, reward mean : 0.12271000850014388, total_step : 398436, cur_epsilon : 0.02\n",
      "episode : 9990, reward mean : 0.1241500085350126, total_step : 398805, cur_epsilon : 0.02\n",
      "episode : 10000, reward mean : 0.12397000856138765, total_step : 399306, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10000, reward mean : -0.4584999830462039, step mean : 76.15, eval_episode_rewards : [ 0.84 -1.    0.96 -1.    0.89 -1.   -1.   -1.    0.42  0.73  0.99 -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 10010, reward mean : 0.12106000853702426, total_step : 399708, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 10020, reward mean : 0.1287400085888803, total_step : 400124, cur_epsilon : 0.02\n",
      "episode : 10030, reward mean : 0.11711000864766538, total_step : 400648, cur_epsilon : 0.02\n",
      "episode : 10040, reward mean : 0.11640000859647989, total_step : 400883, cur_epsilon : 0.02\n",
      "episode : 10050, reward mean : 0.1180800086259842, total_step : 401331, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10050, reward mean : -0.22249998608604074, step mean : 62.65, eval_episode_rewards : [-1.   -1.   -1.    0.98 -1.   -1.   -1.    0.93 -1.   -1.   -1.    0.96\n",
      " -1.    0.91  1.    0.92  0.92 -1.    0.93 -1.  ]\n",
      "episode : 10060, reward mean : 0.12037000861950219, total_step : 401769, cur_epsilon : 0.02\n",
      "episode : 10070, reward mean : 0.10972000861167908, total_step : 402071, cur_epsilon : 0.02\n",
      "episode : 10080, reward mean : 0.10797000865079462, total_step : 402445, cur_epsilon : 0.02\n",
      "episode : 10090, reward mean : 0.10305000864900649, total_step : 402948, cur_epsilon : 0.02\n",
      "episode : 10100, reward mean : 0.10258000863716006, total_step : 403255, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10100, reward mean : -0.5629999818280339, step mean : 81.55, eval_episode_rewards : [-1.   -1.    0.86 -1.   -1.   -1.   -1.   -1.    0.97 -1.    0.75 -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.75  0.41 -1.  ]\n",
      "episode : 10110, reward mean : 0.10245000864006579, total_step : 403642, cur_epsilon : 0.02\n",
      "episode : 10120, reward mean : 0.09969000865705312, total_step : 404127, cur_epsilon : 0.02\n",
      "episode : 10130, reward mean : 0.09520000866800547, total_step : 404479, cur_epsilon : 0.02\n",
      "episode : 10140, reward mean : 0.09041000873036682, total_step : 404996, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 10150, reward mean : 0.08963000874780118, total_step : 405468, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10150, reward mean : -0.57099998164922, step mean : 82.35, eval_episode_rewards : [ 0.98 -1.   -1.   -1.   -1.   -1.    0.53  0.39 -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.71  0.97 -1.   -1.   -1.  ]\n",
      "episode : 10160, reward mean : 0.08621000877954066, total_step : 406053, cur_epsilon : 0.02\n",
      "episode : 10170, reward mean : 0.08806000871583819, total_step : 406451, cur_epsilon : 0.02\n",
      "episode : 10180, reward mean : 0.08506000873818993, total_step : 406847, cur_epsilon : 0.02\n",
      "episode : 10190, reward mean : 0.08698000871762633, total_step : 407227, cur_epsilon : 0.02\n",
      "episode : 10200, reward mean : 0.08952000870555639, total_step : 407525, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10200, reward mean : -0.4229999827221036, step mean : 77.65, eval_episode_rewards : [-1.   -1.    0.25  0.92 -1.   -1.   -1.    0.51 -1.    0.57  0.93 -1.\n",
      " -1.   -1.   -1.   -1.    0.44  0.92 -1.   -1.  ]\n",
      "episode : 10210, reward mean : 0.0881700087133795, total_step : 407934, cur_epsilon : 0.02\n",
      "episode : 10220, reward mean : 0.08597000876255334, total_step : 408400, cur_epsilon : 0.02\n",
      "episode : 10230, reward mean : 0.08380000874400138, total_step : 408981, cur_epsilon : 0.02\n",
      "episode : 10240, reward mean : 0.0812100088018924, total_step : 409684, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 10250, reward mean : 0.08039000879786909, total_step : 410135, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10250, reward mean : -0.5604999818839133, step mean : 81.3, eval_episode_rewards : [-1.   -1.    0.92 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.48 -1.\n",
      " -1.   -1.    0.89 -1.    0.52 -1.   -1.    0.98]\n",
      "episode : 10260, reward mean : 0.08005000876076519, total_step : 410426, cur_epsilon : 0.02\n",
      "episode : 10270, reward mean : 0.07680000876635314, total_step : 410877, cur_epsilon : 0.02\n",
      "episode : 10280, reward mean : 0.07548000877350569, total_step : 411278, cur_epsilon : 0.02\n",
      "episode : 10290, reward mean : 0.07415000878088177, total_step : 411652, cur_epsilon : 0.02\n",
      "episode : 10300, reward mean : 0.08241000875271856, total_step : 411875, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10300, reward mean : -0.204499985370785, step mean : 65.9, eval_episode_rewards : [ 0.76 -1.   -1.    0.99  0.57 -1.   -1.   -1.   -1.   -1.    0.78  0.67\n",
      " -1.    0.93  0.97 -1.    0.85  0.39 -1.   -1.  ]\n",
      "episode : 10310, reward mean : 0.08609000871516764, total_step : 412275, cur_epsilon : 0.02\n",
      "episode : 10320, reward mean : 0.08116000873595476, total_step : 412794, cur_epsilon : 0.02\n",
      "episode : 10330, reward mean : 0.08553000870533288, total_step : 413143, cur_epsilon : 0.02\n",
      "episode : 10340, reward mean : 0.07834000873193145, total_step : 413582, cur_epsilon : 0.02\n",
      "episode : 10350, reward mean : 0.08549000863917172, total_step : 413829, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10350, reward mean : -0.14649998443201184, step mean : 70.2, eval_episode_rewards : [ 0.84  0.5   0.51  0.55  0.86 -1.    0.24  0.58  0.76 -1.    0.21 -1.\n",
      " -1.   -1.   -1.   -1.    0.96 -1.    0.06 -1.  ]\n",
      "episode : 10360, reward mean : 0.08952000859379769, total_step : 414123, cur_epsilon : 0.02\n",
      "episode : 10370, reward mean : 0.0863600086197257, total_step : 414690, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 10380, reward mean : 0.08976000865548849, total_step : 415128, cur_epsilon : 0.02\n",
      "episode : 10390, reward mean : 0.08880000865459442, total_step : 415618, cur_epsilon : 0.02\n",
      "episode : 10400, reward mean : 0.09045000864006579, total_step : 416069, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10400, reward mean : -0.7079999808222055, step mean : 85.95, eval_episode_rewards : [-1.    0.94 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.9  -1.   -1.   -1.    1.  ]\n",
      "episode : 10410, reward mean : 0.0993600086197257, total_step : 416345, cur_epsilon : 0.02\n",
      "episode : 10420, reward mean : 0.09668000863492489, total_step : 416803, cur_epsilon : 0.02\n",
      "episode : 10430, reward mean : 0.10000000862777234, total_step : 417131, cur_epsilon : 0.02\n",
      "episode : 10440, reward mean : 0.0945000086389482, total_step : 417605, cur_epsilon : 0.02\n",
      "episode : 10450, reward mean : 0.09782000856474042, total_step : 417827, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10450, reward mean : -0.3134999840520322, step mean : 71.75, eval_episode_rewards : [ 0.95  0.89 -1.   -1.   -1.   -1.   -1.    0.05  0.32 -1.    0.85  0.85\n",
      " -1.   -1.    0.94 -1.   -1.   -1.    0.88 -1.  ]\n",
      "episode : 10460, reward mean : 0.09529000857658684, total_step : 418265, cur_epsilon : 0.02\n",
      "episode : 10470, reward mean : 0.09034000864252448, total_step : 418766, cur_epsilon : 0.02\n",
      "episode : 10480, reward mean : 0.08984000863134861, total_step : 419198, cur_epsilon : 0.02\n",
      "episode : 10490, reward mean : 0.09318000860139727, total_step : 419482, cur_epsilon : 0.02\n",
      "episode : 10500, reward mean : 0.08877000861056149, total_step : 419875, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10500, reward mean : -0.014999987371265889, step mean : 57.05, eval_episode_rewards : [-1.    0.84 -1.    0.99 -1.   -1.    1.    0.96 -1.   -1.    0.14  0.9\n",
      "  0.86 -1.    0.48  0.98  0.61  0.94 -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 10510, reward mean : 0.0932500086221844, total_step : 420346, cur_epsilon : 0.02\n",
      "episode : 10520, reward mean : 0.0875700087044388, total_step : 421003, cur_epsilon : 0.02\n",
      "episode : 10530, reward mean : 0.08681000876612961, total_step : 421600, cur_epsilon : 0.02\n",
      "episode : 10540, reward mean : 0.08940000879764556, total_step : 422088, cur_epsilon : 0.02\n",
      "episode : 10550, reward mean : 0.08604000876098872, total_step : 422322, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10550, reward mean : -0.4984999821521342, step mean : 80.15, eval_episode_rewards : [ 0.84 -1.   -1.   -1.   -1.   -1.   -1.    0.54 -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.88  0.88  0.6  -1.   -1.    0.29]\n",
      "episode : 10560, reward mean : 0.08054000874981285, total_step : 422508, cur_epsilon : 0.02\n",
      "episode : 10570, reward mean : 0.07913000873662532, total_step : 422882, cur_epsilon : 0.02\n",
      "episode : 10580, reward mean : 0.0826300087030977, total_step : 423196, cur_epsilon : 0.02\n",
      "episode : 10590, reward mean : 0.081310008732602, total_step : 423628, cur_epsilon : 0.02\n",
      "episode : 10600, reward mean : 0.08674000870063901, total_step : 423976, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10600, reward mean : -0.43749998239800336, step mean : 79.1, eval_episode_rewards : [-1.    0.94  0.13 -1.   -1.   -1.   -1.    0.36 -1.   -1.   -1.    0.75\n",
      "  0.2  -1.   -1.   -1.   -1.   -1.    0.96  0.91]\n",
      "episode : 10610, reward mean : 0.09003000864945353, total_step : 424271, cur_epsilon : 0.02\n",
      "episode : 10620, reward mean : 0.0894700086619705, total_step : 424715, cur_epsilon : 0.02\n",
      "episode : 10630, reward mean : 0.09433000866509975, total_step : 424981, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 10640, reward mean : 0.09104000867158174, total_step : 425307, cur_epsilon : 0.02\n",
      "episode : 10650, reward mean : 0.09469000861234964, total_step : 425479, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10650, reward mean : -0.5274999815039336, step mean : 83.05, eval_episode_rewards : [-1.   -1.    0.97 -1.    0.75 -1.   -1.   -1.    0.43 -1.    0.42  0.76\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.12 -1.  ]\n",
      "episode : 10660, reward mean : 0.09356000861525536, total_step : 425857, cur_epsilon : 0.02\n",
      "episode : 10670, reward mean : 0.09197000860609114, total_step : 426169, cur_epsilon : 0.02\n",
      "episode : 10680, reward mean : 0.08799000862799584, total_step : 426587, cur_epsilon : 0.02\n",
      "episode : 10690, reward mean : 0.09049000863917173, total_step : 427080, cur_epsilon : 0.02\n",
      "episode : 10700, reward mean : 0.08768000861257315, total_step : 427404, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10700, reward mean : -0.7419999800622463, step mean : 89.35, eval_episode_rewards : [ 0.95 -1.   -1.    0.96 -1.   -1.   -1.   -1.   -1.    0.25 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 10710, reward mean : 0.08679000861011446, total_step : 427793, cur_epsilon : 0.02\n",
      "episode : 10720, reward mean : 0.08527000859938562, total_step : 428132, cur_epsilon : 0.02\n",
      "episode : 10730, reward mean : 0.07648000863939523, total_step : 428670, cur_epsilon : 0.02\n",
      "episode : 10740, reward mean : 0.06922000871226192, total_step : 429308, cur_epsilon : 0.02\n",
      "episode : 10750, reward mean : 0.060100008759647605, total_step : 429812, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10750, reward mean : -0.6044999820180237, step mean : 80.65, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.99 -1.    0.99 -1.   -1.    0.93 -1.   -1.\n",
      " -1.    1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 10760, reward mean : 0.05822000877931714, total_step : 430171, cur_epsilon : 0.02\n",
      "episode : 10770, reward mean : 0.0629500087853521, total_step : 430534, cur_epsilon : 0.02\n",
      "episode : 10780, reward mean : 0.06599000873975455, total_step : 430746, cur_epsilon : 0.02\n",
      "episode : 10790, reward mean : 0.06223000873439014, total_step : 431188, cur_epsilon : 0.02\n",
      "episode : 10800, reward mean : 0.06105000869370997, total_step : 431434, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10800, reward mean : -0.6439999811351299, step mean : 84.6, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.55 -1.   -1.    0.95 -1.   -1.    0.76\n",
      " -1.   -1.   -1.   -1.   -1.    0.86 -1.   -1.  ]\n",
      "episode : 10810, reward mean : 0.059910008741542696, total_step : 432061, cur_epsilon : 0.02\n",
      "episode : 10820, reward mean : 0.05895000880770385, total_step : 432616, cur_epsilon : 0.02\n",
      "episode : 10830, reward mean : 0.06362000883743167, total_step : 433073, cur_epsilon : 0.02\n",
      "episode : 10840, reward mean : 0.06638000884279609, total_step : 433566, cur_epsilon : 0.02\n",
      "episode : 10850, reward mean : 0.06234000888839364, total_step : 433932, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10850, reward mean : -0.720999980531633, step mean : 87.25, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.76 -1.    0.85 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.97 -1.   -1.   -1.  ]\n",
      "episode : 10860, reward mean : 0.06213000887073576, total_step : 434147, cur_epsilon : 0.02\n",
      "episode : 10870, reward mean : 0.06316000884771347, total_step : 434365, cur_epsilon : 0.02\n",
      "episode : 10880, reward mean : 0.061490008862689134, total_step : 434771, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 10890, reward mean : 0.06410000889375805, total_step : 435366, cur_epsilon : 0.02\n",
      "episode : 10900, reward mean : 0.06370000883564353, total_step : 435671, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10900, reward mean : -0.47499998155981304, step mean : 82.85, eval_episode_rewards : [-1.    0.01  0.75 -1.   -1.   -1.   -1.    0.48 -1.   -1.   -1.   -1.\n",
      " -1.    0.6   0.33 -1.   -1.   -1.    0.34  0.99]\n",
      "episode : 10910, reward mean : 0.05455000888369978, total_step : 436153, cur_epsilon : 0.02\n",
      "episode : 10920, reward mean : 0.0611700088698417, total_step : 436614, cur_epsilon : 0.02\n",
      "episode : 10930, reward mean : 0.054490008862689135, total_step : 436996, cur_epsilon : 0.02\n",
      "episode : 10940, reward mean : 0.05737000882066786, total_step : 437315, cur_epsilon : 0.02\n",
      "episode : 10950, reward mean : 0.05056000881642103, total_step : 437676, cur_epsilon : 0.02\n",
      "[EVAL] episode : 10950, reward mean : -0.7054999808780849, step mean : 85.7, eval_episode_rewards : [ 0.97 -1.   -1.    0.95 -1.   -1.    0.97 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 10960, reward mean : 0.049070008805021645, total_step : 437981, cur_epsilon : 0.02\n",
      "episode : 10970, reward mean : 0.04206000891700387, total_step : 438735, cur_epsilon : 0.02\n",
      "episode : 10980, reward mean : 0.04572000887989998, total_step : 439031, cur_epsilon : 0.02\n",
      "episode : 10990, reward mean : 0.041580008905380966, total_step : 439513, cur_epsilon : 0.02\n",
      "episode : 11000, reward mean : 0.04212000889331102, total_step : 439960, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11000, reward mean : -0.663499980699271, step mean : 86.55, eval_episode_rewards : [ 1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.13 -1.    0.84 -1.   -1.   -1.    0.76]\n",
      "synced target net\n",
      "episode : 11010, reward mean : 0.04562000888213515, total_step : 440311, cur_epsilon : 0.02\n",
      "episode : 11020, reward mean : 0.0427900089006871, total_step : 440808, cur_epsilon : 0.02\n",
      "episode : 11030, reward mean : 0.04959000886045396, total_step : 441153, cur_epsilon : 0.02\n",
      "episode : 11040, reward mean : 0.04739000893197954, total_step : 441705, cur_epsilon : 0.02\n",
      "episode : 11050, reward mean : 0.04955000890605152, total_step : 442036, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11050, reward mean : -0.21199998520314695, step mean : 66.65, eval_episode_rewards : [-1.   -1.    0.04 -1.    0.95 -1.   -1.    0.94 -1.    0.94  0.85  0.97\n",
      " -1.    0.91  0.7  -1.   -1.   -1.   -1.    0.46]\n",
      "episode : 11060, reward mean : 0.04730000888928771, total_step : 442400, cur_epsilon : 0.02\n",
      "episode : 11070, reward mean : 0.054600008882582186, total_step : 442671, cur_epsilon : 0.02\n",
      "episode : 11080, reward mean : 0.05144000886380672, total_step : 442961, cur_epsilon : 0.02\n",
      "episode : 11090, reward mean : 0.05071000885777176, total_step : 443436, cur_epsilon : 0.02\n",
      "episode : 11100, reward mean : 0.046410008886829016, total_step : 443872, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11100, reward mean : -0.5339999813586473, step mean : 83.7, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.44 -1.   -1.    0.43  0.68 -1.   -1.\n",
      "  0.54 -1.   -1.    0.98 -1.   -1.   -1.    0.25]\n",
      "episode : 11110, reward mean : 0.04091000892035663, total_step : 444409, cur_epsilon : 0.02\n",
      "episode : 11120, reward mean : 0.043090008893981575, total_step : 444777, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11130, reward mean : 0.04657000888325274, total_step : 445080, cur_epsilon : 0.02\n",
      "episode : 11140, reward mean : 0.05568000883609057, total_step : 445389, cur_epsilon : 0.02\n",
      "episode : 11150, reward mean : 0.06239000879786909, total_step : 445693, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11150, reward mean : -0.4504999832250178, step mean : 75.35, eval_episode_rewards : [-1.   -1.    0.95  0.44  0.95 -1.    0.91 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.97  0.77 -1.  ]\n",
      "episode : 11160, reward mean : 0.06171000879071653, total_step : 446247, cur_epsilon : 0.02\n",
      "episode : 11170, reward mean : 0.059390008797869084, total_step : 446677, cur_epsilon : 0.02\n",
      "episode : 11180, reward mean : 0.05421000884659588, total_step : 447290, cur_epsilon : 0.02\n",
      "episode : 11190, reward mean : 0.05061000888235867, total_step : 447828, cur_epsilon : 0.02\n",
      "episode : 11200, reward mean : 0.05509000889398158, total_step : 448177, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11200, reward mean : -0.30649998420849445, step mean : 71.05, eval_episode_rewards : [ 0.23  0.82 -1.   -1.    0.23 -1.   -1.   -1.   -1.    0.88  1.   -1.\n",
      "  0.92  0.81 -1.   -1.    0.98 -1.   -1.   -1.  ]\n",
      "episode : 11210, reward mean : 0.05779000887833536, total_step : 448518, cur_epsilon : 0.02\n",
      "episode : 11220, reward mean : 0.06085000883229077, total_step : 448779, cur_epsilon : 0.02\n",
      "episode : 11230, reward mean : 0.06313000880368054, total_step : 449233, cur_epsilon : 0.02\n",
      "episode : 11240, reward mean : 0.07033000870980323, total_step : 449517, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11250, reward mean : 0.07324000873416663, total_step : 450079, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11250, reward mean : -0.5584999808110297, step mean : 86.15, eval_episode_rewards : [ 0.55 -1.   -1.   -1.   -1.    0.72 -1.    0.02 -1.    0.93  0.08 -1.\n",
      " -1.    0.53 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 11260, reward mean : 0.07274000876769424, total_step : 450519, cur_epsilon : 0.02\n",
      "episode : 11270, reward mean : 0.06912000875920057, total_step : 450932, cur_epsilon : 0.02\n",
      "episode : 11280, reward mean : 0.07022000873461366, total_step : 451223, cur_epsilon : 0.02\n",
      "episode : 11290, reward mean : 0.07068000876903534, total_step : 451751, cur_epsilon : 0.02\n",
      "episode : 11300, reward mean : 0.06436000882089138, total_step : 452204, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11300, reward mean : -0.5689999816939235, step mean : 82.15, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.96 -1.   -1.    0.96 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.8   0.79  0.11 -1.  ]\n",
      "episode : 11310, reward mean : 0.06512000880390406, total_step : 452530, cur_epsilon : 0.02\n",
      "episode : 11320, reward mean : 0.06494000876322388, total_step : 452869, cur_epsilon : 0.02\n",
      "episode : 11330, reward mean : 0.06026000877842307, total_step : 453284, cur_epsilon : 0.02\n",
      "episode : 11340, reward mean : 0.06306000876054167, total_step : 453643, cur_epsilon : 0.02\n",
      "episode : 11350, reward mean : 0.06336000879853963, total_step : 454060, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11350, reward mean : -0.22099998500198126, step mean : 67.55, eval_episode_rewards : [-1.   -1.    0.56 -1.   -1.   -1.    0.94  0.96 -1.    0.97  0.96  0.2\n",
      " -1.    0.96  0.91 -1.   -1.    0.12 -1.   -1.  ]\n",
      "episode : 11360, reward mean : 0.05914000884816051, total_step : 454574, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11370, reward mean : 0.057180008847266435, total_step : 455137, cur_epsilon : 0.02\n",
      "episode : 11380, reward mean : 0.052690008835867046, total_step : 455525, cur_epsilon : 0.02\n",
      "episode : 11390, reward mean : 0.051910008830949664, total_step : 455994, cur_epsilon : 0.02\n",
      "episode : 11400, reward mean : 0.049790008833631876, total_step : 456457, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11400, reward mean : -0.6259999815374613, step mean : 82.8, eval_episode_rewards : [-1.    0.98 -1.    0.93 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.62 -1.   -1.   -1.   -1.    0.95 -1.  ]\n",
      "episode : 11410, reward mean : 0.049220008846372364, total_step : 456790, cur_epsilon : 0.02\n",
      "episode : 11420, reward mean : 0.051130008870735766, total_step : 457358, cur_epsilon : 0.02\n",
      "episode : 11430, reward mean : 0.05413000887073576, total_step : 457687, cur_epsilon : 0.02\n",
      "episode : 11440, reward mean : 0.064290008822456, total_step : 457947, cur_epsilon : 0.02\n",
      "episode : 11450, reward mean : 0.06429000884480775, total_step : 458268, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11450, reward mean : -0.5029999820515514, step mean : 80.6, eval_episode_rewards : [-1.   -1.   -1.   -1.    1.   -1.    0.86 -1.   -1.    0.13 -1.    0.37\n",
      "  0.79 -1.   -1.   -1.   -1.    0.79 -1.   -1.  ]\n",
      "episode : 11460, reward mean : 0.06384000883251428, total_step : 458652, cur_epsilon : 0.02\n",
      "episode : 11470, reward mean : 0.06512000878155232, total_step : 458926, cur_epsilon : 0.02\n",
      "episode : 11480, reward mean : 0.06457000879384578, total_step : 459413, cur_epsilon : 0.02\n",
      "episode : 11490, reward mean : 0.06477000881172716, total_step : 459776, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11500, reward mean : 0.06338000877574086, total_step : 460009, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11500, reward mean : -0.49799998216331004, step mean : 80.1, eval_episode_rewards : [ 0.92 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.27 -1.    0.53  0.63\n",
      "  0.71 -1.   -1.   -1.   -1.   -1.    0.98 -1.  ]\n",
      "episode : 11510, reward mean : 0.05529000873304903, total_step : 460289, cur_epsilon : 0.02\n",
      "episode : 11520, reward mean : 0.06041000868566334, total_step : 460737, cur_epsilon : 0.02\n",
      "episode : 11530, reward mean : 0.05608000867068767, total_step : 461268, cur_epsilon : 0.02\n",
      "episode : 11540, reward mean : 0.05730000862106681, total_step : 461535, cur_epsilon : 0.02\n",
      "episode : 11550, reward mean : 0.06334000862017274, total_step : 461765, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11550, reward mean : -0.3249999837949872, step mean : 72.9, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.85  0.1   0.74 -1.    0.85 -1.   -1.    0.96\n",
      " -1.   -1.   -1.    0.97 -1.    0.98  0.05 -1.  ]\n",
      "episode : 11560, reward mean : 0.06308000869303941, total_step : 462274, cur_epsilon : 0.02\n",
      "episode : 11570, reward mean : 0.06686000872030855, total_step : 462769, cur_epsilon : 0.02\n",
      "episode : 11580, reward mean : 0.06910000873729587, total_step : 463158, cur_epsilon : 0.02\n",
      "episode : 11590, reward mean : 0.06819000875763595, total_step : 463681, cur_epsilon : 0.02\n",
      "episode : 11600, reward mean : 0.05848000879585743, total_step : 464200, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11600, reward mean : -0.1869999857619405, step mean : 64.15, eval_episode_rewards : [ 0.92  0.21 -1.   -1.   -1.    0.88 -1.    0.87 -1.   -1.    0.93  0.91\n",
      " -1.    0.77 -1.   -1.    0.98 -1.   -1.    0.79]\n",
      "episode : 11610, reward mean : 0.057900008875876666, total_step : 464850, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11620, reward mean : 0.05233000886626542, total_step : 465251, cur_epsilon : 0.02\n",
      "episode : 11630, reward mean : 0.04581000890024006, total_step : 465668, cur_epsilon : 0.02\n",
      "episode : 11640, reward mean : 0.04825000891275704, total_step : 466049, cur_epsilon : 0.02\n",
      "episode : 11650, reward mean : 0.0450000089854002, total_step : 466544, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11650, reward mean : -0.5434999822638928, step mean : 79.6, eval_episode_rewards : [ 0.83 -1.    0.98 -1.   -1.   -1.   -1.   -1.    0.96 -1.   -1.    0.46\n",
      " -1.   -1.   -1.   -1.   -1.    0.9  -1.   -1.  ]\n",
      "episode : 11660, reward mean : 0.04315000900439918, total_step : 467006, cur_epsilon : 0.02\n",
      "episode : 11670, reward mean : 0.04224000904709101, total_step : 467508, cur_epsilon : 0.02\n",
      "episode : 11680, reward mean : 0.045490009063854815, total_step : 468001, cur_epsilon : 0.02\n",
      "episode : 11690, reward mean : 0.04721000904776156, total_step : 468421, cur_epsilon : 0.02\n",
      "episode : 11700, reward mean : 0.04958000903949142, total_step : 468708, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11700, reward mean : -0.32049998389557005, step mean : 72.45, eval_episode_rewards : [-1.   -1.    0.57 -1.    0.74 -1.    0.97  0.47 -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.76 -1.    0.93 -1.    0.21  0.94]\n",
      "episode : 11710, reward mean : 0.054420009043067694, total_step : 469114, cur_epsilon : 0.02\n",
      "episode : 11720, reward mean : 0.05511000904999673, total_step : 469483, cur_epsilon : 0.02\n",
      "episode : 11730, reward mean : 0.0614500090200454, total_step : 469888, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11740, reward mean : 0.06245000899769366, total_step : 470428, cur_epsilon : 0.02\n",
      "episode : 11750, reward mean : 0.06907000898383558, total_step : 470870, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11750, reward mean : -0.6099999818950892, step mean : 81.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.94 -1.    0.93\n",
      " -1.    0.98  0.95 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 11760, reward mean : 0.06737000902183354, total_step : 471401, cur_epsilon : 0.02\n",
      "episode : 11770, reward mean : 0.06392000900954008, total_step : 471709, cur_epsilon : 0.02\n",
      "episode : 11780, reward mean : 0.05755000906251371, total_step : 472156, cur_epsilon : 0.02\n",
      "episode : 11790, reward mean : 0.06052000904083252, total_step : 472499, cur_epsilon : 0.02\n",
      "episode : 11800, reward mean : 0.05540000908821821, total_step : 472954, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11800, reward mean : -0.7024999809451401, step mean : 85.4, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.99 -1.   -1.    0.99 -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.97 -1.   -1.   -1.  ]\n",
      "episode : 11810, reward mean : 0.06306000902876258, total_step : 473318, cur_epsilon : 0.02\n",
      "episode : 11820, reward mean : 0.06359000901691615, total_step : 473820, cur_epsilon : 0.02\n",
      "episode : 11830, reward mean : 0.06108000905066729, total_step : 474429, cur_epsilon : 0.02\n",
      "episode : 11840, reward mean : 0.05968000903725624, total_step : 474864, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11850, reward mean : 0.05702000907436013, total_step : 475397, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11850, reward mean : -0.28949998458847404, step mean : 69.35, eval_episode_rewards : [ 0.98  0.57  0.93 -1.   -1.    0.91  0.99  0.46 -1.   -1.    0.67  0.7\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 11860, reward mean : 0.052750009125098585, total_step : 475837, cur_epsilon : 0.02\n",
      "episode : 11870, reward mean : 0.05558000915125012, total_step : 476172, cur_epsilon : 0.02\n",
      "episode : 11880, reward mean : 0.055580009128898385, total_step : 476479, cur_epsilon : 0.02\n",
      "episode : 11890, reward mean : 0.05425000911392271, total_step : 477007, cur_epsilon : 0.02\n",
      "episode : 11900, reward mean : 0.057210009137168526, total_step : 477414, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11900, reward mean : -0.49799998216331004, step mean : 80.1, eval_episode_rewards : [-1.   -1.   -1.    0.93 -1.    0.41 -1.   -1.    0.95  0.99  0.21 -1.\n",
      " -1.    0.55 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 11910, reward mean : 0.061170009138062593, total_step : 477902, cur_epsilon : 0.02\n",
      "episode : 11920, reward mean : 0.0569700091201812, total_step : 478282, cur_epsilon : 0.02\n",
      "episode : 11930, reward mean : 0.061320009157061574, total_step : 478829, cur_epsilon : 0.02\n",
      "episode : 11940, reward mean : 0.061150009183213117, total_step : 479266, cur_epsilon : 0.02\n",
      "episode : 11950, reward mean : 0.06892000916600227, total_step : 479551, cur_epsilon : 0.02\n",
      "[EVAL] episode : 11950, reward mean : 0.08000001162290574, step mean : 52.6, eval_episode_rewards : [-1.    0.6  -1.    0.97 -1.    0.96 -1.   -1.    0.65 -1.    0.94  0.34\n",
      "  0.8  -1.    0.97  0.98  0.79  0.98  0.62 -1.  ]\n",
      "episode : 11960, reward mean : 0.06718000918254256, total_step : 479929, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 11970, reward mean : 0.0737000090815127, total_step : 480233, cur_epsilon : 0.02\n",
      "episode : 11980, reward mean : 0.06782000916823745, total_step : 480913, cur_epsilon : 0.02\n",
      "episode : 11990, reward mean : 0.06663000915013254, total_step : 481314, cur_epsilon : 0.02\n",
      "episode : 12000, reward mean : 0.06584000912308693, total_step : 481640, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12000, reward mean : -0.20049998546019196, step mean : 65.5, eval_episode_rewards : [-1.    0.94  0.77 -1.    0.95  0.98 -1.   -1.   -1.    0.6  -1.    0.03\n",
      "  0.96 -1.   -1.    0.85 -1.   -1.    0.91 -1.  ]\n",
      "episode : 12010, reward mean : 0.06087000916711986, total_step : 482185, cur_epsilon : 0.02\n",
      "episode : 12020, reward mean : 0.06659000912867487, total_step : 482512, cur_epsilon : 0.02\n",
      "episode : 12030, reward mean : 0.07059000917337835, total_step : 483057, cur_epsilon : 0.02\n",
      "episode : 12040, reward mean : 0.0753100091572851, total_step : 483537, cur_epsilon : 0.02\n",
      "episode : 12050, reward mean : 0.06882000921294093, total_step : 484115, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12050, reward mean : -0.1649999862536788, step mean : 61.95, eval_episode_rewards : [ 0.91 -1.    0.98 -1.    0.98  0.89 -1.   -1.    0.95 -1.    0.7  -1.\n",
      " -1.   -1.    0.37 -1.    0.96 -1.   -1.    0.96]\n",
      "episode : 12060, reward mean : 0.07121000920422375, total_step : 484440, cur_epsilon : 0.02\n",
      "episode : 12070, reward mean : 0.07032000924646854, total_step : 484899, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 12080, reward mean : 0.07226000931486487, total_step : 485492, cur_epsilon : 0.02\n",
      "episode : 12090, reward mean : 0.06860000930726529, total_step : 485935, cur_epsilon : 0.02\n",
      "episode : 12100, reward mean : 0.07820000924915076, total_step : 486112, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12100, reward mean : -0.46899998281151056, step mean : 77.2, eval_episode_rewards : [-1.   -1.    0.72 -1.   -1.    0.28  0.86 -1.   -1.    0.98  0.94 -1.\n",
      " -1.   -1.   -1.    0.84 -1.   -1.   -1.   -1.  ]\n",
      "episode : 12110, reward mean : 0.08493000923283398, total_step : 486578, cur_epsilon : 0.02\n",
      "episode : 12120, reward mean : 0.08075000930391252, total_step : 487263, cur_epsilon : 0.02\n",
      "episode : 12130, reward mean : 0.07249000933207571, total_step : 487691, cur_epsilon : 0.02\n",
      "episode : 12140, reward mean : 0.06340000942349434, total_step : 488406, cur_epsilon : 0.02\n",
      "episode : 12150, reward mean : 0.0566400094628334, total_step : 488885, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12150, reward mean : -0.3324999836273491, step mean : 73.65, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.63 -1.    0.57 -1.   -1.    0.75  0.47 -1.\n",
      " -1.    0.95 -1.    0.93  0.13 -1.    0.92 -1.  ]\n",
      "episode : 12160, reward mean : 0.06683000939153134, total_step : 489123, cur_epsilon : 0.02\n",
      "episode : 12170, reward mean : 0.06950000937655568, total_step : 489486, cur_epsilon : 0.02\n",
      "episode : 12180, reward mean : 0.07368000930547715, total_step : 489784, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 12190, reward mean : 0.07598000927641987, total_step : 490195, cur_epsilon : 0.02\n",
      "episode : 12200, reward mean : 0.07817000927217305, total_step : 490527, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12200, reward mean : -0.31749998396262524, step mean : 72.15, eval_episode_rewards : [-1.   -1.   -1.   -1.    1.    0.12 -1.    0.28  0.83 -1.   -1.   -1.\n",
      " -1.    0.66 -1.    0.88 -1.   -1.    0.95  0.93]\n",
      "episode : 12210, reward mean : 0.08125000927038491, total_step : 490861, cur_epsilon : 0.02\n",
      "episode : 12220, reward mean : 0.0732900093588978, total_step : 491514, cur_epsilon : 0.02\n",
      "episode : 12230, reward mean : 0.0753500093575567, total_step : 491964, cur_epsilon : 0.02\n",
      "episode : 12240, reward mean : 0.07114000936225057, total_step : 492269, cur_epsilon : 0.02\n",
      "episode : 12250, reward mean : 0.06907000931911171, total_step : 492638, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12250, reward mean : -0.24499998446553944, step mean : 69.95, eval_episode_rewards : [ 0.53 -1.    0.77 -1.    0.76 -1.   -1.   -1.    0.81 -1.   -1.    0.91\n",
      "  0.79  0.63 -1.   -1.   -1.    0.47  0.43 -1.  ]\n",
      "episode : 12260, reward mean : 0.0694400093331933, total_step : 493140, cur_epsilon : 0.02\n",
      "episode : 12270, reward mean : 0.0750700093191117, total_step : 493489, cur_epsilon : 0.02\n",
      "episode : 12280, reward mean : 0.08121000931598246, total_step : 493766, cur_epsilon : 0.02\n",
      "episode : 12290, reward mean : 0.07942000926658511, total_step : 494073, cur_epsilon : 0.02\n",
      "episode : 12300, reward mean : 0.08276000925898552, total_step : 494493, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12300, reward mean : -0.3784999837167561, step mean : 73.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.6  -1.   -1.    0.89 -1.   -1.\n",
      "  0.89  0.54 -1.    0.65 -1.    0.97  0.89 -1.  ]\n",
      "episode : 12310, reward mean : 0.07708000929653644, total_step : 494985, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 12320, reward mean : 0.07770000930503011, total_step : 495361, cur_epsilon : 0.02\n",
      "episode : 12330, reward mean : 0.07563000928424299, total_step : 495684, cur_epsilon : 0.02\n",
      "episode : 12340, reward mean : 0.08104000925272703, total_step : 495902, cur_epsilon : 0.02\n",
      "episode : 12350, reward mean : 0.07974000923708081, total_step : 496249, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12350, reward mean : -0.39849998326972125, step mean : 75.2, eval_episode_rewards : [-1.   -1.   -1.    0.94 -1.    0.08  0.97 -1.    0.9   0.41 -1.   -1.\n",
      "  0.98  0.75 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 12360, reward mean : 0.07888000923395157, total_step : 496750, cur_epsilon : 0.02\n",
      "episode : 12370, reward mean : 0.08600000916421413, total_step : 497003, cur_epsilon : 0.02\n",
      "episode : 12380, reward mean : 0.08927000918053091, total_step : 497462, cur_epsilon : 0.02\n",
      "episode : 12390, reward mean : 0.08913000916130841, total_step : 497846, cur_epsilon : 0.02\n",
      "episode : 12400, reward mean : 0.09441000915504992, total_step : 498280, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12400, reward mean : -0.458999983035028, step mean : 76.2, eval_episode_rewards : [-1.    0.91 -1.   -1.    0.56 -1.    0.94  0.85 -1.    0.78 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.78]\n",
      "episode : 12410, reward mean : 0.09181000914610922, total_step : 498574, cur_epsilon : 0.02\n",
      "episode : 12420, reward mean : 0.08924000911414623, total_step : 498999, cur_epsilon : 0.02\n",
      "episode : 12430, reward mean : 0.0826600091047585, total_step : 499285, cur_epsilon : 0.02\n",
      "episode : 12440, reward mean : 0.07453000915236771, total_step : 499756, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 12450, reward mean : 0.07280000916868448, total_step : 500149, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12450, reward mean : -0.636499981302768, step mean : 83.85, eval_episode_rewards : [-1.   -1.    0.89 -1.    0.77  0.68  0.93 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 12460, reward mean : 0.0743300091791898, total_step : 500580, cur_epsilon : 0.02\n",
      "episode : 12470, reward mean : 0.07098000923171639, total_step : 501088, cur_epsilon : 0.02\n",
      "episode : 12480, reward mean : 0.06467000921629369, total_step : 501505, cur_epsilon : 0.02\n",
      "episode : 12490, reward mean : 0.05934000924602151, total_step : 502001, cur_epsilon : 0.02\n",
      "episode : 12500, reward mean : 0.06061000926233828, total_step : 502307, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12500, reward mean : -0.1564999853260815, step mean : 66.15, eval_episode_rewards : [ 0.84  0.66 -1.   -1.   -1.    0.36 -1.   -1.   -1.   -1.    0.25 -1.\n",
      "  0.72  0.99  0.87 -1.    0.35 -1.    0.89  0.94]\n",
      "episode : 12510, reward mean : 0.06192000934481621, total_step : 502955, cur_epsilon : 0.02\n",
      "episode : 12520, reward mean : 0.065330009335652, total_step : 503361, cur_epsilon : 0.02\n",
      "episode : 12530, reward mean : 0.06900000932067632, total_step : 503824, cur_epsilon : 0.02\n",
      "episode : 12540, reward mean : 0.07134000933542847, total_step : 504156, cur_epsilon : 0.02\n",
      "episode : 12550, reward mean : 0.06643000940047204, total_step : 504675, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12550, reward mean : -0.17649998599663377, step mean : 63.1, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.63 -1.    0.96 -1.   -1.    0.94 -1.   -1.\n",
      "  0.96  0.54  0.96  0.48  1.   -1.   -1.    1.  ]\n",
      "synced target net\n",
      "episode : 12560, reward mean : 0.06573000939376653, total_step : 505155, cur_epsilon : 0.02\n",
      "episode : 12570, reward mean : 0.0645700093973428, total_step : 505665, cur_epsilon : 0.02\n",
      "episode : 12580, reward mean : 0.06069000941701233, total_step : 506141, cur_epsilon : 0.02\n",
      "episode : 12590, reward mean : 0.06215000940673053, total_step : 506619, cur_epsilon : 0.02\n",
      "episode : 12600, reward mean : 0.0675100093986839, total_step : 507099, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12600, reward mean : -0.4949999822303653, step mean : 79.8, eval_episode_rewards : [-1.   -1.    0.43 -1.    0.84 -1.   -1.   -1.    0.61 -1.   -1.   -1.\n",
      "  0.93 -1.    0.93 -1.    0.36 -1.   -1.   -1.  ]\n",
      "episode : 12610, reward mean : 0.06297000934369862, total_step : 507506, cur_epsilon : 0.02\n",
      "episode : 12620, reward mean : 0.06564000937342644, total_step : 508040, cur_epsilon : 0.02\n",
      "episode : 12630, reward mean : 0.06292000936716795, total_step : 508430, cur_epsilon : 0.02\n",
      "episode : 12640, reward mean : 0.061320009358227255, total_step : 508771, cur_epsilon : 0.02\n",
      "episode : 12650, reward mean : 0.06079000937007368, total_step : 509319, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12650, reward mean : -0.4269999837502837, step mean : 73.0, eval_episode_rewards : [-1.   -1.    0.89  0.9  -1.   -1.    0.97  0.92 -1.   -1.   -1.    0.96\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.82 -1.  ]\n",
      "episode : 12660, reward mean : 0.060050009364262226, total_step : 509756, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 12670, reward mean : 0.058820009369403126, total_step : 510280, cur_epsilon : 0.02\n",
      "episode : 12680, reward mean : 0.058950009344145654, total_step : 510659, cur_epsilon : 0.02\n",
      "episode : 12690, reward mean : 0.058650009306147693, total_step : 510911, cur_epsilon : 0.02\n",
      "episode : 12700, reward mean : 0.05647000931017101, total_step : 511216, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12700, reward mean : -0.4984999821521342, step mean : 80.15, eval_episode_rewards : [-1.   -1.   -1.    0.46 -1.    0.96  0.89  0.96 -1.    0.3  -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.46 -1.  ]\n",
      "episode : 12710, reward mean : 0.0533900092896074, total_step : 511530, cur_epsilon : 0.02\n",
      "episode : 12720, reward mean : 0.05367000935040414, total_step : 512170, cur_epsilon : 0.02\n",
      "episode : 12730, reward mean : 0.05329000935889781, total_step : 512613, cur_epsilon : 0.02\n",
      "episode : 12740, reward mean : 0.05950000930950045, total_step : 512934, cur_epsilon : 0.02\n",
      "episode : 12750, reward mean : 0.060500009287148715, total_step : 513278, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12750, reward mean : 0.06550001194700598, step mean : 54.05, eval_episode_rewards : [ 0.93 -1.    0.75  0.94  0.86 -1.    0.31  0.47 -1.   -1.    0.56  0.71\n",
      "  0.98  0.96 -1.    0.93 -1.    0.91 -1.   -1.  ]\n",
      "episode : 12760, reward mean : 0.06294000925496221, total_step : 513664, cur_epsilon : 0.02\n",
      "episode : 12770, reward mean : 0.0650300092753023, total_step : 514064, cur_epsilon : 0.02\n",
      "episode : 12780, reward mean : 0.0660900092292577, total_step : 514306, cur_epsilon : 0.02\n",
      "episode : 12790, reward mean : 0.06230000924691558, total_step : 514729, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 12800, reward mean : 0.0665100092869252, total_step : 515363, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12800, reward mean : -0.07949998704716563, step mean : 58.45, eval_episode_rewards : [ 1.   -1.    0.91  0.5  -1.    0.91 -1.    0.72 -1.    0.86 -1.   -1.\n",
      "  0.71 -1.   -1.   -1.   -1.    0.86  0.95  0.99]\n",
      "episode : 12810, reward mean : 0.0629300093445927, total_step : 515984, cur_epsilon : 0.02\n",
      "episode : 12820, reward mean : 0.0599900092985481, total_step : 516281, cur_epsilon : 0.02\n",
      "episode : 12830, reward mean : 0.0609200092330575, total_step : 516597, cur_epsilon : 0.02\n",
      "episode : 12840, reward mean : 0.06767000919394195, total_step : 516858, cur_epsilon : 0.02\n",
      "episode : 12850, reward mean : 0.07340000917762518, total_step : 517319, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12850, reward mean : -0.16699998620897533, step mean : 62.15, eval_episode_rewards : [-1.    0.78  0.28 -1.    0.92  0.89  0.96 -1.   -1.   -1.    0.97 -1.\n",
      "  0.98 -1.   -1.    0.93 -1.   -1.   -1.    0.95]\n",
      "episode : 12860, reward mean : 0.06851000919751823, total_step : 517848, cur_epsilon : 0.02\n",
      "episode : 12870, reward mean : 0.059680009193718436, total_step : 518165, cur_epsilon : 0.02\n",
      "episode : 12880, reward mean : 0.05798000918701291, total_step : 518442, cur_epsilon : 0.02\n",
      "episode : 12890, reward mean : 0.05973000914789736, total_step : 518797, cur_epsilon : 0.02\n",
      "episode : 12900, reward mean : 0.05940000911056995, total_step : 519039, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12900, reward mean : -0.45649998309090734, step mean : 75.95, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.96  0.28 -1.   -1.   -1.    0.97  1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.97  0.69 -1.   -1.  ]\n",
      "episode : 12910, reward mean : 0.06444000908732414, total_step : 519423, cur_epsilon : 0.02\n",
      "episode : 12920, reward mean : 0.0710600090958178, total_step : 519842, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 12930, reward mean : 0.0667500090803951, total_step : 520319, cur_epsilon : 0.02\n",
      "episode : 12940, reward mean : 0.06597000903077424, total_step : 520535, cur_epsilon : 0.02\n",
      "episode : 12950, reward mean : 0.06129000904597342, total_step : 520888, cur_epsilon : 0.02\n",
      "[EVAL] episode : 12950, reward mean : -0.16449998514726757, step mean : 66.95, eval_episode_rewards : [ 0.24 -1.    0.92 -1.    0.93 -1.    0.53  0.9   0.58 -1.   -1.   -1.\n",
      "  0.43  0.54 -1.    0.74 -1.   -1.   -1.    0.9 ]\n",
      "episode : 12960, reward mean : 0.06654000901803374, total_step : 521143, cur_epsilon : 0.02\n",
      "episode : 12970, reward mean : 0.06562000903859734, total_step : 521539, cur_epsilon : 0.02\n",
      "episode : 12980, reward mean : 0.0707100089918822, total_step : 522013, cur_epsilon : 0.02\n",
      "episode : 12990, reward mean : 0.07850000895187259, total_step : 522237, cur_epsilon : 0.02\n",
      "episode : 13000, reward mean : 0.07832000897824765, total_step : 522680, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13000, reward mean : -0.13949998570606112, step mean : 64.45, eval_episode_rewards : [-1.    0.21  0.78  0.99  0.56  0.43 -1.   -1.   -1.    0.95  0.81  0.79\n",
      "  0.81 -1.   -1.   -1.   -1.   -1.   -1.    0.88]\n",
      "episode : 13010, reward mean : 0.07858000899478793, total_step : 523302, cur_epsilon : 0.02\n",
      "episode : 13020, reward mean : 0.06410000905022026, total_step : 523875, cur_epsilon : 0.02\n",
      "episode : 13030, reward mean : 0.060250009002164005, total_step : 524207, cur_epsilon : 0.02\n",
      "episode : 13040, reward mean : 0.05739000897668302, total_step : 524575, cur_epsilon : 0.02\n",
      "episode : 13050, reward mean : 0.06313000891543925, total_step : 524882, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13050, reward mean : -0.5119999818503856, step mean : 81.5, eval_episode_rewards : [-1.   -1.   -1.    0.66 -1.    0.48 -1.   -1.   -1.    0.66 -1.    0.95\n",
      "  0.07 -1.    0.94 -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 13060, reward mean : 0.06272000890225173, total_step : 525149, cur_epsilon : 0.02\n",
      "episode : 13070, reward mean : 0.061120008893311026, total_step : 525570, cur_epsilon : 0.02\n",
      "episode : 13080, reward mean : 0.06667000885866582, total_step : 526010, cur_epsilon : 0.02\n",
      "episode : 13090, reward mean : 0.07094000885263085, total_step : 526426, cur_epsilon : 0.02\n",
      "episode : 13100, reward mean : 0.06070000894740224, total_step : 527023, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13100, reward mean : 0.09100001137703657, step mean : 51.5, eval_episode_rewards : [-1.    0.94 -1.    0.76 -1.    0.94  0.95 -1.    0.97 -1.    0.52 -1.\n",
      "  0.9   0.64 -1.    0.52  0.94  0.78 -1.    0.96]\n",
      "episode : 13110, reward mean : 0.05926000893488526, total_step : 527431, cur_epsilon : 0.02\n",
      "episode : 13120, reward mean : 0.061030008872970935, total_step : 527840, cur_epsilon : 0.02\n",
      "episode : 13130, reward mean : 0.06511000884883106, total_step : 528161, cur_epsilon : 0.02\n",
      "episode : 13140, reward mean : 0.06762000874802471, total_step : 528427, cur_epsilon : 0.02\n",
      "episode : 13150, reward mean : 0.06621000873483718, total_step : 528847, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13150, reward mean : 0.2030000088736415, step mean : 40.4, eval_episode_rewards : [ 0.96  0.49  1.    0.96 -1.   -1.    0.94 -1.    0.69 -1.   -1.    0.9\n",
      " -1.    0.88  0.9   0.92 -1.51  0.98  0.97  0.98]\n",
      "episode : 13160, reward mean : 0.061280008755624296, total_step : 529176, cur_epsilon : 0.02\n",
      "episode : 13170, reward mean : 0.053810008788481356, total_step : 529686, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 13180, reward mean : 0.05625000880099833, total_step : 530039, cur_epsilon : 0.02\n",
      "episode : 13190, reward mean : 0.05602000880613923, total_step : 530473, cur_epsilon : 0.02\n",
      "episode : 13200, reward mean : 0.0528900088313967, total_step : 530918, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13200, reward mean : -0.258499985281378, step mean : 66.25, eval_episode_rewards : [-1.   -1.    0.46  0.82 -1.   -1.   -1.   -1.    0.91  0.98 -1.    0.94\n",
      "  0.83 -1.    0.94 -1.   -1.   -1.   -1.    0.95]\n",
      "episode : 13210, reward mean : 0.0488900088313967, total_step : 531250, cur_epsilon : 0.02\n",
      "episode : 13220, reward mean : 0.04962000879272819, total_step : 531731, cur_epsilon : 0.02\n",
      "episode : 13230, reward mean : 0.04939000879786909, total_step : 532202, cur_epsilon : 0.02\n",
      "episode : 13240, reward mean : 0.054190008802339436, total_step : 532526, cur_epsilon : 0.02\n",
      "episode : 13250, reward mean : 0.05870000883564353, total_step : 533044, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13250, reward mean : -0.47449998268857596, step mean : 77.75, eval_episode_rewards : [-1.   -1.   -1.    0.45 -1.   -1.   -1.    0.93  0.35  0.98 -1.   -1.\n",
      " -1.   -1.   -1.    0.83  0.97 -1.   -1.   -1.  ]\n",
      "episode : 13260, reward mean : 0.060450008774176237, total_step : 533274, cur_epsilon : 0.02\n",
      "episode : 13270, reward mean : 0.06080000876635313, total_step : 533590, cur_epsilon : 0.02\n",
      "episode : 13280, reward mean : 0.05780000878870487, total_step : 533967, cur_epsilon : 0.02\n",
      "episode : 13290, reward mean : 0.06280000878870487, total_step : 534275, cur_epsilon : 0.02\n",
      "episode : 13300, reward mean : 0.06287000878714025, total_step : 534688, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13300, reward mean : -0.43599998354911806, step mean : 73.9, eval_episode_rewards : [-1.   -1.    0.96 -1.   -1.    0.94  0.72 -1.   -1.   -1.   -1.    0.75\n",
      " -1.   -1.   -1.   -1.    0.92  0.99 -1.   -1.  ]\n",
      "episode : 13310, reward mean : 0.07071000874601305, total_step : 534998, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 13320, reward mean : 0.06978000876680017, total_step : 535469, cur_epsilon : 0.02\n",
      "episode : 13330, reward mean : 0.07288000878691674, total_step : 535882, cur_epsilon : 0.02\n",
      "episode : 13340, reward mean : 0.06344000881910324, total_step : 536244, cur_epsilon : 0.02\n",
      "episode : 13350, reward mean : 0.05811000884883106, total_step : 536722, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13350, reward mean : -0.5444999822415412, step mean : 79.7, eval_episode_rewards : [ 1.   -1.   -1.   -1.    0.95 -1.   -1.   -1.    0.46  0.97 -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.73 -1.   -1.   -1.  ]\n",
      "episode : 13360, reward mean : 0.061380008798092606, total_step : 536997, cur_epsilon : 0.02\n",
      "episode : 13370, reward mean : 0.05816000882536173, total_step : 537372, cur_epsilon : 0.02\n",
      "episode : 13380, reward mean : 0.059760008811950684, total_step : 537772, cur_epsilon : 0.02\n",
      "episode : 13390, reward mean : 0.05713000882603228, total_step : 538217, cur_epsilon : 0.02\n",
      "episode : 13400, reward mean : 0.05297000878490508, total_step : 538469, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13400, reward mean : -0.08799998685717583, step mean : 59.3, eval_episode_rewards : [ 0.98  0.27  0.91 -1.   -1.   -1.   -1.    0.52  0.99 -1.    0.9   0.94\n",
      " -1.   -1.   -1.    0.97 -1.    0.98  0.78 -1.  ]\n",
      "episode : 13410, reward mean : 0.05110000880435109, total_step : 538849, cur_epsilon : 0.02\n",
      "episode : 13420, reward mean : 0.05494000880792737, total_step : 539290, cur_epsilon : 0.02\n",
      "episode : 13430, reward mean : 0.059400008842349056, total_step : 539730, cur_epsilon : 0.02\n",
      "episode : 13440, reward mean : 0.06180000878870487, total_step : 539963, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 13450, reward mean : 0.06144000881910324, total_step : 540493, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13450, reward mean : -0.08499998692423105, step mean : 59.0, eval_episode_rewards : [-1.    0.88  0.92 -1.   -1.    0.64  0.1   0.92 -1.   -1.    1.    0.95\n",
      " -1.    1.   -1.   -1.    0.97 -1.    0.92 -1.  ]\n",
      "episode : 13460, reward mean : 0.06657000881619751, total_step : 540910, cur_epsilon : 0.02\n",
      "episode : 13470, reward mean : 0.07154000879451633, total_step : 541323, cur_epsilon : 0.02\n",
      "episode : 13480, reward mean : 0.07730000884458423, total_step : 541964, cur_epsilon : 0.02\n",
      "episode : 13490, reward mean : 0.0753200088441372, total_step : 542458, cur_epsilon : 0.02\n",
      "episode : 13500, reward mean : 0.07350000886246562, total_step : 542847, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13500, reward mean : 0.016000011935830116, step mean : 53.95, eval_episode_rewards : [-1.    0.81  0.78  0.98 -1.   -1.   -1.    0.94  0.73  0.95 -1.    0.99\n",
      "  0.68  0.9  -1.   -1.    0.63  0.93 -1.   -1.  ]\n",
      "episode : 13510, reward mean : 0.07713000880368054, total_step : 543233, cur_epsilon : 0.02\n",
      "episode : 13520, reward mean : 0.07746000881865621, total_step : 543707, cur_epsilon : 0.02\n",
      "episode : 13530, reward mean : 0.07584000881016255, total_step : 544134, cur_epsilon : 0.02\n",
      "episode : 13540, reward mean : 0.0747800088338554, total_step : 544572, cur_epsilon : 0.02\n",
      "episode : 13550, reward mean : 0.08031000879965723, total_step : 544940, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13550, reward mean : -0.5414999823085964, step mean : 79.4, eval_episode_rewards : [-1.   -1.   -1.    0.74  0.96 -1.   -1.   -1.   -1.    0.63 -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.89 -1.   -1.    0.95]\n",
      "synced target net\n",
      "episode : 13560, reward mean : 0.08358000874891877, total_step : 545194, cur_epsilon : 0.02\n",
      "episode : 13570, reward mean : 0.08339000873081386, total_step : 545624, cur_epsilon : 0.02\n",
      "episode : 13580, reward mean : 0.08674000872299076, total_step : 546066, cur_epsilon : 0.02\n",
      "episode : 13590, reward mean : 0.08610000871494412, total_step : 546507, cur_epsilon : 0.02\n",
      "episode : 13600, reward mean : 0.0812600086890161, total_step : 546873, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13600, reward mean : -0.12449998715892434, step mean : 57.9, eval_episode_rewards : [-1.   -1.   -1.    0.99  0.96  0.92 -1.   -1.    0.99  0.98 -1.   -1.\n",
      "  1.   -1.    0.82 -1.    0.92 -1.    0.93 -1.  ]\n",
      "episode : 13610, reward mean : 0.08522000866755843, total_step : 547183, cur_epsilon : 0.02\n",
      "episode : 13620, reward mean : 0.08328000868856907, total_step : 547808, cur_epsilon : 0.02\n",
      "episode : 13630, reward mean : 0.09148000868409872, total_step : 548178, cur_epsilon : 0.02\n",
      "episode : 13640, reward mean : 0.08916000869125128, total_step : 548551, cur_epsilon : 0.02\n",
      "episode : 13650, reward mean : 0.08965000863559544, total_step : 548852, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13650, reward mean : -0.5649999817833304, step mean : 81.75, eval_episode_rewards : [-1.   -1.    0.4  -1.   -1.    0.78 -1.   -1.   -1.    0.96 -1.   -1.\n",
      " -1.   -1.    0.84  0.72 -1.   -1.   -1.   -1.  ]\n",
      "episode : 13660, reward mean : 0.09922000857815147, total_step : 549033, cur_epsilon : 0.02\n",
      "episode : 13670, reward mean : 0.10171000854484737, total_step : 549409, cur_epsilon : 0.02\n",
      "episode : 13680, reward mean : 0.09747000852786004, total_step : 549713, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 13690, reward mean : 0.09379000858776271, total_step : 550230, cur_epsilon : 0.02\n",
      "episode : 13700, reward mean : 0.09640000859647989, total_step : 550573, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13700, reward mean : -0.30249998429790137, step mean : 70.65, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.95  0.98 -1.   -1.   -1.   -1.\n",
      "  0.88  0.76 -1.   -1.    0.01  0.74  0.64  0.99]\n",
      "episode : 13710, reward mean : 0.08843000864051283, total_step : 551082, cur_epsilon : 0.02\n",
      "episode : 13720, reward mean : 0.08833000857569277, total_step : 551433, cur_epsilon : 0.02\n",
      "episode : 13730, reward mean : 0.08658000859245658, total_step : 551950, cur_epsilon : 0.02\n",
      "episode : 13740, reward mean : 0.08465000863559544, total_step : 552462, cur_epsilon : 0.02\n",
      "episode : 13750, reward mean : 0.08276000865548849, total_step : 552894, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13750, reward mean : -0.1634999862872064, step mean : 61.8, eval_episode_rewards : [ 0.93 -1.   -1.   -1.   -1.    0.88  0.97 -1.    0.92 -1.    0.92 -1.\n",
      " -1.    0.9   0.9  -1.   -1.    0.53  0.78 -1.  ]\n",
      "episode : 13760, reward mean : 0.07708000867068768, total_step : 553347, cur_epsilon : 0.02\n",
      "episode : 13770, reward mean : 0.06957000868208707, total_step : 553797, cur_epsilon : 0.02\n",
      "episode : 13780, reward mean : 0.06560000874847173, total_step : 554335, cur_epsilon : 0.02\n",
      "episode : 13790, reward mean : 0.0717900087442249, total_step : 554739, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 13800, reward mean : 0.07257000868208706, total_step : 555099, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13800, reward mean : -0.2559999853372574, step mean : 66.0, eval_episode_rewards : [ 0.94  0.83  0.97 -1.    0.81 -1.   -1.    0.66  0.77  0.97 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.93]\n",
      "episode : 13810, reward mean : 0.06926000862196088, total_step : 555453, cur_epsilon : 0.02\n",
      "episode : 13820, reward mean : 0.07497000862844289, total_step : 555779, cur_epsilon : 0.02\n",
      "episode : 13830, reward mean : 0.07674000863358378, total_step : 556120, cur_epsilon : 0.02\n",
      "episode : 13840, reward mean : 0.06893000865168869, total_step : 556461, cur_epsilon : 0.02\n",
      "episode : 13850, reward mean : 0.061790008654817936, total_step : 556933, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13850, reward mean : 0.14250001134350895, step mean : 51.4, eval_episode_rewards : [ 0.77 -1.   -1.    0.36 -1.   -1.    0.99  0.93  0.99  0.34  0.9  -1.\n",
      "  0.94  0.99  0.87  0.78  0.58  0.41 -1.   -1.  ]\n",
      "episode : 13860, reward mean : 0.0621600086465478, total_step : 557425, cur_epsilon : 0.02\n",
      "episode : 13870, reward mean : 0.06611000867001712, total_step : 557846, cur_epsilon : 0.02\n",
      "episode : 13880, reward mean : 0.06701000869460404, total_step : 558233, cur_epsilon : 0.02\n",
      "episode : 13890, reward mean : 0.06329000873304903, total_step : 558758, cur_epsilon : 0.02\n",
      "episode : 13900, reward mean : 0.0598400087878108, total_step : 559244, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13900, reward mean : -0.13349998472258448, step mean : 68.9, eval_episode_rewards : [ 0.98  0.14 -1.    0.65  0.95 -1.   -1.   -1.    0.53 -1.    0.36 -1.\n",
      "  0.53 -1.   -1.    0.99 -1.    0.97  0.15  0.08]\n",
      "episode : 13910, reward mean : 0.05666000876948237, total_step : 559546, cur_epsilon : 0.02\n",
      "episode : 13920, reward mean : 0.050860008765012024, total_step : 559945, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 13930, reward mean : 0.05400000876188278, total_step : 560409, cur_epsilon : 0.02\n",
      "episode : 13940, reward mean : 0.05772000881284475, total_step : 560851, cur_epsilon : 0.02\n",
      "episode : 13950, reward mean : 0.05263000883720815, total_step : 561313, cur_epsilon : 0.02\n",
      "[EVAL] episode : 13950, reward mean : -0.3059999842196703, step mean : 71.0, eval_episode_rewards : [ 1.   -1.   -1.    0.44 -1.   -1.    0.11  0.85  0.73  0.99 -1.   -1.\n",
      "  0.91 -1.    0.85 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 13960, reward mean : 0.0488800088763237, total_step : 561743, cur_epsilon : 0.02\n",
      "episode : 13970, reward mean : 0.04564000883698464, total_step : 561962, cur_epsilon : 0.02\n",
      "episode : 13980, reward mean : 0.04389000887610018, total_step : 562611, cur_epsilon : 0.02\n",
      "episode : 13990, reward mean : 0.039390008887276055, total_step : 562885, cur_epsilon : 0.02\n",
      "episode : 14000, reward mean : 0.03884000887721777, total_step : 563282, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14000, reward mean : -0.12549998601898552, step mean : 63.05, eval_episode_rewards : [-1.    0.98 -1.    0.82 -1.    0.47  0.91 -1.    0.93 -1.   -1.   -1.\n",
      "  0.84  0.76  0.56  0.29 -1.   -1.   -1.    0.93]\n",
      "episode : 14010, reward mean : 0.03811000880412757, total_step : 563579, cur_epsilon : 0.02\n",
      "episode : 14020, reward mean : 0.04279000876657665, total_step : 563985, cur_epsilon : 0.02\n",
      "episode : 14030, reward mean : 0.04272000879049301, total_step : 564421, cur_epsilon : 0.02\n",
      "episode : 14040, reward mean : 0.0399200088083744, total_step : 564867, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 14050, reward mean : 0.04221000882424414, total_step : 565244, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14050, reward mean : 0.16250000977888704, step mean : 44.35, eval_episode_rewards : [ 0.96 -1.    0.97  0.96 -1.   -1.    0.93  0.96  0.91  0.98  0.99 -1.\n",
      "  0.92 -1.   -1.   -1.   -1.    0.95  0.9   0.82]\n",
      "episode : 14060, reward mean : 0.04494000883027911, total_step : 565537, cur_epsilon : 0.02\n",
      "episode : 14070, reward mean : 0.042480008818209174, total_step : 565903, cur_epsilon : 0.02\n",
      "episode : 14080, reward mean : 0.040740008790045974, total_step : 566218, cur_epsilon : 0.02\n",
      "episode : 14090, reward mean : 0.04326000877842307, total_step : 566582, cur_epsilon : 0.02\n",
      "episode : 14100, reward mean : 0.04252000877261162, total_step : 567154, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14100, reward mean : -0.25349998539313673, step mean : 65.75, eval_episode_rewards : [ 0.98 -1.   -1.    0.97 -1.    1.   -1.   -1.   -1.    0.92 -1.   -1.\n",
      "  0.97  0.97 -1.    0.43 -1.    0.69 -1.   -1.  ]\n",
      "episode : 14110, reward mean : 0.044550008727237585, total_step : 567361, cur_epsilon : 0.02\n",
      "episode : 14120, reward mean : 0.046370008753612635, total_step : 567887, cur_epsilon : 0.02\n",
      "episode : 14130, reward mean : 0.04382000881060958, total_step : 568459, cur_epsilon : 0.02\n",
      "episode : 14140, reward mean : 0.04894000880792737, total_step : 568714, cur_epsilon : 0.02\n",
      "episode : 14150, reward mean : 0.049220008824020625, total_step : 569205, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14150, reward mean : -0.339499984588474, step mean : 69.3, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.97  0.97 -1.    0.73 -1.   -1.\n",
      "  0.9  -1.    0.93  0.93  0.78 -1.   -1.   -1.  ]\n",
      "episode : 14160, reward mean : 0.0474300088416785, total_step : 569614, cur_epsilon : 0.02\n",
      "episode : 14170, reward mean : 0.05071000881306827, total_step : 569996, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 14180, reward mean : 0.04969000883586705, total_step : 570451, cur_epsilon : 0.02\n",
      "episode : 14190, reward mean : 0.05272000879049301, total_step : 570683, cur_epsilon : 0.02\n",
      "episode : 14200, reward mean : 0.056770008767023684, total_step : 571022, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14200, reward mean : 0.0520000122487545, step mean : 55.4, eval_episode_rewards : [-1.    0.79  0.93  0.82  0.15  0.31  0.95  0.96  0.95 -1.    1.    0.38\n",
      " -1.   -1.   -1.   -1.    0.89 -1.    0.91 -1.  ]\n",
      "episode : 14210, reward mean : 0.059200008779764175, total_step : 571412, cur_epsilon : 0.02\n",
      "episode : 14220, reward mean : 0.06324000877887011, total_step : 571891, cur_epsilon : 0.02\n",
      "episode : 14230, reward mean : 0.06461000877059996, total_step : 572326, cur_epsilon : 0.02\n",
      "episode : 14240, reward mean : 0.05994000878557563, total_step : 572717, cur_epsilon : 0.02\n",
      "episode : 14250, reward mean : 0.05525000877864659, total_step : 573203, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14250, reward mean : -0.017499987315386534, step mean : 57.3, eval_episode_rewards : [ 0.89 -1.   -1.   -1.    0.84  0.65 -1.    0.45  0.48 -1.   -1.    0.98\n",
      "  0.89  0.94  0.67 -1.    0.93 -1.    0.93 -1.  ]\n",
      "episode : 14260, reward mean : 0.05644000881910324, total_step : 573613, cur_epsilon : 0.02\n",
      "episode : 14270, reward mean : 0.05691000885330141, total_step : 574080, cur_epsilon : 0.02\n",
      "episode : 14280, reward mean : 0.05672000887989998, total_step : 574575, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 14290, reward mean : 0.05251000892929733, total_step : 575104, cur_epsilon : 0.02\n",
      "episode : 14300, reward mean : 0.05398000891879201, total_step : 575471, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14300, reward mean : -0.33449998358264565, step mean : 73.85, eval_episode_rewards : [ 0.59  0.01 -1.   -1.   -1.   -1.    0.81  0.8  -1.    0.83 -1.   -1.\n",
      " -1.   -1.    0.95 -1.   -1.   -1.    0.43  0.89]\n",
      "episode : 14310, reward mean : 0.05888000894337893, total_step : 575891, cur_epsilon : 0.02\n",
      "episode : 14320, reward mean : 0.06329000891186297, total_step : 576220, cur_epsilon : 0.02\n",
      "episode : 14330, reward mean : 0.0639300089199096, total_step : 576668, cur_epsilon : 0.02\n",
      "episode : 14340, reward mean : 0.07182000896707177, total_step : 577239, cur_epsilon : 0.02\n",
      "episode : 14350, reward mean : 0.07333000897802412, total_step : 577766, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14350, reward mean : -0.19649998554959894, step mean : 65.1, eval_episode_rewards : [-1.   -1.    0.98 -1.   -1.    0.85 -1.    0.95  0.61  0.46  0.94 -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.99  0.38  0.91]\n",
      "episode : 14360, reward mean : 0.07313000900484622, total_step : 578160, cur_epsilon : 0.02\n",
      "episode : 14370, reward mean : 0.07408000898361206, total_step : 578440, cur_epsilon : 0.02\n",
      "episode : 14380, reward mean : 0.06549000897444784, total_step : 578798, cur_epsilon : 0.02\n",
      "episode : 14390, reward mean : 0.06613000898249448, total_step : 579278, cur_epsilon : 0.02\n",
      "episode : 14400, reward mean : 0.06592000900954008, total_step : 579650, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14400, reward mean : -0.2504999843426049, step mean : 70.5, eval_episode_rewards : [-1.    0.54 -1.   -1.    0.92  0.42 -1.   -1.    0.84 -1.    0.98 -1.\n",
      " -1.    0.14  0.99 -1.    0.64 -1.    0.52 -1.  ]\n",
      "synced target net\n",
      "episode : 14410, reward mean : 0.06489000903256238, total_step : 580131, cur_epsilon : 0.02\n",
      "episode : 14420, reward mean : 0.07045000899769366, total_step : 580418, cur_epsilon : 0.02\n",
      "episode : 14430, reward mean : 0.06972000899165869, total_step : 580832, cur_epsilon : 0.02\n",
      "episode : 14440, reward mean : 0.06859000901691616, total_step : 581176, cur_epsilon : 0.02\n",
      "episode : 14450, reward mean : 0.06337000904418529, total_step : 581826, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14450, reward mean : -0.169999985024333, step mean : 67.5, eval_episode_rewards : [ 0.93 -1.   -1.    0.79 -1.    0.92  0.11 -1.    0.43  0.43  0.1   0.97\n",
      " -1.   -1.   -1.    0.97  0.95 -1.   -1.   -1.  ]\n",
      "episode : 14460, reward mean : 0.06259000899456442, total_step : 582022, cur_epsilon : 0.02\n",
      "episode : 14470, reward mean : 0.05970000899210572, total_step : 582423, cur_epsilon : 0.02\n",
      "episode : 14480, reward mean : 0.059620008926838636, total_step : 582773, cur_epsilon : 0.02\n",
      "episode : 14490, reward mean : 0.06059000892750919, total_step : 583271, cur_epsilon : 0.02\n",
      "episode : 14500, reward mean : 0.06763000890426338, total_step : 583554, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14500, reward mean : -0.2649999851360917, step mean : 66.9, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.91 -1.    0.79 -1.   -1.    0.71 -1.    0.58\n",
      " -1.   -1.    0.9   0.86 -1.   -1.    0.95  1.  ]\n",
      "episode : 14510, reward mean : 0.06981000890024007, total_step : 583922, cur_epsilon : 0.02\n",
      "episode : 14520, reward mean : 0.06482000890001655, total_step : 584394, cur_epsilon : 0.02\n",
      "episode : 14530, reward mean : 0.06813000887073577, total_step : 584690, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 14540, reward mean : 0.06609000887162983, total_step : 585132, cur_epsilon : 0.02\n",
      "episode : 14550, reward mean : 0.06656000886112451, total_step : 585453, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14550, reward mean : -0.39749998329207303, step mean : 75.1, eval_episode_rewards : [-1.    0.88 -1.   -1.   -1.    0.92  0.57 -1.    0.21 -1.   -1.   -1.\n",
      " -1.    0.96  0.73  0.78 -1.   -1.   -1.   -1.  ]\n",
      "episode : 14560, reward mean : 0.0626300088595599, total_step : 585700, cur_epsilon : 0.02\n",
      "episode : 14570, reward mean : 0.0624200088866055, total_step : 586250, cur_epsilon : 0.02\n",
      "episode : 14580, reward mean : 0.057240008912980554, total_step : 586810, cur_epsilon : 0.02\n",
      "episode : 14590, reward mean : 0.05916000889241695, total_step : 587160, cur_epsilon : 0.02\n",
      "episode : 14600, reward mean : 0.06044000890851021, total_step : 587598, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14600, reward mean : -0.09299998562783003, step mean : 64.85, eval_episode_rewards : [-1.    0.29  0.91  0.51  0.4  -1.   -1.   -1.    0.64  0.49 -1.    0.48\n",
      "  0.93 -1.    0.94 -1.   -1.    0.57  0.98 -1.  ]\n",
      "episode : 14610, reward mean : 0.058000008940696714, total_step : 588051, cur_epsilon : 0.02\n",
      "episode : 14620, reward mean : 0.05883000889979303, total_step : 588496, cur_epsilon : 0.02\n",
      "episode : 14630, reward mean : 0.05558000886067748, total_step : 588692, cur_epsilon : 0.02\n",
      "episode : 14640, reward mean : 0.05837000882066786, total_step : 588887, cur_epsilon : 0.02\n",
      "episode : 14650, reward mean : 0.0608900088313967, total_step : 589235, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14650, reward mean : -0.27899998482316735, step mean : 68.3, eval_episode_rewards : [-1.   -1.    0.88  0.41  0.96 -1.   -1.    0.77 -1.    0.71 -1.   -1.\n",
      "  0.94 -1.    0.83 -1.   -1.   -1.   -1.    0.92]\n",
      "episode : 14660, reward mean : 0.0583600088879466, total_step : 589669, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 14670, reward mean : 0.05687000889889896, total_step : 590095, cur_epsilon : 0.02\n",
      "episode : 14680, reward mean : 0.06023000895790756, total_step : 590663, cur_epsilon : 0.02\n",
      "episode : 14690, reward mean : 0.0652200089134276, total_step : 590984, cur_epsilon : 0.02\n",
      "episode : 14700, reward mean : 0.06398000887408853, total_step : 591152, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14700, reward mean : -0.2474999855272472, step mean : 65.15, eval_episode_rewards : [-1.   -1.   -1.    0.93  0.99 -1.   -1.   -1.    0.8   0.76  0.65 -1.\n",
      " -1.   -1.    1.   -1.    0.96 -1.   -1.    0.96]\n",
      "episode : 14710, reward mean : 0.06731000888906419, total_step : 591728, cur_epsilon : 0.02\n",
      "episode : 14720, reward mean : 0.07049000892974436, total_step : 592260, cur_epsilon : 0.02\n",
      "episode : 14730, reward mean : 0.07267000888101757, total_step : 592561, cur_epsilon : 0.02\n",
      "episode : 14740, reward mean : 0.067280008867383, total_step : 593012, cur_epsilon : 0.02\n",
      "episode : 14750, reward mean : 0.07171000883542002, total_step : 593302, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14750, reward mean : -0.4989999821409583, step mean : 80.2, eval_episode_rewards : [-1.   -1.    0.89  0.14  0.99 -1.   -1.   -1.   -1.   -1.    0.86  0.14\n",
      " -1.   -1.   -1.   -1.   -1.    1.   -1.   -1.  ]\n",
      "episode : 14760, reward mean : 0.07298000885173678, total_step : 593828, cur_epsilon : 0.02\n",
      "episode : 14770, reward mean : 0.08078000883385539, total_step : 594199, cur_epsilon : 0.02\n",
      "episode : 14780, reward mean : 0.08140000879764557, total_step : 594576, cur_epsilon : 0.02\n",
      "episode : 14790, reward mean : 0.07723000880144537, total_step : 594997, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 14800, reward mean : 0.07791000880859793, total_step : 595388, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14800, reward mean : 0.07750001167878509, step mean : 52.85, eval_episode_rewards : [ 0.96  0.96  1.    0.45 -1.    0.96 -1.    0.77 -1.    0.9  -1.   -1.\n",
      " -1.   -1.    0.52  0.92 -1.    0.21  0.95  0.95]\n",
      "episode : 14810, reward mean : 0.08195000878535211, total_step : 595637, cur_epsilon : 0.02\n",
      "episode : 14820, reward mean : 0.08272000879049302, total_step : 595987, cur_epsilon : 0.02\n",
      "episode : 14830, reward mean : 0.08310000880435109, total_step : 596389, cur_epsilon : 0.02\n",
      "episode : 14840, reward mean : 0.07979000883363187, total_step : 596861, cur_epsilon : 0.02\n",
      "episode : 14850, reward mean : 0.08093000883050262, total_step : 597320, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14850, reward mean : -0.5274999826215208, step mean : 78.0, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    1.   -1.    1.   -1.    0.48\n",
      " -1.   -1.   -1.   -1.    1.    0.97 -1.   -1.  ]\n",
      "episode : 14860, reward mean : 0.08364000881463289, total_step : 597741, cur_epsilon : 0.02\n",
      "episode : 14870, reward mean : 0.08119000880233944, total_step : 598108, cur_epsilon : 0.02\n",
      "episode : 14880, reward mean : 0.08422000877931714, total_step : 598392, cur_epsilon : 0.02\n",
      "episode : 14890, reward mean : 0.08908000876009464, total_step : 598833, cur_epsilon : 0.02\n",
      "episode : 14900, reward mean : 0.0862400087788701, total_step : 599401, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14900, reward mean : -0.3214999838732183, step mean : 72.55, eval_episode_rewards : [ 0.09  0.88 -1.   -1.   -1.    0.93 -1.   -1.   -1.    0.95  0.05 -1.\n",
      " -1.   -1.   -1.   -1.    0.77  0.92  0.98 -1.  ]\n",
      "episode : 14910, reward mean : 0.08234000882133842, total_step : 599891, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 14920, reward mean : 0.08250000881776214, total_step : 600274, cur_epsilon : 0.02\n",
      "episode : 14930, reward mean : 0.08316000880300999, total_step : 600672, cur_epsilon : 0.02\n",
      "episode : 14940, reward mean : 0.07447000881843269, total_step : 601183, cur_epsilon : 0.02\n",
      "episode : 14950, reward mean : 0.07786000880971551, total_step : 601607, cur_epsilon : 0.02\n",
      "[EVAL] episode : 14950, reward mean : -0.12749998597428203, step mean : 63.25, eval_episode_rewards : [-1.   -1.   -1.    0.83  0.78 -1.    0.91 -1.    0.34 -1.    0.79  0.92\n",
      "  0.84 -1.   -1.    0.67  0.94  0.43 -1.   -1.  ]\n",
      "episode : 14960, reward mean : 0.07980000876635313, total_step : 601843, cur_epsilon : 0.02\n",
      "episode : 14970, reward mean : 0.07760000881552696, total_step : 602280, cur_epsilon : 0.02\n",
      "episode : 14980, reward mean : 0.07599000880680978, total_step : 602890, cur_epsilon : 0.02\n",
      "episode : 14990, reward mean : 0.0755000087954104, total_step : 603112, cur_epsilon : 0.02\n",
      "episode : 15000, reward mean : 0.0735000087954104, total_step : 603509, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15000, reward mean : -0.032499986980110404, step mean : 58.8, eval_episode_rewards : [ 0.94 -1.    0.87  0.85 -1.   -1.    0.93  0.93  0.57 -1.    0.94 -1.\n",
      "  0.9   0.98 -1.    0.22  0.22 -1.   -1.   -1.  ]\n",
      "episode : 15010, reward mean : 0.07765000879205763, total_step : 603791, cur_epsilon : 0.02\n",
      "episode : 15020, reward mean : 0.08415000875853001, total_step : 604048, cur_epsilon : 0.02\n",
      "episode : 15030, reward mean : 0.08420000873506069, total_step : 604382, cur_epsilon : 0.02\n",
      "episode : 15040, reward mean : 0.09062000872567295, total_step : 604788, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15050, reward mean : 0.08876000872254372, total_step : 605151, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15050, reward mean : -0.01449998738244176, step mean : 57.0, eval_episode_rewards : [-1.   -1.    0.96 -1.    0.99  0.32 -1.   -1.    0.62  1.    0.43 -1.\n",
      " -1.    0.84  0.74 -1.    0.97  0.95 -1.    0.89]\n",
      "episode : 15060, reward mean : 0.08099000878445804, total_step : 605720, cur_epsilon : 0.02\n",
      "episode : 15070, reward mean : 0.08582000876590609, total_step : 606003, cur_epsilon : 0.02\n",
      "episode : 15080, reward mean : 0.08504000878334045, total_step : 606394, cur_epsilon : 0.02\n",
      "episode : 15090, reward mean : 0.08052000879496336, total_step : 606810, cur_epsilon : 0.02\n",
      "episode : 15100, reward mean : 0.08454000874981284, total_step : 607182, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15100, reward mean : -0.09799998663365841, step mean : 60.3, eval_episode_rewards : [ 0.9   1.    1.   -1.    0.54 -1.   -1.    0.9  -1.   -1.   -1.    0.92\n",
      "  0.95  0.82 -1.    0.03 -1.    0.98 -1.   -1.  ]\n",
      "episode : 15110, reward mean : 0.08392000876367092, total_step : 607451, cur_epsilon : 0.02\n",
      "episode : 15120, reward mean : 0.08775000872276724, total_step : 607796, cur_epsilon : 0.02\n",
      "episode : 15130, reward mean : 0.08944000868499279, total_step : 608203, cur_epsilon : 0.02\n",
      "episode : 15140, reward mean : 0.08545000870712101, total_step : 608556, cur_epsilon : 0.02\n",
      "episode : 15150, reward mean : 0.08973000867851079, total_step : 608920, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15150, reward mean : -0.5279999826103449, step mean : 78.05, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.95  0.99 -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.7  -1.   -1.    0.86  0.94 -1.   -1.   -1.  ]\n",
      "episode : 15160, reward mean : 0.08973000865615904, total_step : 609230, cur_epsilon : 0.02\n",
      "episode : 15170, reward mean : 0.09277000863291324, total_step : 609510, cur_epsilon : 0.02\n",
      "episode : 15180, reward mean : 0.08957000863738358, total_step : 609986, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15190, reward mean : 0.09328000864386558, total_step : 610247, cur_epsilon : 0.02\n",
      "episode : 15200, reward mean : 0.08851000863872469, total_step : 610564, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15200, reward mean : -0.24999998435378074, step mean : 70.45, eval_episode_rewards : [-1.    0.39 -1.   -1.    0.35 -1.   -1.   -1.   -1.    0.55 -1.    0.47\n",
      " -1.   -1.    0.76  0.78  0.89  0.94 -1.    0.87]\n",
      "episode : 15210, reward mean : 0.0907900086324662, total_step : 610926, cur_epsilon : 0.02\n",
      "episode : 15220, reward mean : 0.09152000863850117, total_step : 611431, cur_epsilon : 0.02\n",
      "episode : 15230, reward mean : 0.0914800086170435, total_step : 611771, cur_epsilon : 0.02\n",
      "episode : 15240, reward mean : 0.08912000860273837, total_step : 612099, cur_epsilon : 0.02\n",
      "episode : 15250, reward mean : 0.09268000859022141, total_step : 612530, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15250, reward mean : -0.6144999817945063, step mean : 81.65, eval_episode_rewards : [ 0.98 -1.   -1.    0.95 -1.   -1.   -1.   -1.   -1.    0.8  -1.   -1.\n",
      " -1.    0.98 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 15260, reward mean : 0.09050000857189297, total_step : 612857, cur_epsilon : 0.02\n",
      "episode : 15270, reward mean : 0.08828000857681036, total_step : 613346, cur_epsilon : 0.02\n",
      "episode : 15280, reward mean : 0.09342000850662589, total_step : 613529, cur_epsilon : 0.02\n",
      "episode : 15290, reward mean : 0.09777000845409929, total_step : 613823, cur_epsilon : 0.02\n",
      "episode : 15300, reward mean : 0.09477000847645103, total_step : 614290, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15300, reward mean : -0.4729999827221036, step mean : 77.6, eval_episode_rewards : [ 0.91  0.39  0.96 -1.   -1.   -1.    0.93 -1.   -1.   -1.    0.38  0.97\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 15310, reward mean : 0.08600000849366188, total_step : 614787, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15320, reward mean : 0.0862700085323304, total_step : 615289, cur_epsilon : 0.02\n",
      "episode : 15330, reward mean : 0.08394000853970647, total_step : 615770, cur_epsilon : 0.02\n",
      "episode : 15340, reward mean : 0.08372000849992037, total_step : 616165, cur_epsilon : 0.02\n",
      "episode : 15350, reward mean : 0.08815000844560564, total_step : 616451, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15350, reward mean : -0.20849998528137803, step mean : 66.3, eval_episode_rewards : [ 0.95 -1.   -1.   -1.    0.41  0.77  0.82  0.43 -1.   -1.   -1.   -1.\n",
      "  0.85 -1.   -1.   -1.    0.99 -1.    0.63  0.98]\n",
      "episode : 15360, reward mean : 0.0867700084540993, total_step : 616884, cur_epsilon : 0.02\n",
      "episode : 15370, reward mean : 0.07935000848583877, total_step : 617304, cur_epsilon : 0.02\n",
      "episode : 15380, reward mean : 0.08178000849857926, total_step : 617720, cur_epsilon : 0.02\n",
      "episode : 15390, reward mean : 0.08127000853233039, total_step : 618350, cur_epsilon : 0.02\n",
      "episode : 15400, reward mean : 0.09004000849276782, total_step : 618546, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15400, reward mean : -0.5639999818056822, step mean : 81.65, eval_episode_rewards : [-1.    0.73 -1.   -1.    0.34 -1.   -1.    0.71 -1.    0.98  0.96 -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 15410, reward mean : 0.08678000847622752, total_step : 618955, cur_epsilon : 0.02\n",
      "episode : 15420, reward mean : 0.07511000855825842, total_step : 619605, cur_epsilon : 0.02\n",
      "episode : 15430, reward mean : 0.07687000854127109, total_step : 619942, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15440, reward mean : 0.07509000853635371, total_step : 620264, cur_epsilon : 0.02\n",
      "episode : 15450, reward mean : 0.07869000850059092, total_step : 620754, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15450, reward mean : -0.3899999834597111, step mean : 74.35, eval_episode_rewards : [-1.   -1.    0.56 -1.   -1.    0.46 -1.   -1.   -1.   -1.   -1.    0.75\n",
      "  0.98  0.93 -1.   -1.   -1.   -1.    0.77  0.75]\n",
      "episode : 15460, reward mean : 0.07703000856004655, total_step : 621215, cur_epsilon : 0.02\n",
      "episode : 15470, reward mean : 0.07815000857971609, total_step : 621702, cur_epsilon : 0.02\n",
      "episode : 15480, reward mean : 0.08316000857949257, total_step : 622052, cur_epsilon : 0.02\n",
      "episode : 15490, reward mean : 0.09253000854887068, total_step : 622413, cur_epsilon : 0.02\n",
      "episode : 15500, reward mean : 0.08298000860586763, total_step : 622950, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15500, reward mean : -0.5779999814927578, step mean : 83.05, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.98 -1.    0.54  0.62 -1.\n",
      " -1.   -1.    0.47 -1.   -1.   -1.    0.83 -1.  ]\n",
      "episode : 15510, reward mean : 0.08478000858798623, total_step : 623239, cur_epsilon : 0.02\n",
      "episode : 15520, reward mean : 0.0841600086018443, total_step : 623773, cur_epsilon : 0.02\n",
      "episode : 15530, reward mean : 0.08622000857815146, total_step : 623964, cur_epsilon : 0.02\n",
      "episode : 15540, reward mean : 0.0832700085323304, total_step : 624202, cur_epsilon : 0.02\n",
      "episode : 15550, reward mean : 0.08317000855691731, total_step : 624632, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15550, reward mean : -0.636499981302768, step mean : 83.85, eval_episode_rewards : [ 0.84 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.64 -1.   -1.\n",
      " -1.   -1.   -1.    0.96  0.83 -1.   -1.   -1.  ]\n",
      "episode : 15560, reward mean : 0.08561000856943428, total_step : 624936, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15570, reward mean : 0.08656000854820013, total_step : 625391, cur_epsilon : 0.02\n",
      "episode : 15580, reward mean : 0.09010000851377845, total_step : 625797, cur_epsilon : 0.02\n",
      "episode : 15590, reward mean : 0.09406000849232078, total_step : 626052, cur_epsilon : 0.02\n",
      "episode : 15600, reward mean : 0.09849000850506127, total_step : 626548, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15600, reward mean : -0.46949998280033467, step mean : 77.25, eval_episode_rewards : [ 0.9  -1.   -1.   -1.    0.99 -1.    0.86  0.74 -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.97 -1.    0.15 -1.   -1.  ]\n",
      "episode : 15610, reward mean : 0.10471000847779215, total_step : 626880, cur_epsilon : 0.02\n",
      "episode : 15620, reward mean : 0.10222000853344798, total_step : 627572, cur_epsilon : 0.02\n",
      "episode : 15630, reward mean : 0.1026200085915625, total_step : 628025, cur_epsilon : 0.02\n",
      "episode : 15640, reward mean : 0.1022100086454302, total_step : 628459, cur_epsilon : 0.02\n",
      "episode : 15650, reward mean : 0.09602000871673226, total_step : 629123, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15650, reward mean : -0.040499986801296475, step mean : 59.6, eval_episode_rewards : [ 0.97  0.7  -1.    0.85  0.79 -1.    0.98  0.94 -1.    0.48  0.92  0.18\n",
      " -1.   -1.    0.98  0.4  -1.   -1.   -1.   -1.  ]\n",
      "episode : 15660, reward mean : 0.09085000869818032, total_step : 629474, cur_epsilon : 0.02\n",
      "episode : 15670, reward mean : 0.09148000863939523, total_step : 629637, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15680, reward mean : 0.08978000863268971, total_step : 630173, cur_epsilon : 0.02\n",
      "episode : 15690, reward mean : 0.08617000862397253, total_step : 630455, cur_epsilon : 0.02\n",
      "episode : 15700, reward mean : 0.08170000872388482, total_step : 631068, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15700, reward mean : -0.2644999851472676, step mean : 66.85, eval_episode_rewards : [ 0.8  -1.   -1.   -1.   -1.    0.91  1.    0.98  0.96 -1.   -1.   -1.\n",
      " -1.    0.92 -1.   -1.   -1.    0.99  0.15 -1.  ]\n",
      "episode : 15710, reward mean : 0.08114000869169831, total_step : 631502, cur_epsilon : 0.02\n",
      "episode : 15720, reward mean : 0.08028000866621733, total_step : 631920, cur_epsilon : 0.02\n",
      "episode : 15730, reward mean : 0.07868000867962838, total_step : 632280, cur_epsilon : 0.02\n",
      "episode : 15740, reward mean : 0.07988000869750976, total_step : 632809, cur_epsilon : 0.02\n",
      "episode : 15750, reward mean : 0.07582000874355435, total_step : 633303, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15750, reward mean : -0.07599998712539673, step mean : 58.1, eval_episode_rewards : [-1.    0.97 -1.    0.88 -1.   -1.   -1.    0.87  0.84 -1.    0.78  0.84\n",
      "  0.97  0.61  0.76 -1.   -1.   -1.   -1.    0.96]\n",
      "episode : 15760, reward mean : 0.07235000868700445, total_step : 633578, cur_epsilon : 0.02\n",
      "episode : 15770, reward mean : 0.06702000871673226, total_step : 634080, cur_epsilon : 0.02\n",
      "episode : 15780, reward mean : 0.06922000871226192, total_step : 634437, cur_epsilon : 0.02\n",
      "episode : 15790, reward mean : 0.0665200086608529, total_step : 634629, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15800, reward mean : 0.06609000867046416, total_step : 635063, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15800, reward mean : -0.6604999807663262, step mean : 86.25, eval_episode_rewards : [-1.   -1.    0.89 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.82 -1.\n",
      "  0.77 -1.   -1.   -1.   -1.   -1.   -1.    0.31]\n",
      "episode : 15810, reward mean : 0.06479000865481793, total_step : 635242, cur_epsilon : 0.02\n",
      "episode : 15820, reward mean : 0.05919000869058073, total_step : 635750, cur_epsilon : 0.02\n",
      "episode : 15830, reward mean : 0.05613000871427357, total_step : 636256, cur_epsilon : 0.02\n",
      "episode : 15840, reward mean : 0.05655000870488584, total_step : 636686, cur_epsilon : 0.02\n",
      "episode : 15850, reward mean : 0.05236000870913267, total_step : 637164, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15850, reward mean : -0.2859999846667051, step mean : 69.0, eval_episode_rewards : [ 0.72 -1.   -1.   -1.    0.94  0.53  0.5  -1.   -1.   -1.    0.95  1.\n",
      " -1.   -1.   -1.    0.93 -1.    0.71 -1.   -1.  ]\n",
      "episode : 15860, reward mean : 0.05014000869169831, total_step : 637508, cur_epsilon : 0.02\n",
      "episode : 15870, reward mean : 0.05518000869080424, total_step : 637872, cur_epsilon : 0.02\n",
      "episode : 15880, reward mean : 0.04681000874377787, total_step : 638393, cur_epsilon : 0.02\n",
      "episode : 15890, reward mean : 0.04038000875338912, total_step : 638877, cur_epsilon : 0.02\n",
      "episode : 15900, reward mean : 0.04202000869438052, total_step : 639184, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15900, reward mean : -0.21099998522549868, step mean : 66.55, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.    0.49 -1.    0.92 -1.    0.47 -1.\n",
      "  0.94  0.82  0.98  0.99  0.95  0.22 -1.   -1.  ]\n",
      "episode : 15910, reward mean : 0.04832000866532326, total_step : 639545, cur_epsilon : 0.02\n",
      "episode : 15920, reward mean : 0.04897000867314637, total_step : 639962, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 15930, reward mean : 0.05068000867962837, total_step : 640389, cur_epsilon : 0.02\n",
      "episode : 15940, reward mean : 0.055630008658394216, total_step : 640805, cur_epsilon : 0.02\n",
      "episode : 15950, reward mean : 0.05322000864520669, total_step : 641169, cur_epsilon : 0.02\n",
      "[EVAL] episode : 15950, reward mean : -0.5784999814815819, step mean : 83.1, eval_episode_rewards : [-1.   -1.   -1.    0.48 -1.    0.97 -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.95 -1.   -1.   -1.    0.11 -1.    0.92]\n",
      "episode : 15960, reward mean : 0.05442000866308808, total_step : 641483, cur_epsilon : 0.02\n",
      "episode : 15970, reward mean : 0.06174000863358378, total_step : 641790, cur_epsilon : 0.02\n",
      "episode : 15980, reward mean : 0.0672300086002797, total_step : 642251, cur_epsilon : 0.02\n",
      "episode : 15990, reward mean : 0.06256000868231058, total_step : 642837, cur_epsilon : 0.02\n",
      "episode : 16000, reward mean : 0.06134000868722796, total_step : 643257, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16000, reward mean : -0.23799998462200164, step mean : 69.25, eval_episode_rewards : [-1.   -1.   -1.    0.59 -1.   -1.   -1.   -1.    0.99  0.84 -1.    0.96\n",
      "  0.13 -1.   -1.    0.43  0.79  0.97 -1.    0.54]\n",
      "episode : 16010, reward mean : 0.06484000869840384, total_step : 643589, cur_epsilon : 0.02\n",
      "episode : 16020, reward mean : 0.06310000871494413, total_step : 643919, cur_epsilon : 0.02\n",
      "episode : 16030, reward mean : 0.06538000875338912, total_step : 644423, cur_epsilon : 0.02\n",
      "episode : 16040, reward mean : 0.05971000876836479, total_step : 644897, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 16050, reward mean : 0.060320008754730224, total_step : 645199, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16050, reward mean : -0.47899998258799314, step mean : 78.2, eval_episode_rewards : [-1.   -1.    0.55 -1.   -1.   -1.   -1.   -1.    0.92 -1.   -1.   -1.\n",
      " -1.    0.97  0.53  0.71  0.74 -1.   -1.   -1.  ]\n",
      "episode : 16060, reward mean : 0.06342000870779156, total_step : 645559, cur_epsilon : 0.02\n",
      "episode : 16070, reward mean : 0.05729000877775252, total_step : 646152, cur_epsilon : 0.02\n",
      "episode : 16080, reward mean : 0.05190000880882144, total_step : 646682, cur_epsilon : 0.02\n",
      "episode : 16090, reward mean : 0.05794000878557563, total_step : 646995, cur_epsilon : 0.02\n",
      "episode : 16100, reward mean : 0.054040008783340455, total_step : 647357, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16100, reward mean : -0.24649998443201185, step mean : 70.1, eval_episode_rewards : [ 0.99  0.28 -1.   -1.    0.65 -1.    0.93  0.98  0.72 -1.   -1.   -1.\n",
      "  0.66 -1.   -1.   -1.   -1.   -1.    0.39  0.47]\n",
      "episode : 16110, reward mean : 0.049750008856877685, total_step : 647952, cur_epsilon : 0.02\n",
      "episode : 16120, reward mean : 0.04333000893332064, total_step : 648636, cur_epsilon : 0.02\n",
      "episode : 16130, reward mean : 0.05029000891186297, total_step : 648947, cur_epsilon : 0.02\n",
      "episode : 16140, reward mean : 0.04990000892058015, total_step : 649339, cur_epsilon : 0.02\n",
      "episode : 16150, reward mean : 0.05026000893488526, total_step : 649766, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16150, reward mean : -0.38399998359382154, step mean : 73.75, eval_episode_rewards : [ 0.99  0.97 -1.   -1.    0.94 -1.    0.95 -1.   -1.   -1.    0.37  0.15\n",
      " -1.   -1.   -1.   -1.   -1.    0.95 -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 16160, reward mean : 0.051330008955672385, total_step : 650167, cur_epsilon : 0.02\n",
      "episode : 16170, reward mean : 0.05253000895120204, total_step : 650426, cur_epsilon : 0.02\n",
      "episode : 16180, reward mean : 0.0534600089751184, total_step : 651007, cur_epsilon : 0.02\n",
      "episode : 16190, reward mean : 0.043940009031444786, total_step : 651519, cur_epsilon : 0.02\n",
      "episode : 16200, reward mean : 0.04457000901736319, total_step : 651773, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16200, reward mean : -0.381999983638525, step mean : 73.55, eval_episode_rewards : [-1.    0.54  0.32 -1.    0.9  -1.   -1.   -1.    0.74 -1.   -1.    0.98\n",
      " -1.    0.99 -1.   -1.   -1.   -1.   -1.    0.89]\n",
      "episode : 16210, reward mean : 0.042730009036138654, total_step : 652220, cur_epsilon : 0.02\n",
      "episode : 16220, reward mean : 0.047360008977353575, total_step : 652464, cur_epsilon : 0.02\n",
      "episode : 16230, reward mean : 0.04268000899255276, total_step : 652872, cur_epsilon : 0.02\n",
      "episode : 16240, reward mean : 0.04629000902362168, total_step : 653338, cur_epsilon : 0.02\n",
      "episode : 16250, reward mean : 0.04303000902943313, total_step : 653796, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16250, reward mean : -0.2829999836161733, step mean : 73.75, eval_episode_rewards : [ 0.42  0.96 -1.    0.18  0.09 -1.   -1.   -1.   -1.   -1.   -1.    0.88\n",
      "  0.97 -1.   -1.    1.    0.77 -1.    0.07 -1.  ]\n",
      "episode : 16260, reward mean : 0.03788000903278589, total_step : 654139, cur_epsilon : 0.02\n",
      "episode : 16270, reward mean : 0.03723000902496278, total_step : 654594, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 16280, reward mean : 0.02532000909000635, total_step : 655067, cur_epsilon : 0.02\n",
      "episode : 16290, reward mean : 0.0242100090701133, total_step : 655272, cur_epsilon : 0.02\n",
      "episode : 16300, reward mean : 0.025920009054243565, total_step : 655667, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16300, reward mean : -0.2519999843090773, step mean : 70.65, eval_episode_rewards : [-1.   -1.    0.92  0.62 -1.   -1.    0.84 -1.    0.32 -1.    0.76 -1.\n",
      "  0.15  0.94  0.69 -1.   -1.   -1.   -1.    0.72]\n",
      "episode : 16310, reward mean : 0.02517000907100737, total_step : 656235, cur_epsilon : 0.02\n",
      "episode : 16320, reward mean : 0.022240009069442748, total_step : 656727, cur_epsilon : 0.02\n",
      "episode : 16330, reward mean : 0.027290009023621678, total_step : 657004, cur_epsilon : 0.02\n",
      "episode : 16340, reward mean : 0.02611000902764499, total_step : 657416, cur_epsilon : 0.02\n",
      "episode : 16350, reward mean : 0.02575000903569162, total_step : 657738, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16350, reward mean : -0.13149998588487505, step mean : 63.65, eval_episode_rewards : [-1.   -1.    0.96  0.51 -1.   -1.   -1.   -1.   -1.   -1.    0.85  0.09\n",
      " -1.    0.52  0.86  0.71  1.   -1.    0.92  0.95]\n",
      "episode : 16360, reward mean : 0.02522000904753804, total_step : 658224, cur_epsilon : 0.02\n",
      "episode : 16370, reward mean : 0.02917000904865563, total_step : 658651, cur_epsilon : 0.02\n",
      "episode : 16380, reward mean : 0.033280009023845196, total_step : 658957, cur_epsilon : 0.02\n",
      "episode : 16390, reward mean : 0.03858000897243619, total_step : 659360, cur_epsilon : 0.02\n",
      "episode : 16400, reward mean : 0.030460009042173625, total_step : 659867, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16400, reward mean : -0.35499998424202206, step mean : 70.85, eval_episode_rewards : [-1.   -1.    1.   -1.   -1.    0.43 -1.   -1.    0.92  0.85 -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.86  0.87  0.97 -1.  ]\n",
      "synced target net\n",
      "episode : 16410, reward mean : 0.032900009032338855, total_step : 660232, cur_epsilon : 0.02\n",
      "episode : 16420, reward mean : 0.042260008979588745, total_step : 660649, cur_epsilon : 0.02\n",
      "episode : 16430, reward mean : 0.04299000898562372, total_step : 661014, cur_epsilon : 0.02\n",
      "episode : 16440, reward mean : 0.04796000898629427, total_step : 661340, cur_epsilon : 0.02\n",
      "episode : 16450, reward mean : 0.04834000897780061, total_step : 661794, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16450, reward mean : 0.031000012718141078, step mean : 57.5, eval_episode_rewards : [-1.    0.59  0.93  0.96 -1.    0.19 -1.    0.87  0.7   0.33 -1.   -1.\n",
      "  0.59 -1.    0.97 -1.   -1.    0.96  0.57  0.96]\n",
      "episode : 16460, reward mean : 0.047360008977353575, total_step : 662252, cur_epsilon : 0.02\n",
      "episode : 16470, reward mean : 0.045100009005516764, total_step : 662864, cur_epsilon : 0.02\n",
      "episode : 16480, reward mean : 0.04088000901043415, total_step : 663236, cur_epsilon : 0.02\n",
      "episode : 16490, reward mean : 0.03847000904195011, total_step : 663737, cur_epsilon : 0.02\n",
      "episode : 16500, reward mean : 0.041000008985400196, total_step : 664023, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16500, reward mean : -0.25999998636543753, step mean : 61.45, eval_episode_rewards : [ 0.79  0.88 -1.   -1.    0.96 -1.    0.79 -1.   -1.   -1.    0.6  -1.\n",
      "  0.95  0.9  -1.05  0.98 -1.   -1.   -1.   -1.  ]\n",
      "episode : 16510, reward mean : 0.03922000900283456, total_step : 664389, cur_epsilon : 0.02\n",
      "episode : 16520, reward mean : 0.036580009017139675, total_step : 664987, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 16530, reward mean : 0.03412000907212496, total_step : 665422, cur_epsilon : 0.02\n",
      "episode : 16540, reward mean : 0.035720009125769135, total_step : 665898, cur_epsilon : 0.02\n",
      "episode : 16550, reward mean : 0.029960009098052978, total_step : 666205, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16550, reward mean : -0.03449998693540692, step mean : 59.0, eval_episode_rewards : [-1.    0.01 -1.    0.94 -1.   -1.    0.23  0.98  0.92  0.93  0.96  0.56\n",
      " -1.   -1.    0.95  0.9  -1.   -1.   -1.    0.93]\n",
      "episode : 16560, reward mean : 0.03200000909715891, total_step : 666505, cur_epsilon : 0.02\n",
      "episode : 16570, reward mean : 0.03228000906854868, total_step : 666833, cur_epsilon : 0.02\n",
      "episode : 16580, reward mean : 0.03594000905379653, total_step : 667174, cur_epsilon : 0.02\n",
      "episode : 16590, reward mean : 0.025940009143203498, total_step : 667825, cur_epsilon : 0.02\n",
      "episode : 16600, reward mean : 0.021610009150579573, total_step : 668350, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16600, reward mean : -0.1874999857507646, step mean : 64.2, eval_episode_rewards : [-1.   -1.    0.97  0.81  0.92 -1.   -1.    0.5   0.82 -1.    0.32 -1.\n",
      "  0.96  0.99  0.96 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 16610, reward mean : 0.0142400092035532, total_step : 668916, cur_epsilon : 0.02\n",
      "episode : 16620, reward mean : 0.023600009106099606, total_step : 669175, cur_epsilon : 0.02\n",
      "episode : 16630, reward mean : 0.017360009111464025, total_step : 669652, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 16640, reward mean : 0.016240009091794492, total_step : 670000, cur_epsilon : 0.02\n",
      "episode : 16650, reward mean : 0.01924000909179449, total_step : 670665, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16650, reward mean : -0.5454999822191894, step mean : 79.8, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.83 -1.   -1.    0.48 -1.   -1.   -1.\n",
      "  0.92 -1.   -1.   -1.   -1.    0.86 -1.    1.  ]\n",
      "episode : 16660, reward mean : 0.01880000914633274, total_step : 671258, cur_epsilon : 0.02\n",
      "episode : 16670, reward mean : 0.014790009213611484, total_step : 671721, cur_epsilon : 0.02\n",
      "episode : 16680, reward mean : 0.010760009191930294, total_step : 672161, cur_epsilon : 0.02\n",
      "episode : 16690, reward mean : 0.017180009204894306, total_step : 672500, cur_epsilon : 0.02\n",
      "episode : 16700, reward mean : 0.026210009137168526, total_step : 672812, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16700, reward mean : -0.05349998651072383, step mean : 60.9, eval_episode_rewards : [-1.   -1.    0.55 -1.    0.2  -1.    0.7  -1.    0.95  0.97  0.96  0.83\n",
      " -1.   -1.    0.89 -1.   -1.    0.87  0.21  0.8 ]\n",
      "episode : 16710, reward mean : 0.02329000915773213, total_step : 673336, cur_epsilon : 0.02\n",
      "episode : 16720, reward mean : 0.022060009140521288, total_step : 673679, cur_epsilon : 0.02\n",
      "episode : 16730, reward mean : 0.02106000916287303, total_step : 674139, cur_epsilon : 0.02\n",
      "episode : 16740, reward mean : 0.02090000916644931, total_step : 674686, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 16750, reward mean : 0.020440009199082853, total_step : 675325, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16750, reward mean : -0.5559999819844961, step mean : 80.85, eval_episode_rewards : [-1.   -1.   -1.    0.95  0.54 -1.   -1.    0.69 -1.   -1.   -1.   -1.\n",
      " -1.    0.74 -1.   -1.   -1.   -1.   -1.    0.96]\n",
      "episode : 16760, reward mean : 0.02458000921830535, total_step : 675685, cur_epsilon : 0.02\n",
      "episode : 16770, reward mean : 0.027310009179636836, total_step : 676015, cur_epsilon : 0.02\n",
      "episode : 16780, reward mean : 0.030870009189471602, total_step : 676416, cur_epsilon : 0.02\n",
      "episode : 16790, reward mean : 0.03728000920265913, total_step : 676666, cur_epsilon : 0.02\n",
      "episode : 16800, reward mean : 0.032930009143427016, total_step : 676836, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16800, reward mean : -0.3559999842196703, step mean : 70.95, eval_episode_rewards : [-1.   -1.    0.8  -1.   -1.    0.82 -1.   -1.    0.86 -1.   -1.   -1.\n",
      "  0.81 -1.    0.67 -1.    0.95 -1.    0.97 -1.  ]\n",
      "episode : 16810, reward mean : 0.02827000924758613, total_step : 677479, cur_epsilon : 0.02\n",
      "episode : 16820, reward mean : 0.027760009191930293, total_step : 677739, cur_epsilon : 0.02\n",
      "episode : 16830, reward mean : 0.0331100091394037, total_step : 678013, cur_epsilon : 0.02\n",
      "episode : 16840, reward mean : 0.03187000914476812, total_step : 678468, cur_epsilon : 0.02\n",
      "episode : 16850, reward mean : 0.04452000910788775, total_step : 678782, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16850, reward mean : -0.3234999849461019, step mean : 67.7, eval_episode_rewards : [-1.   -1.    0.92 -1.   -1.    0.85  0.99 -1.    0.99 -1.   -1.   -1.\n",
      "  0.97 -1.   -1.   -1.   -1.    0.92 -1.    0.89]\n",
      "episode : 16860, reward mean : 0.04304000916332006, total_step : 679373, cur_epsilon : 0.02\n",
      "episode : 16870, reward mean : 0.0383300091791898, total_step : 679806, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 16880, reward mean : 0.04115000913850963, total_step : 680144, cur_epsilon : 0.02\n",
      "episode : 16890, reward mean : 0.049990009097382423, total_step : 680445, cur_epsilon : 0.02\n",
      "episode : 16900, reward mean : 0.052140009116381406, total_step : 680836, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16900, reward mean : -0.4419999822974205, step mean : 79.55, eval_episode_rewards : [ 0.73  0.93 -1.   -1.    0.33  0.78 -1.   -1.   -1.    0.46 -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.39  0.54 -1.   -1.  ]\n",
      "episode : 16910, reward mean : 0.04947000915370882, total_step : 681363, cur_epsilon : 0.02\n",
      "episode : 16920, reward mean : 0.05293000916577876, total_step : 681834, cur_epsilon : 0.02\n",
      "episode : 16930, reward mean : 0.04663000915013254, total_step : 682192, cur_epsilon : 0.02\n",
      "episode : 16940, reward mean : 0.04584000914543867, total_step : 682588, cur_epsilon : 0.02\n",
      "episode : 16950, reward mean : 0.04748000919818878, total_step : 683186, cur_epsilon : 0.02\n",
      "[EVAL] episode : 16950, reward mean : -0.2904999845661223, step mean : 69.45, eval_episode_rewards : [ 0.95  0.86 -1.    0.63  0.98 -1.   -1.   -1.    0.46  0.97 -1.    0.4\n",
      " -1.   -1.   -1.   -1.    0.94 -1.   -1.   -1.  ]\n",
      "episode : 16960, reward mean : 0.044820009235292675, total_step : 683667, cur_epsilon : 0.02\n",
      "episode : 16970, reward mean : 0.03876000925898552, total_step : 684079, cur_epsilon : 0.02\n",
      "episode : 16980, reward mean : 0.03169000926055014, total_step : 684548, cur_epsilon : 0.02\n",
      "episode : 16990, reward mean : 0.0353800092227757, total_step : 684969, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 17000, reward mean : 0.036070009229704734, total_step : 685419, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17000, reward mean : 0.14600001126527787, step mean : 51.05, eval_episode_rewards : [ 0.96 -1.   -1.   -1.    0.98  0.86  0.51  0.63 -1.    0.28 -1.    0.8\n",
      "  0.95 -1.    0.68 -1.    0.94  0.78  0.69  0.86]\n",
      "episode : 17010, reward mean : 0.03476000921428204, total_step : 685682, cur_epsilon : 0.02\n",
      "episode : 17020, reward mean : 0.031900009233504535, total_step : 686098, cur_epsilon : 0.02\n",
      "episode : 17030, reward mean : 0.03501000918634236, total_step : 686392, cur_epsilon : 0.02\n",
      "episode : 17040, reward mean : 0.04269000914879143, total_step : 686698, cur_epsilon : 0.02\n",
      "episode : 17050, reward mean : 0.04003000914119184, total_step : 686967, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17050, reward mean : -0.4059999831020832, step mean : 75.95, eval_episode_rewards : [ 0.37 -1.   -1.   -1.   -1.   -1.    0.78  0.81 -1.   -1.   -1.    0.38\n",
      "  0.85  0.98 -1.   -1.   -1.    0.71 -1.   -1.  ]\n",
      "episode : 17060, reward mean : 0.03813000918366015, total_step : 687515, cur_epsilon : 0.02\n",
      "episode : 17070, reward mean : 0.043510009130463004, total_step : 687873, cur_epsilon : 0.02\n",
      "episode : 17080, reward mean : 0.04775000910274684, total_step : 688280, cur_epsilon : 0.02\n",
      "episode : 17090, reward mean : 0.045170009093359116, total_step : 688550, cur_epsilon : 0.02\n",
      "episode : 17100, reward mean : 0.043400009088218214, total_step : 688889, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17100, reward mean : -0.5374999823980033, step mean : 79.0, eval_episode_rewards : [ 0.92  0.73 -1.    0.98 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.68 -1.    0.94 -1.   -1.   -1.   -1.  ]\n",
      "episode : 17110, reward mean : 0.04788000901043415, total_step : 689139, cur_epsilon : 0.02\n",
      "episode : 17120, reward mean : 0.05209000893868506, total_step : 689505, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 17130, reward mean : 0.04521000898070633, total_step : 690003, cur_epsilon : 0.02\n",
      "episode : 17140, reward mean : 0.049440008975565435, total_step : 690372, cur_epsilon : 0.02\n",
      "episode : 17150, reward mean : 0.049160008937120435, total_step : 690629, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17150, reward mean : -0.2994999843649566, step mean : 70.35, eval_episode_rewards : [-1.   -1.   -1.    0.89 -1.   -1.    0.97 -1.    0.69 -1.   -1.   -1.\n",
      "  0.46 -1.    0.51  0.99 -1.    0.93  0.57 -1.  ]\n",
      "episode : 17160, reward mean : 0.04899000896327198, total_step : 691148, cur_epsilon : 0.02\n",
      "episode : 17170, reward mean : 0.04724000898003578, total_step : 691483, cur_epsilon : 0.02\n",
      "episode : 17180, reward mean : 0.04782000892236829, total_step : 691808, cur_epsilon : 0.02\n",
      "episode : 17190, reward mean : 0.050420008908957245, total_step : 692260, cur_epsilon : 0.02\n",
      "episode : 17200, reward mean : 0.05009000891633332, total_step : 692547, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17200, reward mean : -0.3339999847114086, step mean : 68.75, eval_episode_rewards : [-1.   -1.   -1.    0.95 -1.   -1.   -1.   -1.    0.9   0.97  0.93 -1.\n",
      "  0.96 -1.   -1.    0.64 -1.   -1.   -1.    0.97]\n",
      "episode : 17210, reward mean : 0.051250008912757036, total_step : 692977, cur_epsilon : 0.02\n",
      "episode : 17220, reward mean : 0.04613000896014273, total_step : 693432, cur_epsilon : 0.02\n",
      "episode : 17230, reward mean : 0.05173000892437994, total_step : 693680, cur_epsilon : 0.02\n",
      "episode : 17240, reward mean : 0.055430008886381985, total_step : 693976, cur_epsilon : 0.02\n",
      "episode : 17250, reward mean : 0.0535100088622421, total_step : 694325, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17250, reward mean : -0.32899998370558026, step mean : 73.3, eval_episode_rewards : [ 0.95 -1.    0.27 -1.    0.83 -1.    0.95 -1.    0.35 -1.   -1.    0.8\n",
      " -1.   -1.   -1.   -1.   -1.    0.28  0.99 -1.  ]\n",
      "episode : 17260, reward mean : 0.057660008881241086, total_step : 694752, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 17270, reward mean : 0.056440008886158466, total_step : 695230, cur_epsilon : 0.02\n",
      "episode : 17280, reward mean : 0.06152000886201858, total_step : 695595, cur_epsilon : 0.02\n",
      "episode : 17290, reward mean : 0.06255000892840326, total_step : 696095, cur_epsilon : 0.02\n",
      "episode : 17300, reward mean : 0.06269000894762576, total_step : 696575, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17300, reward mean : -0.1924999856390059, step mean : 64.7, eval_episode_rewards : [ 0.97 -1.    0.96 -1.    0.78  0.93  0.78 -1.    0.75 -1.    0.96 -1.\n",
      " -1.    0.98 -1.    0.04 -1.   -1.   -1.   -1.  ]\n",
      "episode : 17310, reward mean : 0.06632000893354416, total_step : 697081, cur_epsilon : 0.02\n",
      "episode : 17320, reward mean : 0.07197000891901553, total_step : 697511, cur_epsilon : 0.02\n",
      "episode : 17330, reward mean : 0.06599000894092023, total_step : 697885, cur_epsilon : 0.02\n",
      "episode : 17340, reward mean : 0.065550008950755, total_step : 698339, cur_epsilon : 0.02\n",
      "episode : 17350, reward mean : 0.06433000895567238, total_step : 698682, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17350, reward mean : -0.04999998658895492, step mean : 60.55, eval_episode_rewards : [ 0.5   0.9   0.83 -1.   -1.    0.77 -1.    0.89 -1.   -1.    0.91 -1.\n",
      "  0.96  0.02 -1.    0.68 -1.    0.93 -1.    0.61]\n",
      "episode : 17360, reward mean : 0.06978000892326236, total_step : 699023, cur_epsilon : 0.02\n",
      "episode : 17370, reward mean : 0.06908000893890857, total_step : 699518, cur_epsilon : 0.02\n",
      "episode : 17380, reward mean : 0.06893000894226134, total_step : 699839, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 17390, reward mean : 0.06980000890046358, total_step : 700056, cur_epsilon : 0.02\n",
      "episode : 17400, reward mean : 0.069140008892864, total_step : 700530, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17400, reward mean : -0.4194999828003347, step mean : 77.3, eval_episode_rewards : [-1.    0.14 -1.   -1.   -1.   -1.    0.15  0.84 -1.   -1.   -1.   -1.\n",
      " -1.   -1.    1.    0.73  0.89 -1.    0.86 -1.  ]\n",
      "episode : 17410, reward mean : 0.06734000891074539, total_step : 700976, cur_epsilon : 0.02\n",
      "episode : 17420, reward mean : 0.06200000894069672, total_step : 701525, cur_epsilon : 0.02\n",
      "episode : 17430, reward mean : 0.05420000895857811, total_step : 701969, cur_epsilon : 0.02\n",
      "episode : 17440, reward mean : 0.051000009007751944, total_step : 702514, cur_epsilon : 0.02\n",
      "episode : 17450, reward mean : 0.05043000897578895, total_step : 702825, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17450, reward mean : -0.501499982085079, step mean : 80.45, eval_episode_rewards : [-1.   -1.    0.97  1.   -1.   -1.    0.54 -1.   -1.   -1.   -1.    0.07\n",
      " -1.   -1.    0.97 -1.   -1.   -1.   -1.    0.42]\n",
      "episode : 17460, reward mean : 0.053460008952766654, total_step : 703182, cur_epsilon : 0.02\n",
      "episode : 17470, reward mean : 0.05968000888079405, total_step : 703475, cur_epsilon : 0.02\n",
      "episode : 17480, reward mean : 0.0626500088814646, total_step : 703849, cur_epsilon : 0.02\n",
      "episode : 17490, reward mean : 0.06345000886358321, total_step : 704270, cur_epsilon : 0.02\n",
      "episode : 17500, reward mean : 0.0627900089006871, total_step : 704721, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17500, reward mean : -0.16199998520314693, step mean : 66.7, eval_episode_rewards : [ 0.62 -1.    0.34 -1.   -1.    0.52 -1.   -1.   -1.    0.87  0.39 -1.\n",
      "  0.98  0.89  0.97  0.28 -1.   -1.    0.9  -1.  ]\n",
      "episode : 17510, reward mean : 0.06484000887721778, total_step : 704983, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 17520, reward mean : 0.07374000881239771, total_step : 705293, cur_epsilon : 0.02\n",
      "episode : 17530, reward mean : 0.07049000881798566, total_step : 705754, cur_epsilon : 0.02\n",
      "episode : 17540, reward mean : 0.0724400087967515, total_step : 706136, cur_epsilon : 0.02\n",
      "episode : 17550, reward mean : 0.07315000882558524, total_step : 706572, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17550, reward mean : -0.30849998416379093, step mean : 71.25, eval_episode_rewards : [-1.    0.73 -1.   -1.   -1.    0.99  0.97 -1.    0.89  0.55 -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.64  0.56 -1.    0.5 ]\n",
      "episode : 17560, reward mean : 0.07430000884458422, total_step : 706955, cur_epsilon : 0.02\n",
      "episode : 17570, reward mean : 0.07170000888034701, total_step : 707442, cur_epsilon : 0.02\n",
      "episode : 17580, reward mean : 0.06783000887744128, total_step : 707770, cur_epsilon : 0.02\n",
      "episode : 17590, reward mean : 0.07068000883609056, total_step : 708239, cur_epsilon : 0.02\n",
      "episode : 17600, reward mean : 0.07508000880479812, total_step : 708627, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17600, reward mean : -0.219999985024333, step mean : 67.45, eval_episode_rewards : [-1.   -1.    0.96 -1.    0.86 -1.    0.91 -1.   -1.    0.81  0.26 -1.\n",
      " -1.   -1.    0.44 -1.    0.81 -1.    0.76  0.79]\n",
      "episode : 17610, reward mean : 0.08123000875674188, total_step : 708980, cur_epsilon : 0.02\n",
      "episode : 17620, reward mean : 0.07651000879518688, total_step : 709410, cur_epsilon : 0.02\n",
      "episode : 17630, reward mean : 0.08138000879809261, total_step : 709901, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 17640, reward mean : 0.0790300088506192, total_step : 710482, cur_epsilon : 0.02\n",
      "episode : 17650, reward mean : 0.07872000881284476, total_step : 710980, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17650, reward mean : 0.00250001335516572, step mean : 60.35, eval_episode_rewards : [ 0.79 -1.    0.9   0.75 -1.    0.97  0.93  0.9   0.67  0.08  0.32  0.15\n",
      " -1.    0.61  0.98 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 17660, reward mean : 0.08519000871293246, total_step : 711128, cur_epsilon : 0.02\n",
      "episode : 17670, reward mean : 0.08644000870734453, total_step : 711565, cur_epsilon : 0.02\n",
      "episode : 17680, reward mean : 0.09060000865906477, total_step : 711791, cur_epsilon : 0.02\n",
      "episode : 17690, reward mean : 0.08786000865325332, total_step : 712105, cur_epsilon : 0.02\n",
      "episode : 17700, reward mean : 0.07813000869192183, total_step : 712588, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17700, reward mean : -0.17149998610839248, step mean : 62.6, eval_episode_rewards : [-1.    0.95  0.97  0.97  0.94 -1.   -1.    0.63 -1.    0.42 -1.   -1.\n",
      " -1.   -1.   -1.    0.96  0.89 -1.   -1.    0.84]\n",
      "episode : 17710, reward mean : 0.0854900086838752, total_step : 713077, cur_epsilon : 0.02\n",
      "episode : 17720, reward mean : 0.08767000867985189, total_step : 713402, cur_epsilon : 0.02\n",
      "episode : 17730, reward mean : 0.0910100086722523, total_step : 713829, cur_epsilon : 0.02\n",
      "episode : 17740, reward mean : 0.09434000864252448, total_step : 714245, cur_epsilon : 0.02\n",
      "episode : 17750, reward mean : 0.08863000861369073, total_step : 714754, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17750, reward mean : -0.04249998787418008, step mean : 54.75, eval_episode_rewards : [ 0.88 -1.   -1.    0.9  -1.   -1.   -1.   -1.    0.93  0.99  0.92  0.99\n",
      "  0.86 -1.    0.74  0.98 -1.    0.96 -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 17760, reward mean : 0.08077000863291323, total_step : 715199, cur_epsilon : 0.02\n",
      "episode : 17770, reward mean : 0.08609000862576068, total_step : 715498, cur_epsilon : 0.02\n",
      "episode : 17780, reward mean : 0.08783000860922038, total_step : 715826, cur_epsilon : 0.02\n",
      "episode : 17790, reward mean : 0.08236000864207745, total_step : 716223, cur_epsilon : 0.02\n",
      "episode : 17800, reward mean : 0.08524000868946315, total_step : 716604, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17800, reward mean : -0.219999985024333, step mean : 67.45, eval_episode_rewards : [-1.    0.97 -1.    0.98  0.5  -1.   -1.   -1.   -1.    0.94 -1.    0.78\n",
      " -1.    0.91 -1.   -1.    0.06  0.65 -1.    0.81]\n",
      "episode : 17810, reward mean : 0.09274000863358378, total_step : 716998, cur_epsilon : 0.02\n",
      "episode : 17820, reward mean : 0.09179000869952142, total_step : 717551, cur_epsilon : 0.02\n",
      "episode : 17830, reward mean : 0.08540000873059034, total_step : 717963, cur_epsilon : 0.02\n",
      "episode : 17840, reward mean : 0.09154000874981284, total_step : 718503, cur_epsilon : 0.02\n",
      "episode : 17850, reward mean : 0.09036000875383615, total_step : 718834, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17850, reward mean : -0.005999987572431564, step mean : 56.15, eval_episode_rewards : [ 0.95  0.62  0.55  0.96  0.69  0.93 -1.   -1.   -1.    0.83 -1.    0.78\n",
      " -1.    0.97  0.81 -1.   -1.   -1.   -1.    0.79]\n",
      "episode : 17860, reward mean : 0.09905000869370997, total_step : 719158, cur_epsilon : 0.02\n",
      "episode : 17870, reward mean : 0.1006900087017566, total_step : 719627, cur_epsilon : 0.02\n",
      "episode : 17880, reward mean : 0.1026300087030977, total_step : 719971, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 17890, reward mean : 0.10103000871650875, total_step : 720331, cur_epsilon : 0.02\n",
      "episode : 17900, reward mean : 0.10152000870555639, total_step : 720673, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17900, reward mean : -0.022999987192451953, step mean : 57.85, eval_episode_rewards : [ 0.98 -1.    0.08 -1.    0.71  0.87 -1.    0.83  0.96 -1.    0.6   0.82\n",
      " -1.    0.74 -1.   -1.    0.95  1.   -1.   -1.  ]\n",
      "episode : 17910, reward mean : 0.1048100086543709, total_step : 720972, cur_epsilon : 0.02\n",
      "episode : 17920, reward mean : 0.09605000869370997, total_step : 721618, cur_epsilon : 0.02\n",
      "episode : 17930, reward mean : 0.09735000873170793, total_step : 722143, cur_epsilon : 0.02\n",
      "episode : 17940, reward mean : 0.0990600087158382, total_step : 722467, cur_epsilon : 0.02\n",
      "episode : 17950, reward mean : 0.10540000864118337, total_step : 722734, cur_epsilon : 0.02\n",
      "[EVAL] episode : 17950, reward mean : 0.12250001067295671, step mean : 48.35, eval_episode_rewards : [-1.    1.    0.86 -1.   -1.    0.92  0.84  0.8  -1.    0.94 -1.    0.92\n",
      "  0.98 -1.    0.79 -1.    0.87  0.64 -1.    0.89]\n",
      "episode : 17960, reward mean : 0.10829000864364206, total_step : 723227, cur_epsilon : 0.02\n",
      "episode : 17970, reward mean : 0.1114700086619705, total_step : 723721, cur_epsilon : 0.02\n",
      "episode : 17980, reward mean : 0.11236000864207744, total_step : 724101, cur_epsilon : 0.02\n",
      "episode : 17990, reward mean : 0.10489000865258276, total_step : 724568, cur_epsilon : 0.02\n",
      "episode : 18000, reward mean : 0.11101000862754881, total_step : 724907, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18000, reward mean : 0.007500012125819922, step mean : 54.8, eval_episode_rewards : [-1.    0.98  0.59  0.73 -1.    0.98 -1.   -1.   -1.   -1.   -1.    0.09\n",
      " -1.    0.97  1.    0.93  0.97  0.97  0.94 -1.  ]\n",
      "synced target net\n",
      "episode : 18010, reward mean : 0.10825000866688787, total_step : 725343, cur_epsilon : 0.02\n",
      "episode : 18020, reward mean : 0.11068000867962838, total_step : 725817, cur_epsilon : 0.02\n",
      "episode : 18030, reward mean : 0.11008000867068768, total_step : 726072, cur_epsilon : 0.02\n",
      "episode : 18040, reward mean : 0.10801000869460405, total_step : 726484, cur_epsilon : 0.02\n",
      "episode : 18050, reward mean : 0.11223000871203839, total_step : 726830, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18050, reward mean : -0.4189999828115106, step mean : 77.25, eval_episode_rewards : [ 0.62  0.16 -1.   -1.    0.49  0.9  -1.   -1.    0.52 -1.    0.93 -1.\n",
      " -1.   -1.   -1.   -1.    1.   -1.   -1.   -1.  ]\n",
      "episode : 18060, reward mean : 0.11645000868476928, total_step : 727259, cur_epsilon : 0.02\n",
      "episode : 18070, reward mean : 0.11570000870153307, total_step : 727690, cur_epsilon : 0.02\n",
      "episode : 18080, reward mean : 0.11475000872276724, total_step : 728192, cur_epsilon : 0.02\n",
      "episode : 18090, reward mean : 0.11697000878490507, total_step : 728739, cur_epsilon : 0.02\n",
      "episode : 18100, reward mean : 0.1253600087761879, total_step : 729039, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18100, reward mean : -0.06549998624250293, step mean : 62.1, eval_episode_rewards : [-1.    0.6   0.66  0.97  0.6  -1.    0.39  0.6  -1.    0.99 -1.    0.85\n",
      " -1.   -1.    0.12  0.94  0.97 -1.   -1.   -1.  ]\n",
      "episode : 18110, reward mean : 0.12332000879943371, total_step : 729392, cur_epsilon : 0.02\n",
      "episode : 18120, reward mean : 0.12317000880278647, total_step : 729773, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 18130, reward mean : 0.12063000877015292, total_step : 730125, cur_epsilon : 0.02\n",
      "episode : 18140, reward mean : 0.11349000875093043, total_step : 730408, cur_epsilon : 0.02\n",
      "episode : 18150, reward mean : 0.1104100087750703, total_step : 730773, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18150, reward mean : -0.011999986320734023, step mean : 61.8, eval_episode_rewards : [-1.    0.95  1.    0.92 -1.    0.89  0.59 -1.   -1.    0.78 -1.    0.44\n",
      " -1.    0.06 -1.    0.24 -1.    0.01  0.94  0.94]\n",
      "episode : 18160, reward mean : 0.11390000871941447, total_step : 731044, cur_epsilon : 0.02\n",
      "episode : 18170, reward mean : 0.11321000869013369, total_step : 731248, cur_epsilon : 0.02\n",
      "episode : 18180, reward mean : 0.11382000867649913, total_step : 731512, cur_epsilon : 0.02\n",
      "episode : 18190, reward mean : 0.11526000866666436, total_step : 731921, cur_epsilon : 0.02\n",
      "episode : 18200, reward mean : 0.11637000870890915, total_step : 732396, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18200, reward mean : -0.2764999848790467, step mean : 68.05, eval_episode_rewards : [ 0.15  0.8  -1.   -1.   -1.   -1.    0.8  -1.   -1.   -1.   -1.   -1.\n",
      "  0.96 -1.   -1.    0.93  0.97  0.95  0.91 -1.  ]\n",
      "episode : 18210, reward mean : 0.11521000871248543, total_step : 732843, cur_epsilon : 0.02\n",
      "episode : 18220, reward mean : 0.11313000871427357, total_step : 733306, cur_epsilon : 0.02\n",
      "episode : 18230, reward mean : 0.11410000873729587, total_step : 733657, cur_epsilon : 0.02\n",
      "episode : 18240, reward mean : 0.11584000872075557, total_step : 733880, cur_epsilon : 0.02\n",
      "episode : 18250, reward mean : 0.119310008732602, total_step : 734282, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18250, reward mean : -0.374499982688576, step mean : 77.85, eval_episode_rewards : [ 0.69 -1.   -1.   -1.   -1.    0.23 -1.   -1.   -1.   -1.   -1.    0.22\n",
      "  0.24  0.85 -1.   -1.    0.47  0.86 -1.    0.95]\n",
      "episode : 18260, reward mean : 0.11681000874377787, total_step : 734759, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 18270, reward mean : 0.11655000870488584, total_step : 735063, cur_epsilon : 0.02\n",
      "episode : 18280, reward mean : 0.11485000876523555, total_step : 735695, cur_epsilon : 0.02\n",
      "episode : 18290, reward mean : 0.10994000876322389, total_step : 736187, cur_epsilon : 0.02\n",
      "episode : 18300, reward mean : 0.10619000873528421, total_step : 736543, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18300, reward mean : -0.3494999843649566, step mean : 70.3, eval_episode_rewards : [-1.    0.77  0.94 -1.   -1.   -1.   -1.    0.85  0.86  0.95  0.93 -1.\n",
      " -1.   -1.   -1.    0.71 -1.   -1.   -1.   -1.  ]\n",
      "episode : 18310, reward mean : 0.10833000868745148, total_step : 736837, cur_epsilon : 0.02\n",
      "episode : 18320, reward mean : 0.10935000864230096, total_step : 737066, cur_epsilon : 0.02\n",
      "episode : 18330, reward mean : 0.11521000862307847, total_step : 737355, cur_epsilon : 0.02\n",
      "episode : 18340, reward mean : 0.12224000857770442, total_step : 737609, cur_epsilon : 0.02\n",
      "episode : 18350, reward mean : 0.1215900086145848, total_step : 738116, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18350, reward mean : -0.4449999833479524, step mean : 74.8, eval_episode_rewards : [ 0.88 -1.    0.69  0.96 -1.   -1.   -1.   -1.   -1.    0.96 -1.   -1.\n",
      "  0.81 -1.    0.8  -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 18360, reward mean : 0.12274000861123205, total_step : 738441, cur_epsilon : 0.02\n",
      "episode : 18370, reward mean : 0.1284300086181611, total_step : 738969, cur_epsilon : 0.02\n",
      "episode : 18380, reward mean : 0.12406000867113472, total_step : 739523, cur_epsilon : 0.02\n",
      "episode : 18390, reward mean : 0.12227000866644085, total_step : 739719, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 18400, reward mean : 0.12157000863738358, total_step : 740063, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18400, reward mean : 1.2293457984924316e-08, step mean : 55.55, eval_episode_rewards : [-1.    0.93 -1.    0.96 -1.    0.97  0.85 -1.   -1.   -1.    0.87  0.95\n",
      "  0.95 -1.   -1.    0.88  0.3   0.41 -1.    0.93]\n",
      "episode : 18410, reward mean : 0.1274000085964799, total_step : 740326, cur_epsilon : 0.02\n",
      "episode : 18420, reward mean : 0.12617000860162078, total_step : 740899, cur_epsilon : 0.02\n",
      "episode : 18430, reward mean : 0.13333000857569277, total_step : 741227, cur_epsilon : 0.02\n",
      "episode : 18440, reward mean : 0.12812000858038664, total_step : 741792, cur_epsilon : 0.02\n",
      "episode : 18450, reward mean : 0.130900008585304, total_step : 742126, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18450, reward mean : -0.11699998620897531, step mean : 62.2, eval_episode_rewards : [-1.    0.25  0.96  0.32  0.98 -1.   -1.   -1.    0.97 -1.    0.99 -1.\n",
      "  0.34  0.94 -1.   -1.    0.95 -1.    0.96 -1.  ]\n",
      "episode : 18460, reward mean : 0.13122000857815147, total_step : 742451, cur_epsilon : 0.02\n",
      "episode : 18470, reward mean : 0.12422000862285495, total_step : 742943, cur_epsilon : 0.02\n",
      "episode : 18480, reward mean : 0.11865000865794718, total_step : 743472, cur_epsilon : 0.02\n",
      "episode : 18490, reward mean : 0.11885000865347684, total_step : 743875, cur_epsilon : 0.02\n",
      "episode : 18500, reward mean : 0.12013000864721834, total_step : 744299, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18500, reward mean : -0.16699998620897533, step mean : 62.15, eval_episode_rewards : [-1.    0.84  0.95 -1.    0.94 -1.    0.58 -1.    0.71 -1.    0.92 -1.\n",
      " -1.    0.95 -1.   -1.   -1.   -1.    0.81  0.96]\n",
      "episode : 18510, reward mean : 0.11713000869192183, total_step : 744758, cur_epsilon : 0.02\n",
      "episode : 18520, reward mean : 0.11197000867314637, total_step : 744984, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 18530, reward mean : 0.11509000864811242, total_step : 745333, cur_epsilon : 0.02\n",
      "episode : 18540, reward mean : 0.11427000862173736, total_step : 745597, cur_epsilon : 0.02\n",
      "episode : 18550, reward mean : 0.11738000861927866, total_step : 746021, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18550, reward mean : -0.09849998662248254, step mean : 60.35, eval_episode_rewards : [ 0.83 -1.    0.75 -1.    0.91  0.13  0.66 -1.   -1.   -1.   -1.   -1.\n",
      "  0.98 -1.   -1.   -1.    0.99  0.97  0.85  0.96]\n",
      "episode : 18560, reward mean : 0.11273000858910381, total_step : 746271, cur_epsilon : 0.02\n",
      "episode : 18570, reward mean : 0.11461000854708255, total_step : 746572, cur_epsilon : 0.02\n",
      "episode : 18580, reward mean : 0.1097500085439533, total_step : 746887, cur_epsilon : 0.02\n",
      "episode : 18590, reward mean : 0.11179000852070749, total_step : 747253, cur_epsilon : 0.02\n",
      "episode : 18600, reward mean : 0.10861000852473081, total_step : 747660, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18600, reward mean : 0.18600001148879527, step mean : 52.1, eval_episode_rewards : [ 0.9  -1.    0.96  1.    0.62  0.89  0.81  0.29  0.97 -1.    0.97  0.02\n",
      " -1.    0.8  -1.    0.86 -1.    0.14  0.49 -1.  ]\n",
      "episode : 18610, reward mean : 0.10892000851780176, total_step : 747984, cur_epsilon : 0.02\n",
      "episode : 18620, reward mean : 0.1122200084887445, total_step : 748284, cur_epsilon : 0.02\n",
      "episode : 18630, reward mean : 0.11424000844359398, total_step : 748575, cur_epsilon : 0.02\n",
      "episode : 18640, reward mean : 0.11468000838905573, total_step : 748914, cur_epsilon : 0.02\n",
      "episode : 18650, reward mean : 0.11503000833652914, total_step : 749177, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18650, reward mean : 0.09200001135468483, step mean : 51.4, eval_episode_rewards : [-1.    0.79  0.9   0.33  0.93 -1.    0.93 -1.   -1.    0.82  0.91  0.85\n",
      "  0.97  0.95  0.98 -1.   -1.   -1.    0.48 -1.  ]\n",
      "episode : 18660, reward mean : 0.11068000838905573, total_step : 749558, cur_epsilon : 0.02\n",
      "episode : 18670, reward mean : 0.11721000835485756, total_step : 749843, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 18680, reward mean : 0.11832000839710236, total_step : 750257, cur_epsilon : 0.02\n",
      "episode : 18690, reward mean : 0.12039000839553773, total_step : 750564, cur_epsilon : 0.02\n",
      "episode : 18700, reward mean : 0.1250500083807856, total_step : 750981, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18700, reward mean : -0.107999986410141, step mean : 61.3, eval_episode_rewards : [-1.    0.96 -1.   -1.    0.95 -1.   -1.    0.77  0.27 -1.    0.91 -1.\n",
      "  0.97  0.58 -1.   -1.    0.88  0.7  -1.    0.85]\n",
      "episode : 18710, reward mean : 0.12449000834859908, total_step : 751328, cur_epsilon : 0.02\n",
      "episode : 18720, reward mean : 0.12053000837005674, total_step : 751748, cur_epsilon : 0.02\n",
      "episode : 18730, reward mean : 0.11732000837475061, total_step : 752195, cur_epsilon : 0.02\n",
      "episode : 18740, reward mean : 0.11856000841408967, total_step : 752786, cur_epsilon : 0.02\n",
      "episode : 18750, reward mean : 0.12436000841856003, total_step : 753317, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18750, reward mean : 0.2860000092536211, step mean : 42.1, eval_episode_rewards : [-1.    1.    0.95  0.99  0.85 -1.   -1.    0.82  0.93  0.65  0.65 -1.\n",
      "  0.94  0.2  -1.    0.87 -1.    0.95  0.98  0.94]\n",
      "episode : 18760, reward mean : 0.1313100084196776, total_step : 753768, cur_epsilon : 0.02\n",
      "episode : 18770, reward mean : 0.1239600084722042, total_step : 754299, cur_epsilon : 0.02\n",
      "episode : 18780, reward mean : 0.1206500084567815, total_step : 754558, cur_epsilon : 0.02\n",
      "episode : 18790, reward mean : 0.12464000845700503, total_step : 754956, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 18800, reward mean : 0.12742000841721893, total_step : 755160, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18800, reward mean : -0.27549998490139843, step mean : 67.95, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.97  0.96  0.82  1.   -1.    0.81 -1.   -1.\n",
      "  0.63  0.3  -1.   -1.   -1.    1.   -1.   -1.  ]\n",
      "episode : 18810, reward mean : 0.12583000840805472, total_step : 755515, cur_epsilon : 0.02\n",
      "episode : 18820, reward mean : 0.1293600083962083, total_step : 756016, cur_epsilon : 0.02\n",
      "episode : 18830, reward mean : 0.1294300084169954, total_step : 756520, cur_epsilon : 0.02\n",
      "episode : 18840, reward mean : 0.13116000835597516, total_step : 756788, cur_epsilon : 0.02\n",
      "episode : 18850, reward mean : 0.1299300083387643, total_step : 757044, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18850, reward mean : -0.15749998530372977, step mean : 66.25, eval_episode_rewards : [-1.    0.95  0.05  0.95  0.84 -1.   -1.    0.24 -1.   -1.   -1.    0.69\n",
      "  0.6   0.64 -1.   -1.    0.96 -1.   -1.    0.93]\n",
      "episode : 18860, reward mean : 0.12381000834144652, total_step : 757380, cur_epsilon : 0.02\n",
      "episode : 18870, reward mean : 0.1265700082797557, total_step : 757575, cur_epsilon : 0.02\n",
      "episode : 18880, reward mean : 0.12645000826008618, total_step : 757832, cur_epsilon : 0.02\n",
      "episode : 18890, reward mean : 0.12287000827305018, total_step : 758249, cur_epsilon : 0.02\n",
      "episode : 18900, reward mean : 0.12306000826880335, total_step : 758572, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18900, reward mean : -0.12599998600780965, step mean : 63.1, eval_episode_rewards : [ 0.83 -1.   -1.    0.76 -1.   -1.   -1.   -1.    0.97  0.99 -1.    0.95\n",
      "  0.33  0.91  0.96  0.2   0.58 -1.   -1.   -1.  ]\n",
      "episode : 18910, reward mean : 0.12207000829093158, total_step : 758970, cur_epsilon : 0.02\n",
      "episode : 18920, reward mean : 0.12358000825718045, total_step : 759467, cur_epsilon : 0.02\n",
      "episode : 18930, reward mean : 0.12748000821471214, total_step : 759804, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 18940, reward mean : 0.1281100082229823, total_step : 760166, cur_epsilon : 0.02\n",
      "episode : 18950, reward mean : 0.12983000822924076, total_step : 760461, cur_epsilon : 0.02\n",
      "[EVAL] episode : 18950, reward mean : 0.05250001223757863, step mean : 55.35, eval_episode_rewards : [ 0.91  0.97 -1.   -1.    0.98 -1.    0.41  0.43  0.95  0.85  0.69  0.37\n",
      " -1.    0.77 -1.   -1.    0.81 -1.   -1.    0.91]\n",
      "episode : 18960, reward mean : 0.12771000820957124, total_step : 760865, cur_epsilon : 0.02\n",
      "episode : 18970, reward mean : 0.12941000817157328, total_step : 761189, cur_epsilon : 0.02\n",
      "episode : 18980, reward mean : 0.1309300081823021, total_step : 761617, cur_epsilon : 0.02\n",
      "episode : 18990, reward mean : 0.13911000815592706, total_step : 761967, cur_epsilon : 0.02\n",
      "episode : 19000, reward mean : 0.1413200081512332, total_step : 762285, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19000, reward mean : -0.3629999840632081, step mean : 71.65, eval_episode_rewards : [-1.   -1.    0.12 -1.    0.9   0.99 -1.   -1.   -1.    0.84 -1.   -1.\n",
      " -1.   -1.    0.98 -1.    0.96 -1.    0.95 -1.  ]\n",
      "episode : 19010, reward mean : 0.14602000813558697, total_step : 762653, cur_epsilon : 0.02\n",
      "episode : 19020, reward mean : 0.14655000816844405, total_step : 763272, cur_epsilon : 0.02\n",
      "episode : 19030, reward mean : 0.14012000822275877, total_step : 763768, cur_epsilon : 0.02\n",
      "episode : 19040, reward mean : 0.137570008167997, total_step : 763936, cur_epsilon : 0.02\n",
      "episode : 19050, reward mean : 0.13520000817626715, total_step : 764319, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19050, reward mean : -0.19499998558312653, step mean : 64.95, eval_episode_rewards : [ 0.96  0.08 -1.    0.93 -1.    0.93  0.85 -1.   -1.   -1.    0.81 -1.\n",
      "  0.95  0.68  0.91 -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 19060, reward mean : 0.13132000819593667, total_step : 764835, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 19070, reward mean : 0.13363000814430415, total_step : 765037, cur_epsilon : 0.02\n",
      "episode : 19080, reward mean : 0.13506000811234117, total_step : 765396, cur_epsilon : 0.02\n",
      "episode : 19090, reward mean : 0.13655000805668532, total_step : 765695, cur_epsilon : 0.02\n",
      "episode : 19100, reward mean : 0.12832000812888145, total_step : 766315, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19100, reward mean : -0.17949998592957855, step mean : 63.4, eval_episode_rewards : [ 1.    0.95 -1.   -1.   -1.    0.91  0.7  -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.19  0.95 -1.   -1.    0.9   0.83  0.98]\n",
      "episode : 19110, reward mean : 0.1279100081603974, total_step : 766808, cur_epsilon : 0.02\n",
      "episode : 19120, reward mean : 0.1253500081729144, total_step : 767243, cur_epsilon : 0.02\n",
      "episode : 19130, reward mean : 0.132150008155033, total_step : 767517, cur_epsilon : 0.02\n",
      "episode : 19140, reward mean : 0.1331800081767142, total_step : 767897, cur_epsilon : 0.02\n",
      "episode : 19150, reward mean : 0.12950000816956161, total_step : 768230, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19150, reward mean : -0.12749998597428203, step mean : 63.25, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.59 -1.    0.77  0.64  0.74  0.75 -1.    0.86\n",
      " -1.    0.82  0.97  0.41 -1.    0.9  -1.   -1.  ]\n",
      "episode : 19160, reward mean : 0.1291300082001835, total_step : 768637, cur_epsilon : 0.02\n",
      "episode : 19170, reward mean : 0.13063000823371113, total_step : 768990, cur_epsilon : 0.02\n",
      "episode : 19180, reward mean : 0.13179000827483833, total_step : 769437, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 19190, reward mean : 0.12811000831238925, total_step : 770012, cur_epsilon : 0.02\n",
      "episode : 19200, reward mean : 0.12359000830166042, total_step : 770438, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19200, reward mean : 0.049500011187046764, step mean : 50.6, eval_episode_rewards : [-1.    0.86  0.87 -1.   -1.    0.78  1.    0.88 -1.   -1.    0.96 -1.\n",
      " -1.   -1.    0.97  1.    0.98  0.72  0.97 -1.  ]\n",
      "episode : 19210, reward mean : 0.11987000831775367, total_step : 770954, cur_epsilon : 0.02\n",
      "episode : 19220, reward mean : 0.12759000825695693, total_step : 771146, cur_epsilon : 0.02\n",
      "episode : 19230, reward mean : 0.12267000827752053, total_step : 771587, cur_epsilon : 0.02\n",
      "episode : 19240, reward mean : 0.11503000833652914, total_step : 772071, cur_epsilon : 0.02\n",
      "episode : 19250, reward mean : 0.11441000835038721, total_step : 772535, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19250, reward mean : 0.0520000122487545, step mean : 55.4, eval_episode_rewards : [ 0.46  0.99  0.98  1.   -1.   -1.   -1.    0.87 -1.    1.   -1.   -1.\n",
      "  0.09  0.58  0.48  0.85 -1.    0.75  0.99 -1.  ]\n",
      "episode : 19260, reward mean : 0.11434000832960009, total_step : 772920, cur_epsilon : 0.02\n",
      "episode : 19270, reward mean : 0.1127900083642453, total_step : 773379, cur_epsilon : 0.02\n",
      "episode : 19280, reward mean : 0.12114000828936695, total_step : 773679, cur_epsilon : 0.02\n",
      "episode : 19290, reward mean : 0.12230000833049416, total_step : 774354, cur_epsilon : 0.02\n",
      "episode : 19300, reward mean : 0.13029000830836593, total_step : 774612, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19300, reward mean : -0.3269999837502837, step mean : 73.1, eval_episode_rewards : [ 0.56  0.78  0.88 -1.   -1.    0.97 -1.   -1.   -1.   -1.    0.73  0.63\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.04  0.87]\n",
      "synced target net\n",
      "episode : 19310, reward mean : 0.13111000833474099, total_step : 775024, cur_epsilon : 0.02\n",
      "episode : 19320, reward mean : 0.13100000835955142, total_step : 775363, cur_epsilon : 0.02\n",
      "episode : 19330, reward mean : 0.12477000838704408, total_step : 775774, cur_epsilon : 0.02\n",
      "episode : 19340, reward mean : 0.11680000838637351, total_step : 776025, cur_epsilon : 0.02\n",
      "episode : 19350, reward mean : 0.11828000837564469, total_step : 776485, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19350, reward mean : -0.34449998335912824, step mean : 74.85, eval_episode_rewards : [ 0.96  0.86 -1.   -1.   -1.   -1.    0.93  0.11 -1.   -1.   -1.    0.38\n",
      " -1.   -1.    0.8  -1.    0.95 -1.   -1.    0.12]\n",
      "episode : 19360, reward mean : 0.11196000840514898, total_step : 776943, cur_epsilon : 0.02\n",
      "episode : 19370, reward mean : 0.1063300083745271, total_step : 777335, cur_epsilon : 0.02\n",
      "episode : 19380, reward mean : 0.11095000836066901, total_step : 777831, cur_epsilon : 0.02\n",
      "episode : 19390, reward mean : 0.1084900084156543, total_step : 778273, cur_epsilon : 0.02\n",
      "episode : 19400, reward mean : 0.11085000842995942, total_step : 778680, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19400, reward mean : -0.10399998649954796, step mean : 60.9, eval_episode_rewards : [ 0.84  0.73 -1.   -1.    0.91  0.44 -1.   -1.   -1.    0.98  0.89 -1.\n",
      " -1.   -1.    0.89  0.66  0.62 -1.   -1.    0.96]\n",
      "episode : 19410, reward mean : 0.1021900085117668, total_step : 779306, cur_epsilon : 0.02\n",
      "episode : 19420, reward mean : 0.10165000847913325, total_step : 779733, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 19430, reward mean : 0.09955000850372016, total_step : 780170, cur_epsilon : 0.02\n",
      "episode : 19440, reward mean : 0.10701000847108662, total_step : 780591, cur_epsilon : 0.02\n",
      "episode : 19450, reward mean : 0.10789000849612057, total_step : 781035, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19450, reward mean : -0.039499987941235305, step mean : 54.45, eval_episode_rewards : [ 0.98 -1.    0.97 -1.   -1.   -1.    0.99  0.95  0.98 -1.   -1.    0.87\n",
      " -1.    0.97  0.81 -1.   -1.    0.69  1.   -1.  ]\n",
      "episode : 19460, reward mean : 0.10512000853568315, total_step : 781534, cur_epsilon : 0.02\n",
      "episode : 19470, reward mean : 0.10680000854283571, total_step : 782058, cur_epsilon : 0.02\n",
      "episode : 19480, reward mean : 0.10266000854596495, total_step : 782603, cur_epsilon : 0.02\n",
      "episode : 19490, reward mean : 0.10592000849545002, total_step : 782779, cur_epsilon : 0.02\n",
      "episode : 19500, reward mean : 0.10857000850327313, total_step : 783237, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19500, reward mean : -0.03599998690187931, step mean : 59.15, eval_episode_rewards : [-1.    0.66 -1.   -1.    0.91  0.7   0.91  0.3   0.73 -1.   -1.    0.62\n",
      " -1.   -1.    0.81 -1.    0.94  0.79  0.91 -1.  ]\n",
      "episode : 19510, reward mean : 0.10665000852383673, total_step : 783787, cur_epsilon : 0.02\n",
      "episode : 19520, reward mean : 0.11161000854708254, total_step : 784117, cur_epsilon : 0.02\n",
      "episode : 19530, reward mean : 0.11365000854618848, total_step : 784462, cur_epsilon : 0.02\n",
      "episode : 19540, reward mean : 0.11664000859111548, total_step : 784926, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 19550, reward mean : 0.1174100085515529, total_step : 785174, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19550, reward mean : -0.29699998442083597, step mean : 70.1, eval_episode_rewards : [ 0.86 -1.   -1.    0.94 -1.   -1.   -1.    0.95 -1.    0.76 -1.    0.08\n",
      " -1.   -1.   -1.    0.83 -1.   -1.    0.79  0.85]\n",
      "episode : 19560, reward mean : 0.11774000861123204, total_step : 785690, cur_epsilon : 0.02\n",
      "episode : 19570, reward mean : 0.11820000862330199, total_step : 786044, cur_epsilon : 0.02\n",
      "episode : 19580, reward mean : 0.12300000862777233, total_step : 786378, cur_epsilon : 0.02\n",
      "episode : 19590, reward mean : 0.11683000865392387, total_step : 786858, cur_epsilon : 0.02\n",
      "episode : 19600, reward mean : 0.11627000868879259, total_step : 787418, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19600, reward mean : 0.09950001118704677, step mean : 50.65, eval_episode_rewards : [ 0.68  0.67 -1.   -1.    0.97  0.98 -1.   -1.    0.9   0.35  0.91 -1.\n",
      "  0.98 -1.   -1.    0.97  0.7   0.9  -1.    0.98]\n",
      "episode : 19610, reward mean : 0.11341000870801508, total_step : 787826, cur_epsilon : 0.02\n",
      "episode : 19620, reward mean : 0.10737000875361263, total_step : 788330, cur_epsilon : 0.02\n",
      "episode : 19630, reward mean : 0.1047000087685883, total_step : 788686, cur_epsilon : 0.02\n",
      "episode : 19640, reward mean : 0.11082000878825783, total_step : 789112, cur_epsilon : 0.02\n",
      "episode : 19650, reward mean : 0.10932000882178546, total_step : 789525, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19650, reward mean : -0.19049998568370938, step mean : 64.5, eval_episode_rewards : [ 0.53  0.26 -1.   -1.   -1.    0.98 -1.    0.97 -1.    1.   -1.    0.89\n",
      "  0.98 -1.   -1.   -1.   -1.    0.96  0.62 -1.  ]\n",
      "episode : 19660, reward mean : 0.10860000879317522, total_step : 789780, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 19670, reward mean : 0.10472000881284475, total_step : 790152, cur_epsilon : 0.02\n",
      "episode : 19680, reward mean : 0.1076700087916106, total_step : 790471, cur_epsilon : 0.02\n",
      "episode : 19690, reward mean : 0.1033200087994337, total_step : 790811, cur_epsilon : 0.02\n",
      "episode : 19700, reward mean : 0.10458000879362225, total_step : 791203, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19700, reward mean : 0.07300001177936792, step mean : 53.3, eval_episode_rewards : [-1.    0.95 -1.   -1.    1.    0.9   0.96  0.96 -1.    0.52  0.68  0.95\n",
      " -1.   -1.    0.93  0.83 -1.   -1.    0.03  0.75]\n",
      "episode : 19710, reward mean : 0.10547000877372921, total_step : 791461, cur_epsilon : 0.02\n",
      "episode : 19720, reward mean : 0.10366000876948238, total_step : 791862, cur_epsilon : 0.02\n",
      "episode : 19730, reward mean : 0.10387000874243676, total_step : 792189, cur_epsilon : 0.02\n",
      "episode : 19740, reward mean : 0.09994000874087214, total_step : 792771, cur_epsilon : 0.02\n",
      "episode : 19750, reward mean : 0.10460000870376826, total_step : 793137, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19750, reward mean : -0.47349998271092775, step mean : 77.65, eval_episode_rewards : [-1.   -1.    0.88 -1.    0.97  0.82 -1.   -1.    0.93 -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.    0.01 -1.    0.92 -1.  ]\n",
      "episode : 19760, reward mean : 0.1064900086838752, total_step : 793500, cur_epsilon : 0.02\n",
      "episode : 19770, reward mean : 0.11343000859580935, total_step : 793640, cur_epsilon : 0.02\n",
      "episode : 19780, reward mean : 0.10740000866353512, total_step : 794199, cur_epsilon : 0.02\n",
      "episode : 19790, reward mean : 0.10671000863425434, total_step : 794466, cur_epsilon : 0.02\n",
      "episode : 19800, reward mean : 0.10527000866644085, total_step : 794812, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19800, reward mean : 0.1470000112429261, step mean : 50.95, eval_episode_rewards : [ 0.94 -1.   -1.    0.98  0.73 -1.    0.57  0.96  0.73  0.44  0.91  0.98\n",
      "  0.81 -1.    0.97  0.86 -1.   -1.   -1.    0.06]\n",
      "synced target net\n",
      "episode : 19810, reward mean : 0.10458000868186354, total_step : 795236, cur_epsilon : 0.02\n",
      "episode : 19820, reward mean : 0.10022000868991017, total_step : 795771, cur_epsilon : 0.02\n",
      "episode : 19830, reward mean : 0.10370000867918133, total_step : 796228, cur_epsilon : 0.02\n",
      "episode : 19840, reward mean : 0.10277000869996845, total_step : 796587, cur_epsilon : 0.02\n",
      "episode : 19850, reward mean : 0.09939000877551735, total_step : 797179, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19850, reward mean : -0.362499984074384, step mean : 71.6, eval_episode_rewards : [-1.    0.97 -1.   -1.    0.98 -1.   -1.   -1.    0.99  0.28 -1.    0.8\n",
      " -1.   -1.    0.73 -1.   -1.   -1.   -1.    1.  ]\n",
      "episode : 19860, reward mean : 0.1014700087737292, total_step : 797507, cur_epsilon : 0.02\n",
      "episode : 19870, reward mean : 0.10101000880636275, total_step : 797847, cur_epsilon : 0.02\n",
      "episode : 19880, reward mean : 0.10114000882580876, total_step : 798190, cur_epsilon : 0.02\n",
      "episode : 19890, reward mean : 0.10626000880077481, total_step : 798497, cur_epsilon : 0.02\n",
      "episode : 19900, reward mean : 0.10854000881686807, total_step : 798893, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19900, reward mean : -0.08549998691305519, step mean : 59.05, eval_episode_rewards : [-1.   -1.    0.84  0.98 -1.   -1.    0.93  0.94 -1.    0.94  0.63 -1.\n",
      " -1.    0.7   0.98 -1.    0.73 -1.   -1.    0.62]\n",
      "episode : 19910, reward mean : 0.10597000882960855, total_step : 799348, cur_epsilon : 0.02\n",
      "episode : 19920, reward mean : 0.108580008815974, total_step : 799784, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 19930, reward mean : 0.10181000883318483, total_step : 800198, cur_epsilon : 0.02\n",
      "episode : 19940, reward mean : 0.10224000884592534, total_step : 800616, cur_epsilon : 0.02\n",
      "episode : 19950, reward mean : 0.09858000888302922, total_step : 801077, cur_epsilon : 0.02\n",
      "[EVAL] episode : 19950, reward mean : -0.38849998349323867, step mean : 74.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.45  0.97  0.5  -1.   -1.   -1.    0.97\n",
      " -1.   -1.   -1.   -1.    0.66 -1.    0.83  0.85]\n",
      "episode : 19960, reward mean : 0.10046000886335969, total_step : 801394, cur_epsilon : 0.02\n",
      "episode : 19970, reward mean : 0.10285000885464252, total_step : 801681, cur_epsilon : 0.02\n",
      "episode : 19980, reward mean : 0.1080900088045746, total_step : 801886, cur_epsilon : 0.02\n",
      "episode : 19990, reward mean : 0.1066700087916106, total_step : 802178, cur_epsilon : 0.02\n",
      "episode : 20000, reward mean : 0.10388000878691674, total_step : 802476, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20000, reward mean : -0.1754999860189855, step mean : 63.0, eval_episode_rewards : [-1.   -1.    0.95 -1.   -1.   -1.    0.58  0.98  0.87  0.85 -1.    0.63\n",
      " -1.   -1.    0.67  0.96 -1.   -1.    1.   -1.  ]\n",
      "episode : 20010, reward mean : 0.0999300088081509, total_step : 802938, cur_epsilon : 0.02\n",
      "episode : 20020, reward mean : 0.09757000874914229, total_step : 803294, cur_epsilon : 0.02\n",
      "episode : 20030, reward mean : 0.09807000869326293, total_step : 803542, cur_epsilon : 0.02\n",
      "episode : 20040, reward mean : 0.09465000876970589, total_step : 804050, cur_epsilon : 0.02\n",
      "episode : 20050, reward mean : 0.09738000877574086, total_step : 804461, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20050, reward mean : -0.31049998411908747, step mean : 71.45, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.92 -1.   -1.    0.86  0.35 -1.    0.7\n",
      " -1.   -1.   -1.    0.85  0.35 -1.    0.76  1.  ]\n",
      "episode : 20060, reward mean : 0.0982200087569654, total_step : 804893, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 20070, reward mean : 0.09978000878915191, total_step : 805240, cur_epsilon : 0.02\n",
      "episode : 20080, reward mean : 0.10138000877574087, total_step : 805540, cur_epsilon : 0.02\n",
      "episode : 20090, reward mean : 0.10201000880636275, total_step : 805976, cur_epsilon : 0.02\n",
      "episode : 20100, reward mean : 0.10708000876009464, total_step : 806392, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20100, reward mean : -0.204499985370785, step mean : 65.9, eval_episode_rewards : [-1.    0.89 -1.   -1.    0.83  0.69 -1.    1.    0.37 -1.   -1.    0.82\n",
      " -1.    0.57  0.98 -1.   -1.   -1.    0.76 -1.  ]\n",
      "episode : 20110, reward mean : 0.10479000874422491, total_step : 806815, cur_epsilon : 0.02\n",
      "episode : 20120, reward mean : 0.10309000873751939, total_step : 807220, cur_epsilon : 0.02\n",
      "episode : 20130, reward mean : 0.09754000879451633, total_step : 807746, cur_epsilon : 0.02\n",
      "episode : 20140, reward mean : 0.09466000879183412, total_step : 808115, cur_epsilon : 0.02\n",
      "episode : 20150, reward mean : 0.10209000875987112, total_step : 808305, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20150, reward mean : -0.2644999851472676, step mean : 66.85, eval_episode_rewards : [-1.   -1.    0.77 -1.    0.6   0.96 -1.   -1.    1.   -1.   -1.    0.98\n",
      "  0.54 -1.    0.88 -1.    0.98 -1.   -1.   -1.  ]\n",
      "episode : 20160, reward mean : 0.09818000873550772, total_step : 808604, cur_epsilon : 0.02\n",
      "episode : 20170, reward mean : 0.09787000874243677, total_step : 808988, cur_epsilon : 0.02\n",
      "episode : 20180, reward mean : 0.09490000874176621, total_step : 809431, cur_epsilon : 0.02\n",
      "episode : 20190, reward mean : 0.09964000870287418, total_step : 809833, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 20200, reward mean : 0.10178000872209668, total_step : 810346, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20200, reward mean : -0.2009999854490161, step mean : 65.55, eval_episode_rewards : [-1.    1.    0.83  0.94  0.49 -1.   -1.   -1.   -1.    0.54 -1.    0.98\n",
      " -1.   -1.   -1.    0.76 -1.    0.87  0.57 -1.  ]\n",
      "episode : 20210, reward mean : 0.10368000870198012, total_step : 810773, cur_epsilon : 0.02\n",
      "episode : 20220, reward mean : 0.10104000873863697, total_step : 811128, cur_epsilon : 0.02\n",
      "episode : 20230, reward mean : 0.10204000873863697, total_step : 811570, cur_epsilon : 0.02\n",
      "episode : 20240, reward mean : 0.10498000869527459, total_step : 811863, cur_epsilon : 0.02\n",
      "episode : 20250, reward mean : 0.10161000865884125, total_step : 812165, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20250, reward mean : -0.6729999804869294, step mean : 87.5, eval_episode_rewards : [ 0.23 -1.   -1.    0.78 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.    0.77 -1.    0.76]\n",
      "episode : 20260, reward mean : 0.10867000861279666, total_step : 812345, cur_epsilon : 0.02\n",
      "episode : 20270, reward mean : 0.11051000857166945, total_step : 812620, cur_epsilon : 0.02\n",
      "episode : 20280, reward mean : 0.10146000857278704, total_step : 812924, cur_epsilon : 0.02\n",
      "episode : 20290, reward mean : 0.09795000853948295, total_step : 813451, cur_epsilon : 0.02\n",
      "episode : 20300, reward mean : 0.09341000857390463, total_step : 813862, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20300, reward mean : -0.43249998250976207, step mean : 78.6, eval_episode_rewards : [-1.   -1.   -1.    0.86 -1.   -1.    0.34 -1.    0.8   0.72 -1.   -1.\n",
      "  0.96 -1.   -1.   -1.   -1.   -1.    0.03  0.64]\n",
      "episode : 20310, reward mean : 0.09346000859513878, total_step : 814368, cur_epsilon : 0.02\n",
      "episode : 20320, reward mean : 0.09004000860452652, total_step : 814750, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 20330, reward mean : 0.09067000861279667, total_step : 815197, cur_epsilon : 0.02\n",
      "episode : 20340, reward mean : 0.08668000865727662, total_step : 815645, cur_epsilon : 0.02\n",
      "episode : 20350, reward mean : 0.08601000864990055, total_step : 816073, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20350, reward mean : -0.39699998330324887, step mean : 75.05, eval_episode_rewards : [-1.    0.22 -1.   -1.    0.25  0.92 -1.   -1.    0.93  0.92 -1.    0.95\n",
      " -1.   -1.   -1.   -1.   -1.    0.87 -1.   -1.  ]\n",
      "episode : 20360, reward mean : 0.08772000863403082, total_step : 816459, cur_epsilon : 0.02\n",
      "episode : 20370, reward mean : 0.09178000861033797, total_step : 816744, cur_epsilon : 0.02\n",
      "episode : 20380, reward mean : 0.09059000859223307, total_step : 817157, cur_epsilon : 0.02\n",
      "episode : 20390, reward mean : 0.09291000854037702, total_step : 817367, cur_epsilon : 0.02\n",
      "episode : 20400, reward mean : 0.08623000853322446, total_step : 817743, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20400, reward mean : -0.1219999860972166, step mean : 62.7, eval_episode_rewards : [ 0.54  0.93  0.15 -1.    0.84  0.97 -1.    0.97 -1.    0.56 -1.   -1.\n",
      "  0.86 -1.    0.83  0.91 -1.   -1.   -1.   -1.  ]\n",
      "episode : 20410, reward mean : 0.09498000847175718, total_step : 818096, cur_epsilon : 0.02\n",
      "episode : 20420, reward mean : 0.09458000850304961, total_step : 818662, cur_epsilon : 0.02\n",
      "episode : 20430, reward mean : 0.09628000850975514, total_step : 819129, cur_epsilon : 0.02\n",
      "episode : 20440, reward mean : 0.09351000848226249, total_step : 819427, cur_epsilon : 0.02\n",
      "episode : 20450, reward mean : 0.09263000847958028, total_step : 819860, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20450, reward mean : -0.14799998551607133, step mean : 65.3, eval_episode_rewards : [-1.    0.74  0.64 -1.    0.9  -1.   -1.    0.97  0.83 -1.   -1.   -1.\n",
      "  0.28 -1.    0.31 -1.   -1.    0.97  0.58  0.82]\n",
      "synced target net\n",
      "episode : 20460, reward mean : 0.08757000845856965, total_step : 820267, cur_epsilon : 0.02\n",
      "episode : 20470, reward mean : 0.09087000842951239, total_step : 820661, cur_epsilon : 0.02\n",
      "episode : 20480, reward mean : 0.10290000838413835, total_step : 821003, cur_epsilon : 0.02\n",
      "episode : 20490, reward mean : 0.09550000843778253, total_step : 821418, cur_epsilon : 0.02\n",
      "episode : 20500, reward mean : 0.09060000843554736, total_step : 821867, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20500, reward mean : -0.3149999840185046, step mean : 71.9, eval_episode_rewards : [ 0.96 -1.    1.   -1.   -1.    0.76  0.96 -1.    0.53 -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.19 -1.    0.42 -1.    0.88]\n",
      "episode : 20510, reward mean : 0.09282000840827823, total_step : 822298, cur_epsilon : 0.02\n",
      "episode : 20520, reward mean : 0.07975000845454633, total_step : 822834, cur_epsilon : 0.02\n",
      "episode : 20530, reward mean : 0.07150000848248601, total_step : 823303, cur_epsilon : 0.02\n",
      "episode : 20540, reward mean : 0.06334000848606229, total_step : 823783, cur_epsilon : 0.02\n",
      "episode : 20550, reward mean : 0.05490000856295228, total_step : 824372, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20550, reward mean : -0.19949998548254372, step mean : 65.4, eval_episode_rewards : [-1.    0.94 -1.    0.91  0.26  0.82 -1.    0.18 -1.   -1.    0.93  0.98\n",
      " -1.   -1.    0.99 -1.   -1.   -1.    1.   -1.  ]\n",
      "episode : 20560, reward mean : 0.055130008535459636, total_step : 824764, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 20570, reward mean : 0.053070008536800745, total_step : 825124, cur_epsilon : 0.02\n",
      "episode : 20580, reward mean : 0.05038000857457518, total_step : 825626, cur_epsilon : 0.02\n",
      "episode : 20590, reward mean : 0.05363000859133899, total_step : 826181, cur_epsilon : 0.02\n",
      "episode : 20600, reward mean : 0.055460008572787045, total_step : 826659, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20600, reward mean : -0.022499986086040737, step mean : 62.85, eval_episode_rewards : [-1.   -1.   -1.    0.02  0.87 -1.    0.87  0.04  0.54 -1.   -1.    0.99\n",
      "  0.41  0.97 -1.    0.72  0.24  0.93  0.95 -1.  ]\n",
      "episode : 20610, reward mean : 0.05495000856183469, total_step : 827019, cur_epsilon : 0.02\n",
      "episode : 20620, reward mean : 0.05628000855445862, total_step : 827491, cur_epsilon : 0.02\n",
      "episode : 20630, reward mean : 0.058090008581057194, total_step : 827967, cur_epsilon : 0.02\n",
      "episode : 20640, reward mean : 0.05292000860720873, total_step : 828508, cur_epsilon : 0.02\n",
      "episode : 20650, reward mean : 0.05543000861816108, total_step : 828969, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20650, reward mean : 0.012000012025237084, step mean : 54.35, eval_episode_rewards : [-1.   -1.    0.95  0.36 -1.    0.99  0.66  0.97 -1.    0.98 -1.    0.97\n",
      " -1.    0.93  0.95 -1.    0.97 -1.    0.51 -1.  ]\n",
      "episode : 20660, reward mean : 0.056130008647218343, total_step : 829352, cur_epsilon : 0.02\n",
      "episode : 20670, reward mean : 0.052740008655935526, total_step : 829764, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 20680, reward mean : 0.05074000867828727, total_step : 830182, cur_epsilon : 0.02\n",
      "episode : 20690, reward mean : 0.05343000868521631, total_step : 830554, cur_epsilon : 0.02\n",
      "episode : 20700, reward mean : 0.05054000868275762, total_step : 830934, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20700, reward mean : -0.4624999829567969, step mean : 76.55, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.85  0.5  -1.    0.9  -1.   -1.   -1.   -1.\n",
      "  0.86 -1.    0.72 -1.   -1.   -1.    0.92 -1.  ]\n",
      "episode : 20710, reward mean : 0.055950008695945144, total_step : 831251, cur_epsilon : 0.02\n",
      "episode : 20720, reward mean : 0.061890008697286246, total_step : 831658, cur_epsilon : 0.02\n",
      "episode : 20730, reward mean : 0.06541000870801508, total_step : 832033, cur_epsilon : 0.02\n",
      "episode : 20740, reward mean : 0.06820000869035721, total_step : 832536, cur_epsilon : 0.02\n",
      "episode : 20750, reward mean : 0.06180000867694616, total_step : 832843, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20750, reward mean : -0.7449999799951911, step mean : 89.65, eval_episode_rewards : [ 0.31  0.97 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.82 -1.   -1.   -1.   -1.  ]\n",
      "episode : 20760, reward mean : 0.05537000866420567, total_step : 833148, cur_epsilon : 0.02\n",
      "episode : 20770, reward mean : 0.04521000873483717, total_step : 833603, cur_epsilon : 0.02\n",
      "episode : 20780, reward mean : 0.04696000871807337, total_step : 834088, cur_epsilon : 0.02\n",
      "episode : 20790, reward mean : 0.044040008760988715, total_step : 834546, cur_epsilon : 0.02\n",
      "episode : 20800, reward mean : 0.05022000871226191, total_step : 834676, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20800, reward mean : -0.3004999843426049, step mean : 70.45, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.99  0.94 -1.    0.74  0.49 -1.   -1.    0.97\n",
      " -1.    0.18 -1.    0.85 -1.   -1.    0.83 -1.  ]\n",
      "synced target net\n",
      "episode : 20810, reward mean : 0.04983000869862735, total_step : 835038, cur_epsilon : 0.02\n",
      "episode : 20820, reward mean : 0.05527000866644084, total_step : 835431, cur_epsilon : 0.02\n",
      "episode : 20830, reward mean : 0.05352000866085291, total_step : 835863, cur_epsilon : 0.02\n",
      "episode : 20840, reward mean : 0.05144000868499279, total_step : 836331, cur_epsilon : 0.02\n",
      "episode : 20850, reward mean : 0.04908000869303942, total_step : 836957, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20850, reward mean : -0.18999998569488524, step mean : 64.45, eval_episode_rewards : [ 0.92  0.75  0.9   1.    0.04 -1.    0.96 -1.   -1.   -1.    0.98 -1.\n",
      "  0.88  0.77 -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 20860, reward mean : 0.049110008714720606, total_step : 837379, cur_epsilon : 0.02\n",
      "episode : 20870, reward mean : 0.04401000873930752, total_step : 837829, cur_epsilon : 0.02\n",
      "episode : 20880, reward mean : 0.04623000873439014, total_step : 838150, cur_epsilon : 0.02\n",
      "episode : 20890, reward mean : 0.04180000876635313, total_step : 838599, cur_epsilon : 0.02\n",
      "episode : 20900, reward mean : 0.0375300087723881, total_step : 839020, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20900, reward mean : -0.5064999819733202, step mean : 80.95, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.92 -1.    0.13 -1.   -1.   -1.   -1.\n",
      " -1.    0.1  -1.    0.87  0.89  0.96 -1.   -1.  ]\n",
      "episode : 20910, reward mean : 0.03658000874891877, total_step : 839370, cur_epsilon : 0.02\n",
      "episode : 20920, reward mean : 0.037180008735507725, total_step : 839746, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 20930, reward mean : 0.04190000874176621, total_step : 840189, cur_epsilon : 0.02\n",
      "episode : 20940, reward mean : 0.04466000870242715, total_step : 840432, cur_epsilon : 0.02\n",
      "episode : 20950, reward mean : 0.04036000868678093, total_step : 840822, cur_epsilon : 0.02\n",
      "[EVAL] episode : 20950, reward mean : -0.4574999830685556, step mean : 76.05, eval_episode_rewards : [-1.   -1.    0.93  0.61 -1.   -1.   -1.    0.88 -1.   -1.    0.98 -1.\n",
      " -1.   -1.    0.58  0.87 -1.   -1.   -1.   -1.  ]\n",
      "episode : 20960, reward mean : 0.04075000867806375, total_step : 841100, cur_epsilon : 0.02\n",
      "episode : 20970, reward mean : 0.03730000871047377, total_step : 841532, cur_epsilon : 0.02\n",
      "episode : 20980, reward mean : 0.03536000875383615, total_step : 841929, cur_epsilon : 0.02\n",
      "episode : 20990, reward mean : 0.037550008749589324, total_step : 842202, cur_epsilon : 0.02\n",
      "episode : 21000, reward mean : 0.03363000881485641, total_step : 842789, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21000, reward mean : -0.721499980520457, step mean : 87.3, eval_episode_rewards : [-1.   -1.   -1.    0.82 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.95  0.8  -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 21010, reward mean : 0.03433000879921019, total_step : 843183, cur_epsilon : 0.02\n",
      "episode : 21020, reward mean : 0.036770008811727165, total_step : 843594, cur_epsilon : 0.02\n",
      "episode : 21030, reward mean : 0.036470008907839656, total_step : 844268, cur_epsilon : 0.02\n",
      "episode : 21040, reward mean : 0.030900008875876666, total_step : 844634, cur_epsilon : 0.02\n",
      "episode : 21050, reward mean : 0.030790008833631873, total_step : 844856, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21050, reward mean : -0.2939999844878912, step mean : 69.8, eval_episode_rewards : [-1.    0.92 -1.    0.96 -1.   -1.   -1.   -1.    0.62 -1.   -1.   -1.\n",
      "  0.1   0.72  0.98  0.96 -1.   -1.    0.86 -1.  ]\n",
      "synced target net\n",
      "episode : 21060, reward mean : 0.03707000880502164, total_step : 845161, cur_epsilon : 0.02\n",
      "episode : 21070, reward mean : 0.035410008842125536, total_step : 845672, cur_epsilon : 0.02\n",
      "episode : 21080, reward mean : 0.030790008855983615, total_step : 846034, cur_epsilon : 0.02\n",
      "episode : 21090, reward mean : 0.027260008845478297, total_step : 846423, cur_epsilon : 0.02\n",
      "episode : 21100, reward mean : 0.023140008825808765, total_step : 846752, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21100, reward mean : -0.39649998331442476, step mean : 75.0, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.91 -1.   -1.    0.92  0.97 -1.   -1.   -1.\n",
      " -1.    0.74 -1.    0.32  0.97 -1.    0.24 -1.  ]\n",
      "episode : 21110, reward mean : 0.025820008810609578, total_step : 847107, cur_epsilon : 0.02\n",
      "episode : 21120, reward mean : 0.026610008815303444, total_step : 847532, cur_epsilon : 0.02\n",
      "episode : 21130, reward mean : 0.029190008802339434, total_step : 848002, cur_epsilon : 0.02\n",
      "episode : 21140, reward mean : 0.035650008836761116, total_step : 848523, cur_epsilon : 0.02\n",
      "episode : 21150, reward mean : 0.03382000887766481, total_step : 848896, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21150, reward mean : -0.5559999819844961, step mean : 80.85, eval_episode_rewards : [ 0.62 -1.   -1.   -1.   -1.   -1.    0.96 -1.   -1.   -1.    0.97 -1.\n",
      " -1.   -1.    0.84 -1.    0.49 -1.   -1.   -1.  ]\n",
      "episode : 21160, reward mean : 0.03584000887721777, total_step : 849193, cur_epsilon : 0.02\n",
      "episode : 21170, reward mean : 0.033000008918344974, total_step : 849760, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 21180, reward mean : 0.03108000893890858, total_step : 850294, cur_epsilon : 0.02\n",
      "episode : 21190, reward mean : 0.02760000892728567, total_step : 850644, cur_epsilon : 0.02\n",
      "episode : 21200, reward mean : 0.03155000888369978, total_step : 850962, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21200, reward mean : -0.3769999837502837, step mean : 73.05, eval_episode_rewards : [-1.    0.67  0.86 -1.    0.36  0.92 -1.   -1.   -1.    0.75 -1.   -1.\n",
      "  0.95 -1.   -1.   -1.    0.95 -1.   -1.   -1.  ]\n",
      "episode : 21210, reward mean : 0.03710000884905457, total_step : 851236, cur_epsilon : 0.02\n",
      "episode : 21220, reward mean : 0.03203000882826745, total_step : 851498, cur_epsilon : 0.02\n",
      "episode : 21230, reward mean : 0.035340008798986675, total_step : 851809, cur_epsilon : 0.02\n",
      "episode : 21240, reward mean : 0.038220008801668884, total_step : 852113, cur_epsilon : 0.02\n",
      "episode : 21250, reward mean : 0.04476000878959894, total_step : 852361, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21250, reward mean : -0.2804999847896397, step mean : 68.45, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    1.   -1.    0.95  0.23  0.89  0.96 -1.\n",
      " -1.    0.9  -1.   -1.   -1.    0.69  0.77 -1.  ]\n",
      "episode : 21260, reward mean : 0.036730008834972974, total_step : 852742, cur_epsilon : 0.02\n",
      "episode : 21270, reward mean : 0.04428000884503126, total_step : 853062, cur_epsilon : 0.02\n",
      "episode : 21280, reward mean : 0.05106000884994864, total_step : 853389, cur_epsilon : 0.02\n",
      "episode : 21290, reward mean : 0.05582000883296132, total_step : 853840, cur_epsilon : 0.02\n",
      "episode : 21300, reward mean : 0.054760008834302425, total_step : 854256, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21300, reward mean : -0.10849998639896512, step mean : 61.35, eval_episode_rewards : [ 0.67 -1.    0.93 -1.   -1.    0.11 -1.   -1.   -1.   -1.   -1.    0.71\n",
      "  0.95  0.9  -1.    0.76 -1.    0.94  0.87  0.99]\n",
      "episode : 21310, reward mean : 0.05231000884436071, total_step : 854807, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 21320, reward mean : 0.04921000884659588, total_step : 855198, cur_epsilon : 0.02\n",
      "episode : 21330, reward mean : 0.053100008849054574, total_step : 855656, cur_epsilon : 0.02\n",
      "episode : 21340, reward mean : 0.05880000883340836, total_step : 856035, cur_epsilon : 0.02\n",
      "episode : 21350, reward mean : 0.06207000880502164, total_step : 856336, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21350, reward mean : -0.003499986510723829, step mean : 60.95, eval_episode_rewards : [ 0.17 -1.   -1.   -1.   -1.    0.91  0.27  0.8   0.06 -1.    0.69  0.98\n",
      " -1.    0.97  0.98  0.95  0.92 -1.   -1.    0.23]\n",
      "episode : 21360, reward mean : 0.06393000880815089, total_step : 856736, cur_epsilon : 0.02\n",
      "episode : 21370, reward mean : 0.05990000885352492, total_step : 857222, cur_epsilon : 0.02\n",
      "episode : 21380, reward mean : 0.05890000885352492, total_step : 857636, cur_epsilon : 0.02\n",
      "episode : 21390, reward mean : 0.054740008924156425, total_step : 858159, cur_epsilon : 0.02\n",
      "episode : 21400, reward mean : 0.06270000894740224, total_step : 858638, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21400, reward mean : -0.3274999837391078, step mean : 73.15, eval_episode_rewards : [-1.    0.86  0.81 -1.   -1.   -1.    0.04 -1.    0.91 -1.    0.7  -1.\n",
      " -1.   -1.    0.55  0.78 -1.   -1.    0.8  -1.  ]\n",
      "episode : 21410, reward mean : 0.0607700089905411, total_step : 859182, cur_epsilon : 0.02\n",
      "episode : 21420, reward mean : 0.06065000897087157, total_step : 859661, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 21430, reward mean : 0.06134000895544887, total_step : 860059, cur_epsilon : 0.02\n",
      "episode : 21440, reward mean : 0.05677000903524458, total_step : 860711, cur_epsilon : 0.02\n",
      "episode : 21450, reward mean : 0.060010009007528425, total_step : 861020, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21450, reward mean : -0.4459999822080135, step mean : 79.95, eval_episode_rewards : [-1.   -1.   -1.    0.83  0.95  0.16  0.19 -1.   -1.   -1.   -1.    0.93\n",
      " -1.   -1.    0.21 -1.   -1.    0.81 -1.   -1.  ]\n",
      "episode : 21460, reward mean : 0.06634000900015236, total_step : 861394, cur_epsilon : 0.02\n",
      "episode : 21470, reward mean : 0.06907000898383558, total_step : 861717, cur_epsilon : 0.02\n",
      "episode : 21480, reward mean : 0.06364000901579857, total_step : 862202, cur_epsilon : 0.02\n",
      "episode : 21490, reward mean : 0.06571000901423395, total_step : 862610, cur_epsilon : 0.02\n",
      "episode : 21500, reward mean : 0.07236000897735358, total_step : 862895, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21500, reward mean : -0.5619999818503857, step mean : 81.45, eval_episode_rewards : [-1.   -1.    0.1  -1.   -1.   -1.   -1.   -1.   -1.    0.94  0.8  -1.\n",
      " -1.    0.92 -1.    1.   -1.   -1.   -1.   -1.  ]\n",
      "episode : 21510, reward mean : 0.06999000900797546, total_step : 863462, cur_epsilon : 0.02\n",
      "episode : 21520, reward mean : 0.07668000899255276, total_step : 863929, cur_epsilon : 0.02\n",
      "episode : 21530, reward mean : 0.07723000895790756, total_step : 864244, cur_epsilon : 0.02\n",
      "episode : 21540, reward mean : 0.07960000892728566, total_step : 864588, cur_epsilon : 0.02\n",
      "episode : 21550, reward mean : 0.08480000883340835, total_step : 864760, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21550, reward mean : -0.2784999848343432, step mean : 68.25, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.98  0.99  0.68 -1.   -1.   -1.    0.94\n",
      "  0.94  0.8   0.98 -1.   -1.    0.12 -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 21560, reward mean : 0.08162000883743167, total_step : 865171, cur_epsilon : 0.02\n",
      "episode : 21570, reward mean : 0.08105000882782042, total_step : 865487, cur_epsilon : 0.02\n",
      "episode : 21580, reward mean : 0.08368000879138708, total_step : 865827, cur_epsilon : 0.02\n",
      "episode : 21590, reward mean : 0.08741000875271857, total_step : 866211, cur_epsilon : 0.02\n",
      "episode : 21600, reward mean : 0.08337000875361264, total_step : 866693, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21600, reward mean : -0.12149998610839248, step mean : 62.65, eval_episode_rewards : [ 0.64 -1.    0.69 -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.73\n",
      "  0.95 -1.    0.85  0.94  0.64  0.34  0.83  0.96]\n",
      "episode : 21610, reward mean : 0.0921700087133795, total_step : 866874, cur_epsilon : 0.02\n",
      "episode : 21620, reward mean : 0.08761000872589647, total_step : 867402, cur_epsilon : 0.02\n",
      "episode : 21630, reward mean : 0.08501000871695578, total_step : 867837, cur_epsilon : 0.02\n",
      "episode : 21640, reward mean : 0.08524000871181488, total_step : 868355, cur_epsilon : 0.02\n",
      "episode : 21650, reward mean : 0.08182000872120261, total_step : 868857, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21650, reward mean : 0.011000012047588825, step mean : 54.45, eval_episode_rewards : [ 0.91 -1.   -1.    0.63 -1.    0.82 -1.    0.87  0.85  0.82  0.9   0.73\n",
      " -1.    0.96 -1.   -1.   -1.    0.78 -1.    0.95]\n",
      "episode : 21660, reward mean : 0.07751000872813166, total_step : 869273, cur_epsilon : 0.02\n",
      "episode : 21670, reward mean : 0.08114000869169831, total_step : 869522, cur_epsilon : 0.02\n",
      "episode : 21680, reward mean : 0.08197000867314637, total_step : 869859, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 21690, reward mean : 0.08290000865235925, total_step : 870138, cur_epsilon : 0.02\n",
      "episode : 21700, reward mean : 0.08532000864297151, total_step : 870478, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21700, reward mean : -0.13199998587369918, step mean : 63.7, eval_episode_rewards : [ 0.75 -1.    0.85  0.99 -1.   -1.    0.99  0.91 -1.   -1.   -1.    0.09\n",
      "  0.71 -1.   -1.    0.76  0.73  0.58 -1.   -1.  ]\n",
      "episode : 21710, reward mean : 0.07491000869683921, total_step : 871034, cur_epsilon : 0.02\n",
      "episode : 21720, reward mean : 0.07099000869505108, total_step : 871433, cur_epsilon : 0.02\n",
      "episode : 21730, reward mean : 0.07027000868879259, total_step : 871781, cur_epsilon : 0.02\n",
      "episode : 21740, reward mean : 0.07189000865258277, total_step : 872124, cur_epsilon : 0.02\n",
      "episode : 21750, reward mean : 0.07216000869125128, total_step : 872602, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21750, reward mean : -0.1389999868348241, step mean : 59.35, eval_episode_rewards : [-1.   -1.    0.9  -1.    0.98  0.97 -1.    0.97 -1.   -1.   -1.   -1.\n",
      "  0.84 -1.    0.96  0.81  0.97 -1.   -1.    0.82]\n",
      "episode : 21760, reward mean : 0.07322000871226192, total_step : 873001, cur_epsilon : 0.02\n",
      "episode : 21770, reward mean : 0.07815000871382653, total_step : 873462, cur_epsilon : 0.02\n",
      "episode : 21780, reward mean : 0.07339000866375864, total_step : 873724, cur_epsilon : 0.02\n",
      "episode : 21790, reward mean : 0.06766000870242715, total_step : 874353, cur_epsilon : 0.02\n",
      "episode : 21800, reward mean : 0.05868000876903534, total_step : 874779, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21800, reward mean : -0.254999984242022, step mean : 70.95, eval_episode_rewards : [-1.   -1.    0.55 -1.   -1.   -1.   -1.    0.62 -1.    0.76 -1.    0.57\n",
      "  0.96 -1.    0.27 -1.    0.75  0.54 -1.    0.88]\n",
      "synced target net\n",
      "episode : 21810, reward mean : 0.060840008787810804, total_step : 875224, cur_epsilon : 0.02\n",
      "episode : 21820, reward mean : 0.05900000880658626, total_step : 875700, cur_epsilon : 0.02\n",
      "episode : 21830, reward mean : 0.05706000887230039, total_step : 876423, cur_epsilon : 0.02\n",
      "episode : 21840, reward mean : 0.05424000889062881, total_step : 876971, cur_epsilon : 0.02\n",
      "episode : 21850, reward mean : 0.05886000885441899, total_step : 877438, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21850, reward mean : -0.07999998703598976, step mean : 58.5, eval_episode_rewards : [ 0.87 -1.    0.87 -1.    0.92 -1.    0.91 -1.   -1.   -1.    0.16 -1.\n",
      " -1.    0.87  0.98 -1.    0.96 -1.    1.    0.86]\n",
      "episode : 21860, reward mean : 0.05583000887744129, total_step : 877963, cur_epsilon : 0.02\n",
      "episode : 21870, reward mean : 0.05756000888347626, total_step : 878440, cur_epsilon : 0.02\n",
      "episode : 21880, reward mean : 0.05624000891298055, total_step : 878893, cur_epsilon : 0.02\n",
      "episode : 21890, reward mean : 0.05486000889912248, total_step : 879280, cur_epsilon : 0.02\n",
      "episode : 21900, reward mean : 0.05738000888749957, total_step : 879649, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21900, reward mean : -0.2764999848790467, step mean : 68.05, eval_episode_rewards : [-1.    0.43 -1.   -1.   -1.   -1.   -1.    0.47  0.94 -1.   -1.   -1.\n",
      " -1.   -1.   -1.    1.    0.97  0.96  0.82  0.88]\n",
      "synced target net\n",
      "episode : 21910, reward mean : 0.060920008897781375, total_step : 880045, cur_epsilon : 0.02\n",
      "episode : 21920, reward mean : 0.05798000891879201, total_step : 880515, cur_epsilon : 0.02\n",
      "episode : 21930, reward mean : 0.056480008885264395, total_step : 880807, cur_epsilon : 0.02\n",
      "episode : 21940, reward mean : 0.05271000892482698, total_step : 881227, cur_epsilon : 0.02\n",
      "episode : 21950, reward mean : 0.05315000893734396, total_step : 881672, cur_epsilon : 0.02\n",
      "[EVAL] episode : 21950, reward mean : -0.5859999813139438, step mean : 83.85, eval_episode_rewards : [-1.   -1.    0.02  0.99 -1.   -1.    0.98 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.    0.61 -1.    0.68 -1.   -1.  ]\n",
      "episode : 21960, reward mean : 0.051550008950755, total_step : 882009, cur_epsilon : 0.02\n",
      "episode : 21970, reward mean : 0.0486900089699775, total_step : 882525, cur_epsilon : 0.02\n",
      "episode : 21980, reward mean : 0.043260008979588746, total_step : 882966, cur_epsilon : 0.02\n",
      "episode : 21990, reward mean : 0.04127000900171697, total_step : 883337, cur_epsilon : 0.02\n",
      "episode : 22000, reward mean : 0.0399500089418143, total_step : 883658, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22000, reward mean : -0.46149998186156155, step mean : 81.5, eval_episode_rewards : [ 0.42 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.77 -1.    0.64  0.8\n",
      "  0.95 -1.   -1.   -1.    0.14 -1.    0.05 -1.  ]\n",
      "episode : 22010, reward mean : 0.0411000089161098, total_step : 883937, cur_epsilon : 0.02\n",
      "episode : 22020, reward mean : 0.03989000889845192, total_step : 884271, cur_epsilon : 0.02\n",
      "episode : 22030, reward mean : 0.04281000885553658, total_step : 884756, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22040, reward mean : 0.05069000888057053, total_step : 885233, cur_epsilon : 0.02\n",
      "episode : 22050, reward mean : 0.045220008868724106, total_step : 885402, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22050, reward mean : -0.4189999828115106, step mean : 77.25, eval_episode_rewards : [ 0.24  0.87 -1.   -1.   -1.    0.38  0.99 -1.   -1.   -1.   -1.    0.85\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.94  0.35]\n",
      "episode : 22060, reward mean : 0.04418000884726644, total_step : 885611, cur_epsilon : 0.02\n",
      "episode : 22070, reward mean : 0.043000008784234524, total_step : 885842, cur_epsilon : 0.02\n",
      "episode : 22080, reward mean : 0.043140008803457024, total_step : 886289, cur_epsilon : 0.02\n",
      "episode : 22090, reward mean : 0.044540008816868065, total_step : 886738, cur_epsilon : 0.02\n",
      "episode : 22100, reward mean : 0.046710008835420015, total_step : 887149, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22100, reward mean : 0.22650000946596266, step mean : 43.0, eval_episode_rewards : [ 0.96  0.93  0.56  0.88 -1.   -1.    0.86  0.88  0.98 -1.   -1.   -1.\n",
      "  0.98  0.87  0.78 -1.    0.99  0.86 -1.    1.  ]\n",
      "episode : 22110, reward mean : 0.04372000885754824, total_step : 887601, cur_epsilon : 0.02\n",
      "episode : 22120, reward mean : 0.04801000885106623, total_step : 887999, cur_epsilon : 0.02\n",
      "episode : 22130, reward mean : 0.045520008839666844, total_step : 888417, cur_epsilon : 0.02\n",
      "episode : 22140, reward mean : 0.04471000881306827, total_step : 888821, cur_epsilon : 0.02\n",
      "episode : 22150, reward mean : 0.04268000879138708, total_step : 889096, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22150, reward mean : -0.3614999840967357, step mean : 71.5, eval_episode_rewards : [ 0.94  0.94 -1.    0.41 -1.    0.65 -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.92  0.97  0.94 -1.   -1.   -1.  ]\n",
      "episode : 22160, reward mean : 0.039240008868277075, total_step : 889735, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22170, reward mean : 0.042220008824020626, total_step : 890105, cur_epsilon : 0.02\n",
      "episode : 22180, reward mean : 0.046220008801668884, total_step : 890542, cur_epsilon : 0.02\n",
      "episode : 22190, reward mean : 0.04333000879921019, total_step : 890880, cur_epsilon : 0.02\n",
      "episode : 22200, reward mean : 0.04154000881686807, total_step : 891278, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22200, reward mean : 0.12400001063942909, step mean : 48.2, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.   -1.   -1.    0.54  0.81  0.78  1.    0.97\n",
      "  0.91  0.79  0.99  0.83  0.92  1.    0.94 -1.  ]\n",
      "episode : 22210, reward mean : 0.03670000885799527, total_step : 891735, cur_epsilon : 0.02\n",
      "episode : 22220, reward mean : 0.038320008911192416, total_step : 892233, cur_epsilon : 0.02\n",
      "episode : 22230, reward mean : 0.037260008957237, total_step : 892749, cur_epsilon : 0.02\n",
      "episode : 22240, reward mean : 0.02873000899143517, total_step : 893205, cur_epsilon : 0.02\n",
      "episode : 22250, reward mean : 0.02907000900618732, total_step : 893518, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22250, reward mean : -0.3269999848678708, step mean : 68.05, eval_episode_rewards : [-1.   -1.   -1.    0.89 -1.   -1.    0.98  0.77 -1.    0.9  -1.    0.98\n",
      " -1.    0.95 -1.   -1.    0.99 -1.   -1.   -1.  ]\n",
      "episode : 22260, reward mean : 0.025860009033232927, total_step : 894018, cur_epsilon : 0.02\n",
      "episode : 22270, reward mean : 0.01798000905290246, total_step : 894425, cur_epsilon : 0.02\n",
      "episode : 22280, reward mean : 0.01382000905647874, total_step : 894768, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22290, reward mean : 0.00736000906676054, total_step : 895263, cur_epsilon : 0.02\n",
      "episode : 22300, reward mean : 0.005730009080842137, total_step : 895743, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22300, reward mean : -0.2304999859072268, step mean : 63.45, eval_episode_rewards : [ 0.99 -1.    0.82 -1.    0.99 -1.   -1.   -1.   -1.   -1.    0.83 -1.\n",
      "  0.98 -1.   -1.    0.86  0.98 -1.    0.94 -1.  ]\n",
      "episode : 22310, reward mean : 0.0067500090580433604, total_step : 896192, cur_epsilon : 0.02\n",
      "episode : 22320, reward mean : 0.007270009091123938, total_step : 896731, cur_epsilon : 0.02\n",
      "episode : 22330, reward mean : 0.005750009080395102, total_step : 897143, cur_epsilon : 0.02\n",
      "episode : 22340, reward mean : 0.0017000091038644313, total_step : 897626, cur_epsilon : 0.02\n",
      "episode : 22350, reward mean : -0.0006799908429384232, total_step : 898163, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22350, reward mean : -0.01949998727068305, step mean : 57.5, eval_episode_rewards : [-1.    0.95 -1.   -1.    0.73  1.   -1.   -1.    0.61 -1.   -1.    0.93\n",
      "  0.98 -1.   -1.    0.96  0.95  0.31  0.22  0.97]\n",
      "episode : 22360, reward mean : -0.0027899908404797314, total_step : 898574, cur_epsilon : 0.02\n",
      "episode : 22370, reward mean : -0.0014999908469617367, total_step : 899034, cur_epsilon : 0.02\n",
      "episode : 22380, reward mean : -0.003809990817680955, total_step : 899578, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22390, reward mean : -0.0016099908221513034, total_step : 900083, cur_epsilon : 0.02\n",
      "episode : 22400, reward mean : -0.0050299908351153135, total_step : 900504, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22400, reward mean : -0.41499998290091755, step mean : 76.85, eval_episode_rewards : [-1.   -1.   -1.    0.98 -1.    0.96 -1.   -1.   -1.    0.4  -1.   -1.\n",
      "  0.44  0.33 -1.   -1.   -1.    0.94 -1.    0.65]\n",
      "episode : 22410, reward mean : -0.004269990852102637, total_step : 900974, cur_epsilon : 0.02\n",
      "episode : 22420, reward mean : 0.0013200091123580933, total_step : 901294, cur_epsilon : 0.02\n",
      "episode : 22430, reward mean : -0.0013199909180402756, total_step : 901558, cur_epsilon : 0.02\n",
      "episode : 22440, reward mean : 0.006750009013339877, total_step : 901906, cur_epsilon : 0.02\n",
      "episode : 22450, reward mean : -0.0008999909721314907, total_step : 902281, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22450, reward mean : -0.05049998657777906, step mean : 60.6, eval_episode_rewards : [ 0.74  0.52 -1.    0.89 -1.    0.82  0.09  0.8   0.96 -1.    0.75 -1.\n",
      " -1.   -1.   -1.    0.49  0.95 -1.    0.98 -1.  ]\n",
      "episode : 22460, reward mean : -0.00470999095402658, total_step : 902735, cur_epsilon : 0.02\n",
      "episode : 22470, reward mean : -0.008949990903958678, total_step : 903280, cur_epsilon : 0.02\n",
      "episode : 22480, reward mean : -0.006119990944862366, total_step : 903583, cur_epsilon : 0.02\n",
      "episode : 22490, reward mean : -0.010709990954026579, total_step : 903951, cur_epsilon : 0.02\n",
      "episode : 22500, reward mean : -0.018299990873783825, total_step : 904591, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22500, reward mean : 0.17100001070648432, step mean : 48.55, eval_episode_rewards : [ 1.    0.97 -1.   -1.    0.78  0.97  0.98  0.9  -1.   -1.    0.93 -1.\n",
      "  0.89  0.97  0.38  0.96 -1.    0.51 -1.    0.18]\n",
      "episode : 22510, reward mean : -0.009989990947768092, total_step : 904828, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22520, reward mean : -0.00620999096520245, total_step : 905216, cur_epsilon : 0.02\n",
      "episode : 22530, reward mean : 0.0011500090267509222, total_step : 905496, cur_epsilon : 0.02\n",
      "episode : 22540, reward mean : 0.0021000090278685095, total_step : 905846, cur_epsilon : 0.02\n",
      "episode : 22550, reward mean : 0.0015100090857595205, total_step : 906275, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22550, reward mean : -0.1514999854378402, step mean : 65.65, eval_episode_rewards : [ 0.03  0.48 -1.   -1.   -1.    0.99 -1.    0.78  0.79  0.91 -1.   -1.\n",
      "  0.98  0.5   0.56 -1.    0.95 -1.   -1.   -1.  ]\n",
      "episode : 22560, reward mean : 0.003330009112134576, total_step : 906805, cur_epsilon : 0.02\n",
      "episode : 22570, reward mean : 0.00607000914029777, total_step : 907247, cur_epsilon : 0.02\n",
      "episode : 22580, reward mean : 0.0009000091888010502, total_step : 907803, cur_epsilon : 0.02\n",
      "episode : 22590, reward mean : 0.004300009179860354, total_step : 908148, cur_epsilon : 0.02\n",
      "episode : 22600, reward mean : 0.006140009205788374, total_step : 908745, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22600, reward mean : -0.47349998271092775, step mean : 77.65, eval_episode_rewards : [ 0.85 -1.   -1.   -1.   -1.   -1.   -1.    0.97 -1.   -1.   -1.    0.82\n",
      "  0.23 -1.   -1.   -1.   -1.    0.73  0.93 -1.  ]\n",
      "episode : 22610, reward mean : -0.0016999907307326794, total_step : 909209, cur_epsilon : 0.02\n",
      "episode : 22620, reward mean : -0.00018999071978032588, total_step : 909784, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22630, reward mean : -0.003479990668594837, total_step : 910445, cur_epsilon : 0.02\n",
      "episode : 22640, reward mean : -0.003449990691617131, total_step : 910861, cur_epsilon : 0.02\n",
      "episode : 22650, reward mean : 0.0002800092920660973, total_step : 911293, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22650, reward mean : -0.2234999849461019, step mean : 67.8, eval_episode_rewards : [ 0.18 -1.   -1.   -1.    0.76 -1.    0.41  0.94 -1.   -1.    0.82 -1.\n",
      "  0.95  0.92 -1.    0.57  0.98 -1.   -1.   -1.  ]\n",
      "episode : 22660, reward mean : 0.0010500093195587397, total_step : 911830, cur_epsilon : 0.02\n",
      "episode : 22670, reward mean : 0.003460009332746267, total_step : 912139, cur_epsilon : 0.02\n",
      "episode : 22680, reward mean : 0.0023100093584507704, total_step : 912591, cur_epsilon : 0.02\n",
      "episode : 22690, reward mean : 0.00020000940561294556, total_step : 913081, cur_epsilon : 0.02\n",
      "episode : 22700, reward mean : 0.00030000940337777135, total_step : 913411, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22700, reward mean : -0.3324999836273491, step mean : 73.65, eval_episode_rewards : [ 0.86 -1.    0.97 -1.   -1.   -1.    0.72 -1.    0.93  0.35 -1.   -1.\n",
      "  0.58  0.93 -1.    0.01 -1.   -1.   -1.   -1.  ]\n",
      "episode : 22710, reward mean : 0.0025900093521922826, total_step : 913740, cur_epsilon : 0.02\n",
      "episode : 22720, reward mean : -0.0003199906051158905, total_step : 914327, cur_epsilon : 0.02\n",
      "episode : 22730, reward mean : -0.007209990607574582, total_step : 914663, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22740, reward mean : -0.011399990603327751, total_step : 915025, cur_epsilon : 0.02\n",
      "episode : 22750, reward mean : -0.008989990612491965, total_step : 915462, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22750, reward mean : -0.5499999821186066, step mean : 80.25, eval_episode_rewards : [-1.   -1.   -1.    0.3  -1.   -1.   -1.   -1.    0.95 -1.   -1.   -1.\n",
      " -1.    0.94  0.88 -1.   -1.   -1.    0.93 -1.  ]\n",
      "episode : 22760, reward mean : -0.01161999062076211, total_step : 915825, cur_epsilon : 0.02\n",
      "episode : 22770, reward mean : -0.00826999062858522, total_step : 916252, cur_epsilon : 0.02\n",
      "episode : 22780, reward mean : -0.002099990587681532, total_step : 916695, cur_epsilon : 0.02\n",
      "episode : 22790, reward mean : 0.0024200093783438206, total_step : 917175, cur_epsilon : 0.02\n",
      "episode : 22800, reward mean : 0.008080009363591672, total_step : 917536, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22800, reward mean : -0.33049998367205263, step mean : 73.45, eval_episode_rewards : [-1.   -1.   -1.    0.99 -1.    0.92 -1.    0.28 -1.    0.83 -1.    0.12\n",
      " -1.   -1.    0.52 -1.    0.79 -1.   -1.    0.94]\n",
      "episode : 22810, reward mean : -0.0010999906100332738, total_step : 918099, cur_epsilon : 0.02\n",
      "episode : 22820, reward mean : -0.0016099905762821437, total_step : 918725, cur_epsilon : 0.02\n",
      "episode : 22830, reward mean : -0.0010099906120449305, total_step : 919289, cur_epsilon : 0.02\n",
      "episode : 22840, reward mean : 0.0032200093157589435, total_step : 919517, cur_epsilon : 0.02\n",
      "episode : 22850, reward mean : -0.0025099906902760266, total_step : 919957, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22850, reward mean : -0.5959999810904264, step mean : 84.85, eval_episode_rewards : [ 0.88 -1.    0.11 -1.   -1.   -1.   -1.    0.54 -1.    0.93  0.62 -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 22860, reward mean : -0.0037199907079339027, total_step : 920405, cur_epsilon : 0.02\n",
      "episode : 22870, reward mean : 0.0014100092444568873, total_step : 920670, cur_epsilon : 0.02\n",
      "episode : 22880, reward mean : -0.0009699907917529345, total_step : 920961, cur_epsilon : 0.02\n",
      "episode : 22890, reward mean : 0.0006600091941654682, total_step : 921286, cur_epsilon : 0.02\n",
      "episode : 22900, reward mean : 0.0011700092051178217, total_step : 921705, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22900, reward mean : -0.41999998278915884, step mean : 77.35, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.64  0.52  0.98  0.71 -1.   -1.   -1.    0.72\n",
      " -1.   -1.    0.6  -1.   -1.    0.43 -1.   -1.  ]\n",
      "episode : 22910, reward mean : -0.005479990758001804, total_step : 922263, cur_epsilon : 0.02\n",
      "episode : 22920, reward mean : -0.0027799907736480237, total_step : 922663, cur_epsilon : 0.02\n",
      "episode : 22930, reward mean : -0.0003399907611310482, total_step : 923012, cur_epsilon : 0.02\n",
      "episode : 22940, reward mean : 0.00370000921562314, total_step : 923329, cur_epsilon : 0.02\n",
      "episode : 22950, reward mean : 0.004300009179860354, total_step : 923615, cur_epsilon : 0.02\n",
      "[EVAL] episode : 22950, reward mean : -0.15749998642131685, step mean : 61.2, eval_episode_rewards : [-1.    0.98 -1.    0.71  0.88 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      " -1.    0.82  0.81  0.89  0.92  0.94 -1.    0.9 ]\n",
      "episode : 22960, reward mean : -0.00040999078191816807, total_step : 924122, cur_epsilon : 0.02\n",
      "episode : 22970, reward mean : 0.003050009185448289, total_step : 924494, cur_epsilon : 0.02\n",
      "episode : 22980, reward mean : 0.011020009186118842, total_step : 924938, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 22990, reward mean : 0.010530009174719452, total_step : 925259, cur_epsilon : 0.02\n",
      "episode : 23000, reward mean : 0.018890009189024566, total_step : 925645, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23000, reward mean : 0.19550001015886664, step mean : 46.1, eval_episode_rewards : [ 0.98  0.86 -1.   -1.    0.78  0.94  0.78 -1.    0.96 -1.   -1.    0.96\n",
      "  0.94  0.62  0.98 -1.    0.83  0.93  0.35 -1.  ]\n",
      "episode : 23010, reward mean : 0.016800009213387965, total_step : 926032, cur_epsilon : 0.02\n",
      "episode : 23020, reward mean : 0.019150009227916598, total_step : 926430, cur_epsilon : 0.02\n",
      "episode : 23030, reward mean : 0.013900009233504533, total_step : 926939, cur_epsilon : 0.02\n",
      "episode : 23040, reward mean : 0.0145500092189759, total_step : 927351, cur_epsilon : 0.02\n",
      "episode : 23050, reward mean : 0.014500009264796972, total_step : 927725, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23050, reward mean : -0.27049998501315714, step mean : 67.45, eval_episode_rewards : [-1.   -1.    0.97 -1.   -1.   -1.    0.88 -1.   -1.   -1.    0.99 -1.\n",
      " -1.    0.85  0.88 -1.    0.1   0.96  0.96 -1.  ]\n",
      "episode : 23060, reward mean : 0.009360009312629699, total_step : 928147, cur_epsilon : 0.02\n",
      "episode : 23070, reward mean : 0.008990009343251585, total_step : 928514, cur_epsilon : 0.02\n",
      "episode : 23080, reward mean : 0.01263000930659473, total_step : 928797, cur_epsilon : 0.02\n",
      "episode : 23090, reward mean : 0.013740009281784296, total_step : 929135, cur_epsilon : 0.02\n",
      "episode : 23100, reward mean : 0.013910009322687983, total_step : 929727, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23100, reward mean : 0.06400001086294652, step mean : 49.15, eval_episode_rewards : [ 0.97  0.92  0.96  1.   -1.    1.   -1.    0.71  0.98  0.97 -1.    0.95\n",
      " -1.    0.92 -1.   -1.   -1.   -1.    0.9  -1.  ]\n",
      "synced target net\n",
      "episode : 23110, reward mean : 0.016920009300112723, total_step : 930080, cur_epsilon : 0.02\n",
      "episode : 23120, reward mean : 0.015100009273737669, total_step : 930361, cur_epsilon : 0.02\n",
      "episode : 23130, reward mean : 0.021740009259432553, total_step : 930715, cur_epsilon : 0.02\n",
      "episode : 23140, reward mean : 0.01561000926233828, total_step : 931130, cur_epsilon : 0.02\n",
      "episode : 23150, reward mean : 0.015440009266138076, total_step : 931422, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23150, reward mean : 0.11300001200288534, step mean : 54.35, eval_episode_rewards : [ 0.99  0.96 -1.   -1.   -1.   -1.    0.23  0.21  0.86 -1.    0.9  -1.\n",
      "  0.88 -1.    0.98  0.3   0.94  0.51  0.9   0.6 ]\n",
      "episode : 23160, reward mean : 0.017420009199529886, total_step : 931764, cur_epsilon : 0.02\n",
      "episode : 23170, reward mean : 0.018410009199753403, total_step : 932136, cur_epsilon : 0.02\n",
      "episode : 23180, reward mean : 0.014270009225234389, total_step : 932686, cur_epsilon : 0.02\n",
      "episode : 23190, reward mean : 0.020090009251609444, total_step : 933143, cur_epsilon : 0.02\n",
      "episode : 23200, reward mean : 0.01565000928379595, total_step : 933682, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23200, reward mean : -0.10949998637661337, step mean : 61.45, eval_episode_rewards : [ 0.7   0.63  0.94  0.86 -1.   -1.   -1.   -1.    0.6  -1.    0.97  0.6\n",
      "  0.89  0.81 -1.   -1.    0.81 -1.   -1.   -1.  ]\n",
      "episode : 23210, reward mean : 0.016890009233728052, total_step : 933916, cur_epsilon : 0.02\n",
      "episode : 23220, reward mean : 0.01896000920981169, total_step : 934310, cur_epsilon : 0.02\n",
      "episode : 23230, reward mean : 0.01913000918366015, total_step : 934710, cur_epsilon : 0.02\n",
      "episode : 23240, reward mean : 0.02842000913247466, total_step : 934938, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 23250, reward mean : 0.02722000913694501, total_step : 935272, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23250, reward mean : 0.008500013221055269, step mean : 59.75, eval_episode_rewards : [ 0.94  0.91  0.03  0.79 -1.   -1.   -1.    0.82 -1.    0.36  0.74 -1.\n",
      "  0.99  0.72 -1.   -1.    0.99  0.33  0.55 -1.  ]\n",
      "episode : 23260, reward mean : 0.02980000914633274, total_step : 935817, cur_epsilon : 0.02\n",
      "episode : 23270, reward mean : 0.03296000912040472, total_step : 936108, cur_epsilon : 0.02\n",
      "episode : 23280, reward mean : 0.03636000915616751, total_step : 936610, cur_epsilon : 0.02\n",
      "episode : 23290, reward mean : 0.04216000911593437, total_step : 936927, cur_epsilon : 0.02\n",
      "episode : 23300, reward mean : 0.04362000910565257, total_step : 937360, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23300, reward mean : -0.3229999849572778, step mean : 67.65, eval_episode_rewards : [ 0.98 -1.   -1.    1.   -1.    0.96 -1.   -1.   -1.    0.84 -1.    0.83\n",
      " -1.   -1.   -1.   -1.   -1.   -1.    0.98  0.95]\n",
      "episode : 23310, reward mean : 0.037640009082853795, total_step : 937708, cur_epsilon : 0.02\n",
      "episode : 23320, reward mean : 0.04079000903479755, total_step : 938033, cur_epsilon : 0.02\n",
      "episode : 23330, reward mean : 0.03824000904709101, total_step : 938498, cur_epsilon : 0.02\n",
      "episode : 23340, reward mean : 0.04294000900909305, total_step : 938812, cur_epsilon : 0.02\n",
      "episode : 23350, reward mean : 0.04303000898472965, total_step : 939241, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23350, reward mean : -0.3849999835714698, step mean : 73.85, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.94 -1.   -1.    0.16 -1.   -1.    0.62  0.91\n",
      " -1.    0.99 -1.   -1.    0.97  0.71 -1.   -1.  ]\n",
      "episode : 23360, reward mean : 0.043550009017810226, total_step : 939798, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 23370, reward mean : 0.045400009021162986, total_step : 940271, cur_epsilon : 0.02\n",
      "episode : 23380, reward mean : 0.04813000896014273, total_step : 940544, cur_epsilon : 0.02\n",
      "episode : 23390, reward mean : 0.04828000893443823, total_step : 940933, cur_epsilon : 0.02\n",
      "episode : 23400, reward mean : 0.04799000896327198, total_step : 941480, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23400, reward mean : 0.08100001160055399, step mean : 52.5, eval_episode_rewards : [ 0.91 -1.    0.48  0.95  0.97  0.94 -1.    0.49 -1.    0.88  0.57 -1.\n",
      "  0.62 -1.   -1.   -1.   -1.    0.97  0.96  0.88]\n",
      "episode : 23410, reward mean : 0.05055000892840326, total_step : 941795, cur_epsilon : 0.02\n",
      "episode : 23420, reward mean : 0.047920008942484854, total_step : 942178, cur_epsilon : 0.02\n",
      "episode : 23430, reward mean : 0.04895000898651779, total_step : 942638, cur_epsilon : 0.02\n",
      "episode : 23440, reward mean : 0.047510009018704294, total_step : 943128, cur_epsilon : 0.02\n",
      "episode : 23450, reward mean : 0.04769000903703272, total_step : 943584, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23450, reward mean : 0.17100001070648432, step mean : 48.55, eval_episode_rewards : [ 1.   -1.    0.99  0.77  0.8  -1.   -1.    0.62 -1.    0.7   0.39 -1.\n",
      "  0.5   0.89  0.9   0.89  0.99  0.98 -1.   -1.  ]\n",
      "episode : 23460, reward mean : 0.05143000904284418, total_step : 944064, cur_epsilon : 0.02\n",
      "episode : 23470, reward mean : 0.049550008995458486, total_step : 944399, cur_epsilon : 0.02\n",
      "episode : 23480, reward mean : 0.047240009024739266, total_step : 944832, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 23490, reward mean : 0.05194000905379653, total_step : 945330, cur_epsilon : 0.02\n",
      "episode : 23500, reward mean : 0.05678000901266932, total_step : 945789, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23500, reward mean : -0.2234999849461019, step mean : 67.8, eval_episode_rewards : [-1.    0.9   0.97  0.14 -1.    0.32 -1.   -1.    0.98 -1.   -1.   -1.\n",
      " -1.   -1.    0.43 -1.    0.95  0.97  0.87 -1.  ]\n",
      "episode : 23510, reward mean : 0.04591000912152231, total_step : 946511, cur_epsilon : 0.02\n",
      "episode : 23520, reward mean : 0.04146000913158059, total_step : 946944, cur_epsilon : 0.02\n",
      "episode : 23530, reward mean : 0.04062000915035605, total_step : 947308, cur_epsilon : 0.02\n",
      "episode : 23540, reward mean : 0.04335000915639103, total_step : 947684, cur_epsilon : 0.02\n",
      "episode : 23550, reward mean : 0.05065000912733376, total_step : 947985, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23550, reward mean : -0.36949998391792177, step mean : 72.3, eval_episode_rewards : [-1.    0.82  0.51 -1.    0.99 -1.   -1.    0.81  0.58  0.99 -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.91]\n",
      "episode : 23560, reward mean : 0.055240009114146234, total_step : 948454, cur_epsilon : 0.02\n",
      "episode : 23570, reward mean : 0.05119000918231904, total_step : 949198, cur_epsilon : 0.02\n",
      "episode : 23580, reward mean : 0.05926000913605094, total_step : 949549, cur_epsilon : 0.02\n",
      "episode : 23590, reward mean : 0.05591000912152231, total_step : 949829, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 23600, reward mean : 0.06027000909112394, total_step : 950291, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23600, reward mean : -0.27899998482316735, step mean : 68.3, eval_episode_rewards : [ 0.92 -1.    0.17  1.    0.82 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  0.95 -1.   -1.   -1.    0.84  0.76  0.96 -1.  ]\n",
      "episode : 23610, reward mean : 0.06271000905893744, total_step : 950612, cur_epsilon : 0.02\n",
      "episode : 23620, reward mean : 0.06658000906184315, total_step : 951198, cur_epsilon : 0.02\n",
      "episode : 23630, reward mean : 0.06976000899076462, total_step : 951545, cur_epsilon : 0.02\n",
      "episode : 23640, reward mean : 0.06755000899545849, total_step : 951982, cur_epsilon : 0.02\n",
      "episode : 23650, reward mean : 0.06752000899612903, total_step : 952415, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23650, reward mean : -0.12899998482316732, step mean : 68.45, eval_episode_rewards : [ 1.    0.25  0.92  0.93  0.1  -1.   -1.    0.84 -1.   -1.    0.27 -1.\n",
      "  0.1   0.95 -1.    0.21  0.85 -1.   -1.   -1.  ]\n",
      "episode : 23660, reward mean : 0.06693000896461308, total_step : 952811, cur_epsilon : 0.02\n",
      "episode : 23670, reward mean : 0.06077000901289284, total_step : 953334, cur_epsilon : 0.02\n",
      "episode : 23680, reward mean : 0.058790009012445806, total_step : 953782, cur_epsilon : 0.02\n",
      "episode : 23690, reward mean : 0.056440008975565434, total_step : 954107, cur_epsilon : 0.02\n",
      "episode : 23700, reward mean : 0.057230008980259296, total_step : 954457, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23700, reward mean : -0.38449998358264564, step mean : 73.8, eval_episode_rewards : [ 0.96  0.94 -1.   -1.   -1.   -1.   -1.    0.35 -1.   -1.   -1.   -1.\n",
      " -1.   -1.    0.83 -1.    0.64  0.99  0.6  -1.  ]\n",
      "synced target net\n",
      "episode : 23710, reward mean : 0.0549000090546906, total_step : 955116, cur_epsilon : 0.02\n",
      "episode : 23720, reward mean : 0.05917000900395215, total_step : 955478, cur_epsilon : 0.02\n",
      "episode : 23730, reward mean : 0.06332000902295112, total_step : 955898, cur_epsilon : 0.02\n",
      "episode : 23740, reward mean : 0.06543000902049244, total_step : 956249, cur_epsilon : 0.02\n",
      "episode : 23750, reward mean : 0.0643700089994818, total_step : 956592, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23750, reward mean : -0.13799998573958874, step mean : 64.3, eval_episode_rewards : [-1.   -1.   -1.   -1.    0.31  1.   -1.    0.18  0.69 -1.    0.97 -1.\n",
      "  0.81 -1.    0.95  0.94  0.49  0.9  -1.   -1.  ]\n",
      "episode : 23760, reward mean : 0.07498000898584724, total_step : 956894, cur_epsilon : 0.02\n",
      "episode : 23770, reward mean : 0.06915000898204744, total_step : 957304, cur_epsilon : 0.02\n",
      "episode : 23780, reward mean : 0.07604000893980264, total_step : 957561, cur_epsilon : 0.02\n",
      "episode : 23790, reward mean : 0.07857000892795622, total_step : 957988, cur_epsilon : 0.02\n",
      "episode : 23800, reward mean : 0.07590000892058014, total_step : 958317, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23800, reward mean : 0.026500011701136826, step mean : 52.9, eval_episode_rewards : [ 0.93  0.92 -1.   -1.    0.93  0.68 -1.   -1.   -1.    0.88  0.96  0.93\n",
      " -1.    0.94  0.88 -1.   -1.    0.97 -1.    0.51]\n",
      "episode : 23810, reward mean : 0.08689000885374844, total_step : 958583, cur_epsilon : 0.02\n",
      "episode : 23820, reward mean : 0.09057000879384577, total_step : 958944, cur_epsilon : 0.02\n",
      "episode : 23830, reward mean : 0.09259000877104699, total_step : 959409, cur_epsilon : 0.02\n",
      "episode : 23840, reward mean : 0.09732000875473022, total_step : 959564, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 23850, reward mean : 0.09616000878065825, total_step : 960116, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23850, reward mean : -0.26549998512491585, step mean : 66.95, eval_episode_rewards : [-1.   -1.   -1.    0.97  0.88 -1.   -1.   -1.    0.85 -1.   -1.   -1.\n",
      "  0.96  0.83 -1.    0.98  0.34 -1.   -1.    0.88]\n",
      "episode : 23860, reward mean : 0.0999300087634474, total_step : 960486, cur_epsilon : 0.02\n",
      "episode : 23870, reward mean : 0.09525000886805356, total_step : 961215, cur_epsilon : 0.02\n",
      "episode : 23880, reward mean : 0.09508000891655684, total_step : 961721, cur_epsilon : 0.02\n",
      "episode : 23890, reward mean : 0.09235000897757709, total_step : 962315, cur_epsilon : 0.02\n",
      "episode : 23900, reward mean : 0.09263000899367034, total_step : 962805, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23900, reward mean : -0.204499985370785, step mean : 65.9, eval_episode_rewards : [ 0.94 -1.   -1.   -1.   -1.    0.52  0.92  0.96 -1.    0.63 -1.    0.83\n",
      " -1.   -1.   -1.    0.59  0.79 -1.   -1.    0.73]\n",
      "episode : 23910, reward mean : 0.0997800089456141, total_step : 963151, cur_epsilon : 0.02\n",
      "episode : 23920, reward mean : 0.10336000893265009, total_step : 963492, cur_epsilon : 0.02\n",
      "episode : 23930, reward mean : 0.09606000896170735, total_step : 963971, cur_epsilon : 0.02\n",
      "episode : 23940, reward mean : 0.093120008982718, total_step : 964380, cur_epsilon : 0.02\n",
      "episode : 23950, reward mean : 0.09874000899121166, total_step : 964704, cur_epsilon : 0.02\n",
      "[EVAL] episode : 23950, reward mean : -0.05599998757243156, step mean : 56.1, eval_episode_rewards : [-1.   -1.   -1.   -1.   -1.    0.94 -1.    0.84  0.73  1.    0.95 -1.\n",
      " -1.   -1.   -1.    0.88  0.97  0.67  0.97  0.93]\n",
      "synced target net\n",
      "episode : 23960, reward mean : 0.10309000898338855, total_step : 965176, cur_epsilon : 0.02\n",
      "episode : 23970, reward mean : 0.10133000897802412, total_step : 965522, cur_epsilon : 0.02\n",
      "episode : 23980, reward mean : 0.0937400090135634, total_step : 966123, cur_epsilon : 0.02\n",
      "episode : 23990, reward mean : 0.09269000905938446, total_step : 966648, cur_epsilon : 0.02\n",
      "episode : 24000, reward mean : 0.07933000904507935, total_step : 966970, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24000, reward mean : 0.09400001130998134, step mean : 51.2, eval_episode_rewards : [ 0.93  0.63 -1.   -1.    0.89  0.93  0.61 -1.   -1.    0.93 -1.    0.94\n",
      " -1.    0.96  0.98 -1.    0.47  0.98  0.63 -1.  ]\n",
      "episode : 24010, reward mean : 0.08337000904418528, total_step : 967353, cur_epsilon : 0.02\n",
      "episode : 24020, reward mean : 0.07687000910006464, total_step : 967997, cur_epsilon : 0.02\n",
      "episode : 24030, reward mean : 0.07828000911325217, total_step : 968565, cur_epsilon : 0.02\n",
      "episode : 24040, reward mean : 0.07818000909313559, total_step : 968888, cur_epsilon : 0.02\n",
      "episode : 24050, reward mean : 0.07902000911906362, total_step : 969376, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24050, reward mean : 0.12100001070648432, step mean : 48.6, eval_episode_rewards : [ 0.94  0.88  0.86  0.1   0.4   0.81 -1.    1.   -1.   -1.   -1.   -1.\n",
      "  0.79  0.85 -1.26  0.92 -1.    0.93  0.82  0.38]\n",
      "episode : 24060, reward mean : 0.08839000906608999, total_step : 969562, cur_epsilon : 0.02\n",
      "episode : 24070, reward mean : 0.09198000907525421, total_step : 969970, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 24080, reward mean : 0.09057000910677016, total_step : 970394, cur_epsilon : 0.02\n",
      "episode : 24090, reward mean : 0.08510000909492374, total_step : 970679, cur_epsilon : 0.02\n",
      "episode : 24100, reward mean : 0.08441000904329121, total_step : 971041, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24100, reward mean : 0.11900001186877489, step mean : 53.75, eval_episode_rewards : [-1.   -1.    0.45  0.97  0.45  0.98  0.96  0.7  -1.    0.99 -1.    0.98\n",
      " -1.   -1.    0.82  0.88  0.06 -1.    0.15  0.99]\n",
      "episode : 24110, reward mean : 0.08729000906832517, total_step : 971506, cur_epsilon : 0.02\n",
      "episode : 24120, reward mean : 0.08965000908263028, total_step : 971850, cur_epsilon : 0.02\n",
      "episode : 24130, reward mean : 0.08794000909850001, total_step : 972276, cur_epsilon : 0.02\n",
      "episode : 24140, reward mean : 0.09351000908575952, total_step : 972635, cur_epsilon : 0.02\n",
      "episode : 24150, reward mean : 0.09448000913113355, total_step : 973131, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24150, reward mean : -0.16549998624250292, step mean : 62.0, eval_episode_rewards : [-1.    0.61 -1.   -1.   -1.    0.86 -1.    0.6   0.9   0.88 -1.    0.91\n",
      " -1.   -1.    0.98  0.98 -1.   -1.   -1.    0.97]\n",
      "episode : 24160, reward mean : 0.1003400091342628, total_step : 973487, cur_epsilon : 0.02\n",
      "episode : 24170, reward mean : 0.10033000913448632, total_step : 973860, cur_epsilon : 0.02\n",
      "episode : 24180, reward mean : 0.10190000912174582, total_step : 974350, cur_epsilon : 0.02\n",
      "episode : 24190, reward mean : 0.09164000910520553, total_step : 974732, cur_epsilon : 0.02\n",
      "episode : 24200, reward mean : 0.09701000902988016, total_step : 974936, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24200, reward mean : -0.37699998263269663, step mean : 78.1, eval_episode_rewards : [-1.    0.97  0.37  0.8  -1.   -1.   -1.   -1.   -1.    0.42  0.38  0.4\n",
      " -1.   -1.    0.97 -1.   -1.    0.15 -1.   -1.  ]\n",
      "synced target net\n",
      "episode : 24210, reward mean : 0.09650000904127956, total_step : 975221, cur_epsilon : 0.02\n",
      "episode : 24220, reward mean : 0.09155000904016197, total_step : 975609, cur_epsilon : 0.02\n",
      "episode : 24230, reward mean : 0.09209000902809203, total_step : 975955, cur_epsilon : 0.02\n",
      "episode : 24240, reward mean : 0.08763000906072557, total_step : 976328, cur_epsilon : 0.02\n",
      "episode : 24250, reward mean : 0.08642000906541944, total_step : 976682, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24250, reward mean : -0.027499987091869117, step mean : 58.3, eval_episode_rewards : [-1.    0.59  0.97 -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.36  0.88\n",
      "  0.95  0.55  0.88  0.92  0.75  0.71  0.89 -1.  ]\n",
      "episode : 24260, reward mean : 0.082520009085536, total_step : 977314, cur_epsilon : 0.02\n",
      "episode : 24270, reward mean : 0.07796000907570123, total_step : 977562, cur_epsilon : 0.02\n",
      "episode : 24280, reward mean : 0.08062000903859734, total_step : 977899, cur_epsilon : 0.02\n",
      "episode : 24290, reward mean : 0.08516000904887915, total_step : 978263, cur_epsilon : 0.02\n",
      "episode : 24300, reward mean : 0.08112000902742147, total_step : 978601, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24300, reward mean : 0.30850000875070693, step mean : 39.85, eval_episode_rewards : [ 0.95 -1.    0.87  0.99  0.86  0.97 -1.    0.84  0.83  0.99 -1.    0.82\n",
      " -1.    0.85  0.57  0.95 -1.    0.78  0.9  -1.  ]\n",
      "episode : 24310, reward mean : 0.08793000903166831, total_step : 978969, cur_epsilon : 0.02\n",
      "episode : 24320, reward mean : 0.08237000906653702, total_step : 979448, cur_epsilon : 0.02\n",
      "episode : 24330, reward mean : 0.08714000907167792, total_step : 979937, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 24340, reward mean : 0.08779000907950103, total_step : 980287, cur_epsilon : 0.02\n",
      "episode : 24350, reward mean : 0.08636000911146402, total_step : 980857, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24350, reward mean : -0.05299998763948679, step mean : 55.8, eval_episode_rewards : [ 0.97  0.98  0.93  1.   -1.    0.96 -1.    0.97 -1.    0.99  0.58 -1.\n",
      " -1.    0.95 -1.    0.61 -1.   -1.   -1.   -1.  ]\n",
      "episode : 24360, reward mean : 0.08656000908464194, total_step : 981297, cur_epsilon : 0.02\n",
      "episode : 24370, reward mean : 0.08436000908911229, total_step : 981790, cur_epsilon : 0.02\n",
      "episode : 24380, reward mean : 0.08394000916555523, total_step : 982404, cur_epsilon : 0.02\n",
      "episode : 24390, reward mean : 0.08676000914722681, total_step : 982713, cur_epsilon : 0.02\n",
      "episode : 24400, reward mean : 0.08647000913135708, total_step : 983192, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24400, reward mean : -0.3579999841749668, step mean : 71.15, eval_episode_rewards : [ 0.92 -1.    0.94 -1.   -1.   -1.   -1.   -1.   -1.    0.93 -1.    0.94\n",
      " -1.    0.86 -1.    0.31  0.94 -1.   -1.   -1.  ]\n",
      "episode : 24410, reward mean : 0.08614000913873315, total_step : 983540, cur_epsilon : 0.02\n",
      "episode : 24420, reward mean : 0.08683000914566219, total_step : 983953, cur_epsilon : 0.02\n",
      "episode : 24430, reward mean : 0.08505000916309655, total_step : 984490, cur_epsilon : 0.02\n",
      "episode : 24440, reward mean : 0.0834200091548264, total_step : 984945, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 24450, reward mean : 0.08644000915437937, total_step : 985398, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24450, reward mean : -0.11599998623132705, step mean : 62.1, eval_episode_rewards : [-1.    0.56  0.72  0.97  0.61  0.94 -1.    0.76 -1.    0.92 -1.   -1.\n",
      " -1.   -1.    0.97  0.37  0.86 -1.   -1.   -1.  ]\n",
      "episode : 24460, reward mean : 0.081080009162426, total_step : 985913, cur_epsilon : 0.02\n",
      "episode : 24470, reward mean : 0.07736000922322274, total_step : 986517, cur_epsilon : 0.02\n",
      "episode : 24480, reward mean : 0.0812200091816485, total_step : 986764, cur_epsilon : 0.02\n",
      "episode : 24490, reward mean : 0.07797000918723643, total_step : 987286, cur_epsilon : 0.02\n",
      "episode : 24500, reward mean : 0.07668000919371844, total_step : 987773, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24500, reward mean : -0.0804999870248139, step mean : 58.55, eval_episode_rewards : [ 0.9   0.86 -1.    0.95 -1.   -1.    0.95 -1.   -1.    0.95 -1.    1.\n",
      " -1.    0.7   0.2   0.95 -1.    0.93 -1.   -1.  ]\n",
      "episode : 24510, reward mean : 0.0835000090636313, total_step : 987916, cur_epsilon : 0.02\n",
      "episode : 24520, reward mean : 0.08032000906765462, total_step : 988366, cur_epsilon : 0.02\n",
      "episode : 24530, reward mean : 0.07729000904597343, total_step : 988633, cur_epsilon : 0.02\n",
      "episode : 24540, reward mean : 0.07339000906608999, total_step : 989098, cur_epsilon : 0.02\n",
      "episode : 24550, reward mean : 0.06797000905312597, total_step : 989341, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24550, reward mean : 0.20250001111999155, step mean : 50.45, eval_episode_rewards : [ 0.78  0.97  0.89  0.17  0.94 -1.    0.88 -1.   -1.   -1.    0.93  0.8\n",
      " -1.   -1.    0.91  0.57  0.18  0.44  0.97  0.62]\n",
      "episode : 24560, reward mean : 0.06801000902988016, total_step : 989707, cur_epsilon : 0.02\n",
      "episode : 24570, reward mean : 0.07260000892728567, total_step : 989996, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 24580, reward mean : 0.06727000895701349, total_step : 990476, cur_epsilon : 0.02\n",
      "episode : 24590, reward mean : 0.07173000896908342, total_step : 990809, cur_epsilon : 0.02\n",
      "episode : 24600, reward mean : 0.07395000891946256, total_step : 991051, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24600, reward mean : 0.19750001123175026, step mean : 50.95, eval_episode_rewards : [ 0.98  0.16  0.87 -1.   -1.    0.45  0.94  0.88  0.86  0.21  0.92  0.87\n",
      "  0.96 -1.   -1.   -1.    0.99  0.06 -1.    0.8 ]\n",
      "episode : 24610, reward mean : 0.06849000892974436, total_step : 991417, cur_epsilon : 0.02\n",
      "episode : 24620, reward mean : 0.07163000890426338, total_step : 991891, cur_epsilon : 0.02\n",
      "episode : 24630, reward mean : 0.065550008950755, total_step : 992444, cur_epsilon : 0.02\n",
      "episode : 24640, reward mean : 0.06866000892594457, total_step : 992772, cur_epsilon : 0.02\n",
      "episode : 24650, reward mean : 0.06917000891454518, total_step : 993154, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24650, reward mean : -0.017499987315386534, step mean : 57.3, eval_episode_rewards : [-1.    0.82  0.02  0.94 -1.    0.86  0.81 -1.    0.78 -1.   -1.   -1.\n",
      " -1.    0.7   0.93  0.97  0.95 -1.    0.87 -1.  ]\n",
      "episode : 24660, reward mean : 0.07211000893823802, total_step : 993656, cur_epsilon : 0.02\n",
      "episode : 24670, reward mean : 0.07149000890739263, total_step : 994041, cur_epsilon : 0.02\n",
      "episode : 24680, reward mean : 0.07693000883050263, total_step : 994147, cur_epsilon : 0.02\n",
      "episode : 24690, reward mean : 0.07751000879518688, total_step : 994315, cur_epsilon : 0.02\n",
      "episode : 24700, reward mean : 0.07463000883720815, total_step : 994852, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24700, reward mean : -0.04699998665601015, step mean : 60.25, eval_episode_rewards : [ 1.    0.99  0.84  0.09  0.97 -1.   -1.    0.82  0.99 -1.   -1.    0.04\n",
      "  0.91 -1.   -1.   -1.   -1.    0.48 -1.    0.93]\n",
      "synced target net\n",
      "episode : 24710, reward mean : 0.08193000874109566, total_step : 995084, cur_epsilon : 0.02\n",
      "episode : 24720, reward mean : 0.07890000880882143, total_step : 995747, cur_epsilon : 0.02\n",
      "episode : 24730, reward mean : 0.07574000879004597, total_step : 996085, cur_epsilon : 0.02\n",
      "episode : 24740, reward mean : 0.07293000878579915, total_step : 996418, cur_epsilon : 0.02\n",
      "episode : 24750, reward mean : 0.07311000878177583, total_step : 996745, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24750, reward mean : -0.2999999843537807, step mean : 70.4, eval_episode_rewards : [ 0.76 -1.   -1.   -1.   -1.    0.36  0.93 -1.   -1.    0.77 -1.    0.7\n",
      "  0.95 -1.    0.58 -1.    0.95 -1.   -1.   -1.  ]\n",
      "episode : 24760, reward mean : 0.06782000883296133, total_step : 997275, cur_epsilon : 0.02\n",
      "episode : 24770, reward mean : 0.07520000880211591, total_step : 997547, cur_epsilon : 0.02\n",
      "episode : 24780, reward mean : 0.07261000881530344, total_step : 997861, cur_epsilon : 0.02\n",
      "episode : 24790, reward mean : 0.0780400088056922, total_step : 998246, cur_epsilon : 0.02\n",
      "episode : 24800, reward mean : 0.07599000880680978, total_step : 998580, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24800, reward mean : 0.0260000117123127, step mean : 52.95, eval_episode_rewards : [ 0.97  0.82  0.39 -1.    0.92  0.91  0.98 -1.   -1.    0.91  1.   -1.\n",
      " -1.   -1.   -1.    0.77 -1.    0.94  0.91 -1.  ]\n",
      "episode : 24810, reward mean : 0.07108000882714986, total_step : 998937, cur_epsilon : 0.02\n",
      "episode : 24820, reward mean : 0.07249000884033739, total_step : 999357, cur_epsilon : 0.02\n",
      "episode : 24830, reward mean : 0.07181000885553658, total_step : 999886, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 24840, reward mean : 0.0667800089456141, total_step : 1000441, cur_epsilon : 0.02\n",
      "episode : 24850, reward mean : 0.07604000889509917, total_step : 1000772, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24850, reward mean : -0.3834999836049974, step mean : 73.7, eval_episode_rewards : [-1.    0.88  0.98 -1.   -1.   -1.   -1.   -1.    0.84 -1.   -1.    0.29\n",
      "  0.95 -1.   -1.   -1.    0.93 -1.   -1.    0.46]\n",
      "episode : 24860, reward mean : 0.07744000888615847, total_step : 1001103, cur_epsilon : 0.02\n",
      "episode : 24870, reward mean : 0.08243000877462328, total_step : 1001337, cur_epsilon : 0.02\n",
      "episode : 24880, reward mean : 0.08162000881507993, total_step : 1002023, cur_epsilon : 0.02\n",
      "episode : 24890, reward mean : 0.08007000880502164, total_step : 1002574, cur_epsilon : 0.02\n",
      "episode : 24900, reward mean : 0.07682000878825784, total_step : 1002989, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24900, reward mean : 0.12150001069530844, step mean : 48.45, eval_episode_rewards : [ 0.64 -1.    0.99 -1.    0.35  0.9   0.88 -1.    0.93 -1.    0.96 -1.\n",
      "  0.93  0.89  0.96 -1.   -1.   -1.    1.    1.  ]\n",
      "episode : 24910, reward mean : 0.07621000877954066, total_step : 1003297, cur_epsilon : 0.02\n",
      "episode : 24920, reward mean : 0.07311000882647932, total_step : 1003849, cur_epsilon : 0.02\n",
      "episode : 24930, reward mean : 0.07558000883832573, total_step : 1004380, cur_epsilon : 0.02\n",
      "episode : 24940, reward mean : 0.07900000880658627, total_step : 1004649, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 24950, reward mean : 0.07832000882178546, total_step : 1005041, cur_epsilon : 0.02\n",
      "[EVAL] episode : 24950, reward mean : 0.13400001153349878, step mean : 52.25, eval_episode_rewards : [-1.    0.32  0.95  0.98 -1.   -1.   -1.   -1.    0.94  0.19  0.97 -1.\n",
      "  0.96  0.83  0.7   0.98  0.87  0.77  0.22 -1.  ]\n",
      "episode : 24960, reward mean : 0.07855000881664455, total_step : 1005490, cur_epsilon : 0.02\n",
      "episode : 24970, reward mean : 0.08132000882178544, total_step : 1005860, cur_epsilon : 0.02\n",
      "episode : 24980, reward mean : 0.08633000877685845, total_step : 1006263, cur_epsilon : 0.02\n",
      "episode : 24990, reward mean : 0.08483000878803432, total_step : 1006835, cur_epsilon : 0.02\n",
      "episode : 25000, reward mean : 0.09142000884190202, total_step : 1007397, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25000, reward mean : 0.31250000866129996, step mean : 39.45, eval_episode_rewards : [ 0.86 -1.    0.87  0.9   0.99  0.91  0.89  0.73  0.97 -1.    0.75  0.62\n",
      "  0.98 -1.    0.83 -1.   -1.    0.97 -1.    0.98]\n",
      "episode : 25010, reward mean : 0.08463000883720816, total_step : 1007760, cur_epsilon : 0.02\n",
      "episode : 25020, reward mean : 0.09029000875540077, total_step : 1008042, cur_epsilon : 0.02\n",
      "episode : 25030, reward mean : 0.08840000877529383, total_step : 1008696, cur_epsilon : 0.02\n",
      "episode : 25040, reward mean : 0.08471000881306827, total_step : 1009188, cur_epsilon : 0.02\n",
      "episode : 25050, reward mean : 0.085590008771047, total_step : 1009489, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25050, reward mean : 0.3335000081919134, step mean : 37.35, eval_episode_rewards : [ 0.27  0.96  0.99  0.95  0.96  1.   -1.   -1.   -1.    0.88  0.94  0.86\n",
      "  0.95 -1.    0.99  0.98  0.98  0.96 -1.   -1.  ]\n",
      "episode : 25060, reward mean : 0.0814100087750703, total_step : 1009693, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 25070, reward mean : 0.07110000882670284, total_step : 1010330, cur_epsilon : 0.02\n",
      "episode : 25080, reward mean : 0.0716400088146329, total_step : 1010700, cur_epsilon : 0.02\n",
      "episode : 25090, reward mean : 0.08086000880971551, total_step : 1010964, cur_epsilon : 0.02\n",
      "episode : 25100, reward mean : 0.08196000880748033, total_step : 1011317, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25100, reward mean : 0.16100001093000174, step mean : 49.55, eval_episode_rewards : [-1.    0.97  0.44  1.    0.9  -1.   -1.    0.97 -1.    1.    0.94  0.98\n",
      "  0.45 -1.    0.45 -1.    0.16  0.96 -1.    1.  ]\n",
      "episode : 25110, reward mean : 0.07780000876635312, total_step : 1011598, cur_epsilon : 0.02\n",
      "episode : 25120, reward mean : 0.07140000879764558, total_step : 1012081, cur_epsilon : 0.02\n",
      "episode : 25130, reward mean : 0.06486000874266028, total_step : 1012262, cur_epsilon : 0.02\n",
      "episode : 25140, reward mean : 0.06548000875115395, total_step : 1012660, cur_epsilon : 0.02\n",
      "episode : 25150, reward mean : 0.061030008783563974, total_step : 1013298, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25150, reward mean : 0.08600001148879528, step mean : 52.0, eval_episode_rewards : [ 0.8  -1.    0.13 -1.    0.97  0.99 -1.    0.94 -1.   -1.    0.74 -1.\n",
      " -1.    0.76  1.    0.56  0.99 -1.    0.87  0.97]\n",
      "episode : 25160, reward mean : 0.056310008799657224, total_step : 1013726, cur_epsilon : 0.02\n",
      "episode : 25170, reward mean : 0.05806000878289342, total_step : 1014023, cur_epsilon : 0.02\n",
      "episode : 25180, reward mean : 0.05928000877797603, total_step : 1014492, cur_epsilon : 0.02\n",
      "episode : 25190, reward mean : 0.07071000874601305, total_step : 1014733, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 25200, reward mean : 0.06997000876255334, total_step : 1015011, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25200, reward mean : -0.22599998600780963, step mean : 63.0, eval_episode_rewards : [-1.   -1.    0.93 -1.   -1.    0.96 -1.   -1.   -1.   -1.   -1.    0.94\n",
      "  1.    0.93 -1.   -1.    0.96  0.91  0.85 -1.  ]\n",
      "episode : 25210, reward mean : 0.06774000883474945, total_step : 1015616, cur_epsilon : 0.02\n",
      "episode : 25220, reward mean : 0.07376000883430242, total_step : 1016002, cur_epsilon : 0.02\n",
      "episode : 25230, reward mean : 0.06737000886537135, total_step : 1016486, cur_epsilon : 0.02\n",
      "episode : 25240, reward mean : 0.07129000888951123, total_step : 1016968, cur_epsilon : 0.02\n",
      "episode : 25250, reward mean : 0.07524000889062882, total_step : 1017327, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25250, reward mean : 0.3460000090301037, step mean : 41.15, eval_episode_rewards : [ 0.85 -1.    0.81  0.36 -1.    0.89  0.51  0.72  0.83 -1.    0.51  0.95\n",
      "  0.91  0.98 -1.    0.95  0.96 -1.    0.83  0.86]\n",
      "episode : 25260, reward mean : 0.0830800088495016, total_step : 1017777, cur_epsilon : 0.02\n",
      "episode : 25270, reward mean : 0.0829400088749826, total_step : 1018138, cur_epsilon : 0.02\n",
      "episode : 25280, reward mean : 0.0807100088801235, total_step : 1018498, cur_epsilon : 0.02\n",
      "episode : 25290, reward mean : 0.07655000892840326, total_step : 1019076, cur_epsilon : 0.02\n",
      "episode : 25300, reward mean : 0.08513000896014274, total_step : 1019556, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25300, reward mean : 0.11150001091882586, step mean : 49.45, eval_episode_rewards : [ 0.95  0.95  0.78 -1.    0.98 -1.    0.52 -1.   -1.    0.79  1.   -1.\n",
      " -1.    0.98  0.9   0.94  0.46 -1.   -1.    0.98]\n",
      "episode : 25310, reward mean : 0.08305000896193088, total_step : 1019930, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 25320, reward mean : 0.08618000895902514, total_step : 1020397, cur_epsilon : 0.02\n",
      "episode : 25330, reward mean : 0.08385000896640121, total_step : 1020919, cur_epsilon : 0.02\n",
      "episode : 25340, reward mean : 0.08434000895544887, total_step : 1021220, cur_epsilon : 0.02\n",
      "episode : 25350, reward mean : 0.08745000890828669, total_step : 1021582, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25350, reward mean : 0.04800001122057438, step mean : 50.75, eval_episode_rewards : [-1.    0.87  0.79  0.98  0.85  0.98  0.92 -1.    0.82  0.99 -1.   -1.\n",
      "  0.84  0.96 -1.   -1.    0.96 -1.   -1.   -1.  ]\n",
      "episode : 25360, reward mean : 0.0869600088968873, total_step : 1021970, cur_epsilon : 0.02\n",
      "episode : 25370, reward mean : 0.09008000887185336, total_step : 1022353, cur_epsilon : 0.02\n",
      "episode : 25380, reward mean : 0.09052000883966685, total_step : 1022822, cur_epsilon : 0.02\n",
      "episode : 25390, reward mean : 0.08429000886715948, total_step : 1023253, cur_epsilon : 0.02\n",
      "episode : 25400, reward mean : 0.08718000884726644, total_step : 1023643, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25400, reward mean : -0.20149998543784023, step mean : 65.6, eval_episode_rewards : [-1.    0.56  0.95 -1.    0.87 -1.   -1.   -1.    0.67 -1.    0.96  0.87\n",
      " -1.    0.95 -1.   -1.   -1.   -1.    0.84  0.3 ]\n",
      "episode : 25410, reward mean : 0.0806900088582188, total_step : 1024040, cur_epsilon : 0.02\n",
      "episode : 25420, reward mean : 0.08401000882871448, total_step : 1024323, cur_epsilon : 0.02\n",
      "episode : 25430, reward mean : 0.08157000879384577, total_step : 1024704, cur_epsilon : 0.02\n",
      "episode : 25440, reward mean : 0.08486000874266028, total_step : 1024931, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 25450, reward mean : 0.08693000871874391, total_step : 1025278, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25450, reward mean : -0.13799998573958874, step mean : 64.3, eval_episode_rewards : [ 0.29 -1.    0.91  0.96 -1.   -1.   -1.    0.92 -1.    0.94 -1.    0.02\n",
      " -1.    1.    0.85 -1.    0.36 -1.    0.99 -1.  ]\n",
      "episode : 25460, reward mean : 0.0885700087044388, total_step : 1025730, cur_epsilon : 0.02\n",
      "episode : 25470, reward mean : 0.08786000867560506, total_step : 1026205, cur_epsilon : 0.02\n",
      "episode : 25480, reward mean : 0.0837700087223202, total_step : 1026661, cur_epsilon : 0.02\n",
      "episode : 25490, reward mean : 0.08727000871114433, total_step : 1027134, cur_epsilon : 0.02\n",
      "episode : 25500, reward mean : 0.08984000867605209, total_step : 1027465, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25500, reward mean : -0.1334999858401716, step mean : 63.85, eval_episode_rewards : [-1.    0.04  0.75  0.98  0.82  0.34  0.92 -1.   -1.   -1.   -1.   -1.\n",
      "  0.97 -1.    0.95 -1.    0.62  0.94 -1.   -1.  ]\n",
      "episode : 25510, reward mean : 0.09031000871025026, total_step : 1027761, cur_epsilon : 0.02\n",
      "episode : 25520, reward mean : 0.09717000869102776, total_step : 1028127, cur_epsilon : 0.02\n",
      "episode : 25530, reward mean : 0.09872000872343778, total_step : 1028538, cur_epsilon : 0.02\n",
      "episode : 25540, reward mean : 0.10299000871740281, total_step : 1028978, cur_epsilon : 0.02\n",
      "episode : 25550, reward mean : 0.10095000876300037, total_step : 1029425, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25550, reward mean : -0.11599998623132705, step mean : 62.1, eval_episode_rewards : [-1.    0.95 -1.    0.34 -1.   -1.   -1.   -1.   -1.    0.18  0.93  0.94\n",
      "  0.97  0.47  0.99  0.96 -1.   -1.   -1.    0.95]\n",
      "episode : 25560, reward mean : 0.09891000876389444, total_step : 1029795, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 25570, reward mean : 0.09732000882178544, total_step : 1030342, cur_epsilon : 0.02\n",
      "episode : 25580, reward mean : 0.0996500087697059, total_step : 1030593, cur_epsilon : 0.02\n",
      "episode : 25590, reward mean : 0.09462000879272818, total_step : 1031029, cur_epsilon : 0.02\n",
      "episode : 25600, reward mean : 0.08693000885285437, total_step : 1031539, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25600, reward mean : 0.07000001184642315, step mean : 53.6, eval_episode_rewards : [-1.   -1.    0.85  0.81  0.83  0.89  0.45 -1.    0.87 -1.    0.96 -1.\n",
      "  0.95  0.72 -1.    0.35 -1.   -1.    0.76  0.96]\n",
      "episode : 25610, reward mean : 0.09094000883027911, total_step : 1031805, cur_epsilon : 0.02\n",
      "episode : 25620, reward mean : 0.08490000885352492, total_step : 1032382, cur_epsilon : 0.02\n",
      "episode : 25630, reward mean : 0.09352000877261162, total_step : 1032576, cur_epsilon : 0.02\n",
      "episode : 25640, reward mean : 0.09439000877551734, total_step : 1032916, cur_epsilon : 0.02\n",
      "episode : 25650, reward mean : 0.09417000875808298, total_step : 1033221, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25650, reward mean : 0.10150001114234328, step mean : 50.45, eval_episode_rewards : [ 0.93  0.94  1.   -1.    0.93  0.98  0.92 -1.   -1.    0.98 -1.   -1.\n",
      " -1.   -1.    0.26  1.   -1.    0.94  0.19  0.96]\n",
      "episode : 25660, reward mean : 0.09219000873528421, total_step : 1033622, cur_epsilon : 0.02\n",
      "episode : 25670, reward mean : 0.098310008732602, total_step : 1033995, cur_epsilon : 0.02\n",
      "episode : 25680, reward mean : 0.09579000881128014, total_step : 1034452, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 25690, reward mean : 0.09181000890024006, total_step : 1035016, cur_epsilon : 0.02\n",
      "episode : 25700, reward mean : 0.09225000886805355, total_step : 1035410, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25700, reward mean : 0.4395000080578029, step mean : 36.85, eval_episode_rewards : [-1.    0.82  0.95  0.21  0.92  0.83  0.85  0.91  0.67  0.96  0.45  0.98\n",
      "  0.96  0.8  -1.    0.94  0.61 -1.    0.93 -1.  ]\n",
      "episode : 25710, reward mean : 0.09282000887766481, total_step : 1035684, cur_epsilon : 0.02\n",
      "episode : 25720, reward mean : 0.09514000882580877, total_step : 1036117, cur_epsilon : 0.02\n",
      "episode : 25730, reward mean : 0.09960000881552697, total_step : 1036409, cur_epsilon : 0.02\n",
      "episode : 25740, reward mean : 0.0971200088262558, total_step : 1036790, cur_epsilon : 0.02\n",
      "episode : 25750, reward mean : 0.09579000885598361, total_step : 1037250, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25750, reward mean : 0.2975000089965761, step mean : 40.95, eval_episode_rewards : [ 0.92  0.94  0.31  0.98  0.86 -1.    0.89 -1.    0.73  0.98  0.92  0.91\n",
      " -1.    0.93 -1.    0.77  0.85 -1.    0.96 -1.  ]\n",
      "episode : 25760, reward mean : 0.0980100088287145, total_step : 1037657, cur_epsilon : 0.02\n",
      "episode : 25770, reward mean : 0.0912500088904053, total_step : 1038203, cur_epsilon : 0.02\n",
      "episode : 25780, reward mean : 0.08849000888504088, total_step : 1038494, cur_epsilon : 0.02\n",
      "episode : 25790, reward mean : 0.0852500088904053, total_step : 1038902, cur_epsilon : 0.02\n",
      "episode : 25800, reward mean : 0.09308000889420509, total_step : 1039253, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25800, reward mean : -0.09049998568370939, step mean : 64.6, eval_episode_rewards : [ 0.96 -1.    0.9   0.64 -1.    0.09  0.76  0.93 -1.   -1.    0.39 -1.\n",
      "  0.96  0.17  0.68 -1.   -1.   -1.   -1.    0.71]\n",
      "episode : 25810, reward mean : 0.09685000894404948, total_step : 1039831, cur_epsilon : 0.02\n",
      "synced target net\n",
      "episode : 25820, reward mean : 0.0945200089290738, total_step : 1040183, cur_epsilon : 0.02\n",
      "episode : 25830, reward mean : 0.09108000893890858, total_step : 1040757, cur_epsilon : 0.02\n",
      "episode : 25840, reward mean : 0.0886700089033693, total_step : 1041155, cur_epsilon : 0.02\n",
      "episode : 25850, reward mean : 0.08316000895947218, total_step : 1041734, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25850, reward mean : -0.19849998550489545, step mean : 65.3, eval_episode_rewards : [ 0.85 -1.   -1.    0.93 -1.    0.52  0.83 -1.   -1.    0.78 -1.   -1.\n",
      "  0.8  -1.    0.4   0.95 -1.   -1.   -1.    0.97]\n",
      "episode : 25860, reward mean : 0.0796900089699775, total_step : 1042111, cur_epsilon : 0.02\n",
      "episode : 25870, reward mean : 0.07640000899881125, total_step : 1042472, cur_epsilon : 0.02\n",
      "episode : 25880, reward mean : 0.07548000895231963, total_step : 1042951, cur_epsilon : 0.02\n",
      "episode : 25890, reward mean : 0.07405000893957912, total_step : 1043447, cur_epsilon : 0.02\n",
      "episode : 25900, reward mean : 0.07139000897668302, total_step : 1044027, cur_epsilon : 0.02\n",
      "[EVAL] episode : 25900, reward mean : -0.1254999849013984, step mean : 68.1, eval_episode_rewards : [-1.    0.59 -1.    0.7   0.21 -1.    0.93 -1.    0.1   0.98  0.94  0.69\n",
      " -1.   -1.    0.1   0.25 -1.    1.   -1.   -1.  ]\n",
      "episode : 25910, reward mean : 0.0676300089713186, total_step : 1044311, cur_epsilon : 0.02\n",
      "episode : 25920, reward mean : 0.07023000893555581, total_step : 1044704, cur_epsilon : 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m         done_reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilistic_decision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done_reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m             episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 60\u001b[0m, in \u001b[0;36mAgent.play_step\u001b[0;34m(self, epsilon, sync_target, probabilistic_decision)\u001b[0m\n\u001b[1;32m     57\u001b[0m         _, act_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(q_vals_v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_v\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 60\u001b[0m next_state, reward, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_buffer\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state, action, reward, is_done, next_state\n\u001b[1;32m     65\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgResizeGrayWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 11\u001b[0m         next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         next_state \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(next_state[:, :, \u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         return next_state, reward, is_done, info\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(next_state[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), reward, is_done, info\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/envs/unity_gym_env.py:200\u001b[0m, in \u001b[0;36mUnityToGymWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    197\u001b[0m     action_tuple\u001b[38;5;241m.\u001b[39madd_discrete(action)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mset_actions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, action_tuple)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m decision_step, terminal_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_steps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_agents(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(decision_step), \u001b[38;5;28mlen\u001b[39m(terminal_step)))\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/environment.py:353\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_behavior_specs(outputs)\n\u001b[1;32m    352\u001b[0m rl_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mrl_output\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrl_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/environment.py:306\u001b[0m, in \u001b[0;36mUnityEnvironment._update_state\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m brain_name \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39magentInfos:\n\u001b[1;32m    305\u001b[0m     agent_info_list \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39magentInfos[brain_name]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_state[brain_name] \u001b[38;5;241m=\u001b[39m \u001b[43msteps_from_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_info_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_specs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbrain_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_state[brain_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    311\u001b[0m         DecisionSteps\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_specs[brain_name]),\n\u001b[1;32m    312\u001b[0m         TerminalSteps\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_specs[brain_name]),\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_utils.py:366\u001b[0m, in \u001b[0;36msteps_from_proto\u001b[0;34m(agent_info_list, behavior_spec)\u001b[0m\n\u001b[1;32m    357\u001b[0m decision_group_rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    358\u001b[0m     [agent_info\u001b[38;5;241m.\u001b[39mgroup_reward \u001b[38;5;28;01mfor\u001b[39;00m agent_info \u001b[38;5;129;01min\u001b[39;00m decision_agent_info_list],\n\u001b[1;32m    359\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m    360\u001b[0m )\n\u001b[1;32m    361\u001b[0m terminal_group_rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    362\u001b[0m     [agent_info\u001b[38;5;241m.\u001b[39mgroup_reward \u001b[38;5;28;01mfor\u001b[39;00m agent_info \u001b[38;5;129;01min\u001b[39;00m terminal_agent_info_list],\n\u001b[1;32m    363\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[43m_raise_on_nan_and_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m _raise_on_nan_and_inf(terminal_rewards, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    368\u001b[0m _raise_on_nan_and_inf(decision_group_rewards, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_utils.py:279\u001b[0m, in \u001b[0;36m_raise_on_nan_and_inf\u001b[0;34m(data, source)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m--> 279\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m has_nan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(d)\n\u001b[1;32m    281\u001b[0m has_inf \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(d)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3438\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/numpy/core/_methods.py:189\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret \u001b[38;5;241m/\u001b[39m rcount)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m         ret \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(\u001b[43mret\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrcount\u001b[49m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ret \u001b[38;5;241m/\u001b[39m rcount\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# probabilistic decision ver\n",
    "# exp_buffer = NStepPriorityReplayBuffer(\n",
    "#     max_size=150000,\n",
    "#     prob_alpha=0.6,\n",
    "#     beta_start=0.4,\n",
    "#     beta_frames=200000,\n",
    "#     n_step=4,\n",
    "#     gamma=0.99,\n",
    "# )\n",
    "# agent = Agent(\n",
    "#     env=env,\n",
    "#     exp_buffer=exp_buffer,\n",
    "#     net=net,\n",
    "#     epsilon_start=0.9,\n",
    "#     epsilon_final=0.02,\n",
    "#     epsilon_decay_last_step=100000,\n",
    "#     tgt_sync_steps=5000,\n",
    "#     learning_rate=1e-4,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "episode = 0\n",
    "reward_history = []\n",
    "spisode_steps_history = []\n",
    "spisode_start_steps = 0\n",
    "PRINT_EPISODE_INTERVAL = 10\n",
    "print_episode_rewards = []\n",
    "EVAL_EPISODE_INTERVAL = 50\n",
    "N_EVAL_EPISODES = 20\n",
    "\n",
    "while True:\n",
    "\n",
    "    for stp in range(20):\n",
    "        done_reward = agent.play_step(probabilistic_decision=True)\n",
    "        if done_reward is not None:\n",
    "            episode += 1\n",
    "            reward_history.append(done_reward)\n",
    "            # print_episode_rewards.append(done_reward)\n",
    "            spisode_steps_history.append(agent._total_step-spisode_start_steps)\n",
    "            spisode_start_steps = agent._total_step\n",
    "            if episode % PRINT_EPISODE_INTERVAL == 0:\n",
    "                # rm = sum(print_episode_rewards) / len(print_episode_rewards)\n",
    "                rm = sum(reward_history[-1000:]) / len(reward_history[-1000:])\n",
    "                print(f'episode : {episode}, reward mean : {rm}, total_step : {agent._total_step}, cur_epsilon : {agent._epsilon}')\n",
    "            if episode % EVAL_EPISODE_INTERVAL == 0:\n",
    "                eval_episode_rewards = []\n",
    "                eval_episode_steps = []\n",
    "                for i in range(N_EVAL_EPISODES):\n",
    "                    reward, steps = agent.simulate_episode()\n",
    "                    eval_episode_rewards.append(reward)\n",
    "                    eval_episode_steps.append(steps)\n",
    "                mr = sum(eval_episode_rewards) / len(eval_episode_rewards)\n",
    "                ms = sum(eval_episode_steps) / len(eval_episode_steps)\n",
    "                print(f'[EVAL] episode : {episode}, reward mean : {mr}, step mean : {ms}, eval_episode_rewards : {np.round(eval_episode_rewards, 3)}')\n",
    "\n",
    "    agent.train(n_iter=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f1cdd82-c09a-47ff-84e4-4ea59ba805ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 10, reward mean : 0.02800000607967377, total_step : 278, cur_epsilon : 0.89861\n",
      "episode : 20, reward mean : -0.2959999945014715, total_step : 508, cur_epsilon : 0.89746\n",
      "episode : 30, reward mean : -0.341333327939113, total_step : 750, cur_epsilon : 0.89625\n",
      "episode : 40, reward mean : -0.4169999945908785, total_step : 1004, cur_epsilon : 0.89498\n",
      "episode : 50, reward mean : -0.42519999362528327, total_step : 1470, cur_epsilon : 0.89265\n",
      "[EVAL] episode : 50, reward mean : -0.36999999061226846, step mean : 42.95, eval_episode_rewards : [ 0.75  0.62  0.73  0.69 -1.42 -1.84  0.96 -1.   -1.68 -1.87 -1.27 -1.27\n",
      "  0.24 -1.22  0.29  0.99  0.92  0.96 -1.98 -1.  ]\n",
      "episode : 60, reward mean : -0.42933332671721774, total_step : 1829, cur_epsilon : 0.8908550000000001\n",
      "episode : 70, reward mean : -0.45914285039263114, total_step : 2176, cur_epsilon : 0.88912\n",
      "episode : 80, reward mean : -0.4294999934732914, total_step : 2408, cur_epsilon : 0.88796\n",
      "episode : 90, reward mean : -0.37444443781342773, total_step : 2750, cur_epsilon : 0.88625\n",
      "episode : 100, reward mean : -0.3251999934017658, total_step : 3042, cur_epsilon : 0.8847900000000001\n",
      "[EVAL] episode : 100, reward mean : -0.40299999099224804, step mean : 41.2, eval_episode_rewards : [ 0.8  -1.29 -1.73 -1.18  0.94 -1.23  1.    0.32 -1.21  0.02  0.95 -1.3\n",
      " -1.55 -1.   -1.21  0.48 -1.   -1.    1.    0.13]\n",
      "episode : 110, reward mean : -0.30836362987756727, total_step : 3291, cur_epsilon : 0.883545\n",
      "episode : 120, reward mean : -0.3374999933876097, total_step : 3658, cur_epsilon : 0.88171\n",
      "episode : 130, reward mean : -0.31484614715266684, total_step : 4010, cur_epsilon : 0.87995\n",
      "episode : 140, reward mean : -0.3183571362202721, total_step : 4283, cur_epsilon : 0.8785850000000001\n",
      "episode : 150, reward mean : -0.3003999932855368, total_step : 4641, cur_epsilon : 0.876795\n",
      "[EVAL] episode : 150, reward mean : -0.0624999918974936, step mean : 37.15, eval_episode_rewards : [ 1.    0.67 -1.34  0.34  0.93  0.84  0.26 -1.52 -1.04  0.78  0.96  0.64\n",
      " -1.9   0.96 -1.44  0.83 -1.03 -1.    0.81 -1.  ]\n",
      "episode : 160, reward mean : -0.2623124934383668, total_step : 4841, cur_epsilon : 0.875795\n",
      "synced target net\n",
      "episode : 170, reward mean : -0.25399999353377256, total_step : 5071, cur_epsilon : 0.874645\n",
      "episode : 180, reward mean : -0.21688888255092834, total_step : 5267, cur_epsilon : 0.873665\n",
      "episode : 190, reward mean : -0.18873683576913255, total_step : 5559, cur_epsilon : 0.872205\n",
      "episode : 200, reward mean : -0.201899993699044, total_step : 5821, cur_epsilon : 0.870895\n",
      "[EVAL] episode : 200, reward mean : 0.11550000635907054, step mean : 29.45, eval_episode_rewards : [ 0.82  0.94  0.71  0.81 -1.39  0.98  0.05  0.82  0.7   0.21  0.97 -1.19\n",
      " -1.03  0.99  1.   -1.68 -1.27  0.37 -1.05  0.55]\n",
      "episode : 210, reward mean : -0.1760476126646002, total_step : 6189, cur_epsilon : 0.869055\n",
      "episode : 220, reward mean : -0.17140908453782852, total_step : 6473, cur_epsilon : 0.867635\n",
      "episode : 230, reward mean : -0.1482173849788049, total_step : 6721, cur_epsilon : 0.866395\n",
      "episode : 240, reward mean : -0.14904166044822584, total_step : 6899, cur_epsilon : 0.865505\n",
      "episode : 250, reward mean : -0.1465999938622117, total_step : 7096, cur_epsilon : 0.8645200000000001\n",
      "[EVAL] episode : 250, reward mean : 0.017500008549541236, step mean : 39.15, eval_episode_rewards : [-1.   -1.42  0.94 -1.9  -1.52 -1.    0.81  0.98  0.97  0.52  0.91  0.97\n",
      "  0.67  0.08 -1.07 -1.28  0.39  0.96  0.41  0.93]\n",
      "episode : 260, reward mean : -0.14580768621168458, total_step : 7331, cur_epsilon : 0.863345\n",
      "episode : 270, reward mean : -0.13448147524700121, total_step : 7779, cur_epsilon : 0.861105\n",
      "episode : 280, reward mean : -0.15167856524432344, total_step : 8005, cur_epsilon : 0.859975\n",
      "episode : 290, reward mean : -0.13555171802511504, total_step : 8198, cur_epsilon : 0.85901\n",
      "episode : 300, reward mean : -0.13523332733040055, total_step : 8334, cur_epsilon : 0.85833\n",
      "[EVAL] episode : 300, reward mean : 0.3680000029504299, step mean : 14.2, eval_episode_rewards : [-1.01 -1.37 -1.36  0.55  0.89 -1.04  0.91 -1.    1.    1.    1.    0.95\n",
      "  0.88  0.89  0.99  0.58  0.62  0.93  1.    0.95]\n",
      "episode : 310, reward mean : -0.12751612300954518, total_step : 8640, cur_epsilon : 0.8568\n",
      "episode : 320, reward mean : -0.12440624400624074, total_step : 8878, cur_epsilon : 0.85561\n",
      "episode : 330, reward mean : -0.12218181219967929, total_step : 9139, cur_epsilon : 0.854305\n",
      "episode : 340, reward mean : -0.11414705278790173, total_step : 9497, cur_epsilon : 0.852515\n",
      "episode : 350, reward mean : -0.0895714225460376, total_step : 9761, cur_epsilon : 0.851195\n",
      "[EVAL] episode : 350, reward mean : 0.5550000032410025, step mean : 15.5, eval_episode_rewards : [ 0.97 -1.08  0.91  1.    1.    0.79  0.94  0.45  0.83  1.    0.52  0.98\n",
      "  0.69 -1.18  0.95  0.74  1.    0.78 -1.03  0.84]\n",
      "synced target net\n",
      "episode : 360, reward mean : -0.10666666061927875, total_step : 10076, cur_epsilon : 0.84962\n",
      "episode : 370, reward mean : -0.10410810209609367, total_step : 10298, cur_epsilon : 0.84851\n",
      "episode : 380, reward mean : -0.07971052035787388, total_step : 10485, cur_epsilon : 0.847575\n",
      "episode : 390, reward mean : -0.06710255818966872, total_step : 10683, cur_epsilon : 0.846585\n",
      "episode : 400, reward mean : -0.04877499410416931, total_step : 10926, cur_epsilon : 0.8453700000000001\n",
      "[EVAL] episode : 400, reward mean : 0.034500007051974535, step mean : 32.4, eval_episode_rewards : [ 0.77  0.79 -1.24  0.76 -1.06  0.88  0.89  0.94  0.99 -1.85  0.87  0.95\n",
      " -1.    0.33 -1.    0.75 -1.    0.98 -1.06 -1.  ]\n",
      "episode : 410, reward mean : -0.04697560390866384, total_step : 11111, cur_epsilon : 0.844445\n",
      "episode : 420, reward mean : -0.03638094651202361, total_step : 11422, cur_epsilon : 0.84289\n",
      "episode : 430, reward mean : -0.023279063997053824, total_step : 11505, cur_epsilon : 0.842475\n",
      "episode : 440, reward mean : -0.016909085197204892, total_step : 11658, cur_epsilon : 0.8417100000000001\n",
      "episode : 450, reward mean : -0.0163111053324408, total_step : 12058, cur_epsilon : 0.8397100000000001\n",
      "[EVAL] episode : 450, reward mean : -0.18799999244511129, step mean : 34.75, eval_episode_rewards : [ 0.68 -1.02 -1.34 -1.78  0.73  0.78  0.03  0.98 -1.36 -1.51  0.91 -1.\n",
      "  0.98 -1.   -1.03  0.79  0.35  0.13 -1.08  1.  ]\n",
      "episode : 460, reward mean : -0.02704347250578196, total_step : 12277, cur_epsilon : 0.838615\n",
      "episode : 470, reward mean : -0.018404249581409262, total_step : 12508, cur_epsilon : 0.83746\n",
      "episode : 480, reward mean : -0.010583327601974208, total_step : 12761, cur_epsilon : 0.836195\n",
      "episode : 490, reward mean : -0.012285708537211223, total_step : 13065, cur_epsilon : 0.8346750000000001\n",
      "episode : 500, reward mean : -0.007939994279295206, total_step : 13270, cur_epsilon : 0.83365\n",
      "[EVAL] episode : 500, reward mean : 0.3345000036992133, step mean : 17.55, eval_episode_rewards : [ 1.    1.    0.96  0.91  0.59  0.65 -1.26 -1.29  0.76  0.4  -1.23  0.97\n",
      " -1.01 -1.08  0.94  0.98  0.81  0.8   0.95  0.84]\n",
      "episode : 510, reward mean : -0.0049607785929944, total_step : 13536, cur_epsilon : 0.8323200000000001\n",
      "episode : 520, reward mean : 0.006480774931752911, total_step : 13756, cur_epsilon : 0.8312200000000001\n",
      "episode : 530, reward mean : 0.009679251055250753, total_step : 14188, cur_epsilon : 0.82906\n",
      "episode : 540, reward mean : 0.007407413202303427, total_step : 14511, cur_epsilon : 0.827445\n",
      "episode : 550, reward mean : 0.003581823953850703, total_step : 14724, cur_epsilon : 0.82638\n",
      "[EVAL] episode : 550, reward mean : 0.21250000642612576, step mean : 29.75, eval_episode_rewards : [ 0.96  0.98  1.    0.61 -1.54  0.94  0.4   0.72  0.35  0.37  0.86  0.79\n",
      "  0.18  0.82 -1.03 -1.38  0.55 -1.15 -1.    0.82]\n",
      "episode : 560, reward mean : 5.357717496476003e-05, total_step : 14928, cur_epsilon : 0.82536\n",
      "synced target net\n",
      "episode : 570, reward mean : 0.006719303977463329, total_step : 15158, cur_epsilon : 0.82421\n",
      "episode : 580, reward mean : 0.0059482816638874595, total_step : 15604, cur_epsilon : 0.82198\n",
      "episode : 590, reward mean : 0.009389836284316192, total_step : 15805, cur_epsilon : 0.820975\n",
      "episode : 600, reward mean : 0.009600005745887756, total_step : 15993, cur_epsilon : 0.8200350000000001\n",
      "[EVAL] episode : 600, reward mean : 0.10100000780075788, step mean : 35.85, eval_episode_rewards : [ 0.53  0.95  0.14  0.87 -1.15 -1.27  0.66 -1.04  0.23  0.03  0.62  0.8\n",
      " -1.   -1.    1.    0.87  0.71 -1.47  0.62  0.92]\n",
      "episode : 610, reward mean : 0.014836071324763727, total_step : 16274, cur_epsilon : 0.81863\n",
      "episode : 620, reward mean : 0.021241941209882497, total_step : 16472, cur_epsilon : 0.81764\n",
      "episode : 630, reward mean : 0.01738095809483812, total_step : 16704, cur_epsilon : 0.81648\n",
      "episode : 640, reward mean : 0.02303125573671423, total_step : 17034, cur_epsilon : 0.81483\n",
      "episode : 650, reward mean : 0.022030775031218162, total_step : 17484, cur_epsilon : 0.8125800000000001\n",
      "[EVAL] episode : 650, reward mean : 0.16750000966712833, step mean : 44.05, eval_episode_rewards : [ 1.    0.79 -1.38  0.68 -1.08  0.69 -1.   -1.    0.69  0.7  -1.    0.86\n",
      "  0.15  0.72  0.77 -1.    0.55  1.    0.62  0.59]\n",
      "episode : 660, reward mean : 0.02090909674086354, total_step : 17845, cur_epsilon : 0.810775\n",
      "episode : 670, reward mean : 0.020507468500021678, total_step : 18061, cur_epsilon : 0.809695\n",
      "episode : 680, reward mean : 0.022470594044117366, total_step : 18317, cur_epsilon : 0.808415\n",
      "episode : 690, reward mean : 0.030507252173687237, total_step : 18550, cur_epsilon : 0.80725\n",
      "episode : 700, reward mean : 0.032557148643370185, total_step : 18786, cur_epsilon : 0.8060700000000001\n",
      "[EVAL] episode : 700, reward mean : 0.4555000032298267, step mean : 15.45, eval_episode_rewards : [ 1.    0.89  0.98  1.    0.73  0.97 -1.62  0.97  0.92 -1.17  0.91  0.88\n",
      "  0.98  0.86 -1.08  0.86  0.67  0.66 -1.17  0.87]\n",
      "episode : 710, reward mean : 0.03559155504835743, total_step : 18948, cur_epsilon : 0.80526\n",
      "episode : 720, reward mean : 0.040347227963825893, total_step : 19180, cur_epsilon : 0.8041\n",
      "episode : 730, reward mean : 0.037232882432537535, total_step : 19377, cur_epsilon : 0.803115\n",
      "episode : 740, reward mean : 0.039594600294288754, total_step : 19575, cur_epsilon : 0.802125\n",
      "episode : 750, reward mean : 0.051306672314802806, total_step : 19667, cur_epsilon : 0.8016650000000001\n",
      "[EVAL] episode : 750, reward mean : 0.29850000673905014, step mean : 31.15, eval_episode_rewards : [ 0.96  0.84  0.8   0.96  0.05  0.63 -1.13 -1.27 -1.1   0.77  1.    0.97\n",
      "  0.9  -1.96  0.93  0.31  0.72  0.88  0.34  0.37]\n",
      "synced target net\n",
      "episode : 760, reward mean : 0.052828953040175534, total_step : 20009, cur_epsilon : 0.799955\n",
      "episode : 770, reward mean : 0.05488312256510382, total_step : 20307, cur_epsilon : 0.798465\n",
      "episode : 780, reward mean : 0.06471795437809749, total_step : 20495, cur_epsilon : 0.797525\n",
      "episode : 790, reward mean : 0.06989873985795281, total_step : 20829, cur_epsilon : 0.795855\n",
      "episode : 800, reward mean : 0.06607500567566603, total_step : 21075, cur_epsilon : 0.794625\n",
      "[EVAL] episode : 800, reward mean : 0.2925000046379864, step mean : 21.75, eval_episode_rewards : [ 0.98  0.98  0.76 -1.24 -1.31  0.42  0.69 -1.06  0.45  0.78  0.96  0.76\n",
      "  0.68  0.97 -1.09  0.77  0.64  0.91  0.93 -1.13]\n",
      "episode : 810, reward mean : 0.0658148204907775, total_step : 21339, cur_epsilon : 0.793305\n",
      "episode : 820, reward mean : 0.0681585422853326, total_step : 21691, cur_epsilon : 0.791545\n",
      "episode : 830, reward mean : 0.06866265631179852, total_step : 21991, cur_epsilon : 0.790045\n",
      "episode : 840, reward mean : 0.06478571999551995, total_step : 22258, cur_epsilon : 0.78871\n",
      "episode : 850, reward mean : 0.06734118219684151, total_step : 22586, cur_epsilon : 0.78707\n",
      "[EVAL] episode : 850, reward mean : 0.1990000044927001, step mean : 21.1, eval_episode_rewards : [ 0.73 -1.2  -1.07 -1.14  1.    0.62  0.86  0.95  0.81 -1.18  0.99 -1.49\n",
      "  0.46 -1.28  0.84  0.92  0.22  0.99  0.97  0.98]\n",
      "episode : 860, reward mean : 0.06631395920547982, total_step : 22817, cur_epsilon : 0.785915\n",
      "episode : 870, reward mean : 0.06094253446435791, total_step : 23127, cur_epsilon : 0.784365\n",
      "episode : 880, reward mean : 0.06370455119335516, total_step : 23433, cur_epsilon : 0.7828350000000001\n",
      "episode : 890, reward mean : 0.06735955628911766, total_step : 23654, cur_epsilon : 0.78173\n",
      "episode : 900, reward mean : 0.06933333908518155, total_step : 24019, cur_epsilon : 0.7799050000000001\n",
      "[EVAL] episode : 900, reward mean : 0.31700000520795585, step mean : 24.25, eval_episode_rewards : [ 0.94  0.62  0.98  0.97  0.61  0.93  0.95  0.92  0.26  0.9  -1.02 -1.12\n",
      "  0.71  0.91  0.99 -1.28 -1.55 -1.    0.64  0.98]\n",
      "episode : 910, reward mean : 0.0705714343136156, total_step : 24247, cur_epsilon : 0.778765\n",
      "episode : 920, reward mean : 0.0709130492547284, total_step : 24654, cur_epsilon : 0.77673\n",
      "episode : 930, reward mean : 0.06980645740705152, total_step : 24995, cur_epsilon : 0.7750250000000001\n",
      "synced target net\n",
      "episode : 940, reward mean : 0.07061702709327987, total_step : 25358, cur_epsilon : 0.77321\n",
      "episode : 950, reward mean : 0.07041053210630228, total_step : 25517, cur_epsilon : 0.7724150000000001\n",
      "[EVAL] episode : 950, reward mean : 0.35000000670552256, step mean : 30.95, eval_episode_rewards : [ 0.95 -1.27  0.92  0.6   0.97  0.6  -1.22  0.99 -1.25 -1.    0.68  0.47\n",
      "  0.49  0.55  0.93  0.78  0.76  0.4   0.92  0.73]\n",
      "episode : 960, reward mean : 0.06973958913198051, total_step : 25820, cur_epsilon : 0.7709\n",
      "episode : 970, reward mean : 0.07193815014181064, total_step : 26147, cur_epsilon : 0.7692650000000001\n",
      "episode : 980, reward mean : 0.07121429153508982, total_step : 26455, cur_epsilon : 0.767725\n",
      "episode : 990, reward mean : 0.07017172300868263, total_step : 26796, cur_epsilon : 0.76602\n",
      "episode : 1000, reward mean : 0.06993000581301749, total_step : 26960, cur_epsilon : 0.7652\n",
      "[EVAL] episode : 1000, reward mean : 0.15350000550970436, step mean : 25.55, eval_episode_rewards : [ 0.89  1.    0.91  0.61 -1.39  0.71  0.99 -1.   -1.1  -1.01  0.94  0.82\n",
      "  0.99 -1.03 -1.23  0.49  0.87  0.93 -1.    0.68]\n",
      "episode : 1010, reward mean : 0.07566337212551348, total_step : 27121, cur_epsilon : 0.764395\n",
      "episode : 1020, reward mean : 0.07215686851710665, total_step : 27312, cur_epsilon : 0.76344\n",
      "episode : 1030, reward mean : 0.06914563683557857, total_step : 27560, cur_epsilon : 0.7622\n",
      "episode : 1040, reward mean : 0.06979808271325265, total_step : 27932, cur_epsilon : 0.76034\n",
      "episode : 1050, reward mean : 0.07646667245065882, total_step : 28172, cur_epsilon : 0.75914\n",
      "[EVAL] episode : 1050, reward mean : 0.137000004760921, step mean : 22.25, eval_episode_rewards : [-1.02  0.98 -1.68  0.91 -1.07  0.68 -1.    0.78 -1.05  0.94  0.91  0.76\n",
      "  0.36  0.71 -1.21  0.95 -1.13  0.99  0.93  1.  ]\n",
      "episode : 1060, reward mean : 0.07891510012471732, total_step : 28446, cur_epsilon : 0.75777\n",
      "episode : 1070, reward mean : 0.08195327680000913, total_step : 28652, cur_epsilon : 0.75674\n",
      "episode : 1080, reward mean : 0.07665741318595354, total_step : 28952, cur_epsilon : 0.75524\n",
      "episode : 1090, reward mean : 0.07253211589376314, total_step : 29335, cur_epsilon : 0.753325\n",
      "episode : 1100, reward mean : 0.07275455124506897, total_step : 29547, cur_epsilon : 0.752265\n",
      "[EVAL] episode : 1100, reward mean : 0.3315000060014427, step mean : 27.85, eval_episode_rewards : [ 0.63  0.85  0.81  0.78  0.86  0.48  0.91  0.96 -1.03  0.85 -1.26  0.96\n",
      " -1.69  0.75  0.94  0.76  0.96  0.04  0.56 -1.49]\n",
      "synced target net\n",
      "episode : 1110, reward mean : 0.07336036619134582, total_step : 30015, cur_epsilon : 0.7499250000000001\n",
      "episode : 1120, reward mean : 0.07375000583540116, total_step : 30308, cur_epsilon : 0.74846\n",
      "episode : 1130, reward mean : 0.07734513859042025, total_step : 30638, cur_epsilon : 0.74681\n",
      "episode : 1140, reward mean : 0.07521930411294625, total_step : 31013, cur_epsilon : 0.744935\n",
      "episode : 1150, reward mean : 0.07692174501069214, total_step : 31352, cur_epsilon : 0.74324\n",
      "[EVAL] episode : 1150, reward mean : 0.18100000601261854, step mean : 27.85, eval_episode_rewards : [ 0.59  0.32 -1.09  0.93  0.03 -1.24  0.95  0.87  0.61  0.99 -1.47  0.99\n",
      "  0.83  0.96  0.79  0.95 -1.16 -1.   -1.08  0.85]\n",
      "episode : 1160, reward mean : 0.07575000589874027, total_step : 31720, cur_epsilon : 0.7414000000000001\n",
      "episode : 1170, reward mean : 0.07700855289068487, total_step : 31907, cur_epsilon : 0.740465\n",
      "episode : 1180, reward mean : 0.07922034486496853, total_step : 32179, cur_epsilon : 0.739105\n",
      "episode : 1190, reward mean : 0.0857899218249847, total_step : 32328, cur_epsilon : 0.73836\n",
      "episode : 1200, reward mean : 0.08247500586789101, total_step : 32650, cur_epsilon : 0.73675\n",
      "[EVAL] episode : 1200, reward mean : 0.2820000026375055, step mean : 12.8, eval_episode_rewards : [ 0.87  0.94  0.61  0.99  0.9  -1.05 -1.02 -1.04  0.98  0.99  0.96  0.86\n",
      "  0.35 -1.15  0.99 -1.15  0.9   0.97  0.81 -1.07]\n",
      "episode : 1210, reward mean : 0.08457851826467297, total_step : 32923, cur_epsilon : 0.735385\n",
      "episode : 1220, reward mean : 0.08908197308172945, total_step : 33198, cur_epsilon : 0.73401\n",
      "episode : 1230, reward mean : 0.09273984326670567, total_step : 33469, cur_epsilon : 0.7326550000000001\n",
      "episode : 1240, reward mean : 0.09464516716017839, total_step : 33748, cur_epsilon : 0.73126\n",
      "episode : 1250, reward mean : 0.09545600587725639, total_step : 34061, cur_epsilon : 0.729695\n",
      "[EVAL] episode : 1250, reward mean : 0.2855000059120357, step mean : 27.4, eval_episode_rewards : [ 0.94 -1.02 -1.14 -1.   -1.02  0.93  0.86  0.93  0.79  0.73  0.84 -1.28\n",
      "  0.65  0.86  0.47  0.93  0.61  0.36  0.86  0.41]\n",
      "episode : 1260, reward mean : 0.09672222811403493, total_step : 34416, cur_epsilon : 0.72792\n",
      "episode : 1270, reward mean : 0.09729134446875316, total_step : 34657, cur_epsilon : 0.726715\n",
      "episode : 1280, reward mean : 0.09524219338636612, total_step : 34931, cur_epsilon : 0.725345\n",
      "synced target net\n",
      "episode : 1290, reward mean : 0.09442636248359615, total_step : 35250, cur_epsilon : 0.72375\n",
      "episode : 1300, reward mean : 0.09116154436738445, total_step : 35588, cur_epsilon : 0.72206\n",
      "[EVAL] episode : 1300, reward mean : -0.22749999491497874, step mean : 23.75, eval_episode_rewards : [-1.51 -1.07  0.92  0.83  0.99  0.99 -1.37  0.65  0.57  0.96 -1.38 -1.17\n",
      "  0.52  0.66 -1.07  0.95 -1.01 -1.15 -1.36 -1.5 ]\n",
      "episode : 1310, reward mean : 0.09343512040058165, total_step : 35809, cur_epsilon : 0.720955\n",
      "episode : 1320, reward mean : 0.09683333922911322, total_step : 36077, cur_epsilon : 0.719615\n",
      "episode : 1330, reward mean : 0.10006015627343852, total_step : 36361, cur_epsilon : 0.718195\n",
      "episode : 1340, reward mean : 0.09911940887625982, total_step : 36597, cur_epsilon : 0.717015\n",
      "episode : 1350, reward mean : 0.09920741330280348, total_step : 36896, cur_epsilon : 0.71552\n",
      "[EVAL] episode : 1350, reward mean : 0.39250000575557353, step mean : 26.7, eval_episode_rewards : [ 0.93  0.42  0.71  0.78  0.83  0.33  0.87  0.85  0.99  0.71  0.93  0.93\n",
      " -1.06  0.94 -1.   -1.02  0.83  0.7  -1.56  0.74]\n",
      "episode : 1360, reward mean : 0.09521324118626688, total_step : 37150, cur_epsilon : 0.71425\n",
      "episode : 1370, reward mean : 0.09140146575650594, total_step : 37486, cur_epsilon : 0.71257\n",
      "episode : 1380, reward mean : 0.09231884648510512, total_step : 37777, cur_epsilon : 0.711115\n",
      "episode : 1390, reward mean : 0.093374106608224, total_step : 37948, cur_epsilon : 0.71026\n",
      "episode : 1400, reward mean : 0.09679286302599524, total_step : 38186, cur_epsilon : 0.7090700000000001\n",
      "[EVAL] episode : 1400, reward mean : 0.33450000593438745, step mean : 27.55, eval_episode_rewards : [ 0.78  0.49  0.49  0.82  0.65  0.96  0.97  0.95 -1.16 -1.29  0.92  0.2\n",
      "  0.97  0.49 -1.02  0.38  0.86  0.85  0.6  -1.22]\n",
      "episode : 1410, reward mean : 0.09856738176804485, total_step : 38449, cur_epsilon : 0.707755\n",
      "episode : 1420, reward mean : 0.10209155518325491, total_step : 38759, cur_epsilon : 0.706205\n",
      "episode : 1430, reward mean : 0.10492308283081421, total_step : 39161, cur_epsilon : 0.704195\n",
      "episode : 1440, reward mean : 0.10360417257761582, total_step : 39456, cur_epsilon : 0.70272\n",
      "episode : 1450, reward mean : 0.10302069555325755, total_step : 39647, cur_epsilon : 0.701765\n",
      "[EVAL] episode : 1450, reward mean : 0.187000004760921, step mean : 22.3, eval_episode_rewards : [ 0.77  0.91 -1.24 -1.25 -1.01  0.78  0.97  0.91  0.87  0.7  -1.13 -1.47\n",
      "  0.87  0.86  0.57  0.89  0.5  -1.13  0.45  0.92]\n",
      "episode : 1460, reward mean : 0.10342466342837027, total_step : 39895, cur_epsilon : 0.7005250000000001\n",
      "synced target net\n",
      "episode : 1470, reward mean : 0.10544218275518645, total_step : 40105, cur_epsilon : 0.6994750000000001\n",
      "episode : 1480, reward mean : 0.10864865454165516, total_step : 40434, cur_epsilon : 0.6978300000000001\n",
      "episode : 1490, reward mean : 0.10820134820404069, total_step : 40901, cur_epsilon : 0.695495\n",
      "episode : 1500, reward mean : 0.10804000592976809, total_step : 41227, cur_epsilon : 0.693865\n",
      "[EVAL] episode : 1500, reward mean : 0.12150000846013427, step mean : 38.85, eval_episode_rewards : [ 0.27  0.47  0.39 -1.32  0.54  0.23  0.18  0.6   0.11  0.78 -1.04  0.31\n",
      " -1.01  0.97  0.66  0.63  0.84  0.99 -1.06 -1.11]\n",
      "episode : 1510, reward mean : 0.11187417812226032, total_step : 41550, cur_epsilon : 0.69225\n",
      "episode : 1520, reward mean : 0.11373684803318036, total_step : 41765, cur_epsilon : 0.691175\n",
      "episode : 1530, reward mean : 0.11365360070409533, total_step : 42074, cur_epsilon : 0.68963\n",
      "episode : 1540, reward mean : 0.11448701891878789, total_step : 42341, cur_epsilon : 0.688295\n",
      "episode : 1550, reward mean : 0.1153548446537987, total_step : 42702, cur_epsilon : 0.68649\n",
      "[EVAL] episode : 1550, reward mean : 0.2370000069960952, step mean : 32.25, eval_episode_rewards : [ 0.54 -1.06  0.08 -1.07  0.84  0.78  0.97  0.2  -1.05  0.68 -1.28  0.68\n",
      "  0.94 -1.    0.81  0.94  0.75  0.79  0.67  0.53]\n",
      "episode : 1560, reward mean : 0.11687180081334633, total_step : 42960, cur_epsilon : 0.6852\n",
      "episode : 1570, reward mean : 0.11897452823271987, total_step : 43222, cur_epsilon : 0.68389\n",
      "episode : 1580, reward mean : 0.12126582873207105, total_step : 43551, cur_epsilon : 0.682245\n",
      "episode : 1590, reward mean : 0.1224717040634099, total_step : 43848, cur_epsilon : 0.68076\n",
      "episode : 1600, reward mean : 0.12121250597992912, total_step : 44335, cur_epsilon : 0.6783250000000001\n",
      "[EVAL] episode : 1600, reward mean : 0.16750000854954122, step mean : 39.1, eval_episode_rewards : [ 0.8   0.18 -1.    0.9  -1.38 -1.    0.97  0.75  0.26  0.41 -1.09 -1.\n",
      " -1.04  0.68  0.88  0.76  0.74  0.95  0.67  0.91]\n",
      "episode : 1610, reward mean : 0.12173292523840014, total_step : 44639, cur_epsilon : 0.676805\n",
      "episode : 1620, reward mean : 0.12011728991673869, total_step : 44789, cur_epsilon : 0.6760550000000001\n",
      "synced target net\n",
      "episode : 1630, reward mean : 0.12076074216354844, total_step : 45073, cur_epsilon : 0.674635\n",
      "episode : 1640, reward mean : 0.12009756694570548, total_step : 45370, cur_epsilon : 0.67315\n",
      "episode : 1650, reward mean : 0.12267273324118419, total_step : 45635, cur_epsilon : 0.671825\n",
      "[EVAL] episode : 1650, reward mean : -0.2529999932274222, step mean : 31.25, eval_episode_rewards : [-1.21 -1.35 -1.07  0.86  0.76  0.66  0.62 -1.96  0.84  0.88  0.85 -1.21\n",
      " -1.07  0.9  -1.   -1.33 -1.24  0.7   0.47 -1.16]\n",
      "episode : 1660, reward mean : 0.12515060837795755, total_step : 45911, cur_epsilon : 0.670445\n",
      "episode : 1670, reward mean : 0.1265449161579955, total_step : 46262, cur_epsilon : 0.66869\n",
      "episode : 1680, reward mean : 0.12672619646459463, total_step : 46614, cur_epsilon : 0.66693\n",
      "episode : 1690, reward mean : 0.12695858587880107, total_step : 46957, cur_epsilon : 0.665215\n",
      "episode : 1700, reward mean : 0.12834118246374762, total_step : 47205, cur_epsilon : 0.663975\n",
      "[EVAL] episode : 1700, reward mean : 0.3305000060237944, step mean : 27.95, eval_episode_rewards : [ 0.71 -1.35  0.74  0.97  0.53  0.56  0.85  0.39  0.59  0.91  0.81 -1.12\n",
      "  0.61  0.74 -1.04 -1.28  0.9   0.71  0.69  0.69]\n",
      "episode : 1710, reward mean : 0.1258479591872957, total_step : 47313, cur_epsilon : 0.663435\n",
      "episode : 1720, reward mean : 0.1284069827108016, total_step : 47557, cur_epsilon : 0.662215\n",
      "episode : 1730, reward mean : 0.13178035279014552, total_step : 47855, cur_epsilon : 0.660725\n",
      "episode : 1740, reward mean : 0.13127012093626392, total_step : 48321, cur_epsilon : 0.6583950000000001\n",
      "episode : 1750, reward mean : 0.13405143455415963, total_step : 48513, cur_epsilon : 0.657435\n",
      "[EVAL] episode : 1750, reward mean : 0.6070000065490604, step mean : 30.3, eval_episode_rewards : [ 0.88  0.95  0.91  0.71  0.93  0.89  0.11  0.8   0.41  0.47  0.62  0.7\n",
      "  0.83  0.75  0.93  0.76 -1.69  0.3   0.91  0.97]\n",
      "episode : 1760, reward mean : 0.13179546054029329, total_step : 48885, cur_epsilon : 0.655575\n",
      "episode : 1770, reward mean : 0.13353672914501638, total_step : 49055, cur_epsilon : 0.654725\n",
      "episode : 1780, reward mean : 0.1373202306909089, total_step : 49258, cur_epsilon : 0.65371\n",
      "episode : 1790, reward mean : 0.13888827411759666, total_step : 49450, cur_epsilon : 0.65275\n",
      "episode : 1800, reward mean : 0.1387111171003845, total_step : 49951, cur_epsilon : 0.650245\n",
      "[EVAL] episode : 1800, reward mean : -0.14099999461323023, step mean : 25.1, eval_episode_rewards : [ 0.32  0.61  0.98  0.98 -1.25  0.81 -1.08  0.93  0.97 -1.14 -1.39 -1.73\n",
      " -1.04 -1.28  0.71  0.36  0.9  -1.22 -1.15  0.89]\n",
      "synced target net\n",
      "episode : 1810, reward mean : 0.1388784590357328, total_step : 50291, cur_epsilon : 0.648545\n",
      "episode : 1820, reward mean : 0.1412747312664658, total_step : 50526, cur_epsilon : 0.64737\n",
      "episode : 1830, reward mean : 0.14109836665027942, total_step : 50827, cur_epsilon : 0.645865\n",
      "episode : 1840, reward mean : 0.14014674513297073, total_step : 51170, cur_epsilon : 0.64415\n",
      "episode : 1850, reward mean : 0.13890270871991242, total_step : 51569, cur_epsilon : 0.642155\n",
      "[EVAL] episode : 1850, reward mean : 0.5275000060908497, step mean : 28.25, eval_episode_rewards : [ 0.38  0.95  0.85  0.9  -1.02  0.91  0.71  0.66  0.85  0.91  0.5   0.84\n",
      "  0.83  0.84  0.8   0.9   0.02 -1.27  0.63  0.36]\n",
      "episode : 1860, reward mean : 0.14131721031141056, total_step : 51791, cur_epsilon : 0.6410450000000001\n",
      "episode : 1870, reward mean : 0.1440909151097431, total_step : 52141, cur_epsilon : 0.639295\n",
      "episode : 1880, reward mean : 0.1426542613328375, total_step : 52376, cur_epsilon : 0.63812\n",
      "episode : 1890, reward mean : 0.14358201660905723, total_step : 52767, cur_epsilon : 0.6361650000000001\n",
      "episode : 1900, reward mean : 0.14300000601497137, total_step : 52944, cur_epsilon : 0.6352800000000001\n",
      "[EVAL] episode : 1900, reward mean : 0.59450000571087, step mean : 26.5, eval_episode_rewards : [ 0.96  0.63  0.96  0.85 -1.05  0.04  0.98  0.98  0.7   0.88  0.7  -1.\n",
      "  0.67  0.88  0.49  0.99  0.94  0.9   0.59  0.8 ]\n",
      "episode : 1910, reward mean : 0.14310995369018373, total_step : 53489, cur_epsilon : 0.632555\n",
      "episode : 1920, reward mean : 0.14318229772131114, total_step : 53841, cur_epsilon : 0.630795\n",
      "episode : 1930, reward mean : 0.14446114595554807, total_step : 54160, cur_epsilon : 0.6292\n",
      "episode : 1940, reward mean : 0.14345361431580536, total_step : 54520, cur_epsilon : 0.6274\n",
      "episode : 1950, reward mean : 0.14484103171107096, total_step : 54815, cur_epsilon : 0.6259250000000001\n",
      "[EVAL] episode : 1950, reward mean : 0.4460000067949295, step mean : 31.35, eval_episode_rewards : [ 0.2   0.97  0.93 -1.06  0.48  0.86  0.45  0.58  0.91 -1.07  0.67  0.96\n",
      "  0.75  0.6   0.71 -1.    0.92  0.91  0.74  0.41]\n",
      "synced target net\n",
      "episode : 1960, reward mean : 0.14732143464118092, total_step : 55093, cur_epsilon : 0.6245350000000001\n",
      "episode : 1970, reward mean : 0.14876650354262386, total_step : 55471, cur_epsilon : 0.622645\n",
      "episode : 1980, reward mean : 0.1475252585974757, total_step : 55677, cur_epsilon : 0.621615\n",
      "episode : 1990, reward mean : 0.147974880431877, total_step : 55850, cur_epsilon : 0.62075\n",
      "episode : 2000, reward mean : 0.1490750060668215, total_step : 56191, cur_epsilon : 0.6190450000000001\n",
      "[EVAL] episode : 2000, reward mean : 0.14550000792369247, step mean : 36.35, eval_episode_rewards : [-1.05  0.42  0.28 -1.32  0.81  0.39 -1.01  0.91 -1.61 -1.    0.87  0.83\n",
      "  0.87  0.86 -1.    0.24  0.76  0.91  0.82  0.93]\n",
      "episode : 2010, reward mean : 0.1492039861722817, total_step : 56525, cur_epsilon : 0.617375\n",
      "episode : 2020, reward mean : 0.15168812489321473, total_step : 56884, cur_epsilon : 0.61558\n",
      "episode : 2030, reward mean : 0.15150739524531834, total_step : 57179, cur_epsilon : 0.614105\n",
      "episode : 2040, reward mean : 0.1522892217708788, total_step : 57476, cur_epsilon : 0.6126199999999999\n",
      "episode : 2050, reward mean : 0.15359024998800055, total_step : 57766, cur_epsilon : 0.61117\n",
      "[EVAL] episode : 2050, reward mean : 0.1510000078007579, step mean : 35.8, eval_episode_rewards : [ 0.8   0.79 -1.02 -1.06  0.99  0.6   0.53  0.8  -1.    0.77 -1.4  -1.\n",
      "  0.63  0.02  0.91 -1.02  0.83  0.37  0.93  0.55]\n",
      "episode : 2060, reward mean : 0.15433495754412221, total_step : 58067, cur_epsilon : 0.609665\n",
      "episode : 2070, reward mean : 0.15643478868988114, total_step : 58288, cur_epsilon : 0.60856\n",
      "episode : 2080, reward mean : 0.15678365993797064, total_step : 58667, cur_epsilon : 0.606665\n",
      "episode : 2090, reward mean : 0.15815311614869598, total_step : 59032, cur_epsilon : 0.60484\n",
      "episode : 2100, reward mean : 0.15855238706228278, total_step : 59398, cur_epsilon : 0.60301\n",
      "[EVAL] episode : 2100, reward mean : 0.5205000073648989, step mean : 33.9, eval_episode_rewards : [ 0.01  0.97  0.57  0.89  0.73 -1.    0.75  0.65  0.74  0.84  0.72  0.44\n",
      "  0.99  0.54 -1.23  0.4   0.95  0.52  0.97  0.96]\n",
      "episode : 2110, reward mean : 0.16008057482452331, total_step : 59626, cur_epsilon : 0.60187\n",
      "synced target net\n",
      "episode : 2120, reward mean : 0.161688685373245, total_step : 60135, cur_epsilon : 0.599325\n",
      "episode : 2130, reward mean : 0.16177465402009622, total_step : 60464, cur_epsilon : 0.59768\n",
      "episode : 2140, reward mean : 0.1634299126758336, total_step : 60758, cur_epsilon : 0.59621\n",
      "episode : 2150, reward mean : 0.1655116340318738, total_step : 60957, cur_epsilon : 0.595215\n",
      "[EVAL] episode : 2150, reward mean : -0.03449999252334237, step mean : 34.25, eval_episode_rewards : [-1.15  0.77  0.8  -1.06 -1.    0.78 -1.04  0.94 -1.05 -1.36  0.88 -1.\n",
      "  0.82 -1.    0.9   0.66  0.59  0.99 -1.    0.84]\n",
      "episode : 2160, reward mean : 0.16597685798295533, total_step : 61301, cur_epsilon : 0.593495\n",
      "episode : 2170, reward mean : 0.16680645775221598, total_step : 61664, cur_epsilon : 0.59168\n",
      "episode : 2180, reward mean : 0.16693578595893646, total_step : 61978, cur_epsilon : 0.59011\n",
      "episode : 2190, reward mean : 0.16523744907808494, total_step : 62391, cur_epsilon : 0.588045\n",
      "episode : 2200, reward mean : 0.16687273342839695, total_step : 62676, cur_epsilon : 0.58662\n",
      "[EVAL] episode : 2200, reward mean : 0.4575000054202974, step mean : 25.25, eval_episode_rewards : [ 0.77  0.99  0.99 -1.11  0.95  0.69  0.88  0.55  0.86  0.98  0.65 -1.34\n",
      "  0.37  0.72  0.69  0.73 -1.12  0.79  0.86  0.25]\n",
      "episode : 2210, reward mean : 0.16867421429638005, total_step : 62921, cur_epsilon : 0.585395\n",
      "episode : 2220, reward mean : 0.16859460075360697, total_step : 63279, cur_epsilon : 0.583605\n",
      "episode : 2230, reward mean : 0.1692556115510485, total_step : 63672, cur_epsilon : 0.58164\n",
      "episode : 2240, reward mean : 0.16843750617367081, total_step : 63995, cur_epsilon : 0.580025\n",
      "episode : 2250, reward mean : 0.16889778394665983, total_step : 64232, cur_epsilon : 0.57884\n",
      "[EVAL] episode : 2250, reward mean : 0.3170000096783042, step mean : 44.25, eval_episode_rewards : [-1.17  0.73  0.48  0.6   0.77  0.73  0.47  0.76 -1.    0.07  0.7  -1.24\n",
      "  0.43  0.65  0.56  0.34  0.79  0.87  0.59  0.21]\n",
      "episode : 2260, reward mean : 0.17034956369367715, total_step : 64545, cur_epsilon : 0.577275\n",
      "episode : 2270, reward mean : 0.17064758327252144, total_step : 64917, cur_epsilon : 0.575415\n",
      "synced target net\n",
      "episode : 2280, reward mean : 0.16993421671765024, total_step : 65317, cur_epsilon : 0.573415\n",
      "episode : 2290, reward mean : 0.17045852148815924, total_step : 65736, cur_epsilon : 0.57132\n",
      "episode : 2300, reward mean : 0.17039565838468462, total_step : 66089, cur_epsilon : 0.569555\n",
      "[EVAL] episode : 2300, reward mean : 0.3265000061132014, step mean : 28.35, eval_episode_rewards : [ 0.68  0.91  0.72  0.79 -1.3   0.98  0.76  0.11  0.92  0.71  0.35  0.51\n",
      " -1.09  0.75  0.77 -1.01 -1.18  0.88  0.37  0.9 ]\n",
      "episode : 2310, reward mean : 0.1707142919229416, total_step : 66354, cur_epsilon : 0.56823\n",
      "episode : 2320, reward mean : 0.17160776482397241, total_step : 66586, cur_epsilon : 0.56707\n",
      "episode : 2330, reward mean : 0.17208584311694033, total_step : 66913, cur_epsilon : 0.565435\n",
      "episode : 2340, reward mean : 0.1739059891153732, total_step : 67224, cur_epsilon : 0.56388\n",
      "episode : 2350, reward mean : 0.17484681471310398, total_step : 67439, cur_epsilon : 0.562805\n",
      "[EVAL] episode : 2350, reward mean : 0.575000006146729, step mean : 28.45, eval_episode_rewards : [ 0.96  0.92  0.66  0.64  0.74  0.76  0.94  0.88  0.75  0.66 -1.    0.76\n",
      "  0.93 -1.74  0.95  0.92  0.93  0.42  0.89  0.53]\n",
      "episode : 2360, reward mean : 0.17483475197943213, total_step : 67875, cur_epsilon : 0.560625\n",
      "episode : 2370, reward mean : 0.1763839724689785, total_step : 68242, cur_epsilon : 0.55879\n",
      "episode : 2380, reward mean : 0.17771429194121802, total_step : 68559, cur_epsilon : 0.557205\n",
      "episode : 2390, reward mean : 0.17863598948336795, total_step : 68771, cur_epsilon : 0.556145\n",
      "episode : 2400, reward mean : 0.177920839554475, total_step : 69073, cur_epsilon : 0.554635\n",
      "[EVAL] episode : 2400, reward mean : 0.4630000086501241, step mean : 39.65, eval_episode_rewards : [ 0.91  0.48  0.78  0.54  0.99  0.92  0.9   0.45  0.31  0.45  0.78  0.29\n",
      "  0.95  0.3   0.95  0.83  0.56  0.28 -1.   -1.41]\n",
      "episode : 2410, reward mean : 0.17765975726837935, total_step : 69467, cur_epsilon : 0.552665\n",
      "episode : 2420, reward mean : 0.17785124590943668, total_step : 69851, cur_epsilon : 0.550745\n",
      "episode : 2430, reward mean : 0.18066667289369637, total_step : 69999, cur_epsilon : 0.5500050000000001\n",
      "synced target net\n",
      "episode : 2440, reward mean : 0.18040574395067258, total_step : 70490, cur_epsilon : 0.54755\n",
      "episode : 2450, reward mean : 0.1828244960368896, total_step : 70727, cur_epsilon : 0.546365\n",
      "[EVAL] episode : 2450, reward mean : 0.7980000045150518, step mean : 21.2, eval_episode_rewards : [0.96 0.81 0.44 0.91 0.66 0.84 0.27 0.84 0.87 0.99 0.91 0.99 0.83 0.78\n",
      " 0.99 0.86 0.81 0.47 0.88 0.85]\n",
      "episode : 2460, reward mean : 0.1805487867376608, total_step : 71114, cur_epsilon : 0.54443\n",
      "episode : 2470, reward mean : 0.1798583058549868, total_step : 71514, cur_epsilon : 0.54243\n",
      "episode : 2480, reward mean : 0.18033871593915166, total_step : 71825, cur_epsilon : 0.540875\n",
      "episode : 2490, reward mean : 0.18095181349844458, total_step : 72201, cur_epsilon : 0.538995\n",
      "episode : 2500, reward mean : 0.18196800626814366, total_step : 72475, cur_epsilon : 0.537625\n",
      "[EVAL] episode : 2500, reward mean : 0.7525000055320561, step mean : 25.75, eval_episode_rewards : [0.86 0.96 0.84 0.99 0.69 0.38 0.79 0.79 0.74 0.96 0.49 0.1  0.95 0.6\n",
      " 0.93 0.67 0.92 0.91 0.65 0.83]\n",
      "episode : 2510, reward mean : 0.18224701823525694, total_step : 72932, cur_epsilon : 0.53534\n",
      "episode : 2520, reward mean : 0.18306349834929855, total_step : 73253, cur_epsilon : 0.5337350000000001\n",
      "episode : 2530, reward mean : 0.1831462513552353, total_step : 73657, cur_epsilon : 0.531715\n",
      "episode : 2540, reward mean : 0.18357480945666943, total_step : 74074, cur_epsilon : 0.52963\n",
      "episode : 2550, reward mean : 0.18405490826888413, total_step : 74378, cur_epsilon : 0.5281100000000001\n",
      "[EVAL] episode : 2550, reward mean : 0.006500007677823305, step mean : 35.2, eval_episode_rewards : [ 0.94 -1.    0.98  0.8  -1.15  0.99 -1.    0.76 -1.43  0.69  0.89  0.39\n",
      " -1.    0.69  0.99  0.91 -1.64 -1.03 -1.15  0.5 ]\n",
      "episode : 2560, reward mean : 0.18512500631331932, total_step : 74730, cur_epsilon : 0.5263500000000001\n",
      "synced target net\n",
      "episode : 2570, reward mean : 0.18433852771710213, total_step : 75056, cur_epsilon : 0.5247200000000001\n",
      "episode : 2580, reward mean : 0.18548450244359616, total_step : 75386, cur_epsilon : 0.52307\n",
      "episode : 2590, reward mean : 0.18504247735739904, total_step : 75624, cur_epsilon : 0.52188\n",
      "episode : 2600, reward mean : 0.18707308323838964, total_step : 75921, cur_epsilon : 0.5203949999999999\n",
      "[EVAL] episode : 2600, reward mean : 0.29550000904127954, step mean : 41.35, eval_episode_rewards : [-1.    0.18  0.87  0.85  0.37  0.64  0.6  -1.01 -1.71  0.98  0.77 -1.\n",
      "  0.89  0.64  0.13  0.66  0.89  0.85  0.92  0.39]\n",
      "episode : 2610, reward mean : 0.18795402929529376, total_step : 76114, cur_epsilon : 0.5194300000000001\n",
      "episode : 2620, reward mean : 0.18708397576884003, total_step : 76364, cur_epsilon : 0.5181800000000001\n",
      "episode : 2630, reward mean : 0.18911027246271703, total_step : 76654, cur_epsilon : 0.51673\n",
      "episode : 2640, reward mean : 0.18951894569873923, total_step : 76967, cur_epsilon : 0.5151650000000001\n",
      "episode : 2650, reward mean : 0.18989057234506, total_step : 77289, cur_epsilon : 0.513555\n",
      "[EVAL] episode : 2650, reward mean : 0.10150000667199492, step mean : 30.85, eval_episode_rewards : [ 0.96  0.75  0.68  0.82 -1.58 -1.38  0.98 -1.21  0.28  0.88  0.62 -1.04\n",
      " -1.5   0.58  0.68  0.02  0.86  0.8   0.91 -1.08]\n",
      "episode : 2660, reward mean : 0.19067669803822848, total_step : 77599, cur_epsilon : 0.512005\n",
      "episode : 2670, reward mean : 0.19148315237730407, total_step : 77902, cur_epsilon : 0.51049\n",
      "episode : 2680, reward mean : 0.19139552869375295, total_step : 78144, cur_epsilon : 0.50928\n",
      "episode : 2690, reward mean : 0.1915836494180571, total_step : 78312, cur_epsilon : 0.50844\n",
      "episode : 2700, reward mean : 0.19005185815885112, total_step : 78744, cur_epsilon : 0.5062800000000001\n",
      "[EVAL] episode : 2700, reward mean : 0.1600000075995922, step mean : 35.0, eval_episode_rewards : [ 0.92 -1.11  0.95 -1.28  0.84  0.29  0.38 -1.8   0.88  0.52  0.87 -1.04\n",
      "  0.56  0.96  0.11  0.82  0.88 -1.5   0.31  0.64]\n",
      "episode : 2710, reward mean : 0.18940960040309113, total_step : 79037, cur_epsilon : 0.504815\n",
      "episode : 2720, reward mean : 0.18956250632250243, total_step : 79513, cur_epsilon : 0.502435\n",
      "episode : 2730, reward mean : 0.18950550083152867, total_step : 79848, cur_epsilon : 0.5007600000000001\n",
      "synced target net\n",
      "episode : 2740, reward mean : 0.1904124150997585, total_step : 80319, cur_epsilon : 0.49840500000000004\n",
      "episode : 2750, reward mean : 0.18953455179252407, total_step : 80580, cur_epsilon : 0.49710000000000004\n",
      "[EVAL] episode : 2750, reward mean : 0.5835000081919134, step mean : 37.6, eval_episode_rewards : [ 0.88  0.55  0.99 -1.    0.34  0.72  0.94  0.9   0.52  0.95  0.73  0.87\n",
      "  0.39  0.22  0.86  0.35  0.75  0.73  0.2   0.78]\n",
      "episode : 2760, reward mean : 0.19165942662301055, total_step : 80814, cur_epsilon : 0.49593000000000004\n",
      "episode : 2770, reward mean : 0.19232130597550623, total_step : 81148, cur_epsilon : 0.49426000000000003\n",
      "episode : 2780, reward mean : 0.1928920926646792, total_step : 81407, cur_epsilon : 0.49296500000000004\n",
      "episode : 2790, reward mean : 0.1933190027498857, total_step : 81705, cur_epsilon : 0.491475\n",
      "episode : 2800, reward mean : 0.19352143490287874, total_step : 81964, cur_epsilon : 0.49018\n",
      "[EVAL] episode : 2800, reward mean : 0.42250000620260836, step mean : 28.65, eval_episode_rewards : [ 0.92  0.2   0.75  0.89  0.81 -1.18  0.92  0.68  0.91  0.66  0.91  0.95\n",
      "  0.71 -1.    0.98  0.75  0.86 -1.    0.97 -1.24]\n",
      "episode : 2810, reward mean : 0.1940960917456135, total_step : 82318, cur_epsilon : 0.48841\n",
      "episode : 2820, reward mean : 0.19367376521282584, total_step : 82752, cur_epsilon : 0.48624\n",
      "episode : 2830, reward mean : 0.19449117242460642, total_step : 83036, cur_epsilon : 0.48482000000000003\n",
      "episode : 2840, reward mean : 0.19445775282031424, total_step : 83261, cur_epsilon : 0.48369500000000004\n",
      "episode : 2850, reward mean : 0.1951964975625538, total_step : 83466, cur_epsilon : 0.48267000000000004\n",
      "[EVAL] episode : 2850, reward mean : 0.3630000064149499, step mean : 29.65, eval_episode_rewards : [ 0.71  0.93  0.85 -1.56  0.73  0.86 -1.07  0.77  0.98 -1.65 -1.    0.5\n",
      "  0.89  0.68  0.65  0.94  0.75  0.64  0.75  0.91]\n",
      "episode : 2860, reward mean : 0.19482168467402772, total_step : 83986, cur_epsilon : 0.48007\n",
      "episode : 2870, reward mean : 0.19382579033279254, total_step : 84386, cur_epsilon : 0.47807\n",
      "episode : 2880, reward mean : 0.19508333969085168, total_step : 84640, cur_epsilon : 0.4768\n",
      "synced target net\n",
      "episode : 2890, reward mean : 0.1945155773060493, total_step : 85118, cur_epsilon : 0.47441\n",
      "episode : 2900, reward mean : 0.19435862706653004, total_step : 85478, cur_epsilon : 0.47261000000000003\n",
      "[EVAL] episode : 2900, reward mean : 0.5885000047273934, step mean : 22.15, eval_episode_rewards : [ 0.93  0.58 -1.27  0.09  0.93  0.96  0.89  0.89  0.83  0.94  0.95  0.88\n",
      "  0.56  0.97  0.95  0.95 -1.18  0.83  0.15  0.94]\n",
      "episode : 2910, reward mean : 0.19448797889181837, total_step : 85856, cur_epsilon : 0.47072\n",
      "episode : 2920, reward mean : 0.1951506913167565, total_step : 86177, cur_epsilon : 0.469115\n",
      "episode : 2930, reward mean : 0.1957610985384968, total_step : 86512, cur_epsilon : 0.46744\n",
      "episode : 2940, reward mean : 0.1957687138725503, total_step : 86823, cur_epsilon : 0.46588500000000005\n",
      "episode : 2950, reward mean : 0.19664407419830054, total_step : 87278, cur_epsilon : 0.46361\n",
      "[EVAL] episode : 2950, reward mean : 0.38700000699609516, step mean : 32.3, eval_episode_rewards : [-1.35  0.19  0.45  0.63 -1.94  0.06  0.85  0.97  0.86  0.92  0.48  0.97\n",
      "  0.76  0.99  0.93  0.74  0.82 -1.02  0.63  0.8 ]\n",
      "episode : 2960, reward mean : 0.19645946585900478, total_step : 87546, cur_epsilon : 0.46227\n",
      "episode : 2970, reward mean : 0.1978518582516748, total_step : 87845, cur_epsilon : 0.46077500000000005\n",
      "episode : 2980, reward mean : 0.1975402748568496, total_step : 88150, cur_epsilon : 0.45925000000000005\n",
      "episode : 2990, reward mean : 0.19861873549698886, total_step : 88440, cur_epsilon : 0.45780000000000004\n",
      "episode : 3000, reward mean : 0.1996700064074248, total_step : 88835, cur_epsilon : 0.45582500000000004\n",
      "[EVAL] episode : 3000, reward mean : 0.3035000077448785, step mean : 35.5, eval_episode_rewards : [ 0.82  0.9   0.99  0.85  0.93  0.99  0.15 -1.   -1.    0.92  0.86  0.71\n",
      " -1.16  0.77  0.48  0.75  0.48  0.73 -1.1  -1.  ]\n",
      "episode : 3010, reward mean : 0.20005316255348068, total_step : 89130, cur_epsilon : 0.45435000000000003\n",
      "episode : 3020, reward mean : 0.20064570177408145, total_step : 89460, cur_epsilon : 0.45270000000000005\n",
      "episode : 3030, reward mean : 0.20211551795120386, total_step : 89624, cur_epsilon : 0.45188\n",
      "episode : 3040, reward mean : 0.20325987481842994, total_step : 89884, cur_epsilon : 0.45058000000000004\n",
      "synced target net\n",
      "episode : 3050, reward mean : 0.20273443263207303, total_step : 90251, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3050, reward mean : 0.1100000075995922, step mean : 34.95, eval_episode_rewards : [ 0.45  0.66 -1.51  0.88  0.62 -1.06  0.89  0.81 -1.28  0.32 -1.    0.78\n",
      "  0.35 -1.45  0.81 -1.05  0.85  0.69  0.6   0.84]\n",
      "episode : 3060, reward mean : 0.20302288221390127, total_step : 90469, cur_epsilon : 0.45\n",
      "episode : 3070, reward mean : 0.20223779143152776, total_step : 91014, cur_epsilon : 0.45\n",
      "episode : 3080, reward mean : 0.20154221420926224, total_step : 91335, cur_epsilon : 0.45\n",
      "episode : 3090, reward mean : 0.2010161876666411, total_step : 91904, cur_epsilon : 0.45\n",
      "episode : 3100, reward mean : 0.20059032902061458, total_step : 92245, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3100, reward mean : 0.3805000060237944, step mean : 27.9, eval_episode_rewards : [-1.18 -1.18  0.95 -1.55  0.88  0.96  0.33  0.81  0.85  0.91 -1.    0.75\n",
      "  0.95  0.69  0.85  0.89  0.74  0.95  0.26  0.75]\n",
      "episode : 3110, reward mean : 0.1991447009924811, total_step : 92801, cur_epsilon : 0.45\n",
      "episode : 3120, reward mean : 0.19946475005648934, total_step : 93211, cur_epsilon : 0.45\n",
      "episode : 3130, reward mean : 0.20016933553956281, total_step : 93500, cur_epsilon : 0.45\n",
      "episode : 3140, reward mean : 0.20043312748716136, total_step : 93827, cur_epsilon : 0.45\n",
      "episode : 3150, reward mean : 0.19897460964759664, total_step : 94195, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3150, reward mean : 0.42000000849366187, step mean : 38.9, eval_episode_rewards : [ 0.85  0.77  0.88  0.96 -1.8  -1.    0.37  0.89  0.75  0.65  0.76 -1.\n",
      "  0.96  0.4   0.5   0.78  0.55  0.64  0.72  0.77]\n",
      "episode : 3160, reward mean : 0.198911398880278, total_step : 94525, cur_epsilon : 0.45\n",
      "episode : 3170, reward mean : 0.1998612052211626, total_step : 94934, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 3180, reward mean : 0.19954088698697728, total_step : 95246, cur_epsilon : 0.45\n",
      "episode : 3190, reward mean : 0.19991850178093185, total_step : 95536, cur_epsilon : 0.45\n",
      "episode : 3200, reward mean : 0.20046563149255234, total_step : 95971, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3200, reward mean : 0.49200000464916227, step mean : 21.8, eval_episode_rewards : [ 0.92  0.76  0.45  0.58  0.9   0.72  0.69  0.57 -1.09  0.9   0.86  0.63\n",
      " -1.07 -1.01  0.99  0.99  0.75  0.64  0.81  0.85]\n",
      "episode : 3210, reward mean : 0.20056386941933652, total_step : 96248, cur_epsilon : 0.45\n",
      "episode : 3220, reward mean : 0.20184162139765316, total_step : 96545, cur_epsilon : 0.45\n",
      "episode : 3230, reward mean : 0.20192260711566307, total_step : 96925, cur_epsilon : 0.45\n",
      "episode : 3240, reward mean : 0.20272531513481137, total_step : 97172, cur_epsilon : 0.45\n",
      "episode : 3250, reward mean : 0.2026492372779892, total_step : 97701, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3250, reward mean : 0.15200001001358032, step mean : 45.7, eval_episode_rewards : [-1.46  0.45  0.69 -1.    0.32  0.85  0.82  0.82  0.5   0.35  0.98 -1.\n",
      " -1.62  0.53  0.45  0.91  0.62  0.58  0.27 -1.02]\n",
      "episode : 3260, reward mean : 0.20429141754970137, total_step : 97973, cur_epsilon : 0.45\n",
      "episode : 3270, reward mean : 0.2036513826613306, total_step : 98388, cur_epsilon : 0.45\n",
      "episode : 3280, reward mean : 0.20321342115881058, total_step : 98837, cur_epsilon : 0.45\n",
      "episode : 3290, reward mean : 0.20413070561302016, total_step : 99142, cur_epsilon : 0.45\n",
      "episode : 3300, reward mean : 0.20532727925276215, total_step : 99452, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3300, reward mean : 0.421500005107373, step mean : 23.8, eval_episode_rewards : [ 0.91 -1.11  0.72  0.71 -1.    0.98  0.95  0.3   0.92  0.96  0.32  0.96\n",
      "  0.74  0.63  0.93  0.73 -1.11  0.98 -1.05  0.96]\n",
      "episode : 3310, reward mean : 0.20569487057254843, total_step : 99735, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 3320, reward mean : 0.20362350049944797, total_step : 100025, cur_epsilon : 0.45\n",
      "episode : 3330, reward mean : 0.20385586237450978, total_step : 100253, cur_epsilon : 0.45\n",
      "episode : 3340, reward mean : 0.20173653347228102, total_step : 100666, cur_epsilon : 0.45\n",
      "episode : 3350, reward mean : 0.20259702143940464, total_step : 100786, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3350, reward mean : 0.4770000083371997, step mean : 38.25, eval_episode_rewards : [ 0.48  0.92  0.31  0.76  0.85  0.06  0.9   0.92  0.83 -1.    0.77  0.71\n",
      "  0.93  0.54  0.24  0.88  0.88  0.86 -1.46  0.16]\n",
      "episode : 3360, reward mean : 0.2027648874830144, total_step : 101335, cur_epsilon : 0.45\n",
      "episode : 3370, reward mean : 0.2039733003056677, total_step : 101735, cur_epsilon : 0.45\n",
      "episode : 3380, reward mean : 0.20332840891863554, total_step : 102258, cur_epsilon : 0.45\n",
      "episode : 3390, reward mean : 0.2044070861936934, total_step : 102499, cur_epsilon : 0.45\n",
      "episode : 3400, reward mean : 0.20481765360407092, total_step : 102765, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3400, reward mean : 0.6575000065378844, step mean : 30.2, eval_episode_rewards : [-1.    0.68  0.96  0.83  0.66  0.83  0.12  0.96  0.88  0.57  0.95  0.94\n",
      "  0.98  0.84  0.87  0.79  0.17  0.52  0.85  0.75]\n",
      "episode : 3410, reward mean : 0.20548094496010771, total_step : 103043, cur_epsilon : 0.45\n",
      "episode : 3420, reward mean : 0.20678948022490531, total_step : 103299, cur_epsilon : 0.45\n",
      "episode : 3430, reward mean : 0.20746064793166458, total_step : 103472, cur_epsilon : 0.45\n",
      "episode : 3440, reward mean : 0.20687791350904072, total_step : 103774, cur_epsilon : 0.45\n",
      "episode : 3450, reward mean : 0.20753333987695152, total_step : 104250, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3450, reward mean : 0.5660000052303076, step mean : 24.4, eval_episode_rewards : [ 0.77  0.7   0.68  0.76 -1.04  0.94  0.9   0.86  0.97  0.51  0.86  0.78\n",
      "  0.68  0.53  0.83 -1.11  0.86  0.96  0.39  0.49]\n",
      "episode : 3460, reward mean : 0.2073410470103253, total_step : 104618, cur_epsilon : 0.45\n",
      "episode : 3470, reward mean : 0.20748992008921993, total_step : 104868, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 3480, reward mean : 0.20773851229894863, total_step : 105283, cur_epsilon : 0.45\n",
      "episode : 3490, reward mean : 0.20830086614445578, total_step : 105489, cur_epsilon : 0.45\n",
      "episode : 3500, reward mean : 0.20687714940894927, total_step : 105888, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3500, reward mean : 0.04800001010298729, step mean : 46.0, eval_episode_rewards : [-1.96  0.82  0.65  0.69  0.93  0.27  0.81  0.77  0.34  0.86 -1.   -1.01\n",
      " -1.   -1.    0.58 -1.    0.8   0.71  0.81 -1.11]\n",
      "episode : 3510, reward mean : 0.20785755641291453, total_step : 106246, cur_epsilon : 0.45\n",
      "episode : 3520, reward mean : 0.2085710292920174, total_step : 106696, cur_epsilon : 0.45\n",
      "episode : 3530, reward mean : 0.20774504905698488, total_step : 106989, cur_epsilon : 0.45\n",
      "episode : 3540, reward mean : 0.20875989356588592, total_step : 107232, cur_epsilon : 0.45\n",
      "episode : 3550, reward mean : 0.20926479529379538, total_step : 107553, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3550, reward mean : 0.6245000061579049, step mean : 28.55, eval_episode_rewards : [ 0.93  0.92  0.15  0.73 -1.28  0.79  0.96  0.89  0.96  0.87  0.85  0.76\n",
      "  0.43  0.08  0.85  0.61  0.65  0.52  0.88  0.94]\n",
      "episode : 3560, reward mean : 0.20956461331493148, total_step : 108045, cur_epsilon : 0.45\n",
      "episode : 3570, reward mean : 0.210182079407243, total_step : 108425, cur_epsilon : 0.45\n",
      "episode : 3580, reward mean : 0.21180726914255563, total_step : 108643, cur_epsilon : 0.45\n",
      "episode : 3590, reward mean : 0.21260724891298827, total_step : 108954, cur_epsilon : 0.45\n",
      "episode : 3600, reward mean : 0.21313056212901654, total_step : 109262, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3600, reward mean : 0.48850000696256757, step mean : 32.05, eval_episode_rewards : [ 0.72  0.84  0.34  0.45  0.97 -1.    0.79  0.63  0.71 -1.01  0.95  0.9\n",
      "  0.66  0.91 -1.    0.88  0.74  0.87  0.71  0.71]\n",
      "episode : 3610, reward mean : 0.21393352457918124, total_step : 109569, cur_epsilon : 0.45\n",
      "episode : 3620, reward mean : 0.2150110562913408, total_step : 109775, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 3630, reward mean : 0.2148650203444084, total_step : 110122, cur_epsilon : 0.45\n",
      "episode : 3640, reward mean : 0.2139807758080149, total_step : 110538, cur_epsilon : 0.45\n",
      "episode : 3650, reward mean : 0.21458904766515918, total_step : 110712, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3650, reward mean : 0.4195000073872507, step mean : 34.0, eval_episode_rewards : [ 0.86 -1.01  0.86  0.08  0.97 -1.    0.91  0.18  0.3   0.78  0.19  0.9\n",
      "  0.89  0.63  0.35  0.96  0.85  0.95  0.82 -1.08]\n",
      "episode : 3660, reward mean : 0.21519946011923294, total_step : 110983, cur_epsilon : 0.45\n",
      "episode : 3670, reward mean : 0.21614714552843767, total_step : 111230, cur_epsilon : 0.45\n",
      "episode : 3680, reward mean : 0.2163641369992705, total_step : 111544, cur_epsilon : 0.45\n",
      "episode : 3690, reward mean : 0.21778862444850489, total_step : 111812, cur_epsilon : 0.45\n",
      "episode : 3700, reward mean : 0.21757027684283015, total_step : 112283, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3700, reward mean : 0.41250000754371285, step mean : 34.7, eval_episode_rewards : [ 0.96  0.72  0.96  0.96  0.86  0.62  0.07  0.87  0.98  0.95 -1.    0.97\n",
      " -1.64  0.74  0.5   0.47  0.52  0.83  0.72 -1.81]\n",
      "episode : 3710, reward mean : 0.21802156991614002, total_step : 112607, cur_epsilon : 0.45\n",
      "episode : 3720, reward mean : 0.21914785603431844, total_step : 112879, cur_epsilon : 0.45\n",
      "episode : 3730, reward mean : 0.21975067681695357, total_step : 113245, cur_epsilon : 0.45\n",
      "episode : 3740, reward mean : 0.21959893705950861, total_step : 113591, cur_epsilon : 0.45\n",
      "episode : 3750, reward mean : 0.21940000658780337, total_step : 114054, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3750, reward mean : 0.09650000678375363, step mean : 31.25, eval_episode_rewards : [-1.01 -1.27 -1.01  0.96 -1.31  0.86  0.7   0.55 -1.    0.61  0.66  0.77\n",
      "  0.6   0.87 -1.    0.97 -1.11  0.86  0.57  0.66]\n",
      "episode : 3760, reward mean : 0.21980851722207476, total_step : 114291, cur_epsilon : 0.45\n",
      "episode : 3770, reward mean : 0.22001592170358178, total_step : 114603, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 3780, reward mean : 0.22000265209812414, total_step : 115097, cur_epsilon : 0.45\n",
      "episode : 3790, reward mean : 0.2192480277078408, total_step : 115472, cur_epsilon : 0.45\n",
      "episode : 3800, reward mean : 0.21825263818235774, total_step : 115841, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3800, reward mean : 0.5390000035986304, step mean : 17.1, eval_episode_rewards : [ 0.82  0.97 -1.02  0.58  0.85  0.9   0.8   0.74  0.77  0.75  0.81 -1.29\n",
      "  0.92  0.79  0.93  0.9  -1.11  0.98  0.79  0.9 ]\n",
      "episode : 3810, reward mean : 0.21859580713160473, total_step : 116201, cur_epsilon : 0.45\n",
      "episode : 3820, reward mean : 0.21861257205049248, total_step : 116485, cur_epsilon : 0.45\n",
      "episode : 3830, reward mean : 0.21961619458927265, total_step : 116692, cur_epsilon : 0.45\n",
      "episode : 3840, reward mean : 0.2197656316071516, total_step : 117122, cur_epsilon : 0.45\n",
      "episode : 3850, reward mean : 0.2202935130995783, total_step : 117408, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3850, reward mean : 0.1045000077225268, step mean : 35.5, eval_episode_rewards : [ 0.62  0.89  0.99  0.3   0.53 -1.   -1.23 -1.8   0.8  -1.2  -1.39  0.72\n",
      "  0.7   0.66  0.55  0.93 -1.31  0.84  0.62  0.87]\n",
      "episode : 3860, reward mean : 0.22054145738260503, total_step : 117702, cur_epsilon : 0.45\n",
      "episode : 3870, reward mean : 0.22101292649635357, total_step : 117909, cur_epsilon : 0.45\n",
      "episode : 3880, reward mean : 0.22067010969346024, total_step : 118231, cur_epsilon : 0.45\n",
      "episode : 3890, reward mean : 0.2222108035092182, total_step : 118421, cur_epsilon : 0.45\n",
      "episode : 3900, reward mean : 0.22227180146922668, total_step : 118783, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3900, reward mean : 0.33650000812485814, step mean : 37.25, eval_episode_rewards : [ 0.75  0.96  0.95  0.92  0.94  0.13 -1.    0.73  0.69  0.79 -1.22  0.81\n",
      " -1.36  0.86  0.17 -1.    0.2   0.94  0.99  0.48]\n",
      "episode : 3910, reward mean : 0.22234016004759255, total_step : 119043, cur_epsilon : 0.45\n",
      "episode : 3920, reward mean : 0.2230561290325939, total_step : 119150, cur_epsilon : 0.45\n",
      "episode : 3930, reward mean : 0.22413995568486494, total_step : 119311, cur_epsilon : 0.45\n",
      "episode : 3940, reward mean : 0.22325635176615288, total_step : 119843, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 3950, reward mean : 0.2228405129315355, total_step : 120392, cur_epsilon : 0.45\n",
      "[EVAL] episode : 3950, reward mean : 0.44150000577792525, step mean : 26.85, eval_episode_rewards : [ 0.92  0.67  0.59  0.69  0.55  0.86  0.99  0.94  0.94  0.81 -1.02  0.92\n",
      "  0.98 -1.03  0.57 -1.9   0.35  0.82  0.94  0.24]\n",
      "episode : 3960, reward mean : 0.22341667326646997, total_step : 120650, cur_epsilon : 0.45\n",
      "episode : 3970, reward mean : 0.22232242474233924, total_step : 121069, cur_epsilon : 0.45\n",
      "episode : 3980, reward mean : 0.22264322268977838, total_step : 121428, cur_epsilon : 0.45\n",
      "episode : 3990, reward mean : 0.22309023216786913, total_step : 121637, cur_epsilon : 0.45\n",
      "episode : 4000, reward mean : 0.22384000660851597, total_step : 122023, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4000, reward mean : 0.0860000092536211, step mean : 42.2, eval_episode_rewards : [ 0.84 -1.42 -1.    0.11 -1.    0.71  0.93  0.86  0.77  0.97  0.78  0.75\n",
      " -1.    0.77  0.94  0.51 -1.04 -1.31  0.55 -1.  ]\n",
      "episode : 4010, reward mean : 0.22446634076359193, total_step : 122158, cur_epsilon : 0.45\n",
      "episode : 4020, reward mean : 0.22458955883951998, total_step : 122494, cur_epsilon : 0.45\n",
      "episode : 4030, reward mean : 0.22502730188073206, total_step : 122703, cur_epsilon : 0.45\n",
      "episode : 4040, reward mean : 0.22552475906945396, total_step : 122986, cur_epsilon : 0.45\n",
      "episode : 4050, reward mean : 0.22578766091922184, total_step : 123363, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4050, reward mean : 0.2840000059455633, step mean : 27.55, eval_episode_rewards : [ 0.94  0.87 -1.    0.48  0.68  0.64  0.72  0.91  0.72  0.88  0.92  0.88\n",
      " -1.33  0.96  0.45  0.61 -1.48  0.96 -1.11 -1.02]\n",
      "episode : 4060, reward mean : 0.22563793764041343, total_step : 123806, cur_epsilon : 0.45\n",
      "episode : 4070, reward mean : 0.22602949063099134, total_step : 124031, cur_epsilon : 0.45\n",
      "episode : 4080, reward mean : 0.22596569287079368, total_step : 124241, cur_epsilon : 0.45\n",
      "episode : 4090, reward mean : 0.22661125354632772, total_step : 124660, cur_epsilon : 0.45\n",
      "episode : 4100, reward mean : 0.22728537245630853, total_step : 124967, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4100, reward mean : -0.3229999927803874, step mean : 33.2, eval_episode_rewards : [ 0.91 -1.22 -1.23 -1.15  0.62  0.84  0.97  0.67 -1.69  0.99  0.97 -1.81\n",
      " -1.01 -1.31 -1.    0.88 -1.   -1.01 -1.66  0.78]\n",
      "synced target net\n",
      "episode : 4110, reward mean : 0.2277323666941873, total_step : 125166, cur_epsilon : 0.45\n",
      "episode : 4120, reward mean : 0.22848301631470835, total_step : 125639, cur_epsilon : 0.45\n",
      "episode : 4130, reward mean : 0.22835351749803628, total_step : 125874, cur_epsilon : 0.45\n",
      "episode : 4140, reward mean : 0.22918116601885877, total_step : 126113, cur_epsilon : 0.45\n",
      "episode : 4150, reward mean : 0.23013494635113033, total_step : 126298, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4150, reward mean : 0.4030000055208802, step mean : 25.65, eval_episode_rewards : [ 0.98  0.84  0.39  0.93  0.96  0.03  0.8   0.96  0.86  0.49 -1.17  0.89\n",
      "  0.89  0.99 -1.18 -1.    0.83 -1.22  0.85  0.94]\n",
      "episode : 4160, reward mean : 0.2274591412229571, total_step : 126888, cur_epsilon : 0.45\n",
      "episode : 4170, reward mean : 0.22856115768764088, total_step : 127211, cur_epsilon : 0.45\n",
      "episode : 4180, reward mean : 0.22881818843034038, total_step : 127584, cur_epsilon : 0.45\n",
      "episode : 4190, reward mean : 0.22962530493816286, total_step : 127827, cur_epsilon : 0.45\n",
      "episode : 4200, reward mean : 0.22967143518850208, total_step : 128287, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4200, reward mean : 0.4460000090301037, step mean : 41.25, eval_episode_rewards : [ 0.39 -1.    0.69  0.88  0.91  0.73  0.49  0.95  0.61 -1.   -1.    0.95\n",
      "  0.08  0.74  0.66  0.77  0.58  0.9   0.62  0.97]\n",
      "episode : 4210, reward mean : 0.23002138428540453, total_step : 128520, cur_epsilon : 0.45\n",
      "episode : 4220, reward mean : 0.22998341893494764, total_step : 128815, cur_epsilon : 0.45\n",
      "episode : 4230, reward mean : 0.2304468151274408, total_step : 129199, cur_epsilon : 0.45\n",
      "episode : 4240, reward mean : 0.2313349122770201, total_step : 129501, cur_epsilon : 0.45\n",
      "episode : 4250, reward mean : 0.2303317713277305, total_step : 129904, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4250, reward mean : 0.18000000715255737, step mean : 32.9, eval_episode_rewards : [-1.    0.95  0.88 -1.52 -1.13  0.66  0.85  0.05  0.42  0.98  0.83 -1.42\n",
      "  0.95  0.83  0.94  0.86  0.79 -1.15  0.83 -1.  ]\n",
      "synced target net\n",
      "episode : 4260, reward mean : 0.23086855121988786, total_step : 130154, cur_epsilon : 0.45\n",
      "episode : 4270, reward mean : 0.2303067981834748, total_step : 130373, cur_epsilon : 0.45\n",
      "episode : 4280, reward mean : 0.23005374493643155, total_step : 130760, cur_epsilon : 0.45\n",
      "episode : 4290, reward mean : 0.22959208120928432, total_step : 131037, cur_epsilon : 0.45\n",
      "episode : 4300, reward mean : 0.22998837871043834, total_step : 131346, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4300, reward mean : 0.34000000804662706, step mean : 36.9, eval_episode_rewards : [ 0.8  -1.    0.67  0.89  0.32  0.88  0.54 -1.    0.46  0.44 -1.09  0.7\n",
      "  0.5   0.89  0.81  0.85  0.59 -1.14  0.86  0.83]\n",
      "episode : 4310, reward mean : 0.22980047065220133, total_step : 131607, cur_epsilon : 0.45\n",
      "episode : 4320, reward mean : 0.23045602513313362, total_step : 131904, cur_epsilon : 0.45\n",
      "episode : 4330, reward mean : 0.23005081492607368, total_step : 132158, cur_epsilon : 0.45\n",
      "episode : 4340, reward mean : 0.22985945361953933, total_step : 132520, cur_epsilon : 0.45\n",
      "episode : 4350, reward mean : 0.23088506408249854, total_step : 132753, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4350, reward mean : 0.5565000065602362, step mean : 30.3, eval_episode_rewards : [ 0.88  0.87  0.7   0.84 -1.    0.21  0.82  0.84  0.88  0.72  0.6   0.76\n",
      "  0.86  0.69  0.5   0.72  0.98  0.93 -1.23  0.56]\n",
      "episode : 4360, reward mean : 0.2317431258721226, total_step : 132958, cur_epsilon : 0.45\n",
      "episode : 4370, reward mean : 0.2324919974499593, total_step : 133209, cur_epsilon : 0.45\n",
      "episode : 4380, reward mean : 0.23338356824106957, total_step : 133396, cur_epsilon : 0.45\n",
      "episode : 4390, reward mean : 0.23408428905552978, total_step : 133665, cur_epsilon : 0.45\n",
      "episode : 4400, reward mean : 0.2344022793242369, total_step : 134000, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4400, reward mean : 0.21950000850483775, step mean : 38.85, eval_episode_rewards : [-1.   -1.   -1.09  0.95  0.73  0.89  0.93  0.88  0.77  0.73  0.18  0.75\n",
      "  0.94  0.95  0.82  0.81 -1.42 -1.    0.57 -1.  ]\n",
      "episode : 4410, reward mean : 0.23519728550446684, total_step : 134225, cur_epsilon : 0.45\n",
      "episode : 4420, reward mean : 0.23543665817985102, total_step : 134593, cur_epsilon : 0.45\n",
      "episode : 4430, reward mean : 0.23595937454387647, total_step : 134936, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 4440, reward mean : 0.23627027686483956, total_step : 135172, cur_epsilon : 0.45\n",
      "episode : 4450, reward mean : 0.2365662987312574, total_step : 135513, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4450, reward mean : 0.5880000047385693, step mean : 22.2, eval_episode_rewards : [ 0.94  0.65  0.9   0.91  0.7   0.95  0.5   0.73  0.8  -1.06  0.75  0.85\n",
      "  0.8   0.93  0.95  0.84 -1.23  0.35  0.81  0.69]\n",
      "episode : 4460, reward mean : 0.2365336388775519, total_step : 135701, cur_epsilon : 0.45\n",
      "episode : 4470, reward mean : 0.2368434070344906, total_step : 135936, cur_epsilon : 0.45\n",
      "episode : 4480, reward mean : 0.2377031315862301, total_step : 136223, cur_epsilon : 0.45\n",
      "episode : 4490, reward mean : 0.23879733397926148, total_step : 136504, cur_epsilon : 0.45\n",
      "episode : 4500, reward mean : 0.2393022288141979, total_step : 136947, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4500, reward mean : 0.6190000051632524, step mean : 24.05, eval_episode_rewards : [ 0.83  0.66 -1.02  0.96  0.77  0.92  0.97  0.97  0.74  0.86  0.85  0.78\n",
      "  0.85  0.78  0.11 -1.    0.8   0.99  0.67  0.89]\n",
      "episode : 4510, reward mean : 0.23990022832127145, total_step : 137248, cur_epsilon : 0.45\n",
      "episode : 4520, reward mean : 0.24087611278766288, total_step : 137577, cur_epsilon : 0.45\n",
      "episode : 4530, reward mean : 0.24156512800340948, total_step : 137834, cur_epsilon : 0.45\n",
      "episode : 4540, reward mean : 0.24230617398789997, total_step : 138066, cur_epsilon : 0.45\n",
      "episode : 4550, reward mean : 0.24212967691390397, total_step : 138314, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4550, reward mean : 0.4695000062696636, step mean : 29.0, eval_episode_rewards : [ 0.96  0.73  0.84  0.78  0.66  0.22  0.99  0.83 -1.01  0.56  0.48 -1.08\n",
      "  0.98  0.77 -1.    0.52  0.73  0.88  0.83  0.72]\n",
      "episode : 4560, reward mean : 0.24337062061429351, total_step : 138516, cur_epsilon : 0.45\n",
      "episode : 4570, reward mean : 0.2436280153341249, total_step : 138864, cur_epsilon : 0.45\n",
      "episode : 4580, reward mean : 0.24315939522653363, total_step : 139144, cur_epsilon : 0.45\n",
      "episode : 4590, reward mean : 0.24342702182784903, total_step : 139388, cur_epsilon : 0.45\n",
      "episode : 4600, reward mean : 0.24397826745215317, total_step : 139800, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4600, reward mean : 0.17950000492855905, step mean : 23.05, eval_episode_rewards : [-1.11 -1.01  0.94  0.78  0.25  0.89  0.81 -1.02 -1.2   0.53  0.95 -1.28\n",
      "  0.37  0.69 -1.01  0.77  0.55  0.77  0.98  0.94]\n",
      "synced target net\n",
      "episode : 4610, reward mean : 0.24514534280447536, total_step : 140028, cur_epsilon : 0.45\n",
      "episode : 4620, reward mean : 0.2458614784369479, total_step : 140262, cur_epsilon : 0.45\n",
      "episode : 4630, reward mean : 0.24630238238810592, total_step : 140622, cur_epsilon : 0.45\n",
      "episode : 4640, reward mean : 0.24648492037968966, total_step : 141099, cur_epsilon : 0.45\n",
      "episode : 4650, reward mean : 0.2477096840006209, total_step : 141293, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4650, reward mean : 0.263000007532537, step mean : 34.7, eval_episode_rewards : [ 0.92  0.55 -1.37  0.63  0.95  0.11 -1.02  0.56  0.61  0.95  0.77 -1.27\n",
      "  0.43  0.39  0.76  0.58  0.63  0.86 -1.06  0.28]\n",
      "episode : 4660, reward mean : 0.24617811818227428, total_step : 141866, cur_epsilon : 0.45\n",
      "episode : 4670, reward mean : 0.24560814363566852, total_step : 142096, cur_epsilon : 0.45\n",
      "episode : 4680, reward mean : 0.246534194626137, total_step : 142427, cur_epsilon : 0.45\n",
      "episode : 4690, reward mean : 0.24711514518403613, total_step : 142718, cur_epsilon : 0.45\n",
      "episode : 4700, reward mean : 0.24795958105752125, total_step : 142983, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4700, reward mean : 0.3995000067166984, step mean : 31.05, eval_episode_rewards : [-1.12  0.11  0.14  0.84  0.85  0.99 -1.01  0.69  0.81 -1.36  0.75  0.71\n",
      "  0.12  0.85  0.92  0.97  0.45  0.88  0.48  0.92]\n",
      "episode : 4710, reward mean : 0.24734395563626174, total_step : 143334, cur_epsilon : 0.45\n",
      "episode : 4720, reward mean : 0.24631144727177728, total_step : 143683, cur_epsilon : 0.45\n",
      "episode : 4730, reward mean : 0.24629598967415964, total_step : 143854, cur_epsilon : 0.45\n",
      "episode : 4740, reward mean : 0.2462763778987156, total_step : 144227, cur_epsilon : 0.45\n",
      "episode : 4750, reward mean : 0.24587158554047345, total_step : 144583, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4750, reward mean : 0.4875000058673322, step mean : 27.2, eval_episode_rewards : [-1.    0.48  0.96  0.88 -1.01  0.81  0.47  0.68  0.89 -1.16  0.94  0.28\n",
      "  0.9   0.79  0.7   0.76  0.94  0.74  0.93  0.77]\n",
      "episode : 4760, reward mean : 0.24530672927880112, total_step : 144816, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 4770, reward mean : 0.24594759569079683, total_step : 145174, cur_epsilon : 0.45\n",
      "episode : 4780, reward mean : 0.2463117220727376, total_step : 145463, cur_epsilon : 0.45\n",
      "episode : 4790, reward mean : 0.24724009009953082, total_step : 145782, cur_epsilon : 0.45\n",
      "episode : 4800, reward mean : 0.24735000659245998, total_step : 146092, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4800, reward mean : 0.5305000071413815, step mean : 32.9, eval_episode_rewards : [ 0.8   0.93  0.04  0.13  0.86  0.75  0.93  0.92  0.59  0.85  0.91  0.79\n",
      "  0.67 -1.    0.99 -1.14  0.82  0.54  0.51  0.72]\n",
      "episode : 4810, reward mean : 0.24778171136719707, total_step : 146247, cur_epsilon : 0.45\n",
      "episode : 4820, reward mean : 0.24799585720578293, total_step : 146506, cur_epsilon : 0.45\n",
      "episode : 4830, reward mean : 0.24791719085280445, total_step : 146904, cur_epsilon : 0.45\n",
      "episode : 4840, reward mean : 0.24771281650312985, total_step : 147165, cur_epsilon : 0.45\n",
      "episode : 4850, reward mean : 0.2483505220519206, total_step : 147517, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4850, reward mean : 0.4300000071525574, step mean : 32.95, eval_episode_rewards : [ 0.34  0.77  0.87  0.91 -1.49 -1.    0.84  0.43  0.01  0.92  0.99  0.83\n",
      "  0.23  0.86  0.94  0.59 -1.18  0.92  0.92  0.9 ]\n",
      "episode : 4860, reward mean : 0.2483395127637059, total_step : 147882, cur_epsilon : 0.45\n",
      "episode : 4870, reward mean : 0.24898973964766677, total_step : 148127, cur_epsilon : 0.45\n",
      "episode : 4880, reward mean : 0.2496495967536218, total_step : 148465, cur_epsilon : 0.45\n",
      "episode : 4890, reward mean : 0.25006340126600307, total_step : 148623, cur_epsilon : 0.45\n",
      "episode : 4900, reward mean : 0.2502347004592267, total_step : 148899, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4900, reward mean : 0.5585000053979456, step mean : 25.15, eval_episode_rewards : [ 0.93  0.91 -1.13  0.6   0.72  0.68  0.83  0.86  0.47  0.72  0.83  0.76\n",
      " -1.12  0.87  0.75  0.19  0.72  0.91  0.91  0.76]\n",
      "episode : 4910, reward mean : 0.2509490900803851, total_step : 149108, cur_epsilon : 0.45\n",
      "episode : 4920, reward mean : 0.25160163259057977, total_step : 149346, cur_epsilon : 0.45\n",
      "episode : 4930, reward mean : 0.25155984430590883, total_step : 149723, cur_epsilon : 0.45\n",
      "episode : 4940, reward mean : 0.2519959579874207, total_step : 149866, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 4950, reward mean : 0.25231313788793003, total_step : 150265, cur_epsilon : 0.45\n",
      "[EVAL] episode : 4950, reward mean : 0.22800000831484796, step mean : 38.0, eval_episode_rewards : [ 0.85  0.32  0.72  0.84 -1.1   0.98  0.89  0.96  0.72 -1.    0.35 -1.\n",
      " -1.    0.88  0.97  0.65  0.78 -1.15  0.9  -1.  ]\n",
      "episode : 4960, reward mean : 0.2529717807650506, total_step : 150496, cur_epsilon : 0.45\n",
      "episode : 4970, reward mean : 0.25290543917094316, total_step : 150886, cur_epsilon : 0.45\n",
      "episode : 4980, reward mean : 0.2525863519574265, total_step : 151202, cur_epsilon : 0.45\n",
      "episode : 4990, reward mean : 0.25341082822105926, total_step : 151548, cur_epsilon : 0.45\n",
      "episode : 5000, reward mean : 0.25422400657981636, total_step : 151898, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5000, reward mean : 0.05850000875070691, step mean : 40.1, eval_episode_rewards : [ 0.98  0.95  0.19  0.93 -1.08  0.32  0.95  0.02  0.74  0.56  0.72  0.73\n",
      "  0.71  0.31 -1.42 -1.21 -1.48  0.92 -1.67 -1.  ]\n",
      "episode : 5010, reward mean : 0.2541137790307403, total_step : 152109, cur_epsilon : 0.45\n",
      "episode : 5020, reward mean : 0.2541593691311866, total_step : 152541, cur_epsilon : 0.45\n",
      "episode : 5030, reward mean : 0.2542107421734461, total_step : 152970, cur_epsilon : 0.45\n",
      "episode : 5040, reward mean : 0.25442064150517424, total_step : 153220, cur_epsilon : 0.45\n",
      "episode : 5050, reward mean : 0.2548178283645227, total_step : 153474, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5050, reward mean : 0.5975000067614019, step mean : 31.25, eval_episode_rewards : [ 0.75  0.54  0.85  0.85  0.73  0.83 -1.53  0.99  0.6   0.38  0.55  0.83\n",
      "  0.78  0.99  0.72  0.84  0.43  0.82  0.79  0.21]\n",
      "episode : 5060, reward mean : 0.2549762911660221, total_step : 153749, cur_epsilon : 0.45\n",
      "episode : 5070, reward mean : 0.25605326101470455, total_step : 153958, cur_epsilon : 0.45\n",
      "episode : 5080, reward mean : 0.2559389829551288, total_step : 154269, cur_epsilon : 0.45\n",
      "episode : 5090, reward mean : 0.25670924036267756, total_step : 154431, cur_epsilon : 0.45\n",
      "episode : 5100, reward mean : 0.25617255560031127, total_step : 154955, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5100, reward mean : 0.4675000085495412, step mean : 39.2, eval_episode_rewards : [ 0.75  0.78  0.27  0.06  0.72  0.94  0.55  0.76  0.91  0.75  0.33  0.7\n",
      "  0.19  0.67  0.84 -1.07  0.95 -1.    0.69  0.56]\n",
      "synced target net\n",
      "episode : 5110, reward mean : 0.25670255061107783, total_step : 155238, cur_epsilon : 0.45\n",
      "episode : 5120, reward mean : 0.25728320970265484, total_step : 155494, cur_epsilon : 0.45\n",
      "episode : 5130, reward mean : 0.2572573165175075, total_step : 155759, cur_epsilon : 0.45\n",
      "episode : 5140, reward mean : 0.2578657653280588, total_step : 155999, cur_epsilon : 0.45\n",
      "episode : 5150, reward mean : 0.2588699094828182, total_step : 156234, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5150, reward mean : 0.5030000066384673, step mean : 30.7, eval_episode_rewards : [ 0.98 -1.97  0.52  0.6   0.86  0.67  0.91  0.76 -1.26  0.55  0.75  0.92\n",
      "  0.63  0.51  0.96  0.05  0.84  0.86  0.95  0.97]\n",
      "episode : 5160, reward mean : 0.2595465181936878, total_step : 156436, cur_epsilon : 0.45\n",
      "episode : 5170, reward mean : 0.2594700259075462, total_step : 156725, cur_epsilon : 0.45\n",
      "episode : 5180, reward mean : 0.2604401610029365, total_step : 156973, cur_epsilon : 0.45\n",
      "episode : 5190, reward mean : 0.26141041118457825, total_step : 157219, cur_epsilon : 0.45\n",
      "episode : 5200, reward mean : 0.2612865450265459, total_step : 157630, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5200, reward mean : 0.35150000667199494, step mean : 30.8, eval_episode_rewards : [-1.44 -1.    0.45  0.89  0.78  0.08 -1.04  0.84  0.9   0.44  0.84  0.99\n",
      "  0.67  0.76  0.99 -1.39  0.88  0.83  0.58  0.98]\n",
      "episode : 5210, reward mean : 0.26157390291368204, total_step : 157829, cur_epsilon : 0.45\n",
      "episode : 5220, reward mean : 0.26199234372640023, total_step : 158159, cur_epsilon : 0.45\n",
      "episode : 5230, reward mean : 0.26197132587995575, total_step : 158615, cur_epsilon : 0.45\n",
      "episode : 5240, reward mean : 0.2623511516083857, total_step : 158964, cur_epsilon : 0.45\n",
      "episode : 5250, reward mean : 0.2628400065611516, total_step : 159055, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5250, reward mean : 0.648500005621463, step mean : 26.15, eval_episode_rewards : [ 0.23  0.92  0.85  0.87 -1.95  0.82  0.59  0.82  0.9   0.99  0.79  0.64\n",
      "  0.71  0.72  0.74  0.85  0.96  0.86  0.67  0.99]\n",
      "episode : 5260, reward mean : 0.263425862071717, total_step : 159294, cur_epsilon : 0.45\n",
      "episode : 5270, reward mean : 0.2636299875803447, total_step : 159533, cur_epsilon : 0.45\n",
      "episode : 5280, reward mean : 0.2633655368579749, total_step : 159819, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 5290, reward mean : 0.2635822371757521, total_step : 160051, cur_epsilon : 0.45\n",
      "episode : 5300, reward mean : 0.26352642165356366, total_step : 160526, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5300, reward mean : 0.40250000888481735, step mean : 40.75, eval_episode_rewards : [ 0.7   0.91 -1.07  0.62  0.55  0.69  0.54  0.49  0.48  0.71  0.7   0.79\n",
      "  0.8  -1.34  0.54  0.21  0.92  0.42  0.27  0.12]\n",
      "episode : 5310, reward mean : 0.26393220995021405, total_step : 160857, cur_epsilon : 0.45\n",
      "episode : 5320, reward mean : 0.26385902911599696, total_step : 161141, cur_epsilon : 0.45\n",
      "episode : 5330, reward mean : 0.26420826171306405, total_step : 161301, cur_epsilon : 0.45\n",
      "episode : 5340, reward mean : 0.26394195411849614, total_step : 161589, cur_epsilon : 0.45\n",
      "episode : 5350, reward mean : 0.2643981373940255, total_step : 161891, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5350, reward mean : 0.34500000569969413, step mean : 26.5, eval_episode_rewards : [-1.12  0.71  0.58  0.91  0.53  0.46  0.13 -1.02  0.91  0.96  0.63  0.81\n",
      "  0.83  0.84  0.92  0.83 -1.26  0.65 -1.25  0.85]\n",
      "episode : 5360, reward mean : 0.264264931926615, total_step : 162207, cur_epsilon : 0.45\n",
      "episode : 5370, reward mean : 0.264398516794351, total_step : 162481, cur_epsilon : 0.45\n",
      "episode : 5380, reward mean : 0.2649089284840154, total_step : 162752, cur_epsilon : 0.45\n",
      "episode : 5390, reward mean : 0.26507050747670413, total_step : 163010, cur_epsilon : 0.45\n",
      "episode : 5400, reward mean : 0.26544445099516045, total_step : 163351, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5400, reward mean : 0.3105000053532422, step mean : 24.9, eval_episode_rewards : [ 0.9  -1.02  0.88  0.81  0.39 -1.03  0.47  0.97 -1.02  0.74  0.9   0.66\n",
      "  0.89  0.33  0.84  0.9   0.81  0.84 -1.   -1.05]\n",
      "episode : 5410, reward mean : 0.2655785647749405, total_step : 163623, cur_epsilon : 0.45\n",
      "episode : 5420, reward mean : 0.2654778663237404, total_step : 163822, cur_epsilon : 0.45\n",
      "episode : 5430, reward mean : 0.26543462901559706, total_step : 164190, cur_epsilon : 0.45\n",
      "episode : 5440, reward mean : 0.26563971243141327, total_step : 164522, cur_epsilon : 0.45\n",
      "episode : 5450, reward mean : 0.2666385386549278, total_step : 164722, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5450, reward mean : 0.2270000061020255, step mean : 28.3, eval_episode_rewards : [ 0.81 -1.05  0.94 -1.57  0.41  0.94  0.17 -1.02  0.86 -1.11  0.83  0.33\n",
      "  0.98  0.13  0.91 -1.18  0.6   0.85  0.77  0.94]\n",
      "synced target net\n",
      "episode : 5460, reward mean : 0.2669908490376513, total_step : 165073, cur_epsilon : 0.45\n",
      "episode : 5470, reward mean : 0.2674607012444729, total_step : 165359, cur_epsilon : 0.45\n",
      "episode : 5480, reward mean : 0.26670073647133624, total_step : 165617, cur_epsilon : 0.45\n",
      "episode : 5490, reward mean : 0.26528233805695084, total_step : 165939, cur_epsilon : 0.45\n",
      "episode : 5500, reward mean : 0.2650781883654947, total_step : 166295, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5500, reward mean : 0.4260000061243773, step mean : 28.4, eval_episode_rewards : [ 0.64  0.9  -1.58  0.85 -1.01  0.98  0.67  0.93  0.76  0.88  0.66 -1.26\n",
      "  0.89  0.45  0.33  0.96  0.58  0.86  0.66  0.37]\n",
      "episode : 5510, reward mean : 0.26501634048463646, total_step : 166573, cur_epsilon : 0.45\n",
      "episode : 5520, reward mean : 0.26595109349987267, total_step : 166802, cur_epsilon : 0.45\n",
      "episode : 5530, reward mean : 0.26671790889486213, total_step : 167122, cur_epsilon : 0.45\n",
      "episode : 5540, reward mean : 0.26728159498880594, total_step : 167353, cur_epsilon : 0.45\n",
      "episode : 5550, reward mean : 0.2666054119391216, total_step : 167471, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5550, reward mean : 0.4510000066831708, step mean : 30.85, eval_episode_rewards : [ 0.96  0.81  0.05 -1.02  0.84  0.85  0.78  0.88  0.59  0.64  0.97  0.93\n",
      "  0.98 -1.18  0.74 -1.    0.51  0.36  0.59  0.74]\n",
      "episode : 5560, reward mean : 0.2673525245203603, total_step : 167799, cur_epsilon : 0.45\n",
      "episode : 5570, reward mean : 0.267915625921392, total_step : 168028, cur_epsilon : 0.45\n",
      "episode : 5580, reward mean : 0.26847850115649324, total_step : 168355, cur_epsilon : 0.45\n",
      "episode : 5590, reward mean : 0.26853131243662515, total_step : 168667, cur_epsilon : 0.45\n",
      "episode : 5600, reward mean : 0.26895714939039733, total_step : 168970, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5600, reward mean : 0.4045000054873526, step mean : 25.5, eval_episode_rewards : [-1.    0.66  0.74 -1.07 -1.12  0.07  0.86  0.88  0.79  0.81  0.68 -1.55\n",
      "  0.98  0.88  0.99  0.91  0.95  0.71  0.98  0.94]\n",
      "episode : 5610, reward mean : 0.2693992935243908, total_step : 169362, cur_epsilon : 0.45\n",
      "episode : 5620, reward mean : 0.2699733161420505, total_step : 169580, cur_epsilon : 0.45\n",
      "episode : 5630, reward mean : 0.2705097756290653, total_step : 169917, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 5640, reward mean : 0.2704095810067226, total_step : 170311, cur_epsilon : 0.45\n",
      "episode : 5650, reward mean : 0.27005133396591496, total_step : 170453, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5650, reward mean : 0.5195000062696635, step mean : 28.95, eval_episode_rewards : [ 0.81  0.59  0.89  0.56  0.81  0.8   0.91 -1.    0.79  0.88  0.93  0.83\n",
      "  0.8   0.84  0.89  0.96  0.86 -1.22  0.46 -1.  ]\n",
      "episode : 5660, reward mean : 0.26888163198976917, total_step : 171150, cur_epsilon : 0.45\n",
      "episode : 5670, reward mean : 0.2695202887340776, total_step : 171428, cur_epsilon : 0.45\n",
      "episode : 5680, reward mean : 0.2698257107671324, total_step : 171595, cur_epsilon : 0.45\n",
      "episode : 5690, reward mean : 0.2693796198982058, total_step : 171888, cur_epsilon : 0.45\n",
      "episode : 5700, reward mean : 0.268977199522615, total_step : 172157, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5700, reward mean : 0.42500000838190316, step mean : 38.4, eval_episode_rewards : [-1.39  0.74  0.98  0.89  0.89  0.96  0.7   0.77 -1.    0.49  0.22  0.74\n",
      "  0.68  0.82 -1.    0.48  0.04  0.82  0.97  0.7 ]\n",
      "episode : 5710, reward mean : 0.2690367841274329, total_step : 172563, cur_epsilon : 0.45\n",
      "episode : 5720, reward mean : 0.26935490165133735, total_step : 172920, cur_epsilon : 0.45\n",
      "episode : 5730, reward mean : 0.2695305475601065, total_step : 173259, cur_epsilon : 0.45\n",
      "episode : 5740, reward mean : 0.2698240483624257, total_step : 173629, cur_epsilon : 0.45\n",
      "episode : 5750, reward mean : 0.26943305002347284, total_step : 173794, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5750, reward mean : 0.4580000065267086, step mean : 30.15, eval_episode_rewards : [-1.    0.76  0.56  0.95  0.89  0.9   0.83  0.94 -1.17  0.67  0.98  0.66\n",
      "  0.84  0.92  0.03  0.41  0.82  0.74 -1.09  0.52]\n",
      "episode : 5760, reward mean : 0.26994618709852347, total_step : 174039, cur_epsilon : 0.45\n",
      "episode : 5770, reward mean : 0.27050607239753266, total_step : 174256, cur_epsilon : 0.45\n",
      "episode : 5780, reward mean : 0.2702699027323908, total_step : 174532, cur_epsilon : 0.45\n",
      "episode : 5790, reward mean : 0.27010536060245494, total_step : 174965, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 5800, reward mean : 0.2700224203324138, total_step : 175153, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5800, reward mean : 0.7660000052303075, step mean : 24.4, eval_episode_rewards : [0.71 0.67 0.94 0.43 0.75 0.47 0.8  0.97 0.91 0.97 0.81 0.81 0.93 0.94\n",
      " 0.92 0.83 0.86 0.75 0.1  0.75]\n",
      "episode : 5810, reward mean : 0.270743552147188, total_step : 175373, cur_epsilon : 0.45\n",
      "episode : 5820, reward mean : 0.27090035018053454, total_step : 175720, cur_epsilon : 0.45\n",
      "episode : 5830, reward mean : 0.271939972226107, total_step : 175853, cur_epsilon : 0.45\n",
      "episode : 5840, reward mean : 0.272393842150572, total_step : 176225, cur_epsilon : 0.45\n",
      "episode : 5850, reward mean : 0.273124792859213, total_step : 176535, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5850, reward mean : 0.34100000355392696, step mean : 16.9, eval_episode_rewards : [-1.73  0.91  0.59  0.87  0.81  0.95  0.74 -1.03  0.96 -1.03  0.92 -1.29\n",
      "  0.82  0.97  0.79  0.98  0.99  0.75 -1.04  0.89]\n",
      "episode : 5860, reward mean : 0.2722116106326466, total_step : 176906, cur_epsilon : 0.45\n",
      "episode : 5870, reward mean : 0.272318575530927, total_step : 177181, cur_epsilon : 0.45\n",
      "episode : 5880, reward mean : 0.27264116300035546, total_step : 177529, cur_epsilon : 0.45\n",
      "episode : 5890, reward mean : 0.2731816703716138, total_step : 177748, cur_epsilon : 0.45\n",
      "episode : 5900, reward mean : 0.27360678619878775, total_step : 178133, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5900, reward mean : 0.2505000044591725, step mean : 20.9, eval_episode_rewards : [-1.    0.79  0.88  0.8   0.65 -1.3  -1.22  0.9   0.99  0.7   0.92  0.85\n",
      "  0.95 -1.03 -1.04  0.78 -1.12  0.89  0.87  0.75]\n",
      "episode : 5910, reward mean : 0.27447377980045357, total_step : 178357, cur_epsilon : 0.45\n",
      "episode : 5920, reward mean : 0.27433446599107636, total_step : 178575, cur_epsilon : 0.45\n",
      "episode : 5930, reward mean : 0.27463575695526693, total_step : 178932, cur_epsilon : 0.45\n",
      "episode : 5940, reward mean : 0.2744124644523744, total_step : 179398, cur_epsilon : 0.45\n",
      "episode : 5950, reward mean : 0.27518824183296003, total_step : 179672, cur_epsilon : 0.45\n",
      "[EVAL] episode : 5950, reward mean : 0.41950000515207647, step mean : 24.0, eval_episode_rewards : [ 0.87  0.73  0.81  0.96  0.84 -1.29  0.8   0.83  0.69  0.88 -1.07  0.73\n",
      "  0.75  0.97 -1.    0.86  0.99 -1.82  0.91  0.95]\n",
      "synced target net\n",
      "episode : 5960, reward mean : 0.27523826157624187, total_step : 180076, cur_epsilon : 0.45\n",
      "episode : 5970, reward mean : 0.2755611455651434, total_step : 180218, cur_epsilon : 0.45\n",
      "episode : 5980, reward mean : 0.2763979998451461, total_step : 180452, cur_epsilon : 0.45\n",
      "episode : 5990, reward mean : 0.277115198520816, total_step : 180756, cur_epsilon : 0.45\n",
      "episode : 6000, reward mean : 0.2777066731962065, total_step : 180934, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6000, reward mean : 0.3305000060237944, step mean : 27.85, eval_episode_rewards : [ 0.92  0.99  0.52 -1.01  0.86  0.57  0.66  0.37  0.88 -1.12  0.94 -1.\n",
      "  0.84  0.9   0.96  0.87 -1.06  0.76 -1.    0.76]\n",
      "episode : 6010, reward mean : 0.27800666209799973, total_step : 181086, cur_epsilon : 0.45\n",
      "episode : 6020, reward mean : 0.2783887108428979, total_step : 181388, cur_epsilon : 0.45\n",
      "episode : 6030, reward mean : 0.2787993431733813, total_step : 181672, cur_epsilon : 0.45\n",
      "episode : 6040, reward mean : 0.2792615959254272, total_step : 181924, cur_epsilon : 0.45\n",
      "episode : 6050, reward mean : 0.27973058503182713, total_step : 182171, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6050, reward mean : 0.47600000724196434, step mean : 33.4, eval_episode_rewards : [ 0.68  0.69 -1.15  0.69  0.11 -1.08  0.37  0.88  0.69  0.93  0.74  0.91\n",
      "  0.64  0.7   0.39  0.64  0.64  0.46  0.75  0.84]\n",
      "episode : 6060, reward mean : 0.28015347186863276, total_step : 182544, cur_epsilon : 0.45\n",
      "episode : 6070, reward mean : 0.28070017126292185, total_step : 182742, cur_epsilon : 0.45\n",
      "episode : 6080, reward mean : 0.2810937565178488, total_step : 183032, cur_epsilon : 0.45\n",
      "episode : 6090, reward mean : 0.2816896616889558, total_step : 183297, cur_epsilon : 0.45\n",
      "episode : 6100, reward mean : 0.28218689176330314, total_step : 183621, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6100, reward mean : 0.31050000758841634, step mean : 34.9, eval_episode_rewards : [ 0.22 -1.22  0.9   0.8  -1.34 -1.04  0.53  0.54  0.51  0.29  0.65  0.94\n",
      "  0.54  0.9   0.83 -1.    0.96  0.53  0.76  0.91]\n",
      "episode : 6110, reward mean : 0.2818723469472653, total_step : 184040, cur_epsilon : 0.45\n",
      "episode : 6120, reward mean : 0.28161438560977675, total_step : 184425, cur_epsilon : 0.45\n",
      "episode : 6130, reward mean : 0.28175857096703855, total_step : 184863, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 6140, reward mean : 0.2823175961057677, total_step : 185147, cur_epsilon : 0.45\n",
      "episode : 6150, reward mean : 0.2830227707565679, total_step : 185441, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6150, reward mean : 0.14350000685080885, step mean : 31.6, eval_episode_rewards : [ 0.94 -1.53  0.8   0.71  0.37 -1.16  0.71 -1.14 -1.    0.87  0.81  0.74\n",
      "  0.64 -1.09 -1.05  0.93  0.74  0.96  0.59  0.03]\n",
      "episode : 6160, reward mean : 0.2826931883481198, total_step : 185771, cur_epsilon : 0.45\n",
      "episode : 6170, reward mean : 0.28263858027236965, total_step : 186031, cur_epsilon : 0.45\n",
      "episode : 6180, reward mean : 0.283448226590457, total_step : 186258, cur_epsilon : 0.45\n",
      "episode : 6190, reward mean : 0.2839143845525018, total_step : 186496, cur_epsilon : 0.45\n",
      "episode : 6200, reward mean : 0.284129038781168, total_step : 186788, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6200, reward mean : 0.41550000635907053, step mean : 29.45, eval_episode_rewards : [ 0.86  0.89  0.91  0.71  0.79  0.45  0.93 -1.02  0.04  0.96  0.91  0.28\n",
      "  0.53  0.32 -1.58  0.93  0.89  0.89  0.63 -1.01]\n",
      "episode : 6210, reward mean : 0.2845813269719433, total_step : 187033, cur_epsilon : 0.45\n",
      "episode : 6220, reward mean : 0.28468007083556995, total_step : 187495, cur_epsilon : 0.45\n",
      "episode : 6230, reward mean : 0.28481862611088, total_step : 187833, cur_epsilon : 0.45\n",
      "episode : 6240, reward mean : 0.28570833985681815, total_step : 188003, cur_epsilon : 0.45\n",
      "episode : 6250, reward mean : 0.2858752065241337, total_step : 188322, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6250, reward mean : 0.640500005800277, step mean : 26.95, eval_episode_rewards : [ 0.99  0.61  0.63  0.99  0.74  0.91  0.32  0.47  0.6  -1.11  0.83  0.77\n",
      "  0.93  0.98  0.71  0.45  0.51  0.65  0.87  0.96]\n",
      "episode : 6260, reward mean : 0.2860415400746769, total_step : 188740, cur_epsilon : 0.45\n",
      "episode : 6270, reward mean : 0.2863429092408486, total_step : 189075, cur_epsilon : 0.45\n",
      "episode : 6280, reward mean : 0.28668312755247255, total_step : 189484, cur_epsilon : 0.45\n",
      "episode : 6290, reward mean : 0.28696344055721035, total_step : 189831, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 6300, reward mean : 0.28680159383734305, total_step : 190155, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6300, reward mean : 0.33450000481680037, step mean : 22.5, eval_episode_rewards : [ 0.95  0.7   0.95  0.97  0.87 -1.23 -1.09  0.86 -1.    0.99  0.83  0.66\n",
      "  0.77 -1.12  0.83 -1.08  0.82  0.89  0.31  0.81]\n",
      "episode : 6310, reward mean : 0.2869128433045844, total_step : 190507, cur_epsilon : 0.45\n",
      "episode : 6320, reward mean : 0.2876471584347442, total_step : 190766, cur_epsilon : 0.45\n",
      "episode : 6330, reward mean : 0.28771564634560975, total_step : 191045, cur_epsilon : 0.45\n",
      "episode : 6340, reward mean : 0.28837224628294494, total_step : 191351, cur_epsilon : 0.45\n",
      "episode : 6350, reward mean : 0.28852599078452024, total_step : 191575, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6350, reward mean : 0.42600000724196435, step mean : 33.35, eval_episode_rewards : [ 0.63  0.67  0.91 -1.   -1.23  0.41  0.81 -1.19  0.24  0.65  0.44  0.99\n",
      "  0.97  0.79  0.88  0.92  0.78  0.88  0.07  0.9 ]\n",
      "episode : 6360, reward mean : 0.2885660442681807, total_step : 191871, cur_epsilon : 0.45\n",
      "episode : 6370, reward mean : 0.2885902734084444, total_step : 192175, cur_epsilon : 0.45\n",
      "episode : 6380, reward mean : 0.28829781217529765, total_step : 192483, cur_epsilon : 0.45\n",
      "episode : 6390, reward mean : 0.2890876434623892, total_step : 192700, cur_epsilon : 0.45\n",
      "episode : 6400, reward mean : 0.2894140690300264, total_step : 193012, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6400, reward mean : 0.4235000061802566, step mean : 28.65, eval_episode_rewards : [-1.29  0.52  0.95  0.97  0.53  0.62  0.92 -1.26  0.93  0.84  0.02  0.59\n",
      "  0.78  0.28  0.69 -1.06  0.85  0.94  0.88  0.77]\n",
      "episode : 6410, reward mean : 0.2894524246265359, total_step : 193308, cur_epsilon : 0.45\n",
      "episode : 6420, reward mean : 0.2897336513909433, total_step : 193646, cur_epsilon : 0.45\n",
      "episode : 6430, reward mean : 0.2892970516321772, total_step : 193946, cur_epsilon : 0.45\n",
      "episode : 6440, reward mean : 0.28964752205878913, total_step : 194241, cur_epsilon : 0.45\n",
      "episode : 6450, reward mean : 0.28915194451318, total_step : 194481, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6450, reward mean : 0.610000005364418, step mean : 24.95, eval_episode_rewards : [ 0.58  0.9  -1.1   0.92  0.97  0.24  0.9   0.89  0.81  0.95  0.92  0.75\n",
      "  0.63  0.41  0.98  0.86  0.86  0.77  0.96 -1.  ]\n",
      "episode : 6460, reward mean : 0.2895154864042882, total_step : 194767, cur_epsilon : 0.45\n",
      "episode : 6470, reward mean : 0.2897496201247668, total_step : 194936, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 6480, reward mean : 0.2900925991194392, total_step : 195333, cur_epsilon : 0.45\n",
      "episode : 6490, reward mean : 0.2894268170062263, total_step : 195685, cur_epsilon : 0.45\n",
      "episode : 6500, reward mean : 0.28955385268364964, total_step : 196022, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6500, reward mean : 0.7820000048726797, step mean : 22.8, eval_episode_rewards : [0.69 0.21 0.94 0.94 0.92 0.9  0.28 0.86 0.89 0.97 0.48 0.97 0.83 0.18\n",
      " 0.95 0.98 0.97 0.89 0.99 0.8 ]\n",
      "episode : 6510, reward mean : 0.2895714351014676, total_step : 196331, cur_epsilon : 0.45\n",
      "episode : 6520, reward mean : 0.2897300678802414, total_step : 196647, cur_epsilon : 0.45\n",
      "episode : 6530, reward mean : 0.28989893455189547, total_step : 196857, cur_epsilon : 0.45\n",
      "episode : 6540, reward mean : 0.2903914438361239, total_step : 197154, cur_epsilon : 0.45\n",
      "episode : 6550, reward mean : 0.2912702355294965, total_step : 197298, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6550, reward mean : 0.5670000029727816, step mean : 14.3, eval_episode_rewards : [ 0.65 -1.06  0.96  0.78  0.91  0.86  0.77  0.67  0.83 -1.13  0.93  0.9\n",
      "  0.95  0.81 -1.03  0.98  0.96  0.95  0.73  0.92]\n",
      "episode : 6560, reward mean : 0.2915884211562892, total_step : 197608, cur_epsilon : 0.45\n",
      "episode : 6570, reward mean : 0.29170320286717455, total_step : 197851, cur_epsilon : 0.45\n",
      "episode : 6580, reward mean : 0.29207751411791766, total_step : 198123, cur_epsilon : 0.45\n",
      "episode : 6590, reward mean : 0.2920956059156921, total_step : 198528, cur_epsilon : 0.45\n",
      "episode : 6600, reward mean : 0.2918590974342078, total_step : 198901, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6600, reward mean : 0.37050000624731183, step mean : 28.9, eval_episode_rewards : [ 0.94 -1.13  0.91  0.88  0.48  0.92  0.62  0.75  0.59  0.9  -1.38  0.82\n",
      "  0.99  0.82  0.8  -1.19  0.85  0.32  0.52 -1.  ]\n",
      "episode : 6610, reward mean : 0.29165961318670486, total_step : 199349, cur_epsilon : 0.45\n",
      "episode : 6620, reward mean : 0.29136254429490815, total_step : 199664, cur_epsilon : 0.45\n",
      "episode : 6630, reward mean : 0.29204223880693203, total_step : 199932, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 6640, reward mean : 0.29178615110724354, total_step : 200220, cur_epsilon : 0.45\n",
      "episode : 6650, reward mean : 0.29206166066201333, total_step : 200454, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6650, reward mean : 0.564000005275011, step mean : 24.6, eval_episode_rewards : [ 0.72  0.82  0.49  0.98  0.79  0.99  0.74  0.93  0.54  0.33 -1.14 -1.14\n",
      "  0.87  0.93  0.72  0.44  0.97  0.61  0.92  0.77]\n",
      "episode : 6660, reward mean : 0.29164415067733007, total_step : 200947, cur_epsilon : 0.45\n",
      "episode : 6670, reward mean : 0.2917571279738305, total_step : 201289, cur_epsilon : 0.45\n",
      "episode : 6680, reward mean : 0.2916916233030131, total_step : 201651, cur_epsilon : 0.45\n",
      "episode : 6690, reward mean : 0.2916263144541259, total_step : 201813, cur_epsilon : 0.45\n",
      "episode : 6700, reward mean : 0.29218060354160064, total_step : 201960, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6700, reward mean : 0.14900000672787428, step mean : 30.95, eval_episode_rewards : [ 0.93 -1.15 -1.34  0.75 -1.    0.91 -1.07  0.75  0.9   0.8  -1.   -1.1\n",
      "  0.94  0.69  0.89  0.72  0.86  0.73 -1.    0.77]\n",
      "episode : 6710, reward mean : 0.29286587835829453, total_step : 202218, cur_epsilon : 0.45\n",
      "episode : 6720, reward mean : 0.2929256017624755, total_step : 202495, cur_epsilon : 0.45\n",
      "episode : 6730, reward mean : 0.2929494864715576, total_step : 202994, cur_epsilon : 0.45\n",
      "episode : 6740, reward mean : 0.29321514006333776, total_step : 203332, cur_epsilon : 0.45\n",
      "episode : 6750, reward mean : 0.29357926579371646, total_step : 203702, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6750, reward mean : 0.24750000899657607, step mean : 41.2, eval_episode_rewards : [ 0.97 -1.59  0.27  0.77  0.65  0.89 -1.12  0.29  0.22  0.83  0.53 -1.1\n",
      "  0.64  0.53  0.85  0.56  0.57 -1.    0.56  0.63]\n",
      "episode : 6760, reward mean : 0.2940503023923017, total_step : 203999, cur_epsilon : 0.45\n",
      "episode : 6770, reward mean : 0.2944904053564482, total_step : 204417, cur_epsilon : 0.45\n",
      "episode : 6780, reward mean : 0.29474484429428227, total_step : 204659, cur_epsilon : 0.45\n",
      "episode : 6790, reward mean : 0.2951266633829483, total_step : 204915, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 6800, reward mean : 0.2950294182953589, total_step : 205096, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6800, reward mean : 0.35500000659376385, step mean : 30.45, eval_episode_rewards : [ 0.92  0.48 -1.46  0.62  0.84  0.92 -1.59  0.9   0.92  0.28 -1.07  0.89\n",
      "  0.58  0.96  0.89 -1.    0.38  0.8   0.93  0.91]\n",
      "episode : 6810, reward mean : 0.2951027965473993, total_step : 205460, cur_epsilon : 0.45\n",
      "episode : 6820, reward mean : 0.2956510329269463, total_step : 205801, cur_epsilon : 0.45\n",
      "episode : 6830, reward mean : 0.29612738574291875, total_step : 206089, cur_epsilon : 0.45\n",
      "episode : 6840, reward mean : 0.2960277843106625, total_step : 206370, cur_epsilon : 0.45\n",
      "episode : 6850, reward mean : 0.29590657587251523, total_step : 206567, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6850, reward mean : 0.19900000896304845, step mean : 41.0, eval_episode_rewards : [ 0.69  0.04  0.66 -1.02 -1.15  0.49 -1.    0.25  0.92  0.83 -1.    0.16\n",
      "  0.57  0.88  0.58  0.77  0.76  0.99  0.83 -1.27]\n",
      "episode : 6860, reward mean : 0.2962244963284267, total_step : 206962, cur_epsilon : 0.45\n",
      "episode : 6870, reward mean : 0.29647598906001277, total_step : 207103, cur_epsilon : 0.45\n",
      "episode : 6880, reward mean : 0.29632268094966663, total_step : 207520, cur_epsilon : 0.45\n",
      "episode : 6890, reward mean : 0.2970914433922494, total_step : 207704, cur_epsilon : 0.45\n",
      "episode : 6900, reward mean : 0.2968420355154228, total_step : 208088, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6900, reward mean : 0.313500006403774, step mean : 29.65, eval_episode_rewards : [ 0.88  0.73  0.96  0.79  0.95  0.3   0.99 -1.63 -1.01  0.67  0.48  0.59\n",
      "  0.11  0.81  0.77  0.96 -1.14 -1.12  0.33  0.85]\n",
      "episode : 6910, reward mean : 0.2974877055157542, total_step : 208355, cur_epsilon : 0.45\n",
      "episode : 6920, reward mean : 0.2972514516146412, total_step : 208631, cur_epsilon : 0.45\n",
      "episode : 6930, reward mean : 0.29730159382874743, total_step : 208909, cur_epsilon : 0.45\n",
      "episode : 6940, reward mean : 0.29768588549111263, total_step : 209254, cur_epsilon : 0.45\n",
      "episode : 6950, reward mean : 0.2982072007682898, total_step : 209404, cur_epsilon : 0.45\n",
      "[EVAL] episode : 6950, reward mean : 0.44450000571087, step mean : 26.55, eval_episode_rewards : [ 0.83  0.35  0.76  0.89  0.63  0.51  0.87  0.64  0.9  -1.4   0.81  0.96\n",
      " -1.08 -1.01  0.71  0.95  0.57  0.35  0.99  0.66]\n",
      "episode : 6960, reward mean : 0.298750006521506, total_step : 209637, cur_epsilon : 0.45\n",
      "episode : 6970, reward mean : 0.29924390895620867, total_step : 209804, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 6980, reward mean : 0.29932235609232993, total_step : 210258, cur_epsilon : 0.45\n",
      "episode : 6990, reward mean : 0.2991201781975545, total_step : 210609, cur_epsilon : 0.45\n",
      "episode : 7000, reward mean : 0.29935000652846483, total_step : 211058, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7000, reward mean : 0.26700000744313, step mean : 34.2, eval_episode_rewards : [-1.07  0.86  0.95  0.98  0.57  0.55  0.95  0.92  0.74 -1.41 -1.    0.6\n",
      "  0.98  0.88 -1.   -1.47  0.55  0.55  0.48  0.73]\n",
      "episode : 7010, reward mean : 0.2995806056740408, total_step : 211407, cur_epsilon : 0.45\n",
      "episode : 7020, reward mean : 0.3001324851631036, total_step : 211730, cur_epsilon : 0.45\n",
      "episode : 7030, reward mean : 0.3000426807797184, total_step : 211903, cur_epsilon : 0.45\n",
      "episode : 7040, reward mean : 0.300401995161453, total_step : 212160, cur_epsilon : 0.45\n",
      "episode : 7050, reward mean : 0.3005177370191994, total_step : 212388, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7050, reward mean : 0.7015000044368207, step mean : 20.85, eval_episode_rewards : [ 0.87  0.66  0.94  0.71  0.75  0.57  0.94  0.74  0.91  0.96  0.88 -1.32\n",
      "  0.99  0.91  0.61  0.59  0.87  0.97  0.59  0.89]\n",
      "episode : 7060, reward mean : 0.30087111133728545, total_step : 212648, cur_epsilon : 0.45\n",
      "episode : 7070, reward mean : 0.30030552279072403, total_step : 213056, cur_epsilon : 0.45\n",
      "episode : 7080, reward mean : 0.3008418144353029, total_step : 213386, cur_epsilon : 0.45\n",
      "episode : 7090, reward mean : 0.30130184009665956, total_step : 213769, cur_epsilon : 0.45\n",
      "episode : 7100, reward mean : 0.30160423188011204, total_step : 214063, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7100, reward mean : 0.6615000053308904, step mean : 24.85, eval_episode_rewards : [-1.65  0.76  0.92  0.97  0.58  0.83  0.82  0.51  0.91  0.86  0.8   0.9\n",
      "  0.52  0.83  0.86  0.92  0.75  0.71  0.69  0.74]\n",
      "episode : 7110, reward mean : 0.30144726391252963, total_step : 214382, cur_epsilon : 0.45\n",
      "episode : 7120, reward mean : 0.3012373660808806, total_step : 214739, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 7130, reward mean : 0.3015259532344124, total_step : 215042, cur_epsilon : 0.45\n",
      "episode : 7140, reward mean : 0.30191737347483344, total_step : 215271, cur_epsilon : 0.45\n",
      "episode : 7150, reward mean : 0.30214126526762347, total_step : 215518, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7150, reward mean : 0.671500005107373, step mean : 23.85, eval_episode_rewards : [ 0.95  0.91  0.62  0.96  0.62  0.79  0.78  0.97  0.94  0.88  0.11 -1.08\n",
      "  0.96  0.71  0.52  0.77  0.87  0.44  0.85  0.86]\n",
      "episode : 7160, reward mean : 0.30199302328651634, total_step : 215831, cur_epsilon : 0.45\n",
      "episode : 7170, reward mean : 0.30158299118503823, total_step : 216132, cur_epsilon : 0.45\n",
      "episode : 7180, reward mean : 0.30179666391000526, total_step : 216487, cur_epsilon : 0.45\n",
      "episode : 7190, reward mean : 0.3018344988782213, total_step : 216768, cur_epsilon : 0.45\n",
      "episode : 7200, reward mean : 0.30094028430706304, total_step : 217118, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7200, reward mean : 0.6275000060908497, step mean : 28.25, eval_episode_rewards : [ 0.07  0.47  0.67  0.88  0.85  0.95  0.93  0.86  0.29 -1.1   0.82  0.65\n",
      "  0.96  0.8   0.54  0.56  0.92  0.92  0.92  0.59]\n",
      "episode : 7210, reward mean : 0.30105271110706416, total_step : 217445, cur_epsilon : 0.45\n",
      "episode : 7220, reward mean : 0.3011745217661283, total_step : 217765, cur_epsilon : 0.45\n",
      "episode : 7230, reward mean : 0.3006763550773634, total_step : 218034, cur_epsilon : 0.45\n",
      "episode : 7240, reward mean : 0.3013245921630865, total_step : 218274, cur_epsilon : 0.45\n",
      "episode : 7250, reward mean : 0.3013655237683962, total_step : 218553, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7250, reward mean : 0.3150000074878335, step mean : 34.45, eval_episode_rewards : [ 0.75 -1.1   0.72  0.89  0.85  0.76  0.57  0.48  0.97 -1.    0.73 -1.82\n",
      "  0.36  0.85  0.7   0.76  0.67  0.68  0.67 -1.19]\n",
      "episode : 7260, reward mean : 0.3014545519797763, total_step : 218797, cur_epsilon : 0.45\n",
      "episode : 7270, reward mean : 0.3021004191780414, total_step : 219036, cur_epsilon : 0.45\n",
      "episode : 7280, reward mean : 0.30183105047940595, total_step : 219340, cur_epsilon : 0.45\n",
      "episode : 7290, reward mean : 0.3011358089898349, total_step : 219555, cur_epsilon : 0.45\n",
      "episode : 7300, reward mean : 0.30181370514814987, total_step : 219769, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7300, reward mean : 0.35850000763311984, step mean : 35.15, eval_episode_rewards : [ 0.86  0.84  0.45  0.66  0.63  0.67 -1.04  0.67  0.5   0.18  0.93 -1.65\n",
      "  0.8  -1.76  0.22  0.97  0.72  0.87  0.85  0.8 ]\n",
      "synced target net\n",
      "episode : 7310, reward mean : 0.3018632076114987, total_step : 220041, cur_epsilon : 0.45\n",
      "episode : 7320, reward mean : 0.30206831252365246, total_step : 220199, cur_epsilon : 0.45\n",
      "episode : 7330, reward mean : 0.3024079191990166, total_step : 220458, cur_epsilon : 0.45\n",
      "episode : 7340, reward mean : 0.30189646427654115, total_step : 220741, cur_epsilon : 0.45\n",
      "episode : 7350, reward mean : 0.30194830583269094, total_step : 221110, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7350, reward mean : 0.40200000554323195, step mean : 25.75, eval_episode_rewards : [ 0.94  0.93  0.95  0.96 -1.   -1.25  0.56  0.8   0.51  0.49 -1.02  0.83\n",
      " -1.01  0.74  0.89  0.88  0.85  0.48  0.75  0.76]\n",
      "episode : 7360, reward mean : 0.30250136520860355, total_step : 221411, cur_epsilon : 0.45\n",
      "episode : 7370, reward mean : 0.30303664152008786, total_step : 221724, cur_epsilon : 0.45\n",
      "episode : 7380, reward mean : 0.30297697128445356, total_step : 222075, cur_epsilon : 0.45\n",
      "episode : 7390, reward mean : 0.30344926226148144, total_step : 222233, cur_epsilon : 0.45\n",
      "episode : 7400, reward mean : 0.3035540605623376, total_step : 222462, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7400, reward mean : 0.5870000058785081, step mean : 27.25, eval_episode_rewards : [-1.35  0.88  0.92  0.69 -1.    0.73  0.8   0.95  0.91  0.51  0.82  0.77\n",
      "  0.85  0.95  0.99  0.46  0.74  0.49  0.82  0.81]\n",
      "episode : 7410, reward mean : 0.3037044599524092, total_step : 222857, cur_epsilon : 0.45\n",
      "episode : 7420, reward mean : 0.3033140226804483, total_step : 223053, cur_epsilon : 0.45\n",
      "episode : 7430, reward mean : 0.3028250401634046, total_step : 223620, cur_epsilon : 0.45\n",
      "episode : 7440, reward mean : 0.3031747377001911, total_step : 223966, cur_epsilon : 0.45\n",
      "episode : 7450, reward mean : 0.3037570534964036, total_step : 224239, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7450, reward mean : 0.3960000079125166, step mean : 36.35, eval_episode_rewards : [ 0.64  0.3   0.5   0.69  0.57  0.58  0.99 -1.1   0.77  0.54  0.9   0.48\n",
      " -1.    0.69  0.83  0.9   0.48  0.76  0.52 -1.12]\n",
      "episode : 7460, reward mean : 0.303869979704503, total_step : 224461, cur_epsilon : 0.45\n",
      "episode : 7470, reward mean : 0.3037456557748109, total_step : 224660, cur_epsilon : 0.45\n",
      "episode : 7480, reward mean : 0.3037607016980668, total_step : 224955, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 7490, reward mean : 0.3040253736673095, total_step : 225263, cur_epsilon : 0.45\n",
      "episode : 7500, reward mean : 0.3044386731746296, total_step : 225459, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7500, reward mean : 0.5505000066943466, step mean : 30.9, eval_episode_rewards : [ 0.15  0.26  0.92  0.88  0.92  0.94  0.7  -1.    0.96  0.47  0.86  0.73\n",
      "  0.97  0.77 -1.36  0.88  0.91  0.8   0.43  0.82]\n",
      "episode : 7510, reward mean : 0.30511718360203677, total_step : 225655, cur_epsilon : 0.45\n",
      "episode : 7520, reward mean : 0.3052526660763877, total_step : 225858, cur_epsilon : 0.45\n",
      "episode : 7530, reward mean : 0.30519389759966853, total_step : 226007, cur_epsilon : 0.45\n",
      "episode : 7540, reward mean : 0.3052612797055759, total_step : 226261, cur_epsilon : 0.45\n",
      "episode : 7550, reward mean : 0.3053390793420917, total_step : 226507, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7550, reward mean : 0.09850000785663723, step mean : 36.1, eval_episode_rewards : [ 0.7  -1.44 -1.27  0.94 -1.55 -1.14  0.32  0.3   0.87 -1.54  0.75  0.13\n",
      "  0.81  0.94  0.98  0.69 -1.    0.64  0.91  0.93]\n",
      "episode : 7560, reward mean : 0.30585185834674766, total_step : 226824, cur_epsilon : 0.45\n",
      "episode : 7570, reward mean : 0.3053500725446045, total_step : 227108, cur_epsilon : 0.45\n",
      "episode : 7580, reward mean : 0.30540370042472686, total_step : 227372, cur_epsilon : 0.45\n",
      "episode : 7590, reward mean : 0.30599605392281837, total_step : 227627, cur_epsilon : 0.45\n",
      "episode : 7600, reward mean : 0.3062460591269069, total_step : 228040, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7600, reward mean : 0.3395000069402158, step mean : 32.0, eval_episode_rewards : [ 0.79  0.8   0.85  0.69  0.82 -1.03  0.08  0.55  0.6   0.99 -1.    0.65\n",
      "  0.32  0.69  0.97 -1.13 -1.05  0.92  0.5   0.78]\n",
      "episode : 7610, reward mean : 0.3062746451281589, total_step : 228322, cur_epsilon : 0.45\n",
      "episode : 7620, reward mean : 0.3059133923179873, total_step : 228501, cur_epsilon : 0.45\n",
      "episode : 7630, reward mean : 0.3062791676947732, total_step : 228726, cur_epsilon : 0.45\n",
      "episode : 7640, reward mean : 0.30585995413075295, total_step : 228950, cur_epsilon : 0.45\n",
      "episode : 7650, reward mean : 0.30584575812411463, total_step : 229364, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7650, reward mean : 0.37100000735372307, step mean : 33.9, eval_episode_rewards : [ 0.19  0.56  0.94  0.22  0.9  -1.28  0.38  0.87  0.8   0.61  0.68  0.82\n",
      "  0.8   0.82  0.84 -1.45  0.86 -1.03  0.72  0.17]\n",
      "episode : 7660, reward mean : 0.30554308743403336, total_step : 229799, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 7670, reward mean : 0.305723604931456, total_step : 230165, cur_epsilon : 0.45\n",
      "episode : 7680, reward mean : 0.30540495441406773, total_step : 230514, cur_epsilon : 0.45\n",
      "episode : 7690, reward mean : 0.30498830298996094, total_step : 230939, cur_epsilon : 0.45\n",
      "episode : 7700, reward mean : 0.3055246818254327, total_step : 231231, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7700, reward mean : 0.36950000738725064, step mean : 33.95, eval_episode_rewards : [ 0.82  0.78  0.42 -1.04  0.32  0.96 -1.25  0.79  0.92 -1.    0.7   0.67\n",
      "  0.9   0.64  0.84 -1.    0.84  0.93  0.64  0.51]\n",
      "episode : 7710, reward mean : 0.305492872905938, total_step : 231459, cur_epsilon : 0.45\n",
      "episode : 7720, reward mean : 0.30606088732668424, total_step : 231725, cur_epsilon : 0.45\n",
      "episode : 7730, reward mean : 0.30661837648393114, total_step : 231998, cur_epsilon : 0.45\n",
      "episode : 7740, reward mean : 0.3067726163126201, total_step : 232182, cur_epsilon : 0.45\n",
      "episode : 7750, reward mean : 0.3068645226205549, total_step : 232414, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7750, reward mean : 0.26250000642612575, step mean : 29.7, eval_episode_rewards : [ 0.74  0.6  -1.2   0.67  0.31  0.41  0.89 -1.02  0.91  0.86  0.65  0.77\n",
      " -1.    0.96  0.6  -1.11  0.89 -1.09  0.63  0.78]\n",
      "episode : 7760, reward mean : 0.30673583123098447, total_step : 232617, cur_epsilon : 0.45\n",
      "episode : 7770, reward mean : 0.3071956306799019, total_step : 232763, cur_epsilon : 0.45\n",
      "episode : 7780, reward mean : 0.30753471085270223, total_step : 233002, cur_epsilon : 0.45\n",
      "episode : 7790, reward mean : 0.307928119447747, total_step : 233297, cur_epsilon : 0.45\n",
      "episode : 7800, reward mean : 0.30817564750830523, total_step : 233606, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7800, reward mean : 0.34450000459328295, step mean : 21.5, eval_episode_rewards : [-1.    0.43  0.4   0.98  0.93  0.95 -1.01 -1.01  0.87  0.8   0.95  0.81\n",
      "  0.8   0.9   0.84  0.86  0.75  0.96 -1.19 -1.13]\n",
      "episode : 7810, reward mean : 0.30834699751601746, total_step : 233774, cur_epsilon : 0.45\n",
      "episode : 7820, reward mean : 0.30865345916322545, total_step : 234036, cur_epsilon : 0.45\n",
      "episode : 7830, reward mean : 0.30882631554738127, total_step : 234400, cur_epsilon : 0.45\n",
      "episode : 7840, reward mean : 0.309241077910434, total_step : 234776, cur_epsilon : 0.45\n",
      "episode : 7850, reward mean : 0.30914268163762465, total_step : 234954, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7850, reward mean : 0.7345000036992133, step mean : 17.55, eval_episode_rewards : [ 0.91  0.78  0.94  0.94 -1.09  0.89  0.99  0.87  0.84  0.95  0.88  0.91\n",
      "  0.54  0.84  0.88  0.58  0.75  0.67  0.89  0.73]\n",
      "synced target net\n",
      "episode : 7860, reward mean : 0.30913868332897854, total_step : 235456, cur_epsilon : 0.45\n",
      "episode : 7870, reward mean : 0.3095362199498802, total_step : 235644, cur_epsilon : 0.45\n",
      "episode : 7880, reward mean : 0.3094200572393702, total_step : 235836, cur_epsilon : 0.45\n",
      "episode : 7890, reward mean : 0.3097680673151925, total_step : 236161, cur_epsilon : 0.45\n",
      "episode : 7900, reward mean : 0.3099493735689151, total_step : 236518, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7900, reward mean : 0.45550000658258794, step mean : 30.4, eval_episode_rewards : [ 0.43  0.64  0.87  0.87  0.76 -1.   -1.15  0.86  0.65  0.95  0.92  0.91\n",
      "  0.73  0.71  0.9   0.08  0.68 -1.21  0.93  0.58]\n",
      "episode : 7910, reward mean : 0.310154241626769, total_step : 236856, cur_epsilon : 0.45\n",
      "episode : 7920, reward mean : 0.31026263274158344, total_step : 237070, cur_epsilon : 0.45\n",
      "episode : 7930, reward mean : 0.31054351215296594, total_step : 237347, cur_epsilon : 0.45\n",
      "episode : 7940, reward mean : 0.31110705937369154, total_step : 237599, cur_epsilon : 0.45\n",
      "episode : 7950, reward mean : 0.3112000064776754, total_step : 237923, cur_epsilon : 0.45\n",
      "[EVAL] episode : 7950, reward mean : 0.2800000060349703, step mean : 27.95, eval_episode_rewards : [ 0.81  0.94 -1.17  0.07  0.72  0.98  0.3   0.74  0.94  0.92 -1.03 -1.32\n",
      " -1.    0.85  0.82 -1.19  0.93  0.96  0.58  0.75]\n",
      "episode : 7960, reward mean : 0.31108668989726534, total_step : 238310, cur_epsilon : 0.45\n",
      "episode : 7970, reward mean : 0.31145546444489564, total_step : 238515, cur_epsilon : 0.45\n",
      "episode : 7980, reward mean : 0.3114774500888149, total_step : 238895, cur_epsilon : 0.45\n",
      "episode : 7990, reward mean : 0.31138548833971963, total_step : 239465, cur_epsilon : 0.45\n",
      "episode : 8000, reward mean : 0.3119962564848829, total_step : 239675, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8000, reward mean : 0.6105000064708292, step mean : 29.95, eval_episode_rewards : [ 0.71  0.83  0.65  0.02  0.87  0.7   0.39  0.82  0.85  0.96  0.79  0.93\n",
      "  0.58  0.76  0.68  0.62  0.53  0.94  0.69 -1.11]\n",
      "episode : 8010, reward mean : 0.31204994406165104, total_step : 239930, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 8020, reward mean : 0.3121172134646045, total_step : 240174, cur_epsilon : 0.45\n",
      "episode : 8030, reward mean : 0.312109595523268, total_step : 240478, cur_epsilon : 0.45\n",
      "episode : 8040, reward mean : 0.31179105125979256, total_step : 240832, cur_epsilon : 0.45\n",
      "episode : 8050, reward mean : 0.311658391574201, total_step : 241037, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8050, reward mean : 0.6345000059343875, step mean : 27.55, eval_episode_rewards : [ 0.21  0.95  0.47  0.69  0.61  0.81  0.91  0.83  0.47  0.78  0.95  0.92\n",
      "  0.94  0.78 -1.03  0.85  0.74  0.92  0.17  0.72]\n",
      "episode : 8060, reward mean : 0.3120161355107967, total_step : 241247, cur_epsilon : 0.45\n",
      "episode : 8070, reward mean : 0.312172249358152, total_step : 241718, cur_epsilon : 0.45\n",
      "episode : 8080, reward mean : 0.31251485796626843, total_step : 241939, cur_epsilon : 0.45\n",
      "episode : 8090, reward mean : 0.3127614403528027, total_step : 242336, cur_epsilon : 0.45\n",
      "episode : 8100, reward mean : 0.3126679077204087, total_step : 242709, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8100, reward mean : 0.5815000048838556, step mean : 22.85, eval_episode_rewards : [ 0.96  0.55  0.78  0.72  0.82  0.92  0.68  0.84  0.86  0.29  0.72  0.85\n",
      " -1.53  0.86  0.91  0.95  0.89  0.72 -1.06  0.9 ]\n",
      "episode : 8110, reward mean : 0.3132392173348185, total_step : 242943, cur_epsilon : 0.45\n",
      "episode : 8120, reward mean : 0.31365148431548084, total_step : 243204, cur_epsilon : 0.45\n",
      "episode : 8130, reward mean : 0.3133727002050384, total_step : 243327, cur_epsilon : 0.45\n",
      "episode : 8140, reward mean : 0.3134705224518346, total_step : 243742, cur_epsilon : 0.45\n",
      "episode : 8150, reward mean : 0.3136282273420801, total_step : 244110, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8150, reward mean : 0.07800000607967376, step mean : 28.15, eval_episode_rewards : [ 0.63  0.37  0.69  0.53  0.63  0.92 -1.03 -1.04 -1.37  0.66  0.67 -1.02\n",
      "  0.57  0.91  0.83 -1.2   0.87  0.96 -1.   -1.02]\n",
      "episode : 8160, reward mean : 0.31374878099841047, total_step : 244607, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 8170, reward mean : 0.31310404566238603, total_step : 245127, cur_epsilon : 0.45\n",
      "episode : 8180, reward mean : 0.3133398597982591, total_step : 245530, cur_epsilon : 0.45\n",
      "episode : 8190, reward mean : 0.31324420674102466, total_step : 245804, cur_epsilon : 0.45\n",
      "episode : 8200, reward mean : 0.31304756747127127, total_step : 246062, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8200, reward mean : 0.4695000062696636, step mean : 29.0, eval_episode_rewards : [ 0.86  0.89  0.72  0.92  0.83  0.48  0.32  0.66  0.95  0.96  0.4  -1.\n",
      "  0.9  -1.08  0.45 -1.18  0.92  0.78  0.68  0.93]\n",
      "episode : 8210, reward mean : 0.31346407470119786, total_step : 246217, cur_epsilon : 0.45\n",
      "episode : 8220, reward mean : 0.31377616221053467, total_step : 246556, cur_epsilon : 0.45\n",
      "episode : 8230, reward mean : 0.3135492166988597, total_step : 246839, cur_epsilon : 0.45\n",
      "episode : 8240, reward mean : 0.3139174822175387, total_step : 247032, cur_epsilon : 0.45\n",
      "episode : 8250, reward mean : 0.31414424891386067, total_step : 247341, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8250, reward mean : 0.2110000064596534, step mean : 29.9, eval_episode_rewards : [ 0.53  0.35 -1.63  0.78  0.98  0.83  0.6   0.93  0.16 -1.04  0.82  0.98\n",
      "  0.98  0.84  0.7   0.14  0.99 -1.54 -1.1  -1.08]\n",
      "episode : 8260, reward mean : 0.31434504281003345, total_step : 247671, cur_epsilon : 0.45\n",
      "episode : 8270, reward mean : 0.3144171769871515, total_step : 248006, cur_epsilon : 0.45\n",
      "episode : 8280, reward mean : 0.31480556204352655, total_step : 248180, cur_epsilon : 0.45\n",
      "episode : 8290, reward mean : 0.31500603785183373, total_step : 248509, cur_epsilon : 0.45\n",
      "episode : 8300, reward mean : 0.3155241028733372, total_step : 248774, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8300, reward mean : 0.4970000067725778, step mean : 31.3, eval_episode_rewards : [ 0.88  0.96  0.79  0.7   0.8  -1.83  0.51  0.43  0.94  0.8   0.26  0.88\n",
      "  0.84  0.84  0.92  0.55  0.63 -1.53  0.89  0.68]\n",
      "episode : 8310, reward mean : 0.31559687772548334, total_step : 249008, cur_epsilon : 0.45\n",
      "episode : 8320, reward mean : 0.31566466994856734, total_step : 249345, cur_epsilon : 0.45\n",
      "episode : 8330, reward mean : 0.3159868012029244, total_step : 249571, cur_epsilon : 0.45\n",
      "episode : 8340, reward mean : 0.3156187115183515, total_step : 249772, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 8350, reward mean : 0.31541557534904535, total_step : 250234, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8350, reward mean : 0.4270000049844384, step mean : 23.25, eval_episode_rewards : [ 0.89  0.75 -1.73  0.88  0.97 -1.15  0.91  0.77  0.44  0.99  0.93  0.91\n",
      "  0.76  0.87 -1.05  0.87  0.88  0.94  0.71 -1.  ]\n",
      "episode : 8360, reward mean : 0.31530503041253527, total_step : 250619, cur_epsilon : 0.45\n",
      "episode : 8370, reward mean : 0.3153978559492309, total_step : 250836, cur_epsilon : 0.45\n",
      "episode : 8380, reward mean : 0.3153830613796477, total_step : 251143, cur_epsilon : 0.45\n",
      "episode : 8390, reward mean : 0.3156615082738439, total_step : 251404, cur_epsilon : 0.45\n",
      "episode : 8400, reward mean : 0.3161011969601913, total_step : 251628, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8400, reward mean : 0.5195000062696635, step mean : 29.05, eval_episode_rewards : [-1.06  0.11 -1.04  0.64  0.96  0.72  0.82  0.97  0.78  0.76  0.43  0.63\n",
      "  0.92  0.48  0.51  0.97  0.86  0.86  0.25  0.82]\n",
      "episode : 8410, reward mean : 0.3159239066067051, total_step : 252069, cur_epsilon : 0.45\n",
      "episode : 8420, reward mean : 0.31593349817388133, total_step : 252355, cur_epsilon : 0.45\n",
      "episode : 8430, reward mean : 0.31552906935904196, total_step : 252689, cur_epsilon : 0.45\n",
      "episode : 8440, reward mean : 0.3159798643086106, total_step : 253003, cur_epsilon : 0.45\n",
      "episode : 8450, reward mean : 0.3165136159546453, total_step : 253246, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8450, reward mean : 0.10950000761076809, step mean : 34.9, eval_episode_rewards : [-1.25 -1.    0.49 -1.   -1.18  0.79  0.84 -1.    0.93  0.8   0.58  0.9\n",
      "  0.77  0.48  0.91 -1.15  0.86  0.94 -1.17  0.65]\n",
      "episode : 8460, reward mean : 0.31690426180271286, total_step : 253409, cur_epsilon : 0.45\n",
      "episode : 8470, reward mean : 0.31715112809144586, total_step : 253792, cur_epsilon : 0.45\n",
      "episode : 8480, reward mean : 0.3175884498825806, total_step : 254114, cur_epsilon : 0.45\n",
      "episode : 8490, reward mean : 0.3179764493619771, total_step : 254477, cur_epsilon : 0.45\n",
      "episode : 8500, reward mean : 0.3183658888399163, total_step : 254737, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8500, reward mean : 0.5235000061802566, step mean : 28.65, eval_episode_rewards : [ 0.93  0.99  0.33  0.89  0.52  0.94  0.99 -1.16  0.75  0.65  0.93  0.7\n",
      "  0.72  0.43  0.13  0.12  0.75  0.93  0.96 -1.03]\n",
      "episode : 8510, reward mean : 0.3185240957899113, total_step : 254894, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 8520, reward mean : 0.31844132103882905, total_step : 255254, cur_epsilon : 0.45\n",
      "episode : 8530, reward mean : 0.3186694085949558, total_step : 255551, cur_epsilon : 0.45\n",
      "episode : 8540, reward mean : 0.3185995381046305, total_step : 256001, cur_epsilon : 0.45\n",
      "episode : 8550, reward mean : 0.31865146847823034, total_step : 256347, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8550, reward mean : 0.4385000069625676, step mean : 32.1, eval_episode_rewards : [ 0.42  0.87  0.87  0.86  0.87  0.7   0.77 -1.29  0.63  0.96  0.72 -1.16\n",
      "  0.87  0.55  0.76  0.66 -1.    0.73  0.25  0.73]\n",
      "episode : 8560, reward mean : 0.3190397261176258, total_step : 256706, cur_epsilon : 0.45\n",
      "episode : 8570, reward mean : 0.319571768450078, total_step : 256941, cur_epsilon : 0.45\n",
      "episode : 8580, reward mean : 0.3197249482110108, total_step : 257100, cur_epsilon : 0.45\n",
      "episode : 8590, reward mean : 0.3193667119574908, total_step : 257397, cur_epsilon : 0.45\n",
      "episode : 8600, reward mean : 0.3195395413708202, total_step : 257739, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8600, reward mean : 0.6980000045150518, step mean : 21.2, eval_episode_rewards : [ 0.86  0.67  0.9   0.56  0.77  0.97  0.98 -1.13  0.87  0.89  0.72  0.84\n",
      "  0.75  0.78  0.61  0.78  0.95  0.59  0.84  0.76]\n",
      "episode : 8610, reward mean : 0.31951800880761133, total_step : 257947, cur_epsilon : 0.45\n",
      "episode : 8620, reward mean : 0.31966009929391564, total_step : 258315, cur_epsilon : 0.45\n",
      "episode : 8630, reward mean : 0.3190903888756316, total_step : 258697, cur_epsilon : 0.45\n",
      "episode : 8640, reward mean : 0.31922685833716835, total_step : 258870, cur_epsilon : 0.45\n",
      "episode : 8650, reward mean : 0.3195965382738868, total_step : 259041, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8650, reward mean : 0.4200000062584877, step mean : 29.0, eval_episode_rewards : [ 0.48 -1.03  0.94  0.83  0.02 -1.14  0.77  0.96  0.76  0.73  0.77  0.77\n",
      "  0.81  0.76  0.75  0.91  0.98  0.05 -1.04  0.32]\n",
      "episode : 8660, reward mean : 0.31995612657381606, total_step : 259319, cur_epsilon : 0.45\n",
      "episode : 8670, reward mean : 0.31977509298783996, total_step : 259665, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 8680, reward mean : 0.3195126792967474, total_step : 260082, cur_epsilon : 0.45\n",
      "episode : 8690, reward mean : 0.31970541500180993, total_step : 260405, cur_epsilon : 0.45\n",
      "episode : 8700, reward mean : 0.31958046625635916, total_step : 260703, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8700, reward mean : 0.47650000723078845, step mean : 33.35, eval_episode_rewards : [ 0.72  0.76  0.74  0.66  0.92  0.84  0.85  0.65 -1.15  0.77  0.61  0.18\n",
      "  0.8   0.92  0.34  0.61  0.05  0.94  0.72 -1.4 ]\n",
      "episode : 8710, reward mean : 0.3193318090148176, total_step : 261109, cur_epsilon : 0.45\n",
      "episode : 8720, reward mean : 0.3188990890595276, total_step : 261476, cur_epsilon : 0.45\n",
      "episode : 8730, reward mean : 0.31920848300644244, total_step : 261697, cur_epsilon : 0.45\n",
      "episode : 8740, reward mean : 0.3193787250252232, total_step : 262039, cur_epsilon : 0.45\n",
      "episode : 8750, reward mean : 0.31975772077441217, total_step : 262297, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8750, reward mean : 0.6235000039450824, step mean : 18.65, eval_episode_rewards : [ 0.79  0.89  0.95  0.63  0.7   0.97  0.74  0.5   0.86  0.83  0.97  0.96\n",
      "  0.51 -1.01  0.96 -1.02  0.8   0.67  0.79  0.98]\n",
      "episode : 8760, reward mean : 0.3199554859411788, total_step : 262614, cur_epsilon : 0.45\n",
      "episode : 8770, reward mean : 0.32003193351109865, total_step : 262837, cur_epsilon : 0.45\n",
      "episode : 8780, reward mean : 0.32037472174628073, total_step : 263026, cur_epsilon : 0.45\n",
      "episode : 8790, reward mean : 0.3205449439142327, total_step : 263366, cur_epsilon : 0.45\n",
      "episode : 8800, reward mean : 0.3203954610338604, total_step : 263785, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8800, reward mean : 0.3185000062920153, step mean : 29.15, eval_episode_rewards : [ 0.62  0.79 -1.79 -1.03  0.93  0.71  0.92  0.84  0.62  0.92 -1.41  0.98\n",
      "  0.69  0.38 -1.22  0.69  0.68  0.72  0.42  0.91]\n",
      "episode : 8810, reward mean : 0.32094098264954046, total_step : 263994, cur_epsilon : 0.45\n",
      "episode : 8820, reward mean : 0.321193884036244, total_step : 264260, cur_epsilon : 0.45\n",
      "episode : 8830, reward mean : 0.32119932698311615, total_step : 264544, cur_epsilon : 0.45\n",
      "episode : 8840, reward mean : 0.32154751779402, total_step : 264725, cur_epsilon : 0.45\n",
      "episode : 8850, reward mean : 0.32169605167845716, total_step : 264981, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8850, reward mean : 0.13800000585615635, step mean : 27.2, eval_episode_rewards : [ 0.78  0.64  0.89  0.91  0.87 -1.04 -1.75 -1.1   0.4  -1.29  0.95  0.99\n",
      " -1.04 -1.46  0.69  0.77  0.99  0.36  0.89  0.31]\n",
      "synced target net\n",
      "episode : 8860, reward mean : 0.32214673334274346, total_step : 265270, cur_epsilon : 0.45\n",
      "episode : 8870, reward mean : 0.3225862522501385, total_step : 265467, cur_epsilon : 0.45\n",
      "episode : 8880, reward mean : 0.3230563127834283, total_step : 265737, cur_epsilon : 0.45\n",
      "episode : 8890, reward mean : 0.3229358894890855, total_step : 265931, cur_epsilon : 0.45\n",
      "episode : 8900, reward mean : 0.32342023119246693, total_step : 266187, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8900, reward mean : 0.456000005453825, step mean : 25.4, eval_episode_rewards : [-1.45 -1.2  -1.19  0.58  0.56  0.79  0.91  0.91  0.65  0.78  0.32  0.93\n",
      "  0.83  0.78  0.89  0.96  0.96  0.73  0.55  0.83]\n",
      "episode : 8910, reward mean : 0.32373962487720215, total_step : 266389, cur_epsilon : 0.45\n",
      "episode : 8920, reward mean : 0.32371413203169314, total_step : 266698, cur_epsilon : 0.45\n",
      "episode : 8930, reward mean : 0.3240503984081068, total_step : 266983, cur_epsilon : 0.45\n",
      "episode : 8940, reward mean : 0.3241040333149167, total_step : 267221, cur_epsilon : 0.45\n",
      "episode : 8950, reward mean : 0.3239318500482087, total_step : 267659, cur_epsilon : 0.45\n",
      "[EVAL] episode : 8950, reward mean : 0.5895000047050416, step mean : 22.05, eval_episode_rewards : [ 0.62  0.5  -1.05  0.82  0.72  0.84  0.76 -1.33  0.31  0.87  0.89  0.93\n",
      "  0.92  0.89  0.81  0.82  0.85  0.87  0.88  0.87]\n",
      "episode : 8960, reward mean : 0.3244285778998996, total_step : 267900, cur_epsilon : 0.45\n",
      "episode : 8970, reward mean : 0.32441918149906124, total_step : 268194, cur_epsilon : 0.45\n",
      "episode : 8980, reward mean : 0.324037868387267, total_step : 268521, cur_epsilon : 0.45\n",
      "episode : 8990, reward mean : 0.3244137995765525, total_step : 268869, cur_epsilon : 0.45\n",
      "episode : 9000, reward mean : 0.32458778424664503, total_step : 268998, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9000, reward mean : 0.6250000039115549, step mean : 18.5, eval_episode_rewards : [ 0.73  0.88  0.84  0.86  0.97  0.97  0.91  0.69  0.89  0.93  0.93 -1.11\n",
      "  0.77  0.98  0.66  0.92 -1.05  0.42  0.74  0.57]\n",
      "episode : 9010, reward mean : 0.3243840242286402, total_step : 269366, cur_epsilon : 0.45\n",
      "episode : 9020, reward mean : 0.32489801090326315, total_step : 269588, cur_epsilon : 0.45\n",
      "episode : 9030, reward mean : 0.32489480159574435, total_step : 269876, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 9040, reward mean : 0.3248108471803515, total_step : 270334, cur_epsilon : 0.45\n",
      "episode : 9050, reward mean : 0.32521879100317946, total_step : 270650, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9050, reward mean : 0.33550000814720987, step mean : 37.35, eval_episode_rewards : [ 0.85  0.33  0.7   0.94  0.64  0.97  0.74 -1.    0.92 -1.16  0.97  0.57\n",
      "  0.49  0.11  0.74  0.88 -1.2  -1.    0.62  0.6 ]\n",
      "episode : 9060, reward mean : 0.32543047005125625, total_step : 271042, cur_epsilon : 0.45\n",
      "episode : 9070, reward mean : 0.32569570658397845, total_step : 271286, cur_epsilon : 0.45\n",
      "episode : 9080, reward mean : 0.3256134425936111, total_step : 271445, cur_epsilon : 0.45\n",
      "episode : 9090, reward mean : 0.3255995664261978, total_step : 271742, cur_epsilon : 0.45\n",
      "episode : 9100, reward mean : 0.32551319328494943, total_step : 272105, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9100, reward mean : 0.37950000604614614, step mean : 28.0, eval_episode_rewards : [ 0.96  0.88 -1.    0.84  0.25 -1.12  0.71  0.71 -1.23  0.5   0.55  0.9\n",
      "  0.86  0.67  0.88  0.72  0.91  0.89 -1.04  0.75]\n",
      "episode : 9110, reward mean : 0.32580900757229025, total_step : 272520, cur_epsilon : 0.45\n",
      "episode : 9120, reward mean : 0.3260361906845662, total_step : 272797, cur_epsilon : 0.45\n",
      "episode : 9130, reward mean : 0.3261423942064795, total_step : 273083, cur_epsilon : 0.45\n",
      "episode : 9140, reward mean : 0.32617396708533036, total_step : 273338, cur_epsilon : 0.45\n",
      "episode : 9150, reward mean : 0.3261847059255659, total_step : 273612, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9150, reward mean : 0.151500007789582, step mean : 35.75, eval_episode_rewards : [-1.26  0.67 -1.32  0.96 -1.03  0.78 -1.   -1.    0.53  0.92  0.6   0.68\n",
      "  0.91  0.15 -1.05  0.94  0.36  0.6   0.68  0.91]\n",
      "episode : 9160, reward mean : 0.32633952612126466, total_step : 273853, cur_epsilon : 0.45\n",
      "episode : 9170, reward mean : 0.3261450446376348, total_step : 274115, cur_epsilon : 0.45\n",
      "episode : 9180, reward mean : 0.3266939062489225, total_step : 274295, cur_epsilon : 0.45\n",
      "episode : 9190, reward mean : 0.3269684504259528, total_step : 274526, cur_epsilon : 0.45\n",
      "episode : 9200, reward mean : 0.32725109341979514, total_step : 274749, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9200, reward mean : 0.3185000074096024, step mean : 34.1, eval_episode_rewards : [-1.11 -1.91  0.59  0.62  0.45  0.99 -1.21  0.91  0.94  0.61 -1.    0.19\n",
      "  0.9   0.98  0.84  0.71  0.91  0.52  0.76  0.68]\n",
      "synced target net\n",
      "episode : 9210, reward mean : 0.32746146140573595, total_step : 275137, cur_epsilon : 0.45\n",
      "episode : 9220, reward mean : 0.3274956680694898, total_step : 275388, cur_epsilon : 0.45\n",
      "episode : 9230, reward mean : 0.32776056984550483, total_step : 275725, cur_epsilon : 0.45\n",
      "episode : 9240, reward mean : 0.32789935711831725, total_step : 276178, cur_epsilon : 0.45\n",
      "episode : 9250, reward mean : 0.32781730376767954, total_step : 276536, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9250, reward mean : 0.4255000072531402, step mean : 33.4, eval_episode_rewards : [ 0.26  0.59 -1.31  0.71  0.66  0.74  0.99 -1.    0.66  0.62  0.99  0.73\n",
      " -1.1   0.58  0.87  0.75  0.66  0.84  0.82  0.45]\n",
      "episode : 9260, reward mean : 0.3280442829276023, total_step : 276808, cur_epsilon : 0.45\n",
      "episode : 9270, reward mean : 0.3280809126173457, total_step : 277056, cur_epsilon : 0.45\n",
      "episode : 9280, reward mean : 0.32838147198339795, total_step : 277259, cur_epsilon : 0.45\n",
      "episode : 9290, reward mean : 0.32816469968668516, total_step : 277641, cur_epsilon : 0.45\n",
      "episode : 9300, reward mean : 0.32844194195022985, total_step : 277865, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9300, reward mean : 0.11800000742077828, step mean : 34.15, eval_episode_rewards : [ 0.19  0.97  0.68  0.84 -1.71  0.69  0.94 -1.42  0.96 -1.12  0.56  0.17\n",
      "  0.88 -1.12  0.94  0.6   0.76 -1.    0.93 -1.38]\n",
      "episode : 9310, reward mean : 0.32864554889368097, total_step : 278157, cur_epsilon : 0.45\n",
      "episode : 9320, reward mean : 0.32880365453088906, total_step : 278291, cur_epsilon : 0.45\n",
      "episode : 9330, reward mean : 0.32914684462018656, total_step : 278652, cur_epsilon : 0.45\n",
      "episode : 9340, reward mean : 0.32906210496622823, total_step : 279012, cur_epsilon : 0.45\n",
      "episode : 9350, reward mean : 0.3294160492423384, total_step : 279162, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9350, reward mean : 0.2925000068731606, step mean : 31.65, eval_episode_rewards : [ 0.38  0.92 -1.13  0.93 -1.11  0.49  0.95  0.87  0.93 -1.55  0.8   0.48\n",
      "  0.58  0.86  0.9  -1.   -1.    0.9   0.85  0.8 ]\n",
      "episode : 9360, reward mean : 0.32952137398493725, total_step : 279544, cur_epsilon : 0.45\n",
      "episode : 9370, reward mean : 0.3294236991013795, total_step : 279916, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 9380, reward mean : 0.32919830071058737, total_step : 280307, cur_epsilon : 0.45\n",
      "episode : 9390, reward mean : 0.3296666731327947, total_step : 280548, cur_epsilon : 0.45\n",
      "episode : 9400, reward mean : 0.3299989426379746, total_step : 280916, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9400, reward mean : 0.5040000054985285, step mean : 25.55, eval_episode_rewards : [ 0.67  0.62  0.85 -1.17  0.77  0.73  0.66 -1.    0.97  0.95  0.87  0.66\n",
      "  0.88  0.83  0.77  0.9  -1.08  0.56  0.79  0.85]\n",
      "episode : 9410, reward mean : 0.32989798734097187, total_step : 281291, cur_epsilon : 0.45\n",
      "episode : 9420, reward mean : 0.32991083449666286, total_step : 281658, cur_epsilon : 0.45\n",
      "episode : 9430, reward mean : 0.3300296989389056, total_step : 281826, cur_epsilon : 0.45\n",
      "episode : 9440, reward mean : 0.33049470985651613, total_step : 282067, cur_epsilon : 0.45\n",
      "episode : 9450, reward mean : 0.3304052974711414, total_step : 282330, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9450, reward mean : 0.5960000045597553, step mean : 21.4, eval_episode_rewards : [ 0.89  0.98  0.66  0.45 -1.58  0.95  0.9   0.84  0.42  0.98  0.85  0.96\n",
      "  0.9   0.95  0.36  0.72 -1.07  0.95  0.89  0.92]\n",
      "episode : 9460, reward mean : 0.3304048690446761, total_step : 282610, cur_epsilon : 0.45\n",
      "episode : 9470, reward mean : 0.33053538133001786, total_step : 282766, cur_epsilon : 0.45\n",
      "episode : 9480, reward mean : 0.3303776435911624, total_step : 282995, cur_epsilon : 0.45\n",
      "episode : 9490, reward mean : 0.33016649750311583, total_step : 283275, cur_epsilon : 0.45\n",
      "episode : 9500, reward mean : 0.33023895382861557, total_step : 283585, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9500, reward mean : 0.1580000076442957, step mean : 35.1, eval_episode_rewards : [ 0.38  0.99  0.79 -1.    0.29  0.67  0.89  0.38  0.9   0.64  0.98  0.95\n",
      " -1.12  0.13 -1.02 -1.    0.72 -1.07 -1.01  0.67]\n",
      "episode : 9510, reward mean : 0.3302597330628525, total_step : 283845, cur_epsilon : 0.45\n",
      "episode : 9520, reward mean : 0.33017227536834337, total_step : 284208, cur_epsilon : 0.45\n",
      "episode : 9530, reward mean : 0.3305603422380882, total_step : 284318, cur_epsilon : 0.45\n",
      "episode : 9540, reward mean : 0.33085220771191537, total_step : 284519, cur_epsilon : 0.45\n",
      "episode : 9550, reward mean : 0.33112356666163467, total_step : 284739, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9550, reward mean : 0.5960000045597553, step mean : 21.4, eval_episode_rewards : [ 0.88 -1.23  0.83  0.86  0.87 -1.08  0.98  0.78  0.19  0.94  0.96  0.48\n",
      "  0.96  0.8   0.95  0.57  0.95  0.99  0.55  0.69]\n",
      "episode : 9560, reward mean : 0.331575320258534, total_step : 284986, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 9570, reward mean : 0.3316332352916976, total_step : 285308, cur_epsilon : 0.45\n",
      "episode : 9580, reward mean : 0.3316993801471523, total_step : 285622, cur_epsilon : 0.45\n",
      "episode : 9590, reward mean : 0.33197915139486694, total_step : 285931, cur_epsilon : 0.45\n",
      "episode : 9600, reward mean : 0.3321645897855827, total_step : 286231, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9600, reward mean : 0.6590000053867697, step mean : 25.1, eval_episode_rewards : [ 0.62  0.94  0.93  0.59  0.68  0.96  0.67  0.91  0.69  0.78 -1.73  0.98\n",
      "  0.94  0.86  0.73  0.85  0.87  0.94  0.91  0.06]\n",
      "episode : 9610, reward mean : 0.33218939250740187, total_step : 286485, cur_epsilon : 0.45\n",
      "episode : 9620, reward mean : 0.3323721478234303, total_step : 286787, cur_epsilon : 0.45\n",
      "episode : 9630, reward mean : 0.3326105983504254, total_step : 287035, cur_epsilon : 0.45\n",
      "episode : 9640, reward mean : 0.33302801474832244, total_step : 287310, cur_epsilon : 0.45\n",
      "episode : 9650, reward mean : 0.3329575194040111, total_step : 287655, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9650, reward mean : 0.6115000075660646, step mean : 34.8, eval_episode_rewards : [ 0.68  0.84  0.87  0.12  0.83  0.2   0.84  0.65  0.52  0.72  0.74  0.55\n",
      "  0.78 -1.    0.78  0.97  0.75  0.6   0.99  0.8 ]\n",
      "episode : 9660, reward mean : 0.3331677083134281, total_step : 287929, cur_epsilon : 0.45\n",
      "episode : 9670, reward mean : 0.33343227118457885, total_step : 288150, cur_epsilon : 0.45\n",
      "episode : 9680, reward mean : 0.3331797585149165, total_step : 288471, cur_epsilon : 0.45\n",
      "episode : 9690, reward mean : 0.33335501160895525, total_step : 288778, cur_epsilon : 0.45\n",
      "episode : 9700, reward mean : 0.3335618621185806, total_step : 289054, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9700, reward mean : 0.19000000581145288, step mean : 26.95, eval_episode_rewards : [ 0.92  0.77 -1.08 -1.04 -1.03  0.89  0.92 -1.    0.9   0.61 -1.12  0.85\n",
      "  0.75  0.81  0.71  0.52  0.15  0.45 -1.02  0.84]\n",
      "episode : 9710, reward mean : 0.33380742148313014, total_step : 289292, cur_epsilon : 0.45\n",
      "episode : 9720, reward mean : 0.33409980068332035, total_step : 289484, cur_epsilon : 0.45\n",
      "episode : 9730, reward mean : 0.33420144529077095, total_step : 289661, cur_epsilon : 0.45\n",
      "episode : 9740, reward mean : 0.33403696742834377, total_step : 289996, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 9750, reward mean : 0.3344338525963517, total_step : 290285, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9750, reward mean : 0.42950000828132034, step mean : 37.95, eval_episode_rewards : [ 0.79  0.95  0.61 -1.    0.65 -1.2   0.58  0.92  0.7   0.43  0.65  0.72\n",
      "  0.64  0.92  0.9   0.59  0.86  0.22 -1.    0.66]\n",
      "episode : 9760, reward mean : 0.3343452933269217, total_step : 290546, cur_epsilon : 0.45\n",
      "episode : 9770, reward mean : 0.3347789214866783, total_step : 290798, cur_epsilon : 0.45\n",
      "episode : 9780, reward mean : 0.334645200714827, total_step : 291103, cur_epsilon : 0.45\n",
      "episode : 9790, reward mean : 0.33516548141224484, total_step : 291269, cur_epsilon : 0.45\n",
      "episode : 9800, reward mean : 0.33534796562368924, total_step : 291664, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9800, reward mean : 0.4340000059455633, step mean : 27.5, eval_episode_rewards : [ 0.38 -1.    0.9   0.86  0.61 -1.15  0.93  0.79  0.95  0.9  -1.03  0.82\n",
      "  0.82  0.5   0.87  0.89  0.74  0.91  0.99 -1.  ]\n",
      "episode : 9810, reward mean : 0.33546178013724287, total_step : 291827, cur_epsilon : 0.45\n",
      "episode : 9820, reward mean : 0.3357576439088844, total_step : 292011, cur_epsilon : 0.45\n",
      "episode : 9830, reward mean : 0.3358321529270234, total_step : 292410, cur_epsilon : 0.45\n",
      "episode : 9840, reward mean : 0.3360802909879892, total_step : 292640, cur_epsilon : 0.45\n",
      "episode : 9850, reward mean : 0.33633706227096205, total_step : 292861, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9850, reward mean : 0.8395000035874546, step mean : 17.05, eval_episode_rewards : [0.94 0.97 0.69 0.83 0.72 0.81 0.67 0.97 0.79 0.88 0.98 0.92 0.91 0.86\n",
      " 0.97 0.88 0.76 0.99 0.93 0.32]\n",
      "episode : 9860, reward mean : 0.33663489487112835, total_step : 293140, cur_epsilon : 0.45\n",
      "episode : 9870, reward mean : 0.3367902799919927, total_step : 293559, cur_epsilon : 0.45\n",
      "episode : 9880, reward mean : 0.3366923141274375, total_step : 293828, cur_epsilon : 0.45\n",
      "episode : 9890, reward mean : 0.33704550694128077, total_step : 294152, cur_epsilon : 0.45\n",
      "episode : 9900, reward mean : 0.3366929357269346, total_step : 294374, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9900, reward mean : 0.6765000072307885, step mean : 33.35, eval_episode_rewards : [0.89 0.44 0.88 0.66 0.97 0.14 0.97 0.95 0.31 0.76 0.32 0.95 0.79 0.85\n",
      " 0.97 0.39 0.12 0.87 0.37 0.93]\n",
      "episode : 9910, reward mean : 0.3371574231828483, total_step : 294587, cur_epsilon : 0.45\n",
      "episode : 9920, reward mean : 0.33760887739819384, total_step : 294812, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 9930, reward mean : 0.3376384757164667, total_step : 295154, cur_epsilon : 0.45\n",
      "episode : 9940, reward mean : 0.33800101246800973, total_step : 295466, cur_epsilon : 0.45\n",
      "episode : 9950, reward mean : 0.33800000643348277, total_step : 295838, cur_epsilon : 0.45\n",
      "[EVAL] episode : 9950, reward mean : 0.4400000058114529, step mean : 27.0, eval_episode_rewards : [ 0.87  0.99 -1.15  0.8   0.62  0.91  0.96  0.52 -1.02 -1.88  0.96  0.81\n",
      "  0.97  0.95  0.44  0.84  0.51  0.88  0.48  0.34]\n",
      "episode : 9960, reward mean : 0.33813554860079237, total_step : 296074, cur_epsilon : 0.45\n",
      "episode : 9970, reward mean : 0.3384142491579758, total_step : 296268, cur_epsilon : 0.45\n",
      "episode : 9980, reward mean : 0.33860822286445624, total_step : 296645, cur_epsilon : 0.45\n",
      "episode : 9990, reward mean : 0.3389809874125996, total_step : 296944, cur_epsilon : 0.45\n",
      "episode : 10000, reward mean : 0.339295006432943, total_step : 297301, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10000, reward mean : 0.5680000051856041, step mean : 24.2, eval_episode_rewards : [ 0.85  0.84  0.7   0.43  0.83  0.7   0.91  0.89  0.85  0.72 -1.25  0.75\n",
      " -1.22  0.34  0.83  0.89  0.9   0.8   0.92  0.68]\n",
      "episode : 10010, reward mean : 0.3394165898529026, total_step : 297749, cur_epsilon : 0.45\n",
      "episode : 10020, reward mean : 0.33955689266457884, total_step : 298079, cur_epsilon : 0.45\n",
      "episode : 10030, reward mean : 0.3397916315602564, total_step : 298314, cur_epsilon : 0.45\n",
      "episode : 10040, reward mean : 0.3402390502586189, total_step : 298535, cur_epsilon : 0.45\n",
      "episode : 10050, reward mean : 0.34064279250281665, total_step : 298799, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10050, reward mean : 0.4030000066384673, step mean : 30.7, eval_episode_rewards : [ 0.9   0.54  0.68  0.96 -1.11 -1.66  0.75  0.77  0.66  0.94  0.78  0.84\n",
      "  0.68  0.19  0.84  0.88  0.84  0.74 -1.77  0.61]\n",
      "episode : 10060, reward mean : 0.34086183545792526, total_step : 299048, cur_epsilon : 0.45\n",
      "episode : 10070, reward mean : 0.3408589935218758, total_step : 299320, cur_epsilon : 0.45\n",
      "episode : 10080, reward mean : 0.34129266516020934, total_step : 299552, cur_epsilon : 0.45\n",
      "episode : 10090, reward mean : 0.34158474379103737, total_step : 299726, cur_epsilon : 0.45\n",
      "episode : 10100, reward mean : 0.341625749000007, total_step : 299953, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10100, reward mean : 0.6140000075101852, step mean : 34.55, eval_episode_rewards : [ 0.72  0.81  0.75  0.73  0.66  0.78  0.85  0.84  0.42  0.76  0.59  0.95\n",
      "  0.91  0.52  0.61  0.2   0.86  0.5   0.82 -1.  ]\n",
      "synced target net\n",
      "episode : 10110, reward mean : 0.3420623209642506, total_step : 300180, cur_epsilon : 0.45\n",
      "episode : 10120, reward mean : 0.3423675953583578, total_step : 300539, cur_epsilon : 0.45\n",
      "episode : 10130, reward mean : 0.342607114025137, total_step : 300764, cur_epsilon : 0.45\n",
      "episode : 10140, reward mean : 0.34280474015109286, total_step : 301031, cur_epsilon : 0.45\n",
      "episode : 10150, reward mean : 0.34300690297422737, total_step : 301293, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10150, reward mean : 0.46550000524148344, step mean : 24.45, eval_episode_rewards : [ 0.95  0.57  0.89  0.96  0.58  0.7   0.52 -1.03 -1.03 -1.02  0.99  0.74\n",
      "  0.74  0.39  0.98  0.37  0.85  0.96  0.45  0.75]\n",
      "episode : 10160, reward mean : 0.3434429198067166, total_step : 301517, cur_epsilon : 0.45\n",
      "episode : 10170, reward mean : 0.34348673208515024, total_step : 301838, cur_epsilon : 0.45\n",
      "episode : 10180, reward mean : 0.34368959384913644, total_step : 302197, cur_epsilon : 0.45\n",
      "episode : 10190, reward mean : 0.34392836755955036, total_step : 302420, cur_epsilon : 0.45\n",
      "episode : 10200, reward mean : 0.34413530053790004, total_step : 302675, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10200, reward mean : 0.3820000048726797, step mean : 22.8, eval_episode_rewards : [ 0.94  0.86 -1.24  0.92 -1.03  0.48  0.87  0.77 -1.06  0.9   0.69  0.78\n",
      "  0.89  0.68  0.77  0.83  0.76  0.78 -1.1   0.15]\n",
      "episode : 10210, reward mean : 0.34360039819115873, total_step : 302887, cur_epsilon : 0.45\n",
      "episode : 10220, reward mean : 0.3435548009414685, total_step : 303299, cur_epsilon : 0.45\n",
      "episode : 10230, reward mean : 0.3435014726970989, total_step : 303620, cur_epsilon : 0.45\n",
      "episode : 10240, reward mean : 0.3438886782958434, total_step : 303890, cur_epsilon : 0.45\n",
      "episode : 10250, reward mean : 0.344053664957423, total_step : 304187, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10250, reward mean : 0.31100000757724044, step mean : 34.85, eval_episode_rewards : [ 0.51 -1.65  0.32 -1.21  0.8   0.9   0.68 -1.    0.89  0.39  0.91  0.55\n",
      "  0.6   0.78 -1.13  0.93  0.57  0.7   0.77  0.91]\n",
      "episode : 10260, reward mean : 0.34422710193720507, total_step : 304475, cur_epsilon : 0.45\n",
      "episode : 10270, reward mean : 0.34366602395056317, total_step : 304915, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 10280, reward mean : 0.34411090136349837, total_step : 305124, cur_epsilon : 0.45\n",
      "episode : 10290, reward mean : 0.34448785870521537, total_step : 305402, cur_epsilon : 0.45\n",
      "episode : 10300, reward mean : 0.3447844724378829, total_step : 305562, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10300, reward mean : 0.36000000424683093, step mean : 19.95, eval_episode_rewards : [ 0.9   0.72  0.85  0.78 -1.08  0.87  0.81  0.84  0.97  0.97  0.96  0.68\n",
      " -1.12  0.91 -1.   -1.01 -1.04  0.75  0.58  0.86]\n",
      "episode : 10310, reward mean : 0.3450029162135405, total_step : 305802, cur_epsilon : 0.45\n",
      "episode : 10320, reward mean : 0.3450407040922708, total_step : 306127, cur_epsilon : 0.45\n",
      "episode : 10330, reward mean : 0.34519361726027287, total_step : 306434, cur_epsilon : 0.45\n",
      "episode : 10340, reward mean : 0.3446972984872845, total_step : 306711, cur_epsilon : 0.45\n",
      "episode : 10350, reward mean : 0.3451082189768049, total_step : 306951, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10350, reward mean : 0.2805000049062073, step mean : 22.85, eval_episode_rewards : [ 0.97 -1.    0.94  0.71  0.6   0.85  0.95 -1.   -1.02  0.98 -1.25 -1.31\n",
      "  0.99  0.63  0.91  0.95  0.81  0.97 -1.04  0.97]\n",
      "episode : 10360, reward mean : 0.3452220141395035, total_step : 307298, cur_epsilon : 0.45\n",
      "episode : 10370, reward mean : 0.34543009320527257, total_step : 307547, cur_epsilon : 0.45\n",
      "episode : 10380, reward mean : 0.3457080989015542, total_step : 307822, cur_epsilon : 0.45\n",
      "episode : 10390, reward mean : 0.3461270516503448, total_step : 308051, cur_epsilon : 0.45\n",
      "episode : 10400, reward mean : 0.3462653910304205, total_step : 308371, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10400, reward mean : 0.6175000040791929, step mean : 19.25, eval_episode_rewards : [ 0.93  0.45  0.81  0.84  0.93  0.73  0.68  0.86 -1.04 -1.08  0.87  0.9\n",
      "  0.93  0.89  0.9   0.92  0.76  0.95  0.23  0.89]\n",
      "episode : 10410, reward mean : 0.34597503042974, total_step : 308636, cur_epsilon : 0.45\n",
      "episode : 10420, reward mean : 0.34603551504954755, total_step : 308837, cur_epsilon : 0.45\n",
      "episode : 10430, reward mean : 0.3464918568414258, total_step : 309025, cur_epsilon : 0.45\n",
      "episode : 10440, reward mean : 0.34684196043299276, total_step : 309323, cur_epsilon : 0.45\n",
      "episode : 10450, reward mean : 0.346700484877069, total_step : 309534, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10450, reward mean : 0.6455000045709312, step mean : 21.4, eval_episode_rewards : [ 0.75  0.92  0.82 -1.    0.99  0.89  0.98  0.85  0.8   0.92  0.89  0.75\n",
      " -1.02  0.83  0.97  0.48  0.81  0.43  0.92  0.93]\n",
      "episode : 10460, reward mean : 0.34709274063285717, total_step : 309787, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 10470, reward mean : 0.34735053171970326, total_step : 310180, cur_epsilon : 0.45\n",
      "episode : 10480, reward mean : 0.3473578308358831, total_step : 310435, cur_epsilon : 0.45\n",
      "episode : 10490, reward mean : 0.3473555831474787, total_step : 310700, cur_epsilon : 0.45\n",
      "episode : 10500, reward mean : 0.34717714926387583, total_step : 310950, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10500, reward mean : 0.6785000049509108, step mean : 23.15, eval_episode_rewards : [ 0.87  0.41  0.51  0.65  0.92  0.98  0.97  0.86  0.86  0.54  0.71  0.78\n",
      "  0.63  0.76  0.87  0.86  0.56 -1.12  0.97  0.98]\n",
      "episode : 10510, reward mean : 0.34700476378166994, total_step : 311293, cur_epsilon : 0.45\n",
      "episode : 10520, reward mean : 0.3468754816908361, total_step : 311492, cur_epsilon : 0.45\n",
      "episode : 10530, reward mean : 0.34692878133253835, total_step : 311699, cur_epsilon : 0.45\n",
      "episode : 10540, reward mean : 0.34694308040653, total_step : 311947, cur_epsilon : 0.45\n",
      "episode : 10550, reward mean : 0.3467952670667197, total_step : 312265, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10550, reward mean : 0.32300000730901957, step mean : 33.65, eval_episode_rewards : [ 0.32  0.52  0.77  0.91  0.92  0.78  0.37  0.75  0.73  0.79 -1.43 -1.66\n",
      " -1.    0.76  0.49  0.91 -1.13  0.94  0.89  0.83]\n",
      "episode : 10560, reward mean : 0.3469697033725397, total_step : 312544, cur_epsilon : 0.45\n",
      "episode : 10570, reward mean : 0.3473699212550074, total_step : 312784, cur_epsilon : 0.45\n",
      "episode : 10580, reward mean : 0.3474139950599078, total_step : 313099, cur_epsilon : 0.45\n",
      "episode : 10590, reward mean : 0.34719830668523444, total_step : 313390, cur_epsilon : 0.45\n",
      "episode : 10600, reward mean : 0.34742076111746567, total_step : 313617, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10600, reward mean : 0.5980000045150519, step mean : 21.2, eval_episode_rewards : [ 0.97  0.92  0.46  0.99  0.94 -1.03  0.97  0.96  0.9   0.43  0.64  0.91\n",
      "  0.85  0.99  0.61  0.96 -1.38  0.89  0.67  0.31]\n",
      "episode : 10610, reward mean : 0.3476032109239422, total_step : 313886, cur_epsilon : 0.45\n",
      "episode : 10620, reward mean : 0.34794727570312395, total_step : 314183, cur_epsilon : 0.45\n",
      "episode : 10630, reward mean : 0.3479068737566429, total_step : 314488, cur_epsilon : 0.45\n",
      "episode : 10640, reward mean : 0.34836936729970647, total_step : 314658, cur_epsilon : 0.45\n",
      "episode : 10650, reward mean : 0.3483718373825852, total_step : 314917, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10650, reward mean : 0.4240000072866678, step mean : 33.55, eval_episode_rewards : [ 0.99  0.69  0.73  0.93  0.67  0.93  0.94  0.95  0.76  0.82 -1.06  0.18\n",
      "  0.22  0.84  0.44  0.86  0.28 -1.02  0.33 -1.  ]\n",
      "synced target net\n",
      "episode : 10660, reward mean : 0.3486866855675854, total_step : 315043, cur_epsilon : 0.45\n",
      "episode : 10670, reward mean : 0.3487010373219834, total_step : 315388, cur_epsilon : 0.45\n",
      "episode : 10680, reward mean : 0.34861049328473737, total_step : 315645, cur_epsilon : 0.45\n",
      "episode : 10690, reward mean : 0.3488512692539156, total_step : 315849, cur_epsilon : 0.45\n",
      "episode : 10700, reward mean : 0.34909813723465966, total_step : 316246, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10700, reward mean : 0.5850000048056245, step mean : 22.5, eval_episode_rewards : [ 0.96 -1.29  0.96 -1.46  0.7   0.92  0.55  0.86  0.85  0.87  0.81  0.89\n",
      "  0.94  0.9   0.4   0.88  0.98  0.93  0.5   0.55]\n",
      "episode : 10710, reward mean : 0.3489738626047329, total_step : 316638, cur_epsilon : 0.45\n",
      "episode : 10720, reward mean : 0.3489589616190642, total_step : 316915, cur_epsilon : 0.45\n",
      "episode : 10730, reward mean : 0.3493644052753633, total_step : 317141, cur_epsilon : 0.45\n",
      "episode : 10740, reward mean : 0.3493407885185001, total_step : 317526, cur_epsilon : 0.45\n",
      "episode : 10750, reward mean : 0.34946233197769455, total_step : 317856, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10750, reward mean : 0.6115000064484775, step mean : 29.75, eval_episode_rewards : [ 0.9   0.79  0.77  0.82  0.61  0.77  0.99  0.68  0.96 -1.    0.39  0.73\n",
      "  0.87  0.93  0.96  0.93  0.54  0.98  0.61 -1.  ]\n",
      "episode : 10760, reward mean : 0.3498373669902811, total_step : 318113, cur_epsilon : 0.45\n",
      "episode : 10770, reward mean : 0.34957753657281204, total_step : 318453, cur_epsilon : 0.45\n",
      "episode : 10780, reward mean : 0.34961781715542245, total_step : 318670, cur_epsilon : 0.45\n",
      "episode : 10790, reward mean : 0.3498304049146946, total_step : 319101, cur_epsilon : 0.45\n",
      "episode : 10800, reward mean : 0.3498416730632178, total_step : 319349, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10800, reward mean : 0.4785000060684979, step mean : 28.1, eval_episode_rewards : [-1.32  0.64  0.92  0.81  0.93  0.99  0.86  0.37  0.77  0.32  0.77 -1.\n",
      "  0.62  0.86  0.99  0.33  0.87  0.98 -1.12  0.98]\n",
      "episode : 10810, reward mean : 0.35022849853234117, total_step : 319591, cur_epsilon : 0.45\n",
      "episode : 10820, reward mean : 0.350264331717354, total_step : 319812, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 10830, reward mean : 0.3502871716763078, total_step : 320146, cur_epsilon : 0.45\n",
      "episode : 10840, reward mean : 0.35029151930896446, total_step : 320401, cur_epsilon : 0.45\n",
      "episode : 10850, reward mean : 0.35068019072443657, total_step : 320639, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10850, reward mean : 0.41450000638142226, step mean : 29.55, eval_episode_rewards : [ 0.44  0.97  0.77  0.84  0.93  0.84 -1.21  0.41 -1.19  0.8   0.97  0.36\n",
      "  0.93  0.9   0.91  0.49  0.12  0.27 -1.06  0.8 ]\n",
      "episode : 10860, reward mean : 0.350893192394923, total_step : 320867, cur_epsilon : 0.45\n",
      "episode : 10870, reward mean : 0.35085741209711024, total_step : 321264, cur_epsilon : 0.45\n",
      "episode : 10880, reward mean : 0.35089890345047664, total_step : 321478, cur_epsilon : 0.45\n",
      "episode : 10890, reward mean : 0.3509035876600621, total_step : 321831, cur_epsilon : 0.45\n",
      "episode : 10900, reward mean : 0.3503256944660952, total_step : 322120, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10900, reward mean : 0.6915000046603381, step mean : 21.85, eval_episode_rewards : [ 0.96  0.96  0.97  0.6   0.97  0.48  0.56  0.97  0.78 -1.17  0.56  0.87\n",
      "  0.96  0.7   0.94  0.56  0.7   0.81  0.93  0.72]\n",
      "episode : 10910, reward mean : 0.35025756826332327, total_step : 322454, cur_epsilon : 0.45\n",
      "episode : 10920, reward mean : 0.35027015291464825, total_step : 322799, cur_epsilon : 0.45\n",
      "episode : 10930, reward mean : 0.35059378498573834, total_step : 323105, cur_epsilon : 0.45\n",
      "episode : 10940, reward mean : 0.3508628948746586, total_step : 323270, cur_epsilon : 0.45\n",
      "episode : 10950, reward mean : 0.3506913305939809, total_step : 323616, cur_epsilon : 0.45\n",
      "[EVAL] episode : 10950, reward mean : 0.6785000049509108, step mean : 23.15, eval_episode_rewards : [ 0.92  0.6   0.59  0.61  0.81  0.86  0.62  0.82  0.94  0.57  0.71  0.93\n",
      "  0.91  0.87  0.72  0.9   0.88  0.72 -1.06  0.65]\n",
      "episode : 10960, reward mean : 0.3511897874112329, total_step : 323729, cur_epsilon : 0.45\n",
      "episode : 10970, reward mean : 0.35144485597878206, total_step : 324007, cur_epsilon : 0.45\n",
      "episode : 10980, reward mean : 0.3519244144002284, total_step : 324139, cur_epsilon : 0.45\n",
      "episode : 10990, reward mean : 0.3517561483318096, total_step : 324382, cur_epsilon : 0.45\n",
      "episode : 11000, reward mean : 0.3517245518411086, total_step : 324774, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11000, reward mean : 0.6330000059679151, step mean : 27.7, eval_episode_rewards : [ 0.74  0.91  0.72  0.04  0.96  0.83  0.9   0.98  0.83  0.45  0.4   0.93\n",
      "  0.56  0.9  -1.06  0.82  0.99  0.64  0.92  0.2 ]\n",
      "synced target net\n",
      "episode : 11010, reward mean : 0.35193370302460686, total_step : 325002, cur_epsilon : 0.45\n",
      "episode : 11020, reward mean : 0.3523629827894771, total_step : 325187, cur_epsilon : 0.45\n",
      "episode : 11030, reward mean : 0.35225295289212727, total_step : 325465, cur_epsilon : 0.45\n",
      "episode : 11040, reward mean : 0.3523876875423003, total_step : 325774, cur_epsilon : 0.45\n",
      "episode : 11050, reward mean : 0.3523701421311336, total_step : 326150, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11050, reward mean : 0.2730000061914325, step mean : 28.65, eval_episode_rewards : [ 0.91  0.84  0.9   0.96  0.82 -1.27  0.79  0.92  0.89  0.87  0.77 -1.84\n",
      "  0.87  0.87  0.57 -1.54  0.77 -1.13  0.49 -1.  ]\n",
      "episode : 11060, reward mean : 0.3525036230213455, total_step : 326460, cur_epsilon : 0.45\n",
      "episode : 11070, reward mean : 0.35239567034108044, total_step : 326736, cur_epsilon : 0.45\n",
      "episode : 11080, reward mean : 0.3523673349041626, total_step : 327025, cur_epsilon : 0.45\n",
      "episode : 11090, reward mean : 0.3527736763554693, total_step : 327232, cur_epsilon : 0.45\n",
      "episode : 11100, reward mean : 0.35271351989660715, total_step : 327556, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11100, reward mean : 0.09750000787898898, step mean : 36.2, eval_episode_rewards : [ 0.84  0.83 -1.    0.03  0.72 -1.1   0.56  0.4   0.66  0.9   0.49 -1.25\n",
      " -1.24  0.72  0.77  0.42  0.59 -1.22  0.85 -1.02]\n",
      "episode : 11110, reward mean : 0.35271197758148937, total_step : 327914, cur_epsilon : 0.45\n",
      "episode : 11120, reward mean : 0.3527500063827489, total_step : 328129, cur_epsilon : 0.45\n",
      "episode : 11130, reward mean : 0.35300539721586716, total_step : 328302, cur_epsilon : 0.45\n",
      "episode : 11140, reward mean : 0.3532450692188095, total_step : 328692, cur_epsilon : 0.45\n",
      "episode : 11150, reward mean : 0.35325740548427065, total_step : 328935, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11150, reward mean : 0.5090000042691827, step mean : 20.1, eval_episode_rewards : [ 0.78  0.67  0.93  0.68  0.91  0.85 -1.14  0.55  0.95  0.16  0.85  0.93\n",
      "  0.97  0.78  0.66 -1.01  0.78  0.91 -1.01  0.98]\n",
      "episode : 11160, reward mean : 0.3534220493914339, total_step : 329208, cur_epsilon : 0.45\n",
      "episode : 11170, reward mean : 0.3532793259871798, total_step : 329523, cur_epsilon : 0.45\n",
      "episode : 11180, reward mean : 0.35352862892299225, total_step : 329901, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 11190, reward mean : 0.35375514489742554, total_step : 330104, cur_epsilon : 0.45\n",
      "episode : 11200, reward mean : 0.3540169706639634, total_step : 330267, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11200, reward mean : 0.36400000415742395, step mean : 19.55, eval_episode_rewards : [ 0.8  -1.02  0.79 -1.    0.31  0.95  0.98  0.91 -1.38  0.97  0.76  0.93\n",
      "  0.72  0.93  0.98 -1.02  0.95 -1.13  0.9   0.95]\n",
      "episode : 11210, reward mean : 0.35395361922455354, total_step : 330594, cur_epsilon : 0.45\n",
      "episode : 11220, reward mean : 0.3538092755395488, total_step : 330812, cur_epsilon : 0.45\n",
      "episode : 11230, reward mean : 0.3538041025476459, total_step : 331074, cur_epsilon : 0.45\n",
      "episode : 11240, reward mean : 0.3540391522860718, total_step : 331466, cur_epsilon : 0.45\n",
      "episode : 11250, reward mean : 0.3542684508230951, total_step : 331763, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11250, reward mean : 0.4045000054873526, step mean : 25.5, eval_episode_rewards : [ 0.76  0.82  0.78  0.98  0.65  0.53  0.69  0.99 -1.01 -1.    0.76  0.94\n",
      "  0.67  0.6   0.77 -1.19  0.88 -1.06  0.69  0.84]\n",
      "episode : 11260, reward mean : 0.354263771919709, total_step : 332024, cur_epsilon : 0.45\n",
      "episode : 11270, reward mean : 0.3544782672459202, total_step : 332238, cur_epsilon : 0.45\n",
      "episode : 11280, reward mean : 0.3542358219778786, total_step : 332466, cur_epsilon : 0.45\n",
      "episode : 11290, reward mean : 0.3541346387945914, total_step : 332836, cur_epsilon : 0.45\n",
      "episode : 11300, reward mean : 0.3545805373471395, total_step : 332988, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11300, reward mean : 0.6700000073760748, step mean : 34.0, eval_episode_rewards : [0.14 0.74 0.94 0.98 0.85 0.73 0.63 0.72 0.01 0.88 0.5  0.23 0.94 0.83\n",
      " 0.31 0.97 0.54 0.67 0.96 0.83]\n",
      "episode : 11310, reward mean : 0.3547347543840185, total_step : 333269, cur_epsilon : 0.45\n",
      "episode : 11320, reward mean : 0.35495937032914676, total_step : 333470, cur_epsilon : 0.45\n",
      "episode : 11330, reward mean : 0.3551482852760436, total_step : 333711, cur_epsilon : 0.45\n",
      "episode : 11340, reward mean : 0.3551684367043382, total_step : 333943, cur_epsilon : 0.45\n",
      "episode : 11350, reward mean : 0.3554264380886232, total_step : 334305, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11350, reward mean : 0.5560000065714121, step mean : 30.35, eval_episode_rewards : [ 0.6   0.65  0.78  0.52 -1.    0.52  0.97  0.76  0.25 -1.01  0.93  0.62\n",
      "  0.39  0.93  0.77  0.88  0.99  0.94  0.87  0.76]\n",
      "episode : 11360, reward mean : 0.3551822246772132, total_step : 334437, cur_epsilon : 0.45\n",
      "episode : 11370, reward mean : 0.3549164531564073, total_step : 334693, cur_epsilon : 0.45\n",
      "episode : 11380, reward mean : 0.3550632752592697, total_step : 334981, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 11390, reward mean : 0.35510975175534715, total_step : 335183, cur_epsilon : 0.45\n",
      "episode : 11400, reward mean : 0.3551535151348237, total_step : 335388, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11400, reward mean : 0.41550000412389637, step mean : 19.45, eval_episode_rewards : [ 0.83  0.87 -1.17  0.94  0.77  0.63  0.89  0.7   0.88  0.87  0.99  0.81\n",
      " -1.11  0.89  0.74 -1.18  0.81 -1.12  0.42  0.85]\n",
      "episode : 11410, reward mean : 0.3553207776155796, total_step : 335652, cur_epsilon : 0.45\n",
      "episode : 11420, reward mean : 0.35574519024776935, total_step : 335822, cur_epsilon : 0.45\n",
      "episode : 11430, reward mean : 0.3560411262202907, total_step : 336138, cur_epsilon : 0.45\n",
      "episode : 11440, reward mean : 0.35642570565939863, total_step : 336352, cur_epsilon : 0.45\n",
      "episode : 11450, reward mean : 0.3565318840886893, total_step : 336682, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11450, reward mean : 0.5510000055655837, step mean : 25.9, eval_episode_rewards : [ 0.94  0.73  0.93 -1.28 -1.01  0.63  0.89  0.23  0.46  0.86  0.76  0.64\n",
      "  0.76  0.76  0.83  0.82  0.5   0.98  0.67  0.92]\n",
      "episode : 11460, reward mean : 0.35656894178732734, total_step : 336992, cur_epsilon : 0.45\n",
      "episode : 11470, reward mean : 0.35692589999433955, total_step : 337236, cur_epsilon : 0.45\n",
      "episode : 11480, reward mean : 0.35710714921494785, total_step : 337481, cur_epsilon : 0.45\n",
      "episode : 11490, reward mean : 0.35733943194316276, total_step : 337667, cur_epsilon : 0.45\n",
      "episode : 11500, reward mean : 0.3570982672220663, total_step : 337797, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11500, reward mean : 0.4505000044591725, step mean : 20.9, eval_episode_rewards : [-1.03  0.93  0.51  0.81 -1.    0.69  0.84  0.99  0.93  0.81 -1.02  0.76\n",
      "  0.8   0.87  0.78  0.87 -1.01  0.9   0.66  0.92]\n",
      "episode : 11510, reward mean : 0.3570729863715822, total_step : 338178, cur_epsilon : 0.45\n",
      "episode : 11520, reward mean : 0.357304693852105, total_step : 338364, cur_epsilon : 0.45\n",
      "episode : 11530, reward mean : 0.357230708864255, total_step : 338502, cur_epsilon : 0.45\n",
      "episode : 11540, reward mean : 0.35734402714490865, total_step : 338723, cur_epsilon : 0.45\n",
      "episode : 11550, reward mean : 0.35754892409702527, total_step : 339038, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11550, reward mean : 0.6050000054761767, step mean : 25.45, eval_episode_rewards : [ 0.99  0.87 -1.    0.95  0.95  0.72  0.82  0.23  0.92  0.94  0.96  0.92\n",
      " -1.04  0.26  0.92  0.36  0.95  0.73  0.8   0.85]\n",
      "episode : 11560, reward mean : 0.357452428492856, total_step : 339301, cur_epsilon : 0.45\n",
      "episode : 11570, reward mean : 0.3574096865551152, total_step : 339603, cur_epsilon : 0.45\n",
      "episode : 11580, reward mean : 0.3577936160178704, total_step : 339811, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 11590, reward mean : 0.3579042341291737, total_step : 340135, cur_epsilon : 0.45\n",
      "episode : 11600, reward mean : 0.358030178760628, total_step : 340441, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11600, reward mean : 0.5315000060014426, step mean : 27.85, eval_episode_rewards : [ 0.5   0.57  0.67 -1.18  0.96  0.93  0.85  0.78  0.91  0.52  0.94  0.83\n",
      "  0.82  0.3   0.97  0.08  0.83  0.71  0.75 -1.11]\n",
      "episode : 11610, reward mean : 0.3577993172861822, total_step : 340761, cur_epsilon : 0.45\n",
      "episode : 11620, reward mean : 0.3579879581533924, total_step : 340994, cur_epsilon : 0.45\n",
      "episode : 11630, reward mean : 0.358030100929313, total_step : 341296, cur_epsilon : 0.45\n",
      "episode : 11640, reward mean : 0.35834107163835865, total_step : 341586, cur_epsilon : 0.45\n",
      "episode : 11650, reward mean : 0.3586918518388476, total_step : 341829, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11650, reward mean : 0.7175000040791929, step mean : 19.25, eval_episode_rewards : [-1.01  0.99  0.97  0.88  0.82  0.87  0.91  0.97  0.55  0.7   0.61  0.73\n",
      "  0.83  0.98  0.68  0.81  0.82  0.61  0.79  0.84]\n",
      "episode : 11660, reward mean : 0.35872813670578096, total_step : 342137, cur_epsilon : 0.45\n",
      "episode : 11670, reward mean : 0.3584430226245328, total_step : 342321, cur_epsilon : 0.45\n",
      "episode : 11680, reward mean : 0.3584152460693101, total_step : 342605, cur_epsilon : 0.45\n",
      "episode : 11690, reward mean : 0.3585876881217439, total_step : 342855, cur_epsilon : 0.45\n",
      "episode : 11700, reward mean : 0.35863932258170894, total_step : 343145, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11700, reward mean : 0.737000003643334, step mean : 17.3, eval_episode_rewards : [ 0.96  0.83  0.86  0.59  0.95  0.8   0.84  0.76  0.65  0.92  0.93  0.92\n",
      "  0.97  0.77  0.88 -1.4   0.72  0.97  0.95  0.87]\n",
      "episode : 11710, reward mean : 0.35877199609669275, total_step : 343540, cur_epsilon : 0.45\n",
      "episode : 11720, reward mean : 0.35897184934629184, total_step : 343757, cur_epsilon : 0.45\n",
      "episode : 11730, reward mean : 0.3593529475176388, total_step : 343961, cur_epsilon : 0.45\n",
      "episode : 11740, reward mean : 0.35955281724448884, total_step : 344276, cur_epsilon : 0.45\n",
      "episode : 11750, reward mean : 0.35979660208380604, total_step : 344440, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11750, reward mean : 0.2990000067278743, step mean : 31.1, eval_episode_rewards : [ 0.69  0.92  0.95  0.99  0.78  0.73  0.81 -1.28 -1.07  0.91 -1.91 -1.36\n",
      "  0.6   0.12  0.28  0.36  0.9   0.9   0.87  0.79]\n",
      "episode : 11760, reward mean : 0.3599821491950971, total_step : 344672, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 11770, reward mean : 0.35994053310291335, total_step : 345070, cur_epsilon : 0.45\n",
      "episode : 11780, reward mean : 0.36022920837761846, total_step : 345380, cur_epsilon : 0.45\n",
      "episode : 11790, reward mean : 0.3602892344994463, total_step : 345658, cur_epsilon : 0.45\n",
      "episode : 11800, reward mean : 0.3604889893893279, total_step : 345872, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11800, reward mean : 0.3750000050291419, step mean : 23.5, eval_episode_rewards : [ 0.91  0.73  0.57  0.42 -1.04  0.84  0.87  0.62  0.93  0.7  -1.21 -1.24\n",
      "  0.59  0.62  0.82  0.68  0.92  0.96  0.88 -1.07]\n",
      "episode : 11810, reward mean : 0.360690099478342, total_step : 346084, cur_epsilon : 0.45\n",
      "episode : 11820, reward mean : 0.36079188450641436, total_step : 346213, cur_epsilon : 0.45\n",
      "episode : 11830, reward mean : 0.3609154754802459, total_step : 346516, cur_epsilon : 0.45\n",
      "episode : 11840, reward mean : 0.3611883509292179, total_step : 346842, cur_epsilon : 0.45\n",
      "episode : 11850, reward mean : 0.36121772785211287, total_step : 347056, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11850, reward mean : 0.5285000038333237, step mean : 18.15, eval_episode_rewards : [-1.53 -1.08  0.91 -1.02  0.83  0.54  0.89  0.98  0.84  0.72  0.68  0.89\n",
      "  0.89  0.8   0.98  0.85  0.64  0.92  0.98  0.86]\n",
      "episode : 11860, reward mean : 0.361537948997085, total_step : 347325, cur_epsilon : 0.45\n",
      "episode : 11870, reward mean : 0.3615880434008248, total_step : 347613, cur_epsilon : 0.45\n",
      "episode : 11880, reward mean : 0.36192172350352536, total_step : 347865, cur_epsilon : 0.45\n",
      "episode : 11890, reward mean : 0.3618267514952047, total_step : 348125, cur_epsilon : 0.45\n",
      "episode : 11900, reward mean : 0.36185630885070236, total_step : 348338, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11900, reward mean : 0.6150000041350723, step mean : 19.5, eval_episode_rewards : [ 0.91  0.69 -1.05  0.88  0.71  0.86  0.96  0.96  0.44  0.92  0.8   0.84\n",
      "  0.97  0.74  0.66  0.82  0.88  0.55  0.88 -1.12]\n",
      "episode : 11910, reward mean : 0.3619009299233468, total_step : 348632, cur_epsilon : 0.45\n",
      "episode : 11920, reward mean : 0.3619454761265446, total_step : 348827, cur_epsilon : 0.45\n",
      "episode : 11930, reward mean : 0.3622782963522447, total_step : 349078, cur_epsilon : 0.45\n",
      "episode : 11940, reward mean : 0.3621574602615544, total_step : 349270, cur_epsilon : 0.45\n",
      "episode : 11950, reward mean : 0.3625213452358438, total_step : 349483, cur_epsilon : 0.45\n",
      "[EVAL] episode : 11950, reward mean : 0.6620000053197146, step mean : 24.8, eval_episode_rewards : [ 0.83  0.93  0.89  0.93  0.81 -1.47  0.92  0.56  0.84  0.95  0.95  0.89\n",
      "  0.89  0.11  0.63  0.74  0.82  0.74  0.81  0.47]\n",
      "episode : 11960, reward mean : 0.36268144445011025, total_step : 349739, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 11970, reward mean : 0.362878034727721, total_step : 350050, cur_epsilon : 0.45\n",
      "episode : 11980, reward mean : 0.3627604403781809, total_step : 350238, cur_epsilon : 0.45\n",
      "episode : 11990, reward mean : 0.36290075694668333, total_step : 350517, cur_epsilon : 0.45\n",
      "episode : 12000, reward mean : 0.3631408396520031, total_step : 350676, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12000, reward mean : 0.49400000683963297, step mean : 31.6, eval_episode_rewards : [ 0.8   0.81  0.76  0.51  0.09  0.78  0.84  0.63  0.49  0.45  0.46  0.99\n",
      "  0.91  0.52  0.96  0.9   0.83  0.77 -1.58 -1.04]\n",
      "episode : 12010, reward mean : 0.36351207958923637, total_step : 350877, cur_epsilon : 0.45\n",
      "episode : 12020, reward mean : 0.3638510878467483, total_step : 351116, cur_epsilon : 0.45\n",
      "episode : 12030, reward mean : 0.3638088176229772, total_step : 351512, cur_epsilon : 0.45\n",
      "episode : 12040, reward mean : 0.36400415914031087, total_step : 351723, cur_epsilon : 0.45\n",
      "episode : 12050, reward mean : 0.36433859552708897, total_step : 351966, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12050, reward mean : 0.41000000536441805, step mean : 24.95, eval_episode_rewards : [ 0.84  0.87 -1.74  0.91  0.65  0.76  0.67  0.88  0.9  -1.    0.78  0.83\n",
      "  0.9  -1.21  0.8   0.88 -1.07  0.84  0.95  0.76]\n",
      "episode : 12060, reward mean : 0.36448425175452215, total_step : 352236, cur_epsilon : 0.45\n",
      "episode : 12070, reward mean : 0.36427755395387235, total_step : 352531, cur_epsilon : 0.45\n",
      "episode : 12080, reward mean : 0.36450580101851865, total_step : 352901, cur_epsilon : 0.45\n",
      "episode : 12090, reward mean : 0.3646038111108236, total_step : 353028, cur_epsilon : 0.45\n",
      "episode : 12100, reward mean : 0.36456281623087267, total_step : 353323, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12100, reward mean : 0.3955000056885183, step mean : 26.4, eval_episode_rewards : [ 0.89 -1.    0.57  0.91  0.7   0.81  0.91  0.87  0.82  0.97  0.76  0.46\n",
      "  0.9   0.62  0.66  0.94  0.33 -1.11 -1.09 -1.01]\n",
      "episode : 12110, reward mean : 0.36457556370295074, total_step : 353553, cur_epsilon : 0.45\n",
      "episode : 12120, reward mean : 0.3647599073011649, total_step : 353775, cur_epsilon : 0.45\n",
      "episode : 12130, reward mean : 0.3649134440680771, total_step : 354034, cur_epsilon : 0.45\n",
      "episode : 12140, reward mean : 0.3646449815965207, total_step : 354205, cur_epsilon : 0.45\n",
      "episode : 12150, reward mean : 0.36487819560807305, total_step : 354466, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12150, reward mean : 0.5955000045709312, step mean : 21.45, eval_episode_rewards : [ 0.53  0.98  0.87  0.85  0.67  0.7   0.64  0.69  0.44  0.85  0.95 -1.01\n",
      "  0.95  0.92  0.95  0.77 -1.01  0.75  0.9   0.52]\n",
      "episode : 12160, reward mean : 0.3647401378869912, total_step : 354778, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 12170, reward mean : 0.36487675240476086, total_step : 355057, cur_epsilon : 0.45\n",
      "episode : 12180, reward mean : 0.36493842995087866, total_step : 355227, cur_epsilon : 0.45\n",
      "episode : 12190, reward mean : 0.36497621631205474, total_step : 355426, cur_epsilon : 0.45\n",
      "episode : 12200, reward mean : 0.365063121055146, total_step : 355565, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12200, reward mean : 0.4410000069066882, step mean : 31.85, eval_episode_rewards : [ 0.51  0.79  0.6   0.96  0.79 -1.01  0.92  0.77  0.86  0.15  0.32 -1.23\n",
      "  0.88  0.9   0.8  -1.    0.95  0.48  0.87  0.51]\n",
      "episode : 12210, reward mean : 0.36518591948701745, total_step : 355860, cur_epsilon : 0.45\n",
      "episode : 12220, reward mean : 0.3654860946805553, total_step : 356138, cur_epsilon : 0.45\n",
      "episode : 12230, reward mean : 0.36571464244282376, total_step : 356503, cur_epsilon : 0.45\n",
      "episode : 12240, reward mean : 0.3660359540138564, total_step : 356754, cur_epsilon : 0.45\n",
      "episode : 12250, reward mean : 0.3661983736475207, total_step : 356999, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12250, reward mean : 0.3145000063814223, step mean : 29.45, eval_episode_rewards : [ 0.56  0.87  0.52  0.94  0.91  0.98  0.43 -1.    0.79  0.88  0.96 -1.\n",
      "  0.89  0.5  -1.11  0.76 -1.07  0.71  0.81 -1.04]\n",
      "episode : 12260, reward mean : 0.36636134398505577, total_step : 357342, cur_epsilon : 0.45\n",
      "episode : 12270, reward mean : 0.36649552382370904, total_step : 357621, cur_epsilon : 0.45\n",
      "episode : 12280, reward mean : 0.3666376284507167, total_step : 357890, cur_epsilon : 0.45\n",
      "episode : 12290, reward mean : 0.36672742696519883, total_step : 358023, cur_epsilon : 0.45\n",
      "episode : 12300, reward mean : 0.36662520955142575, total_step : 358390, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12300, reward mean : 0.7030000055208803, step mean : 25.65, eval_episode_rewards : [ 0.91  0.73  0.85  0.6   0.87  0.89 -1.    0.84  0.94  0.98  0.16  0.97\n",
      "  0.98  0.88  0.22  0.92  0.87  0.87  0.66  0.92]\n",
      "episode : 12310, reward mean : 0.366702687047773, total_step : 358738, cur_epsilon : 0.45\n",
      "episode : 12320, reward mean : 0.3667467595452999, total_step : 358927, cur_epsilon : 0.45\n",
      "episode : 12330, reward mean : 0.36664964133452105, total_step : 359189, cur_epsilon : 0.45\n",
      "episode : 12340, reward mean : 0.3670291797156828, total_step : 359364, cur_epsilon : 0.45\n",
      "episode : 12350, reward mean : 0.3672162006264515, total_step : 359576, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12350, reward mean : 0.6955000045709312, step mean : 21.45, eval_episode_rewards : [ 0.52  0.61  0.59 -1.03  0.87  0.81  0.97  0.97  0.76  0.82  0.76  0.91\n",
      "  0.98  0.47  0.79  0.83  0.89  0.79  0.87  0.73]\n",
      "episode : 12360, reward mean : 0.367436899496152, total_step : 359746, cur_epsilon : 0.45\n",
      "episode : 12370, reward mean : 0.36748747597497466, total_step : 359926, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 12380, reward mean : 0.36783522438251914, total_step : 360138, cur_epsilon : 0.45\n",
      "episode : 12390, reward mean : 0.36786199176129286, total_step : 360446, cur_epsilon : 0.45\n",
      "episode : 12400, reward mean : 0.36819032886876696, total_step : 360681, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12400, reward mean : 0.4855000047944486, step mean : 22.45, eval_episode_rewards : [ 0.89  0.89  0.97 -1.17  0.98  0.55  0.81  0.75  0.3   0.42  0.89  0.78\n",
      "  0.91  0.83  0.84  0.95  0.68  0.55 -1.05 -1.06]\n",
      "episode : 12410, reward mean : 0.3682361062056266, total_step : 360866, cur_epsilon : 0.45\n",
      "episode : 12420, reward mean : 0.3684009724688055, total_step : 361103, cur_epsilon : 0.45\n",
      "episode : 12430, reward mean : 0.3684416796579022, total_step : 361494, cur_epsilon : 0.45\n",
      "episode : 12440, reward mean : 0.3686390738094291, total_step : 361690, cur_epsilon : 0.45\n",
      "episode : 12450, reward mean : 0.3688265123079161, total_step : 361898, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12450, reward mean : 0.6365000058896839, step mean : 27.35, eval_episode_rewards : [ 0.54  0.54 -1.61  0.82  0.88  0.82  0.58  0.88  0.69  0.93  0.57  0.62\n",
      "  0.81  0.87  0.94  0.5   0.91  0.96  0.82  0.66]\n",
      "episode : 12460, reward mean : 0.36869743806571603, total_step : 362199, cur_epsilon : 0.45\n",
      "episode : 12470, reward mean : 0.3687209365151542, total_step : 362411, cur_epsilon : 0.45\n",
      "episode : 12480, reward mean : 0.3688101024335573, total_step : 362541, cur_epsilon : 0.45\n",
      "episode : 12490, reward mean : 0.368939157600572, total_step : 362821, cur_epsilon : 0.45\n",
      "episode : 12500, reward mean : 0.3692880062779784, total_step : 363026, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12500, reward mean : 0.5850000070407987, step mean : 32.5, eval_episode_rewards : [ 0.85  0.23  0.98  0.73  0.81  0.83  0.8  -1.48  0.93  0.89  0.56  0.1\n",
      "  0.64  0.85  0.92  0.52  0.77  0.52  0.78  0.47]\n",
      "episode : 12510, reward mean : 0.369273387572373, total_step : 363285, cur_epsilon : 0.45\n",
      "episode : 12520, reward mean : 0.3693266835951874, total_step : 363659, cur_epsilon : 0.45\n",
      "episode : 12530, reward mean : 0.3696121371646262, total_step : 363942, cur_epsilon : 0.45\n",
      "episode : 12540, reward mean : 0.3695446633766487, total_step : 364267, cur_epsilon : 0.45\n",
      "episode : 12550, reward mean : 0.3697434325724009, total_step : 364458, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12550, reward mean : 0.48300000820308925, step mean : 37.65, eval_episode_rewards : [ 0.04  0.79  0.67  0.6   0.82  0.79  0.33  0.83  0.95  0.76  0.07  0.79\n",
      "  0.68 -1.   -1.28  0.98  0.85  0.8   0.42  0.77]\n",
      "episode : 12560, reward mean : 0.36996178971840793, total_step : 364824, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 12570, reward mean : 0.3701193380202911, total_step : 365066, cur_epsilon : 0.45\n",
      "episode : 12580, reward mean : 0.37015739896487215, total_step : 365357, cur_epsilon : 0.45\n",
      "episode : 12590, reward mean : 0.3700349546516221, total_step : 365751, cur_epsilon : 0.45\n",
      "episode : 12600, reward mean : 0.3700817523096939, total_step : 365932, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12600, reward mean : 0.6440000034868717, step mean : 16.6, eval_episode_rewards : [ 0.92 -1.06  0.89 -1.05  0.93  0.99  0.6   0.49  0.98  0.87  0.62  0.88\n",
      "  0.73  0.71  0.79  0.76  0.99  0.96  0.91  0.97]\n",
      "episode : 12610, reward mean : 0.3702735986650687, total_step : 366229, cur_epsilon : 0.45\n",
      "episode : 12620, reward mean : 0.3703446972422987, total_step : 366379, cur_epsilon : 0.45\n",
      "episode : 12630, reward mean : 0.37055424221969036, total_step : 366554, cur_epsilon : 0.45\n",
      "episode : 12640, reward mean : 0.37058940500601584, total_step : 366749, cur_epsilon : 0.45\n",
      "episode : 12650, reward mean : 0.3706940774183735, total_step : 367056, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12650, reward mean : 0.548000005632639, step mean : 26.2, eval_episode_rewards : [ 0.86  0.6   0.36 -1.2   0.91  0.74  0.46  0.93  0.92  0.78 -1.25  0.61\n",
      "  0.77  0.86  0.91  0.53  0.93  0.45  0.81  0.98]\n",
      "episode : 12660, reward mean : 0.3708949509780048, total_step : 367241, cur_epsilon : 0.45\n",
      "episode : 12670, reward mean : 0.37093134012981815, total_step : 367533, cur_epsilon : 0.45\n",
      "episode : 12680, reward mean : 0.371100163997859, total_step : 367758, cur_epsilon : 0.45\n",
      "episode : 12690, reward mean : 0.371308122894384, total_step : 367933, cur_epsilon : 0.45\n",
      "episode : 12700, reward mean : 0.3716157542978685, total_step : 368181, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12700, reward mean : 0.6305000037886203, step mean : 17.95, eval_episode_rewards : [ 0.93  0.92  0.95  0.72  0.86  0.92  0.95  0.35  0.34  0.76  0.94  0.98\n",
      "  0.75  0.82  0.95 -1.09  0.75  0.84 -1.02  0.99]\n",
      "episode : 12710, reward mean : 0.37171676472286647, total_step : 368390, cur_epsilon : 0.45\n",
      "episode : 12720, reward mean : 0.37177673582245346, total_step : 368552, cur_epsilon : 0.45\n",
      "episode : 12730, reward mean : 0.3721421900784007, total_step : 368725, cur_epsilon : 0.45\n",
      "episode : 12740, reward mean : 0.37245762007445493, total_step : 368961, cur_epsilon : 0.45\n",
      "episode : 12750, reward mean : 0.3726172611606764, total_step : 369195, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12750, reward mean : 0.44250000463798644, step mean : 21.7, eval_episode_rewards : [ 0.74  0.77  0.97  0.97  0.81  0.85  0.95  0.8  -1.32 -1.    0.67  0.94\n",
      "  0.88 -1.13  0.85 -1.52  0.99  0.95  0.8   0.88]\n",
      "episode : 12760, reward mean : 0.3728542382344253, total_step : 369530, cur_epsilon : 0.45\n",
      "episode : 12770, reward mean : 0.37272827563925515, total_step : 369728, cur_epsilon : 0.45\n",
      "episode : 12780, reward mean : 0.3727668294172985, total_step : 369916, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 12790, reward mean : 0.37280610477227055, total_step : 370303, cur_epsilon : 0.45\n",
      "episode : 12800, reward mean : 0.3727836000073876, total_step : 370569, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12800, reward mean : 0.5030000066384673, step mean : 30.7, eval_episode_rewards : [ 0.93  0.44  0.23  0.4   0.88  0.83  0.61 -1.1   0.62  0.92  0.94 -1.26\n",
      "  0.12  0.72  0.69  0.87  0.95  0.8   0.9   0.57]\n",
      "episode : 12810, reward mean : 0.37308977987096703, total_step : 370814, cur_epsilon : 0.45\n",
      "episode : 12820, reward mean : 0.3732808174873454, total_step : 371006, cur_epsilon : 0.45\n",
      "episode : 12830, reward mean : 0.3732104505285249, total_step : 371432, cur_epsilon : 0.45\n",
      "episode : 12840, reward mean : 0.37316355765927756, total_step : 371729, cur_epsilon : 0.45\n",
      "episode : 12850, reward mean : 0.3730459206525222, total_step : 371917, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12850, reward mean : 0.787000004760921, step mean : 22.3, eval_episode_rewards : [0.73 0.68 0.88 0.81 0.94 0.65 0.91 0.68 0.78 0.88 0.6  0.92 0.91 0.93\n",
      " 0.73 0.75 0.78 0.83 0.58 0.77]\n",
      "episode : 12860, reward mean : 0.3730948740625131, total_step : 372190, cur_epsilon : 0.45\n",
      "episode : 12870, reward mean : 0.37327506452894366, total_step : 372395, cur_epsilon : 0.45\n",
      "episode : 12880, reward mean : 0.37312422985534954, total_step : 372626, cur_epsilon : 0.45\n",
      "episode : 12890, reward mean : 0.3734406579197713, total_step : 372855, cur_epsilon : 0.45\n",
      "episode : 12900, reward mean : 0.37366279695053084, total_step : 373205, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12900, reward mean : 0.3065000099129975, step mean : 45.2, eval_episode_rewards : [ 0.32  0.92  0.67  0.14 -1.    0.73  0.62  0.91  0.76  0.78  0.86  0.66\n",
      " -1.   -1.   -1.38  0.61  0.79  0.51  0.53  0.7 ]\n",
      "episode : 12910, reward mean : 0.3735933447473103, total_step : 373331, cur_epsilon : 0.45\n",
      "episode : 12920, reward mean : 0.3737840619758521, total_step : 373521, cur_epsilon : 0.45\n",
      "episode : 12930, reward mean : 0.3737494262034752, total_step : 373901, cur_epsilon : 0.45\n",
      "episode : 12940, reward mean : 0.3734605935763516, total_step : 374210, cur_epsilon : 0.45\n",
      "episode : 12950, reward mean : 0.37333514138552787, total_step : 374508, cur_epsilon : 0.45\n",
      "[EVAL] episode : 12950, reward mean : 0.344000006839633, step mean : 31.55, eval_episode_rewards : [-1.07  0.68  0.84  0.97  0.31  0.91  0.79 -1.42  0.95  0.71 -1.   -1.14\n",
      "  0.95  0.81  0.79  0.78  0.83  0.15  0.36  0.68]\n",
      "episode : 12960, reward mean : 0.37344753711461376, total_step : 374799, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 12970, reward mean : 0.373754054052165, total_step : 375038, cur_epsilon : 0.45\n",
      "episode : 12980, reward mean : 0.3739052450793791, total_step : 375377, cur_epsilon : 0.45\n",
      "episode : 12990, reward mean : 0.3739099369653206, total_step : 375607, cur_epsilon : 0.45\n",
      "episode : 13000, reward mean : 0.37420769855637964, total_step : 375856, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13000, reward mean : 0.775500005017966, step mean : 23.45, eval_episode_rewards : [0.25 0.95 0.85 0.94 0.96 0.59 0.74 0.91 0.76 0.99 0.51 0.69 0.81 0.89\n",
      " 0.68 0.76 0.89 0.98 0.49 0.87]\n",
      "episode : 13010, reward mean : 0.37432744668067924, total_step : 376235, cur_epsilon : 0.45\n",
      "episode : 13020, reward mean : 0.3743233549454394, total_step : 376575, cur_epsilon : 0.45\n",
      "episode : 13030, reward mean : 0.37432080440835047, total_step : 376814, cur_epsilon : 0.45\n",
      "episode : 13040, reward mean : 0.37434816575791985, total_step : 377014, cur_epsilon : 0.45\n",
      "episode : 13050, reward mean : 0.37449042770569124, total_step : 377363, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13050, reward mean : 0.8315000037662685, step mean : 17.85, eval_episode_rewards : [0.89 0.99 0.8  0.57 0.87 0.84 0.76 0.8  0.86 0.91 0.92 0.88 0.68 0.91\n",
      " 0.97 0.85 0.94 0.53 0.68 0.98]\n",
      "episode : 13060, reward mean : 0.37437826046437755, total_step : 377844, cur_epsilon : 0.45\n",
      "episode : 13070, reward mean : 0.3742218884268196, total_step : 378183, cur_epsilon : 0.45\n",
      "episode : 13080, reward mean : 0.37428364539874964, total_step : 378538, cur_epsilon : 0.45\n",
      "episode : 13090, reward mean : 0.3741145975455198, total_step : 378795, cur_epsilon : 0.45\n",
      "episode : 13100, reward mean : 0.3742755787724359, total_step : 379020, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13100, reward mean : 0.5160000063478947, step mean : 29.4, eval_episode_rewards : [ 0.58  0.72 -1.16  0.88 -1.01  0.68  0.99  0.72  0.89  0.84  0.68  0.77\n",
      "  0.28  0.69  0.16  0.59  0.73  0.76  0.83  0.7 ]\n",
      "episode : 13110, reward mean : 0.3742944379835116, total_step : 379231, cur_epsilon : 0.45\n",
      "episode : 13120, reward mean : 0.374285829422013, total_step : 379478, cur_epsilon : 0.45\n",
      "episode : 13130, reward mean : 0.3745544616967122, total_step : 379761, cur_epsilon : 0.45\n",
      "episode : 13140, reward mean : 0.3745723045755694, total_step : 379973, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 13150, reward mean : 0.37475209750305905, total_step : 380172, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13150, reward mean : 0.46700000632554295, step mean : 29.25, eval_episode_rewards : [ 0.99  0.8   0.59  0.78  0.96  0.98  0.94  0.8   0.31 -1.04  0.65  0.92\n",
      "  0.42  0.77  0.43 -1.    0.84 -1.37  0.79  0.78]\n",
      "episode : 13160, reward mean : 0.37497948953176996, total_step : 380508, cur_epsilon : 0.45\n",
      "episode : 13170, reward mean : 0.37509795613502045, total_step : 380787, cur_epsilon : 0.45\n",
      "episode : 13180, reward mean : 0.37525645541323965, total_step : 381013, cur_epsilon : 0.45\n",
      "episode : 13190, reward mean : 0.37548370602116315, total_step : 381348, cur_epsilon : 0.45\n",
      "episode : 13200, reward mean : 0.3757530365514597, total_step : 381627, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13200, reward mean : 0.7155000041238964, step mean : 19.45, eval_episode_rewards : [ 0.82  0.91  0.63  0.92  0.96  0.9   0.53  0.84  0.97  0.98  0.97  0.79\n",
      "  0.91  0.88  0.67 -1.18  0.6   0.96  0.5   0.75]\n",
      "episode : 13210, reward mean : 0.37601287528706107, total_step : 381918, cur_epsilon : 0.45\n",
      "episode : 13220, reward mean : 0.37574811517423, total_step : 382201, cur_epsilon : 0.45\n",
      "episode : 13230, reward mean : 0.3757959246153937, total_step : 382471, cur_epsilon : 0.45\n",
      "episode : 13240, reward mean : 0.37609970413227184, total_step : 382703, cur_epsilon : 0.45\n",
      "episode : 13250, reward mean : 0.37593811945392275, total_step : 382951, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13250, reward mean : 0.456000005453825, step mean : 25.4, eval_episode_rewards : [ 0.69  0.77  0.65  0.8  -1.06  0.09  0.86  0.71  0.59  0.89  0.71  0.94\n",
      "  0.96  0.79  0.91 -1.01  0.07  0.9   0.94 -1.08]\n",
      "episode : 13260, reward mean : 0.37606109221891515, total_step : 383222, cur_epsilon : 0.45\n",
      "episode : 13270, reward mean : 0.37596609516626545, total_step : 383382, cur_epsilon : 0.45\n",
      "episode : 13280, reward mean : 0.3757055785312003, total_step : 383562, cur_epsilon : 0.45\n",
      "episode : 13290, reward mean : 0.3758743478509438, total_step : 383772, cur_epsilon : 0.45\n",
      "episode : 13300, reward mean : 0.37601805135271604, total_step : 384015, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13300, reward mean : 0.4725000073201954, step mean : 33.75, eval_episode_rewards : [ 0.82  0.71  0.45  0.76  0.18  0.78  0.52  0.9   0.82 -1.04  0.53  0.06\n",
      "  0.59 -1.09  0.57  0.57  0.84  0.93  0.92  0.63]\n",
      "episode : 13310, reward mean : 0.376083402182126, total_step : 384261, cur_epsilon : 0.45\n",
      "episode : 13320, reward mean : 0.37633108732055637, total_step : 384565, cur_epsilon : 0.45\n",
      "episode : 13330, reward mean : 0.3763751000126814, total_step : 384839, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 13340, reward mean : 0.3765764680062735, total_step : 385004, cur_epsilon : 0.45\n",
      "episode : 13350, reward mean : 0.3768764107306017, total_step : 385237, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13350, reward mean : 0.5795000060461462, step mean : 28.0, eval_episode_rewards : [-1.06  0.89  0.55  0.76  0.87  0.82  0.49  0.85  0.82  0.69  0.97  0.68\n",
      " -1.    0.97  0.43  0.73  0.77  0.77  0.65  0.94]\n",
      "episode : 13360, reward mean : 0.37674701222285495, total_step : 385443, cur_epsilon : 0.45\n",
      "episode : 13370, reward mean : 0.37695737347597497, total_step : 385795, cur_epsilon : 0.45\n",
      "episode : 13380, reward mean : 0.3770553126636063, total_step : 386097, cur_epsilon : 0.45\n",
      "episode : 13390, reward mean : 0.37723525642285916, total_step : 386388, cur_epsilon : 0.45\n",
      "episode : 13400, reward mean : 0.37713955847282354, total_step : 386549, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13400, reward mean : 0.7995000044815243, step mean : 21.05, eval_episode_rewards : [0.56 0.79 0.9  0.94 0.89 0.84 0.49 0.72 0.4  0.96 0.59 0.93 0.9  0.91\n",
      " 0.84 0.93 0.95 0.95 0.77 0.73]\n",
      "episode : 13410, reward mean : 0.37727517401862065, total_step : 386800, cur_epsilon : 0.45\n",
      "episode : 13420, reward mean : 0.377531302805668, total_step : 387089, cur_epsilon : 0.45\n",
      "episode : 13430, reward mean : 0.3774772958836934, total_step : 387394, cur_epsilon : 0.45\n",
      "episode : 13440, reward mean : 0.37729018480500337, total_step : 387678, cur_epsilon : 0.45\n",
      "episode : 13450, reward mean : 0.37710558244328407, total_step : 388058, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13450, reward mean : 0.5525000066496432, step mean : 30.7, eval_episode_rewards : [ 0.77  0.89  0.88  0.72 -1.24  0.82  0.11 -1.    0.79  0.55  0.9   0.87\n",
      "  0.86  0.86  0.56  0.75  0.67  0.97  0.63  0.69]\n",
      "episode : 13460, reward mean : 0.37715602406365645, total_step : 388223, cur_epsilon : 0.45\n",
      "episode : 13470, reward mean : 0.3770022334037286, total_step : 388463, cur_epsilon : 0.45\n",
      "episode : 13480, reward mean : 0.3769762673611457, total_step : 388830, cur_epsilon : 0.45\n",
      "episode : 13490, reward mean : 0.3771319558247482, total_step : 389053, cur_epsilon : 0.45\n",
      "episode : 13500, reward mean : 0.3773066728976038, total_step : 389250, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13500, reward mean : 0.7350000059232116, step mean : 27.5, eval_episode_rewards : [0.98 0.86 0.9  0.1  0.47 0.74 0.84 0.94 0.9  0.82 0.74 0.77 0.89 0.89\n",
      " 0.98 0.46 0.57 0.62 0.29 0.94]\n",
      "episode : 13510, reward mean : 0.3769296879490643, total_step : 389590, cur_epsilon : 0.45\n",
      "episode : 13520, reward mean : 0.37726923699915055, total_step : 389764, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 13530, reward mean : 0.3775040712710043, total_step : 390079, cur_epsilon : 0.45\n",
      "episode : 13540, reward mean : 0.37766618052747875, total_step : 390292, cur_epsilon : 0.45\n",
      "episode : 13550, reward mean : 0.37777196194866597, total_step : 390581, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13550, reward mean : 0.2855000070296228, step mean : 32.35, eval_episode_rewards : [ 0.5   0.76  0.85 -1.39  0.93  0.88 -1.15  0.74 -1.    0.61 -1.    0.92\n",
      "  0.69  0.88 -1.1   0.36  0.79  0.7   0.84  0.9 ]\n",
      "episode : 13560, reward mean : 0.37784366404368463, total_step : 390716, cur_epsilon : 0.45\n",
      "episode : 13570, reward mean : 0.37791231278269694, total_step : 390855, cur_epsilon : 0.45\n",
      "episode : 13580, reward mean : 0.3780721711713563, total_step : 391070, cur_epsilon : 0.45\n",
      "episode : 13590, reward mean : 0.3780066287367056, total_step : 391191, cur_epsilon : 0.45\n",
      "episode : 13600, reward mean : 0.3781610356324681, total_step : 391512, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13600, reward mean : 0.3720000073313713, step mean : 33.7, eval_episode_rewards : [ 0.38  0.87  0.98  0.78  0.81  0.89 -1.    0.66  0.63  0.43  0.84  0.91\n",
      "  0.7  -1.09  0.81  0.49  0.92 -1.3  -1.    0.73]\n",
      "episode : 13610, reward mean : 0.3783453405319462, total_step : 391693, cur_epsilon : 0.45\n",
      "episode : 13620, reward mean : 0.3782070546759711, total_step : 391913, cur_epsilon : 0.45\n",
      "episode : 13630, reward mean : 0.37837931656115364, total_step : 392110, cur_epsilon : 0.45\n",
      "episode : 13640, reward mean : 0.37851980093694526, total_step : 392350, cur_epsilon : 0.45\n",
      "episode : 13650, reward mean : 0.3788381014521415, total_step : 392547, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13650, reward mean : 0.5705000040121376, step mean : 18.9, eval_episode_rewards : [ 0.94  0.94  0.84  0.96  0.64 -1.   -1.09  0.83  0.92  0.92  0.92  0.8\n",
      " -1.57  0.83  0.95  0.96  0.99  0.85  0.79  0.99]\n",
      "episode : 13660, reward mean : 0.3787328026982878, total_step : 392722, cur_epsilon : 0.45\n",
      "episode : 13670, reward mean : 0.37908120591741196, total_step : 392877, cur_epsilon : 0.45\n",
      "episode : 13680, reward mean : 0.37939693603313346, total_step : 393076, cur_epsilon : 0.45\n",
      "episode : 13690, reward mean : 0.37946092658604774, total_step : 393219, cur_epsilon : 0.45\n",
      "episode : 13700, reward mean : 0.379454020803971, total_step : 393459, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13700, reward mean : 0.6155000063590705, step mean : 29.45, eval_episode_rewards : [ 0.79  0.38  0.74  0.9   0.71 -1.53  0.81  0.76  0.38  0.32  0.45  0.9\n",
      "  0.83  0.62  0.99  0.74  0.92  0.85  0.78  0.97]\n",
      "episode : 13710, reward mean : 0.3794682775389737, total_step : 393670, cur_epsilon : 0.45\n",
      "episode : 13720, reward mean : 0.379594029527315, total_step : 393928, cur_epsilon : 0.45\n",
      "episode : 13730, reward mean : 0.3798354031448914, total_step : 394227, cur_epsilon : 0.45\n",
      "episode : 13740, reward mean : 0.37954512992928013, total_step : 394456, cur_epsilon : 0.45\n",
      "episode : 13750, reward mean : 0.37967564256597647, total_step : 394707, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13750, reward mean : 0.6115000042133033, step mean : 19.85, eval_episode_rewards : [ 0.78  0.62  0.91  0.91  0.73  0.95  0.94  0.8   0.78  0.66  0.74  0.86\n",
      "  0.94 -1.23  0.64  0.76  0.9   0.96  0.86 -1.28]\n",
      "episode : 13760, reward mean : 0.3797209364345817, total_step : 394974, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 13770, reward mean : 0.37973711585941183, total_step : 395182, cur_epsilon : 0.45\n",
      "episode : 13780, reward mean : 0.37978375075609355, total_step : 395348, cur_epsilon : 0.45\n",
      "episode : 13790, reward mean : 0.379882529766675, total_step : 395642, cur_epsilon : 0.45\n",
      "episode : 13800, reward mean : 0.3800471076467817, total_step : 395845, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13800, reward mean : 0.36750000519677994, step mean : 24.25, eval_episode_rewards : [-1.7   0.89  0.98  0.8   0.91  0.82  0.41  0.97  0.35 -1.34  0.82  0.81\n",
      "  0.85  0.93  0.77 -1.06 -1.06  0.81  0.87  0.52]\n",
      "episode : 13810, reward mean : 0.38021289540682685, total_step : 396046, cur_epsilon : 0.45\n",
      "episode : 13820, reward mean : 0.3805636820259373, total_step : 396191, cur_epsilon : 0.45\n",
      "episode : 13830, reward mean : 0.38071222600640914, total_step : 396514, cur_epsilon : 0.45\n",
      "episode : 13840, reward mean : 0.3810202374069203, total_step : 396717, cur_epsilon : 0.45\n",
      "episode : 13850, reward mean : 0.3810397173846241, total_step : 397018, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13850, reward mean : 0.8455000034533441, step mean : 16.45, eval_episode_rewards : [0.86 0.84 0.93 0.82 0.64 0.96 0.94 0.89 0.45 0.84 0.91 0.81 0.83 0.9\n",
      " 0.78 0.84 0.96 0.84 0.96 0.91]\n",
      "episode : 13860, reward mean : 0.3810512327433202, total_step : 397231, cur_epsilon : 0.45\n",
      "episode : 13870, reward mean : 0.380953142455448, total_step : 397396, cur_epsilon : 0.45\n",
      "episode : 13880, reward mean : 0.38127738371000275, total_step : 397575, cur_epsilon : 0.45\n",
      "episode : 13890, reward mean : 0.38114543455452415, total_step : 397886, cur_epsilon : 0.45\n",
      "episode : 13900, reward mean : 0.3811230277710949, total_step : 398146, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13900, reward mean : 0.45750000765547155, step mean : 35.15, eval_episode_rewards : [ 0.92  0.91 -1.05  0.94  0.96  0.74  0.16  0.75  0.64  0.49  0.93  0.91\n",
      "  0.45  0.27  0.91 -1.    0.71  0.54  0.97 -1.  ]\n",
      "episode : 13910, reward mean : 0.3813242333608403, total_step : 398295, cur_epsilon : 0.45\n",
      "episode : 13920, reward mean : 0.38163793721910694, total_step : 398487, cur_epsilon : 0.45\n",
      "episode : 13930, reward mean : 0.38187653166933916, total_step : 398783, cur_epsilon : 0.45\n",
      "episode : 13940, reward mean : 0.38199785410398496, total_step : 399042, cur_epsilon : 0.45\n",
      "episode : 13950, reward mean : 0.3820444506283943, total_step : 399304, cur_epsilon : 0.45\n",
      "[EVAL] episode : 13950, reward mean : 0.36150000533089044, step mean : 24.85, eval_episode_rewards : [ 0.56  0.79  0.71  0.76  0.99  0.86  0.12  0.67  0.56 -1.14  0.88  0.94\n",
      "  0.95 -1.08  0.74 -1.22  0.79  0.96 -1.02  0.41]\n",
      "episode : 13960, reward mean : 0.382035822803715, total_step : 399643, cur_epsilon : 0.45\n",
      "episode : 13970, reward mean : 0.3817559116955316, total_step : 399862, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 13980, reward mean : 0.38171388315082194, total_step : 400149, cur_epsilon : 0.45\n",
      "episode : 13990, reward mean : 0.3818620505000467, total_step : 400370, cur_epsilon : 0.45\n",
      "episode : 14000, reward mean : 0.38190286332528506, total_step : 400640, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14000, reward mean : 0.2270000072196126, step mean : 33.25, eval_episode_rewards : [ 0.73  0.75  0.24  0.87  0.85 -1.32  0.7   0.73 -1.12  0.64 -1.08  0.74\n",
      "  0.83 -1.39 -1.    0.87  0.74  0.91  0.73  0.12]\n",
      "episode : 14010, reward mean : 0.38223912109827607, total_step : 400797, cur_epsilon : 0.45\n",
      "episode : 14020, reward mean : 0.3820285368514129, total_step : 401120, cur_epsilon : 0.45\n",
      "episode : 14030, reward mean : 0.3821297282051675, total_step : 401406, cur_epsilon : 0.45\n",
      "episode : 14040, reward mean : 0.3823796358104116, total_step : 401683, cur_epsilon : 0.45\n",
      "episode : 14050, reward mean : 0.3822761627657876, total_step : 402056, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14050, reward mean : 0.1905000046826899, step mean : 21.95, eval_episode_rewards : [ 0.9  -1.02  0.61  0.8   0.51  0.93  0.68 -1.01  0.93  0.77  0.8  -1.04\n",
      "  0.73 -1.12  0.86  0.95  0.32  0.77 -1.27 -1.29]\n",
      "episode : 14060, reward mean : 0.38259104458737064, total_step : 402241, cur_epsilon : 0.45\n",
      "episode : 14070, reward mean : 0.3826368221021558, total_step : 402604, cur_epsilon : 0.45\n",
      "episode : 14080, reward mean : 0.38291264822645726, total_step : 402843, cur_epsilon : 0.45\n",
      "episode : 14090, reward mean : 0.38306175209899124, total_step : 403060, cur_epsilon : 0.45\n",
      "episode : 14100, reward mean : 0.3831659636266054, total_step : 403340, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14100, reward mean : 0.6060000043362379, step mean : 20.4, eval_episode_rewards : [ 0.73  0.63  0.91  0.98  0.35  0.98  0.93  0.99  0.55  0.93  0.94 -1.4\n",
      "  0.85  0.82  0.93  0.69 -1.23  0.66  0.89  0.99]\n",
      "episode : 14110, reward mean : 0.38334940376828147, total_step : 403508, cur_epsilon : 0.45\n",
      "episode : 14120, reward mean : 0.38365368889605167, total_step : 403705, cur_epsilon : 0.45\n",
      "episode : 14130, reward mean : 0.3834310040542275, total_step : 404046, cur_epsilon : 0.45\n",
      "episode : 14140, reward mean : 0.38361669641588797, total_step : 404210, cur_epsilon : 0.45\n",
      "episode : 14150, reward mean : 0.38364240900234914, total_step : 404499, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14150, reward mean : 0.19950000559911132, step mean : 26.0, eval_episode_rewards : [ 0.97  0.42 -1.06  0.91  0.61 -1.04  0.69  0.94  0.63  0.44  0.78  0.95\n",
      "  0.9   0.97 -1.12 -1.38  0.89  0.58 -1.09 -1.  ]\n",
      "episode : 14160, reward mean : 0.38359958244669173, total_step : 404786, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 14170, reward mean : 0.38386592007752823, total_step : 405035, cur_epsilon : 0.45\n",
      "episode : 14180, reward mean : 0.38406488628550967, total_step : 405179, cur_epsilon : 0.45\n",
      "episode : 14190, reward mean : 0.38405356501623555, total_step : 405421, cur_epsilon : 0.45\n",
      "episode : 14200, reward mean : 0.3843077526505197, total_step : 405686, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14200, reward mean : 0.4670000052079558, step mean : 24.3, eval_episode_rewards : [-1.22  0.91  0.98  0.42  0.53  0.92 -1.15  0.79  0.97  0.96 -1.3   0.93\n",
      "  0.95  0.61  0.93  0.95  0.65  0.74  0.07  0.7 ]\n",
      "episode : 14210, reward mean : 0.3846594009615276, total_step : 405812, cur_epsilon : 0.45\n",
      "episode : 14220, reward mean : 0.38478762923455606, total_step : 406055, cur_epsilon : 0.45\n",
      "episode : 14230, reward mean : 0.38472172085625117, total_step : 406374, cur_epsilon : 0.45\n",
      "episode : 14240, reward mean : 0.38479916347279686, total_step : 406689, cur_epsilon : 0.45\n",
      "episode : 14250, reward mean : 0.38490386581839175, total_step : 406965, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14250, reward mean : 0.6570000054314733, step mean : 25.3, eval_episode_rewards : [ 0.69  0.87  0.91  0.97  0.78  0.26  0.56  0.63  0.98  0.75  0.78  0.96\n",
      "  0.82  0.58  0.51  0.98  0.76 -1.02  0.97  0.4 ]\n",
      "episode : 14260, reward mean : 0.3850441856914543, total_step : 407190, cur_epsilon : 0.45\n",
      "episode : 14270, reward mean : 0.385199725858674, total_step : 407393, cur_epsilon : 0.45\n",
      "episode : 14280, reward mean : 0.38506583249645115, total_step : 407609, cur_epsilon : 0.45\n",
      "episode : 14290, reward mean : 0.3851917486425367, total_step : 407854, cur_epsilon : 0.45\n",
      "episode : 14300, reward mean : 0.38546853763292305, total_step : 408083, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14300, reward mean : 0.5745000061579049, step mean : 28.5, eval_episode_rewards : [ 0.92  0.51  0.86 -1.    0.77  0.78  0.89  0.98  0.55 -1.16  0.93  0.9\n",
      "  0.72  0.17  0.69  0.7   0.81  0.98  0.86  0.63]\n",
      "episode : 14310, reward mean : 0.3855905023203664, total_step : 408333, cur_epsilon : 0.45\n",
      "episode : 14320, reward mean : 0.38553771566126144, total_step : 408633, cur_epsilon : 0.45\n",
      "episode : 14330, reward mean : 0.38557641928156333, total_step : 408802, cur_epsilon : 0.45\n",
      "episode : 14340, reward mean : 0.3857887090916383, total_step : 409122, cur_epsilon : 0.45\n",
      "episode : 14350, reward mean : 0.385578403375939, total_step : 409448, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14350, reward mean : 0.5270000038668513, step mean : 18.3, eval_episode_rewards : [ 0.88  0.92  0.82  0.89  0.84  0.44  0.83  0.94  0.97  0.74  0.63 -1.08\n",
      "  0.81  0.89  0.72 -1.01  0.99  0.72 -1.34  0.94]\n",
      "episode : 14360, reward mean : 0.38562326521436296, total_step : 409608, cur_epsilon : 0.45\n",
      "episode : 14370, reward mean : 0.38559569161691204, total_step : 409872, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 14380, reward mean : 0.38544437333705717, total_step : 410114, cur_epsilon : 0.45\n",
      "episode : 14390, reward mean : 0.3856080673156077, total_step : 410503, cur_epsilon : 0.45\n",
      "episode : 14400, reward mean : 0.38578611727441764, total_step : 410871, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14400, reward mean : 0.514000004157424, step mean : 19.6, eval_episode_rewards : [ 0.75  0.92  0.85  0.95 -1.02  0.69  0.92  0.61  0.71  0.93 -1.37  0.44\n",
      "  0.88  0.98  0.95  0.95  0.94 -1.34  0.7   0.84]\n",
      "episode : 14410, reward mean : 0.38584802836776916, total_step : 411006, cur_epsilon : 0.45\n",
      "episode : 14420, reward mean : 0.38608807828290265, total_step : 411284, cur_epsilon : 0.45\n",
      "episode : 14430, reward mean : 0.3861975113580511, total_step : 411550, cur_epsilon : 0.45\n",
      "episode : 14440, reward mean : 0.38620706987283465, total_step : 411859, cur_epsilon : 0.45\n",
      "episode : 14450, reward mean : 0.38649550788981074, total_step : 412066, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14450, reward mean : 0.7905000046826899, step mean : 21.95, eval_episode_rewards : [0.84 0.64 0.95 0.71 0.93 0.25 0.99 0.87 0.89 0.78 0.29 0.93 0.82 0.94\n",
      " 0.35 0.96 0.95 0.83 0.95 0.94]\n",
      "episode : 14460, reward mean : 0.38656916245341527, total_step : 412383, cur_epsilon : 0.45\n",
      "episode : 14470, reward mean : 0.3864381540543512, total_step : 412796, cur_epsilon : 0.45\n",
      "episode : 14480, reward mean : 0.3863522161063301, total_step : 413043, cur_epsilon : 0.45\n",
      "episode : 14490, reward mean : 0.3861656376316866, total_step : 413337, cur_epsilon : 0.45\n",
      "episode : 14500, reward mean : 0.38633655788415466, total_step : 413513, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14500, reward mean : 0.6515000055544078, step mean : 25.85, eval_episode_rewards : [ 0.81  0.55  0.9   0.94  0.89  0.82 -1.03  0.26  0.99  0.71  0.78  0.19\n",
      "  0.82  0.92  0.66  0.74  0.41  0.78  0.95  0.94]\n",
      "episode : 14510, reward mean : 0.38661268706876994, total_step : 413736, cur_epsilon : 0.45\n",
      "episode : 14520, reward mean : 0.38672865629632325, total_step : 413991, cur_epsilon : 0.45\n",
      "episode : 14530, reward mean : 0.38670475495529233, total_step : 414348, cur_epsilon : 0.45\n",
      "episode : 14540, reward mean : 0.38680949721862007, total_step : 414619, cur_epsilon : 0.45\n",
      "episode : 14550, reward mean : 0.38663230856490094, total_step : 414900, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14550, reward mean : 0.7015000044368207, step mean : 20.85, eval_episode_rewards : [ 0.82  0.97  0.65  0.81  0.93  0.93  0.9  -1.18  0.86  0.79  0.56  0.74\n",
      "  0.74  0.73  0.56  0.96  0.68  0.88  0.77  0.93]\n",
      "synced target net\n",
      "episode : 14560, reward mean : 0.38663874242204027, total_step : 415114, cur_epsilon : 0.45\n",
      "episode : 14570, reward mean : 0.38681126216205325, total_step : 415286, cur_epsilon : 0.45\n",
      "episode : 14580, reward mean : 0.38688615155859823, total_step : 415400, cur_epsilon : 0.45\n",
      "episode : 14590, reward mean : 0.38673886838761645, total_step : 415638, cur_epsilon : 0.45\n",
      "episode : 14600, reward mean : 0.3867705540967569, total_step : 415815, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14600, reward mean : 0.7250000039115548, step mean : 18.5, eval_episode_rewards : [ 0.68  0.86  0.5  -1.14  0.91  0.84  0.95  0.94  0.68  0.86  0.96  0.98\n",
      "  0.93  0.9   0.77  0.97  0.89  0.44  0.74  0.84]\n",
      "episode : 14610, reward mean : 0.3869178706267562, total_step : 416023, cur_epsilon : 0.45\n",
      "episode : 14620, reward mean : 0.38699316620398644, total_step : 416235, cur_epsilon : 0.45\n",
      "episode : 14630, reward mean : 0.387033498970383, total_step : 416399, cur_epsilon : 0.45\n",
      "episode : 14640, reward mean : 0.38693511543828546, total_step : 416766, cur_epsilon : 0.45\n",
      "episode : 14650, reward mean : 0.38713789010833655, total_step : 417092, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14650, reward mean : 0.683000004850328, step mean : 22.7, eval_episode_rewards : [ 0.66  0.84  0.92 -1.01  0.77  0.99  0.86  0.92  0.94  0.65  0.47  0.37\n",
      "  0.8   0.89  0.97  0.69  0.86  0.79  0.7   0.58]\n",
      "episode : 14660, reward mean : 0.38738950137392014, total_step : 417346, cur_epsilon : 0.45\n",
      "episode : 14670, reward mean : 0.3875405651114084, total_step : 417547, cur_epsilon : 0.45\n",
      "episode : 14680, reward mean : 0.3876982350289385, total_step : 417738, cur_epsilon : 0.45\n",
      "episode : 14690, reward mean : 0.3880299584923592, total_step : 417873, cur_epsilon : 0.45\n",
      "episode : 14700, reward mean : 0.388074155803263, total_step : 418129, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14700, reward mean : 0.6900000046938658, step mean : 22.0, eval_episode_rewards : [ 0.88  0.63  0.73  0.96  0.9   0.87  0.85  0.68  0.9   0.95  0.49  0.67\n",
      "  0.51  0.87  0.85  0.93  0.75  0.83  0.88 -1.33]\n",
      "episode : 14710, reward mean : 0.38829096467528595, total_step : 418432, cur_epsilon : 0.45\n",
      "episode : 14720, reward mean : 0.3884640007071025, total_step : 418599, cur_epsilon : 0.45\n",
      "episode : 14730, reward mean : 0.3886048941275526, total_step : 419013, cur_epsilon : 0.45\n",
      "episode : 14740, reward mean : 0.3885854878258137, total_step : 419263, cur_epsilon : 0.45\n",
      "episode : 14750, reward mean : 0.38888339597226707, total_step : 419445, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14750, reward mean : 0.5590000053867697, step mean : 25.1, eval_episode_rewards : [ 0.57  0.87  0.6  -1.33  0.74  0.78  0.69  0.82  0.83  0.79  0.82  0.79\n",
      " -1.23  0.25  0.73  0.91  0.83  0.79  0.96  0.97]\n",
      "episode : 14760, reward mean : 0.38893767551930375, total_step : 419786, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 14770, reward mean : 0.38906906504643046, total_step : 420112, cur_epsilon : 0.45\n",
      "episode : 14780, reward mean : 0.38923478286841906, total_step : 420387, cur_epsilon : 0.45\n",
      "episode : 14790, reward mean : 0.3894232651012231, total_step : 420628, cur_epsilon : 0.45\n",
      "episode : 14800, reward mean : 0.38958446560037396, total_step : 420810, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14800, reward mean : 0.5850000048056245, step mean : 22.5, eval_episode_rewards : [ 0.73  0.61  0.71  0.85  0.88  0.69  0.92  0.94  0.9   0.23  0.79  0.88\n",
      "  0.95  0.88  0.87 -1.08  0.5   0.72  0.78 -1.05]\n",
      "episode : 14810, reward mean : 0.3897272174834581, total_step : 421019, cur_epsilon : 0.45\n",
      "episode : 14820, reward mean : 0.3898400871109534, total_step : 421272, cur_epsilon : 0.45\n",
      "episode : 14830, reward mean : 0.3897127505751052, total_step : 421481, cur_epsilon : 0.45\n",
      "episode : 14840, reward mean : 0.38963747244614433, total_step : 421813, cur_epsilon : 0.45\n",
      "episode : 14850, reward mean : 0.38950303644092393, total_step : 422033, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14850, reward mean : 0.7325000037439168, step mean : 17.75, eval_episode_rewards : [ 0.8   0.65  0.84  0.86  0.96  0.95  0.69  0.81 -1.03  0.95  0.88  0.36\n",
      "  0.94  0.93  0.71  0.84  0.81  0.94  0.97  0.79]\n",
      "episode : 14860, reward mean : 0.3896386333254904, total_step : 422351, cur_epsilon : 0.45\n",
      "episode : 14870, reward mean : 0.38987357708642406, total_step : 422622, cur_epsilon : 0.45\n",
      "episode : 14880, reward mean : 0.3899160007599451, total_step : 422779, cur_epsilon : 0.45\n",
      "episode : 14890, reward mean : 0.3901222358210684, total_step : 423092, cur_epsilon : 0.45\n",
      "episode : 14900, reward mean : 0.390315442379039, total_step : 423424, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14900, reward mean : 0.35900000426918266, step mean : 20.05, eval_episode_rewards : [-1.26  0.78 -1.05 -1.02  0.56  0.89  0.88 -1.    0.79 -1.06  0.93  0.81\n",
      "  0.77  0.97  0.73  0.88  0.82  0.93  0.84  0.99]\n",
      "episode : 14910, reward mean : 0.3905043656270049, total_step : 423661, cur_epsilon : 0.45\n",
      "episode : 14920, reward mean : 0.39079290157765767, total_step : 423850, cur_epsilon : 0.45\n",
      "episode : 14930, reward mean : 0.3909403946135899, total_step : 424049, cur_epsilon : 0.45\n",
      "episode : 14940, reward mean : 0.3911492698529169, total_step : 424156, cur_epsilon : 0.45\n",
      "episode : 14950, reward mean : 0.39116321683241473, total_step : 424354, cur_epsilon : 0.45\n",
      "[EVAL] episode : 14950, reward mean : 0.3720000073313713, step mean : 33.7, eval_episode_rewards : [ 0.68  0.66  0.89 -1.    0.82  0.92  0.9  -1.18 -1.08  0.26  0.8   0.69\n",
      "  0.81  0.85  0.9   0.81  0.3  -1.    0.68  0.73]\n",
      "episode : 14960, reward mean : 0.39114505960693197, total_step : 424699, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 14970, reward mean : 0.3913567195581142, total_step : 425001, cur_epsilon : 0.45\n",
      "episode : 14980, reward mean : 0.3915000061300622, total_step : 425205, cur_epsilon : 0.45\n",
      "episode : 14990, reward mean : 0.3916050761764132, total_step : 425466, cur_epsilon : 0.45\n",
      "episode : 15000, reward mean : 0.39172333946240445, total_step : 425707, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15000, reward mean : 0.6790000071749092, step mean : 33.1, eval_episode_rewards : [0.79 0.95 0.37 0.59 0.69 0.99 0.94 0.92 0.8  0.86 0.5  0.65 0.41 0.96\n",
      " 0.52 0.13 0.68 0.93 0.79 0.11]\n",
      "episode : 15010, reward mean : 0.39171486289043733, total_step : 425938, cur_epsilon : 0.45\n",
      "episode : 15020, reward mean : 0.39182557203991714, total_step : 426190, cur_epsilon : 0.45\n",
      "episode : 15030, reward mean : 0.39195609395122694, total_step : 426412, cur_epsilon : 0.45\n",
      "episode : 15040, reward mean : 0.39209774548740206, total_step : 426617, cur_epsilon : 0.45\n",
      "episode : 15050, reward mean : 0.3924013350275268, total_step : 426778, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15050, reward mean : 0.5390000035986304, step mean : 17.1, eval_episode_rewards : [ 0.84  0.92  0.81  0.91  0.9   0.83  0.96 -1.13  0.97  0.92  0.78  0.81\n",
      "  0.9  -1.39  0.99  0.95  0.91  0.82 -1.89  0.97]\n",
      "episode : 15060, reward mean : 0.39265007252432654, total_step : 427021, cur_epsilon : 0.45\n",
      "episode : 15070, reward mean : 0.3925779755996675, total_step : 427347, cur_epsilon : 0.45\n",
      "episode : 15080, reward mean : 0.392760616201359, total_step : 427489, cur_epsilon : 0.45\n",
      "episode : 15090, reward mean : 0.3927117357441552, total_step : 427780, cur_epsilon : 0.45\n",
      "episode : 15100, reward mean : 0.3926099398965324, total_step : 428050, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15100, reward mean : 0.7770000049844384, step mean : 23.3, eval_episode_rewards : [0.69 0.64 0.9  0.79 0.81 0.33 0.9  0.86 0.84 0.8  0.87 0.84 0.89 0.48\n",
      " 0.94 0.93 0.74 0.84 0.62 0.83]\n",
      "episode : 15110, reward mean : 0.39278690221501866, total_step : 428200, cur_epsilon : 0.45\n",
      "episode : 15120, reward mean : 0.39277381564431363, total_step : 428536, cur_epsilon : 0.45\n",
      "episode : 15130, reward mean : 0.3929537404211631, total_step : 428681, cur_epsilon : 0.45\n",
      "episode : 15140, reward mean : 0.3931142729596337, total_step : 428855, cur_epsilon : 0.45\n",
      "episode : 15150, reward mean : 0.3931544615619696, total_step : 429110, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15150, reward mean : 0.5000000044703483, step mean : 21.0, eval_episode_rewards : [ 0.9   0.81  0.74  0.82  0.6   0.85  0.95  0.75  0.91  0.97  0.13 -1.05\n",
      " -1.04  0.87  0.76 -1.11  0.91  0.84  0.95  0.44]\n",
      "episode : 15160, reward mean : 0.3932269190455293, total_step : 429417, cur_epsilon : 0.45\n",
      "episode : 15170, reward mean : 0.393468694316669, total_step : 429667, cur_epsilon : 0.45\n",
      "episode : 15180, reward mean : 0.393234525218714, total_step : 429839, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 15190, reward mean : 0.3932547789900867, total_step : 430025, cur_epsilon : 0.45\n",
      "episode : 15200, reward mean : 0.39340263769079586, total_step : 430217, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15200, reward mean : 0.6675000029616058, step mean : 14.25, eval_episode_rewards : [ 0.81  0.86  0.85 -1.13  0.99  0.94  0.99  0.77  0.88  0.8   0.91  0.71\n",
      "  0.97  0.93  0.9   0.87 -1.15  0.9   0.75  0.8 ]\n",
      "episode : 15210, reward mean : 0.39364760637425383, total_step : 430461, cur_epsilon : 0.45\n",
      "episode : 15220, reward mean : 0.3935834489500887, total_step : 430775, cur_epsilon : 0.45\n",
      "episode : 15230, reward mean : 0.39384898838250165, total_step : 430987, cur_epsilon : 0.45\n",
      "episode : 15240, reward mean : 0.3937690349828618, total_step : 431325, cur_epsilon : 0.45\n",
      "episode : 15250, reward mean : 0.39366033398176803, total_step : 431705, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15250, reward mean : 0.7420000035315752, step mean : 16.8, eval_episode_rewards : [ 0.88  0.62  0.9   0.9   0.83  0.97  0.98  0.61  0.83 -1.02  0.98  0.76\n",
      "  0.94  0.85  0.97  0.83  0.93  0.64  0.48  0.96]\n",
      "episode : 15260, reward mean : 0.3939744490989744, total_step : 431842, cur_epsilon : 0.45\n",
      "episode : 15270, reward mean : 0.3939744658347344, total_step : 432058, cur_epsilon : 0.45\n",
      "episode : 15280, reward mean : 0.3938841684130191, total_step : 432311, cur_epsilon : 0.45\n",
      "episode : 15290, reward mean : 0.39391105908357177, total_step : 432486, cur_epsilon : 0.45\n",
      "episode : 15300, reward mean : 0.3937980453235408, total_step : 432774, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15300, reward mean : 0.663500006403774, step mean : 29.6, eval_episode_rewards : [ 0.96  0.93  0.91  0.67  0.74  0.34  0.69  0.87  0.74  0.64  0.99  0.71\n",
      "  0.92  0.67  0.73  0.54  0.92  0.48  0.82 -1.  ]\n",
      "episode : 15310, reward mean : 0.3936760348460688, total_step : 432977, cur_epsilon : 0.45\n",
      "episode : 15320, reward mean : 0.3935652802581595, total_step : 433262, cur_epsilon : 0.45\n",
      "episode : 15330, reward mean : 0.39372146729384755, total_step : 433538, cur_epsilon : 0.45\n",
      "episode : 15340, reward mean : 0.39365971927522564, total_step : 433849, cur_epsilon : 0.45\n",
      "episode : 15350, reward mean : 0.39381108102415985, total_step : 434033, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15350, reward mean : 0.52200000397861, step mean : 18.8, eval_episode_rewards : [ 0.93  0.93  0.72  0.83  0.92  0.98  0.67 -1.24  0.72  0.76  0.94  0.89\n",
      " -1.22  0.78  0.84  0.63 -1.05  0.82  0.99  0.6 ]\n",
      "episode : 15360, reward mean : 0.39393425089644246, total_step : 434260, cur_epsilon : 0.45\n",
      "episode : 15370, reward mean : 0.3939193294613235, total_step : 434499, cur_epsilon : 0.45\n",
      "episode : 15380, reward mean : 0.3941384976515389, total_step : 434778, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 15390, reward mean : 0.393990259514813, total_step : 435022, cur_epsilon : 0.45\n",
      "episode : 15400, reward mean : 0.39400065545295065, total_step : 435222, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15400, reward mean : 0.6905000046826899, step mean : 21.95, eval_episode_rewards : [ 0.64  0.9   0.88  0.45  0.64  0.97  0.95  0.92  0.66  0.13  0.92  0.96\n",
      "  0.93  0.89  0.91  0.91  0.82 -1.2   0.67  0.86]\n",
      "episode : 15410, reward mean : 0.394000655030596, total_step : 435438, cur_epsilon : 0.45\n",
      "episode : 15420, reward mean : 0.3939701747132318, total_step : 435701, cur_epsilon : 0.45\n",
      "episode : 15430, reward mean : 0.3941380488759297, total_step : 436058, cur_epsilon : 0.45\n",
      "episode : 15440, reward mean : 0.39433225998860044, total_step : 436374, cur_epsilon : 0.45\n",
      "episode : 15450, reward mean : 0.39446473102064195, total_step : 436585, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15450, reward mean : 0.6390000047162175, step mean : 22.05, eval_episode_rewards : [ 0.83  0.88  0.95  0.99  0.69  0.82  0.97  0.43  0.93  0.94  0.52  0.89\n",
      "  0.83 -1.32  0.81  0.94 -1.    0.89  0.87  0.92]\n",
      "episode : 15460, reward mean : 0.39447866069420423, total_step : 436878, cur_epsilon : 0.45\n",
      "episode : 15470, reward mean : 0.39464318644900764, total_step : 437039, cur_epsilon : 0.45\n",
      "episode : 15480, reward mean : 0.39493217664092145, total_step : 437207, cur_epsilon : 0.45\n",
      "episode : 15490, reward mean : 0.3952162746570913, total_step : 437382, cur_epsilon : 0.45\n",
      "episode : 15500, reward mean : 0.39533097383800536, total_step : 437619, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15500, reward mean : 0.5135000052861869, step mean : 24.6, eval_episode_rewards : [ 0.92  0.92  0.93 -1.   -1.38  0.92  0.63  0.96  0.76  0.96  0.83  0.8\n",
      "  0.38  0.89  0.83 -1.04  0.81  0.78  0.52  0.85]\n",
      "episode : 15510, reward mean : 0.39534430009865956, total_step : 437813, cur_epsilon : 0.45\n",
      "episode : 15520, reward mean : 0.3955818359911701, total_step : 438059, cur_epsilon : 0.45\n",
      "episode : 15530, reward mean : 0.39573664485782156, total_step : 438332, cur_epsilon : 0.45\n",
      "episode : 15540, reward mean : 0.39573488382807187, total_step : 438549, cur_epsilon : 0.45\n",
      "episode : 15550, reward mean : 0.3956578839074377, total_step : 438883, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15550, reward mean : 0.39000000469386575, step mean : 22.0, eval_episode_rewards : [ 0.1   0.87  0.99  0.54  0.98  0.75  0.96  0.89  0.66  0.84  0.7  -1.28\n",
      "  0.83  0.75  0.87  0.89  0.76 -1.11 -1.18 -1.01]\n",
      "episode : 15560, reward mean : 0.39567995467857675, total_step : 439063, cur_epsilon : 0.45\n",
      "episode : 15570, reward mean : 0.39592935740828095, total_step : 439289, cur_epsilon : 0.45\n",
      "episode : 15580, reward mean : 0.39598460172772865, total_step : 439617, cur_epsilon : 0.45\n",
      "episode : 15590, reward mean : 0.3960275878764168, total_step : 439964, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 15600, reward mean : 0.39630385224564146, total_step : 440147, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15600, reward mean : 0.4955000068061054, step mean : 31.35, eval_episode_rewards : [ 0.83  0.91  0.8  -1.88  0.73  0.65  0.64  0.64  0.99  0.88  0.8   0.89\n",
      "  0.95  0.85  0.63  0.92  0.96 -1.   -1.    0.72]\n",
      "episode : 15610, reward mean : 0.39639014062089256, total_step : 440426, cur_epsilon : 0.45\n",
      "episode : 15620, reward mean : 0.3966927077542518, total_step : 440567, cur_epsilon : 0.45\n",
      "episode : 15630, reward mean : 0.39696417755351393, total_step : 440756, cur_epsilon : 0.45\n",
      "episode : 15640, reward mean : 0.39723530020469516, total_step : 440945, cur_epsilon : 0.45\n",
      "episode : 15650, reward mean : 0.397293935797204, total_step : 441066, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15650, reward mean : 0.6590000053867697, step mean : 25.1, eval_episode_rewards : [ 0.3   0.88  0.92  0.94  0.96  0.52  0.86  0.62 -1.41  0.69  0.78  0.96\n",
      "  0.89  0.89  0.57  0.68  0.86  0.69  0.63  0.95]\n",
      "episode : 15660, reward mean : 0.3972720367370095, total_step : 441412, cur_epsilon : 0.45\n",
      "episode : 15670, reward mean : 0.39756988483292705, total_step : 441558, cur_epsilon : 0.45\n",
      "episode : 15680, reward mean : 0.39761352648991827, total_step : 441702, cur_epsilon : 0.45\n",
      "episode : 15690, reward mean : 0.39778139549987174, total_step : 441851, cur_epsilon : 0.45\n",
      "episode : 15700, reward mean : 0.3978191143582771, total_step : 442004, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15700, reward mean : 0.33400000594556334, step mean : 27.6, eval_episode_rewards : [ 0.8   0.43  0.62  0.89  0.33  0.96 -1.56  0.93 -1.59  0.94  0.96  0.96\n",
      "  0.45  0.77 -1.5   0.86  0.85 -1.25  0.89  0.94]\n",
      "episode : 15710, reward mean : 0.3976836470702105, total_step : 442229, cur_epsilon : 0.45\n",
      "episode : 15720, reward mean : 0.39760051498535554, total_step : 442671, cur_epsilon : 0.45\n",
      "episode : 15730, reward mean : 0.39766561320005367, total_step : 442981, cur_epsilon : 0.45\n",
      "episode : 15740, reward mean : 0.3979250378449941, total_step : 443185, cur_epsilon : 0.45\n",
      "episode : 15750, reward mean : 0.3978158790946953, total_step : 443468, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15750, reward mean : 0.22950000716373326, step mean : 33.0, eval_episode_rewards : [ 0.88  0.98 -1.1  -1.03  0.55 -1.    0.89 -1.13  0.92  0.06  0.49  0.49\n",
      "  0.02  0.98  0.96  0.36  0.67 -1.25  0.98  0.87]\n",
      "episode : 15760, reward mean : 0.3979714527776994, total_step : 443635, cur_epsilon : 0.45\n",
      "episode : 15770, reward mean : 0.39801776130803607, total_step : 443873, cur_epsilon : 0.45\n",
      "episode : 15780, reward mean : 0.3982813748966605, total_step : 444069, cur_epsilon : 0.45\n",
      "episode : 15790, reward mean : 0.39813806814217656, total_step : 444505, cur_epsilon : 0.45\n",
      "episode : 15800, reward mean : 0.39825760101345425, total_step : 444728, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15800, reward mean : 0.6545000043697655, step mean : 20.5, eval_episode_rewards : [ 0.99 -1.25  0.96  0.63  0.94  0.73 -1.    0.91  0.84  0.93  0.52  0.92\n",
      "  0.94  0.75  0.89  0.91  0.81  0.92  0.88  0.87]\n",
      "episode : 15810, reward mean : 0.3983769826730036, total_step : 444951, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 15820, reward mean : 0.39803477219575556, total_step : 445304, cur_epsilon : 0.45\n",
      "episode : 15830, reward mean : 0.3979431520034374, total_step : 445659, cur_epsilon : 0.45\n",
      "episode : 15840, reward mean : 0.39771338991486854, total_step : 445835, cur_epsilon : 0.45\n",
      "episode : 15850, reward mean : 0.3977028451924585, total_step : 446064, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15850, reward mean : 0.7695000051520765, step mean : 24.05, eval_episode_rewards : [0.46 0.78 0.39 0.39 0.86 0.79 0.92 0.89 0.97 0.89 0.54 0.59 0.97 0.79\n",
      " 0.92 0.94 0.9  0.74 0.97 0.69]\n",
      "episode : 15860, reward mean : 0.39770492410749986, total_step : 446273, cur_epsilon : 0.45\n",
      "episode : 15870, reward mean : 0.39780655931939607, total_step : 446524, cur_epsilon : 0.45\n",
      "episode : 15880, reward mean : 0.39782053504024234, total_step : 446714, cur_epsilon : 0.45\n",
      "episode : 15890, reward mean : 0.3980239204846702, total_step : 447003, cur_epsilon : 0.45\n",
      "episode : 15900, reward mean : 0.39825409412290314, total_step : 447249, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15900, reward mean : 0.6600000031292439, step mean : 15.0, eval_episode_rewards : [-1.02  0.99  0.87  0.67  0.84  0.63  0.93  0.96  0.88  0.86  0.8   0.94\n",
      "  0.76  0.96  0.89 -1.31  0.77  0.88  0.99  0.91]\n",
      "episode : 15910, reward mean : 0.39839095515998035, total_step : 447443, cur_epsilon : 0.45\n",
      "episode : 15920, reward mean : 0.3986740010445376, total_step : 447604, cur_epsilon : 0.45\n",
      "episode : 15930, reward mean : 0.39880038271649354, total_step : 447814, cur_epsilon : 0.45\n",
      "episode : 15940, reward mean : 0.39867691949718026, total_step : 448319, cur_epsilon : 0.45\n",
      "episode : 15950, reward mean : 0.39865455152589513, total_step : 448566, cur_epsilon : 0.45\n",
      "[EVAL] episode : 15950, reward mean : 0.5690000062808395, step mean : 29.05, eval_episode_rewards : [-1.    0.86 -1.35  0.97  0.56  0.64  0.79  0.78  0.9   0.9   0.73  0.91\n",
      "  0.92  0.9   0.8   0.73  0.53  0.3   0.74  0.77]\n",
      "episode : 15960, reward mean : 0.39855263764878973, total_step : 448740, cur_epsilon : 0.45\n",
      "episode : 15970, reward mean : 0.3986011331811146, total_step : 448874, cur_epsilon : 0.45\n",
      "episode : 15980, reward mean : 0.39879349793012586, total_step : 448978, cur_epsilon : 0.45\n",
      "episode : 15990, reward mean : 0.3988724263280099, total_step : 449263, cur_epsilon : 0.45\n",
      "episode : 16000, reward mean : 0.399104381064768, total_step : 449503, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16000, reward mean : 0.2735000050626695, step mean : 23.65, eval_episode_rewards : [-1.38  0.74  0.51  0.91  0.96 -1.14 -1.26  0.94  0.76  0.91  0.97  0.88\n",
      "  0.86  0.9   0.57 -1.02  0.64  0.57  0.53 -1.38]\n",
      "episode : 16010, reward mean : 0.3990337349833929, total_step : 449726, cur_epsilon : 0.45\n",
      "episode : 16020, reward mean : 0.3993089948264596, total_step : 449896, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 16030, reward mean : 0.39916906407798963, total_step : 450131, cur_epsilon : 0.45\n",
      "episode : 16040, reward mean : 0.39913716316873443, total_step : 450393, cur_epsilon : 0.45\n",
      "episode : 16050, reward mean : 0.3992797568389374, total_step : 450575, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16050, reward mean : 0.8235000039450824, step mean : 18.65, eval_episode_rewards : [0.89 0.97 0.77 0.74 0.96 0.96 0.9  0.91 0.3  0.96 0.99 0.38 0.91 0.93\n",
      " 0.91 0.85 0.81 0.48 0.97 0.88]\n",
      "episode : 16060, reward mean : 0.3994452115376787, total_step : 450720, cur_epsilon : 0.45\n",
      "episode : 16070, reward mean : 0.3994492904393799, total_step : 451023, cur_epsilon : 0.45\n",
      "episode : 16080, reward mean : 0.39950249362134593, total_step : 451348, cur_epsilon : 0.45\n",
      "episode : 16090, reward mean : 0.3994170352707763, total_step : 451696, cur_epsilon : 0.45\n",
      "episode : 16100, reward mean : 0.3995416209659314, total_step : 451906, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16100, reward mean : 0.8055000043474138, step mean : 20.45, eval_episode_rewards : [0.94 0.95 0.61 0.87 0.95 0.94 0.46 0.77 0.72 0.88 0.91 0.87 0.71 0.54\n",
      " 0.89 0.97 0.86 0.72 0.76 0.79]\n",
      "episode : 16110, reward mean : 0.3997964058096811, total_step : 452106, cur_epsilon : 0.45\n",
      "episode : 16120, reward mean : 0.39958809538889983, total_step : 452452, cur_epsilon : 0.45\n",
      "episode : 16130, reward mean : 0.3995716117619221, total_step : 452689, cur_epsilon : 0.45\n",
      "episode : 16140, reward mean : 0.3995470940380079, total_step : 452939, cur_epsilon : 0.45\n",
      "episode : 16150, reward mean : 0.39966440234321743, total_step : 453259, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16150, reward mean : 0.5890000047162175, step mean : 22.1, eval_episode_rewards : [ 0.95  0.46  0.75  0.66  0.8   0.81  0.9   0.83  0.94  0.99  0.72  0.88\n",
      " -1.09  0.84  0.66  0.83  0.74  0.63 -1.09  0.57]\n",
      "episode : 16160, reward mean : 0.39940532783984606, total_step : 453488, cur_epsilon : 0.45\n",
      "episode : 16170, reward mean : 0.3994094061802891, total_step : 453692, cur_epsilon : 0.45\n",
      "episode : 16180, reward mean : 0.39953152645121437, total_step : 453905, cur_epsilon : 0.45\n",
      "episode : 16190, reward mean : 0.3992680727656634, total_step : 454340, cur_epsilon : 0.45\n",
      "episode : 16200, reward mean : 0.3993506233430939, total_step : 454716, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16200, reward mean : 0.5390000069513917, step mean : 32.05, eval_episode_rewards : [ 0.68 -1.44  0.9   0.95  0.84 -1.    0.06  0.76  0.77  0.49  0.49  0.88\n",
      "  0.67  0.92  0.69  0.97  0.75  0.98  0.62  0.8 ]\n",
      "episode : 16210, reward mean : 0.39932141259799836, total_step : 454974, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 16220, reward mean : 0.3995635079075244, total_step : 455192, cur_epsilon : 0.45\n",
      "episode : 16230, reward mean : 0.3997147318741246, total_step : 455456, cur_epsilon : 0.45\n",
      "episode : 16240, reward mean : 0.3998750060559319, total_step : 455606, cur_epsilon : 0.45\n",
      "episode : 16250, reward mean : 0.4000621599024305, total_step : 455912, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16250, reward mean : 0.356000005453825, step mean : 25.4, eval_episode_rewards : [ 0.22  0.63  0.84  0.85  0.55  0.72 -1.04  0.91  0.65 -1.05  0.94 -1.74\n",
      "  0.76  0.93  0.86  0.89  0.97 -1.42  0.84  0.81]\n",
      "episode : 16260, reward mean : 0.4001519125751088, total_step : 456176, cur_epsilon : 0.45\n",
      "episode : 16270, reward mean : 0.40024708657197483, total_step : 456431, cur_epsilon : 0.45\n",
      "episode : 16280, reward mean : 0.4003679421726907, total_step : 456644, cur_epsilon : 0.45\n",
      "episode : 16290, reward mean : 0.4002811601366036, total_step : 456894, cur_epsilon : 0.45\n",
      "episode : 16300, reward mean : 0.4005484723105478, total_step : 457068, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16300, reward mean : 0.6840000048279762, step mean : 22.6, eval_episode_rewards : [ 0.83 -1.04  0.99  0.74  0.7   0.85  0.59  0.98  0.99  0.86  0.63  0.9\n",
      "  0.76  0.47  0.59  0.96  0.9   0.73  0.3   0.95]\n",
      "episode : 16310, reward mean : 0.40069160629943207, total_step : 457444, cur_epsilon : 0.45\n",
      "episode : 16320, reward mean : 0.40095159919012574, total_step : 457629, cur_epsilon : 0.45\n",
      "episode : 16330, reward mean : 0.40120943654762026, total_step : 457817, cur_epsilon : 0.45\n",
      "episode : 16340, reward mean : 0.4013035556242198, total_step : 458171, cur_epsilon : 0.45\n",
      "episode : 16350, reward mean : 0.4011626971835436, total_step : 458410, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16350, reward mean : 0.5225000039674341, step mean : 18.75, eval_episode_rewards : [ 0.97  0.69  0.85  0.95  0.44  0.87  0.59  0.89  0.87 -1.25  0.92  0.77\n",
      " -1.13  0.94  0.72  0.78  0.98  0.78 -1.02  0.84]\n",
      "episode : 16360, reward mean : 0.40136797671208574, total_step : 458683, cur_epsilon : 0.45\n",
      "episode : 16370, reward mean : 0.40119853995549254, total_step : 458969, cur_epsilon : 0.45\n",
      "episode : 16380, reward mean : 0.40130708785841335, total_step : 459200, cur_epsilon : 0.45\n",
      "episode : 16390, reward mean : 0.40135449049511357, total_step : 459630, cur_epsilon : 0.45\n",
      "episode : 16400, reward mean : 0.4016292743443898, total_step : 459788, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16400, reward mean : 0.6185000062920153, step mean : 29.15, eval_episode_rewards : [-1.11  0.91  0.77  0.94  0.69  0.9   0.95  0.63  0.31  0.3   0.64  0.92\n",
      "  0.35  0.89  0.82  0.7   0.62  0.89  0.44  0.81]\n",
      "episode : 16410, reward mean : 0.40167276656163187, total_step : 459925, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 16420, reward mean : 0.4019330145745798, total_step : 460106, cur_epsilon : 0.45\n",
      "episode : 16430, reward mean : 0.4019945282642109, total_step : 460413, cur_epsilon : 0.45\n",
      "episode : 16440, reward mean : 0.40197202551298733, total_step : 460658, cur_epsilon : 0.45\n",
      "episode : 16450, reward mean : 0.40189423097031457, total_step : 460794, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16450, reward mean : 0.7190000040456652, step mean : 19.1, eval_episode_rewards : [ 0.44  0.83  0.8   0.69  0.98  0.78  0.85  0.83  0.87  0.85  0.9   0.8\n",
      "  0.91  0.63  0.66 -1.02  0.93  0.99  0.98  0.68]\n",
      "episode : 16460, reward mean : 0.40190037056794653, total_step : 461192, cur_epsilon : 0.45\n",
      "episode : 16470, reward mean : 0.401952647214216, total_step : 461514, cur_epsilon : 0.45\n",
      "episode : 16480, reward mean : 0.4021771905139016, total_step : 461752, cur_epsilon : 0.45\n",
      "episode : 16490, reward mean : 0.4021928501945145, total_step : 461934, cur_epsilon : 0.45\n",
      "episode : 16500, reward mean : 0.40231455150009554, total_step : 462141, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16500, reward mean : 0.7075000043027103, step mean : 20.25, eval_episode_rewards : [ 0.92  0.99  0.74  0.82  0.65  0.9   0.87  0.58 -1.38  0.67  0.94  0.86\n",
      "  0.7   0.83  0.97  0.82  0.91  0.51  0.97  0.88]\n",
      "episode : 16510, reward mean : 0.4024252028951928, total_step : 462366, cur_epsilon : 0.45\n",
      "episode : 16520, reward mean : 0.40245521185428046, total_step : 462524, cur_epsilon : 0.45\n",
      "episode : 16530, reward mean : 0.40262069569904013, total_step : 462858, cur_epsilon : 0.45\n",
      "episode : 16540, reward mean : 0.4022799334909988, total_step : 463029, cur_epsilon : 0.45\n",
      "episode : 16550, reward mean : 0.4023081631418261, total_step : 463289, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16550, reward mean : 0.49100000467151406, step mean : 21.9, eval_episode_rewards : [-1.03  0.89  0.61  0.91  0.84  0.95  0.69 -1.08  0.65  0.88  0.61  0.91\n",
      " -1.07  0.78  0.65  0.84  0.87  0.72  0.58  0.62]\n",
      "episode : 16560, reward mean : 0.40251932971332455, total_step : 463547, cur_epsilon : 0.45\n",
      "episode : 16570, reward mean : 0.4026710983756963, total_step : 463703, cur_epsilon : 0.45\n",
      "episode : 16580, reward mean : 0.40286550664335075, total_step : 463988, cur_epsilon : 0.45\n",
      "episode : 16590, reward mean : 0.4026889753004176, total_step : 464387, cur_epsilon : 0.45\n",
      "episode : 16600, reward mean : 0.40288072893351135, total_step : 464676, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16600, reward mean : 0.8055000043474138, step mean : 20.45, eval_episode_rewards : [0.78 0.4  0.79 0.96 0.91 0.49 0.61 0.98 0.57 0.76 0.87 0.77 0.94 0.98\n",
      " 0.8  0.96 0.98 0.96 0.76 0.84]\n",
      "episode : 16610, reward mean : 0.4028007284964085, total_step : 464816, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 16620, reward mean : 0.4028026534531324, total_step : 465119, cur_epsilon : 0.45\n",
      "episode : 16630, reward mean : 0.40289417320780646, total_step : 465374, cur_epsilon : 0.45\n",
      "episode : 16640, reward mean : 0.40296454930939946, total_step : 465664, cur_epsilon : 0.45\n",
      "episode : 16650, reward mean : 0.4030456516857038, total_step : 465936, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16650, reward mean : 0.44000000357627866, step mean : 17.0, eval_episode_rewards : [ 0.7  -1.3   0.87  0.81  0.81  0.99 -1.18  0.93  0.94  0.83 -1.08  0.94\n",
      "  0.7   0.9   0.53  0.92  0.81  0.86  0.93 -1.11]\n",
      "episode : 16660, reward mean : 0.4032719148028897, total_step : 466166, cur_epsilon : 0.45\n",
      "episode : 16670, reward mean : 0.40343791845756855, total_step : 466496, cur_epsilon : 0.45\n",
      "episode : 16680, reward mean : 0.4033279436887231, total_step : 466686, cur_epsilon : 0.45\n",
      "episode : 16690, reward mean : 0.4032947933363827, total_step : 466948, cur_epsilon : 0.45\n",
      "episode : 16700, reward mean : 0.4033802455593414, total_step : 467212, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16700, reward mean : 0.7800000049173832, step mean : 23.0, eval_episode_rewards : [0.75 0.67 0.01 0.97 0.66 0.84 0.98 0.62 0.78 0.73 0.87 0.85 0.85 0.99\n",
      " 0.46 0.79 0.98 0.93 0.97 0.9 ]\n",
      "episode : 16710, reward mean : 0.4034871394907078, total_step : 467440, cur_epsilon : 0.45\n",
      "episode : 16720, reward mean : 0.40359510173074203, total_step : 467666, cur_epsilon : 0.45\n",
      "episode : 16730, reward mean : 0.4036951644345005, total_step : 467905, cur_epsilon : 0.45\n",
      "episode : 16740, reward mean : 0.40381661296489435, total_step : 468108, cur_epsilon : 0.45\n",
      "episode : 16750, reward mean : 0.40383164782510766, total_step : 468289, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16750, reward mean : 0.5555000054650009, step mean : 25.45, eval_episode_rewards : [ 0.15  0.87  0.96  0.28  0.83  0.65  0.93  0.94  0.72  0.68  0.89  0.57\n",
      "  0.96  0.94  0.75  0.59  0.75 -1.05 -1.15  0.85]\n",
      "episode : 16760, reward mean : 0.4037506026909342, total_step : 468431, cur_epsilon : 0.45\n",
      "episode : 16770, reward mean : 0.4040041801513583, total_step : 468612, cur_epsilon : 0.45\n",
      "episode : 16780, reward mean : 0.4038915435758034, total_step : 468906, cur_epsilon : 0.45\n",
      "episode : 16790, reward mean : 0.40416796314660425, total_step : 469048, cur_epsilon : 0.45\n",
      "episode : 16800, reward mean : 0.40417083936288867, total_step : 469348, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16800, reward mean : 0.6900000046938658, step mean : 22.0, eval_episode_rewards : [ 0.7   0.8   0.95  0.96  0.84  0.86  0.69  0.82  0.65  0.62  0.9   0.56\n",
      "  0.81  0.72  0.92  0.79  0.63 -1.33  0.93  0.98]\n",
      "episode : 16810, reward mean : 0.4040475967484773, total_step : 469561, cur_epsilon : 0.45\n",
      "episode : 16820, reward mean : 0.40418133777527493, total_step : 469742, cur_epsilon : 0.45\n",
      "episode : 16830, reward mean : 0.40429234114241475, total_step : 469961, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 16840, reward mean : 0.4043735214658391, total_step : 470230, cur_epsilon : 0.45\n",
      "episode : 16850, reward mean : 0.40450208317654246, total_step : 470419, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16850, reward mean : 0.4040000043809414, step mean : 20.6, eval_episode_rewards : [ 0.72 -1.19  0.96 -1.15  0.97  0.44  0.69  0.94  0.88  0.98  0.97  0.83\n",
      "  0.99  0.92 -1.07  0.47  0.91 -1.69  0.97  0.54]\n",
      "episode : 16860, reward mean : 0.40461685062692115, total_step : 470631, cur_epsilon : 0.45\n",
      "episode : 16870, reward mean : 0.4047065857512999, total_step : 470885, cur_epsilon : 0.45\n",
      "episode : 16880, reward mean : 0.40494313398508525, total_step : 471091, cur_epsilon : 0.45\n",
      "episode : 16890, reward mean : 0.40505625232171916, total_step : 471305, cur_epsilon : 0.45\n",
      "episode : 16900, reward mean : 0.4051011894520967, total_step : 471434, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16900, reward mean : 0.7250000039115548, step mean : 18.5, eval_episode_rewards : [ 0.64  0.97  0.76  0.97  0.9   0.76  0.38 -1.06  0.97  0.97  0.93  0.93\n",
      "  0.93  0.61  0.98  0.75  0.8   0.76  0.86  0.69]\n",
      "episode : 16910, reward mean : 0.4053116559310382, total_step : 471683, cur_epsilon : 0.45\n",
      "episode : 16920, reward mean : 0.4054870036560558, total_step : 471991, cur_epsilon : 0.45\n",
      "episode : 16930, reward mean : 0.40551093336914285, total_step : 472355, cur_epsilon : 0.45\n",
      "episode : 16940, reward mean : 0.40547580295137914, total_step : 472619, cur_epsilon : 0.45\n",
      "episode : 16950, reward mean : 0.40572920955948333, total_step : 472794, cur_epsilon : 0.45\n",
      "[EVAL] episode : 16950, reward mean : 0.5620000053197145, step mean : 24.8, eval_episode_rewards : [ 0.94  0.88  0.92  0.74  0.77  0.91  0.99  0.16  0.74  0.78  0.89  0.9\n",
      "  0.93 -1.43  0.72  0.91  0.64 -1.03  0.65  0.23]\n",
      "episode : 16960, reward mean : 0.40579717583110875, total_step : 473083, cur_epsilon : 0.45\n",
      "episode : 16970, reward mean : 0.4060359518054124, total_step : 473282, cur_epsilon : 0.45\n",
      "episode : 16980, reward mean : 0.40628740295495736, total_step : 473459, cur_epsilon : 0.45\n",
      "episode : 16990, reward mean : 0.40637434386287, total_step : 473715, cur_epsilon : 0.45\n",
      "episode : 17000, reward mean : 0.4062553001363926, total_step : 474121, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17000, reward mean : 0.6740000028163194, step mean : 13.6, eval_episode_rewards : [ 0.87  0.97  0.92  0.96  0.78  0.88  0.81 -1.45  0.98  0.88  0.83  0.98\n",
      "  0.95  0.67  0.97  0.84  0.84 -1.02  0.89  0.93]\n",
      "episode : 17010, reward mean : 0.4064250501109258, total_step : 474436, cur_epsilon : 0.45\n",
      "episode : 17020, reward mean : 0.4066239778169121, total_step : 474701, cur_epsilon : 0.45\n",
      "episode : 17030, reward mean : 0.40682619509685847, total_step : 474960, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 17040, reward mean : 0.40692195437505335, total_step : 475200, cur_epsilon : 0.45\n",
      "episode : 17050, reward mean : 0.40680000601735106, total_step : 475411, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17050, reward mean : 0.5875000047497452, step mean : 22.25, eval_episode_rewards : [ 0.64 -1.16  0.11  0.8   0.66  0.85 -1.07  0.68  0.99  0.46  0.87  0.95\n",
      "  0.98  0.74  0.99  0.58  0.88  0.93  0.91  0.96]\n",
      "episode : 17060, reward mean : 0.407018177177329, total_step : 475642, cur_epsilon : 0.45\n",
      "episode : 17070, reward mean : 0.407275342863483, total_step : 475806, cur_epsilon : 0.45\n",
      "episode : 17080, reward mean : 0.40740047439810684, total_step : 475995, cur_epsilon : 0.45\n",
      "episode : 17090, reward mean : 0.40749737289469884, total_step : 476232, cur_epsilon : 0.45\n",
      "episode : 17100, reward mean : 0.4076140411002071, total_step : 476435, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17100, reward mean : 0.8625000030733645, step mean : 14.75, eval_episode_rewards : [0.82 0.52 0.96 0.85 0.94 0.7  0.93 0.92 0.73 0.89 0.96 0.8  0.91 0.98\n",
      " 0.95 0.95 0.65 0.93 0.93 0.93]\n",
      "episode : 17110, reward mean : 0.40768264774254875, total_step : 476720, cur_epsilon : 0.45\n",
      "episode : 17120, reward mean : 0.40763493591917366, total_step : 477004, cur_epsilon : 0.45\n",
      "episode : 17130, reward mean : 0.40763281395357653, total_step : 477410, cur_epsilon : 0.45\n",
      "episode : 17140, reward mean : 0.407698955839389, total_step : 477699, cur_epsilon : 0.45\n",
      "episode : 17150, reward mean : 0.40782682817060034, total_step : 477882, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17150, reward mean : 0.4330000037327409, step mean : 17.7, eval_episode_rewards : [ 0.86 -1.53 -1.01  0.58  0.93  0.77  0.93 -1.13  0.88  0.79  0.71  0.71\n",
      " -1.11  0.99  0.97  0.58  0.92  0.93  0.95  0.94]\n",
      "episode : 17160, reward mean : 0.40779487780775087, total_step : 478139, cur_epsilon : 0.45\n",
      "episode : 17170, reward mean : 0.4079091498674906, total_step : 478345, cur_epsilon : 0.45\n",
      "episode : 17180, reward mean : 0.40804715385807977, total_step : 478609, cur_epsilon : 0.45\n",
      "episode : 17190, reward mean : 0.4081774347480722, total_step : 478787, cur_epsilon : 0.45\n",
      "episode : 17200, reward mean : 0.4081680292654566, total_step : 479005, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17200, reward mean : 0.4500000044703484, step mean : 20.95, eval_episode_rewards : [-1.19  0.81  0.69  0.96  0.85 -1.15  0.96 -1.01  0.97  0.86  0.93  0.73\n",
      "  0.5   0.95  0.72  0.76 -1.    0.92  0.76  0.98]\n",
      "episode : 17210, reward mean : 0.40807205714125194, total_step : 479172, cur_epsilon : 0.45\n",
      "episode : 17220, reward mean : 0.4078977992721378, total_step : 479474, cur_epsilon : 0.45\n",
      "episode : 17230, reward mean : 0.40800755098732366, total_step : 479687, cur_epsilon : 0.45\n",
      "episode : 17240, reward mean : 0.40811311505559894, total_step : 479907, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 17250, reward mean : 0.4081982668761326, total_step : 480162, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17250, reward mean : 0.6010000055655838, step mean : 25.85, eval_episode_rewards : [ 0.4   0.53  0.83  0.93 -1.51  0.92  0.99  0.84  0.98  0.97  0.85  0.99\n",
      "  0.99  0.78  0.95  0.88 -1.    0.69  0.35  0.66]\n",
      "episode : 17260, reward mean : 0.4081633895521213, total_step : 480424, cur_epsilon : 0.45\n",
      "episode : 17270, reward mean : 0.40813897531684523, total_step : 480668, cur_epsilon : 0.45\n",
      "episode : 17280, reward mean : 0.40810764489449664, total_step : 480924, cur_epsilon : 0.45\n",
      "episode : 17290, reward mean : 0.4083574380458453, total_step : 481094, cur_epsilon : 0.45\n",
      "episode : 17300, reward mean : 0.40824567074300666, total_step : 481289, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17300, reward mean : 0.5240000050514937, step mean : 23.55, eval_episode_rewards : [ 0.86  0.86  0.91  0.65  0.64  0.91  0.96  0.69  0.81  0.87  0.73  0.57\n",
      "  0.73  0.75 -1.    0.98  0.81 -1.03 -1.04  0.82]\n",
      "episode : 17310, reward mean : 0.40833102853313424, total_step : 481543, cur_epsilon : 0.45\n",
      "episode : 17320, reward mean : 0.4082586665102227, total_step : 481769, cur_epsilon : 0.45\n",
      "episode : 17330, reward mean : 0.4080069304101569, total_step : 482007, cur_epsilon : 0.45\n",
      "episode : 17340, reward mean : 0.4078985065771772, total_step : 482197, cur_epsilon : 0.45\n",
      "episode : 17350, reward mean : 0.40804092818908716, total_step : 482352, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17350, reward mean : 0.5650000052526594, step mean : 24.5, eval_episode_rewards : [ 0.89  0.93  0.79  0.97  0.33  0.83  0.82 -1.12 -1.65  0.95  0.6   0.84\n",
      "  0.71  0.88  0.87  0.7   0.78  0.76  0.69  0.73]\n",
      "episode : 17360, reward mean : 0.40819873871584317, total_step : 482480, cur_epsilon : 0.45\n",
      "episode : 17370, reward mean : 0.4084582673654895, total_step : 482631, cur_epsilon : 0.45\n",
      "episode : 17380, reward mean : 0.40848504626990584, total_step : 482786, cur_epsilon : 0.45\n",
      "episode : 17390, reward mean : 0.40856699851791944, total_step : 483045, cur_epsilon : 0.45\n",
      "episode : 17400, reward mean : 0.40866322438371766, total_step : 483279, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17400, reward mean : 0.5280000060796738, step mean : 28.1, eval_episode_rewards : [ 0.89  0.88  0.94  0.8  -1.03  0.58  0.89  0.86  0.43  0.9   0.97  0.63\n",
      " -1.    0.74 -1.    0.96  0.53  0.96  0.89  0.74]\n",
      "episode : 17410, reward mean : 0.40880184401552067, total_step : 483439, cur_epsilon : 0.45\n",
      "episode : 17420, reward mean : 0.408666481307882, total_step : 483775, cur_epsilon : 0.45\n",
      "episode : 17430, reward mean : 0.4087119968131407, total_step : 484097, cur_epsilon : 0.45\n",
      "episode : 17440, reward mean : 0.40869151975365714, total_step : 484334, cur_epsilon : 0.45\n",
      "episode : 17450, reward mean : 0.40890143865639383, total_step : 484569, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17450, reward mean : 0.506500004325062, step mean : 20.35, eval_episode_rewards : [-1.22  0.82  0.8   0.83  0.55  0.83  0.86  0.99  0.91  0.75  0.72  0.79\n",
      "  0.93  0.93  0.78  0.49 -1.36 -1.01  0.78  0.96]\n",
      "episode : 17460, reward mean : 0.4087863748350137, total_step : 484870, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 17470, reward mean : 0.40887979992388673, total_step : 485108, cur_epsilon : 0.45\n",
      "episode : 17480, reward mean : 0.4090320426054316, total_step : 485443, cur_epsilon : 0.45\n",
      "episode : 17490, reward mean : 0.40927673555056476, total_step : 485616, cur_epsilon : 0.45\n",
      "episode : 17500, reward mean : 0.4092554345617337, total_step : 485854, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17500, reward mean : 0.7280000038444996, step mean : 18.2, eval_episode_rewards : [ 0.82  0.8   0.84  0.63  0.97  0.99  0.89  0.95  0.95  0.82  0.53  0.78\n",
      " -1.07  0.78  0.91  0.73  0.61  0.78  0.88  0.97]\n",
      "episode : 17510, reward mean : 0.4093500916550903, total_step : 486089, cur_epsilon : 0.45\n",
      "episode : 17520, reward mean : 0.409206056217748, total_step : 486342, cur_epsilon : 0.45\n",
      "episode : 17530, reward mean : 0.4093462695360728, total_step : 486497, cur_epsilon : 0.45\n",
      "episode : 17540, reward mean : 0.4095416251438306, total_step : 486755, cur_epsilon : 0.45\n",
      "episode : 17550, reward mean : 0.40966553305193487, total_step : 486938, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17550, reward mean : 0.5780000060796737, step mean : 28.15, eval_episode_rewards : [ 0.82  0.76 -1.03  0.81  0.84  0.81  0.6   0.74  0.86  0.9   0.47  0.79\n",
      "  0.76  0.79  0.63 -1.    0.98  0.89  0.4   0.74]\n",
      "episode : 17560, reward mean : 0.4098092314972837, total_step : 487086, cur_epsilon : 0.45\n",
      "episode : 17570, reward mean : 0.4097313662587019, total_step : 487423, cur_epsilon : 0.45\n",
      "episode : 17580, reward mean : 0.4097025088293094, total_step : 487674, cur_epsilon : 0.45\n",
      "episode : 17590, reward mean : 0.40987152389339115, total_step : 487977, cur_epsilon : 0.45\n",
      "episode : 17600, reward mean : 0.40986648325734265, total_step : 488186, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17600, reward mean : 0.7090000065043569, step mean : 30.1, eval_episode_rewards : [0.97 0.31 0.39 0.89 0.8  0.94 0.9  0.97 0.73 0.54 0.99 0.86 0.66 0.86\n",
      " 0.41 0.25 0.75 0.47 0.94 0.55]\n",
      "episode : 17610, reward mean : 0.4101016527750465, total_step : 488372, cur_epsilon : 0.45\n",
      "episode : 17620, reward mean : 0.41024461438135157, total_step : 488520, cur_epsilon : 0.45\n",
      "episode : 17630, reward mean : 0.4103840105179581, total_step : 488674, cur_epsilon : 0.45\n",
      "episode : 17640, reward mean : 0.4103582826238168, total_step : 488919, cur_epsilon : 0.45\n",
      "episode : 17650, reward mean : 0.4105920739673479, total_step : 489106, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17650, reward mean : 0.6465000056661665, step mean : 26.35, eval_episode_rewards : [ 0.99  0.96  0.91  0.72  0.82  0.81  0.88  0.33  0.03  0.7   0.79  0.26\n",
      "  0.41 -1.03  0.89  0.94  0.98  0.81  0.9   0.83]\n",
      "episode : 17660, reward mean : 0.41055832987425517, total_step : 489365, cur_epsilon : 0.45\n",
      "episode : 17670, reward mean : 0.41073797994567873, total_step : 489647, cur_epsilon : 0.45\n",
      "episode : 17680, reward mean : 0.41097681593198365, total_step : 489824, cur_epsilon : 0.45\n",
      "episode : 17690, reward mean : 0.4109056023574435, total_step : 489949, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 17700, reward mean : 0.41099605117256277, total_step : 490188, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17700, reward mean : 0.1735000061802566, step mean : 28.6, eval_episode_rewards : [ 0.74 -1.   -1.02  0.77  0.61  0.6   0.64  0.91  0.54 -1.24  0.41  0.84\n",
      "  0.8  -1.06  0.86  0.78  0.88 -1.32 -1.12  0.85]\n",
      "episode : 17710, reward mean : 0.41100113528045995, total_step : 490477, cur_epsilon : 0.45\n",
      "episode : 17720, reward mean : 0.4110744980742007, total_step : 490746, cur_epsilon : 0.45\n",
      "episode : 17730, reward mean : 0.4112904741072127, total_step : 490962, cur_epsilon : 0.45\n",
      "episode : 17740, reward mean : 0.4113624636979465, total_step : 491332, cur_epsilon : 0.45\n",
      "episode : 17750, reward mean : 0.4114512735804622, total_step : 491573, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17750, reward mean : 0.3965000045485795, step mean : 21.35, eval_episode_rewards : [ 0.96  0.95 -1.94  0.97 -1.12  0.51  0.58 -1.11  0.93  0.77  0.83  0.95\n",
      "  0.91  0.59  0.82  0.93  0.95 -1.15  0.84  0.76]\n",
      "episode : 17760, reward mean : 0.41156588435217084, total_step : 491768, cur_epsilon : 0.45\n",
      "episode : 17770, reward mean : 0.4112211652339075, total_step : 492278, cur_epsilon : 0.45\n",
      "episode : 17780, reward mean : 0.4112699722313857, total_step : 492590, cur_epsilon : 0.45\n",
      "episode : 17790, reward mean : 0.41148398574033335, total_step : 492808, cur_epsilon : 0.45\n",
      "episode : 17800, reward mean : 0.4114792194586833, total_step : 493015, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17800, reward mean : 0.4700000051409006, step mean : 24.0, eval_episode_rewards : [ 0.86  0.65  0.78  0.86  0.71  0.91  0.94  0.94  0.68  0.79 -1.23 -1.06\n",
      "  0.96  0.45 -1.02  0.42  0.36  0.65  0.93  0.82]\n",
      "episode : 17810, reward mean : 0.411492987445324, total_step : 493189, cur_epsilon : 0.45\n",
      "episode : 17820, reward mean : 0.4113917006980356, total_step : 493368, cur_epsilon : 0.45\n",
      "episode : 17830, reward mean : 0.4114800957089527, total_step : 493609, cur_epsilon : 0.45\n",
      "episode : 17840, reward mean : 0.4115521360173061, total_step : 493879, cur_epsilon : 0.45\n",
      "episode : 17850, reward mean : 0.4114958042920637, total_step : 494178, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17850, reward mean : 0.6185000040568411, step mean : 19.15, eval_episode_rewards : [ 0.94  0.65  0.6   0.98  0.84  0.97  0.75 -1.09  0.96  0.41  0.9   0.91\n",
      "  0.72 -1.15  0.94  0.42  0.9   0.97  0.88  0.87]\n",
      "episode : 17860, reward mean : 0.41149216722728305, total_step : 494482, cur_epsilon : 0.45\n",
      "episode : 17870, reward mean : 0.41173587614513, total_step : 494645, cur_epsilon : 0.45\n",
      "episode : 17880, reward mean : 0.4119312140249941, total_step : 494894, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 17890, reward mean : 0.41201509820123483, total_step : 495142, cur_epsilon : 0.45\n",
      "episode : 17900, reward mean : 0.41213129088605166, total_step : 495332, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17900, reward mean : 0.8005000044591725, step mean : 20.95, eval_episode_rewards : [0.68 0.76 0.79 0.95 0.88 0.71 0.84 0.7  0.75 0.81 0.27 0.84 0.96 0.94\n",
      " 0.62 0.83 0.91 0.89 0.96 0.92]\n",
      "episode : 17910, reward mean : 0.41228643813119015, total_step : 495652, cur_epsilon : 0.45\n",
      "episode : 17920, reward mean : 0.41249888989820777, total_step : 495869, cur_epsilon : 0.45\n",
      "episode : 17930, reward mean : 0.41248076447434784, total_step : 496099, cur_epsilon : 0.45\n",
      "episode : 17940, reward mean : 0.4124437071951968, total_step : 496363, cur_epsilon : 0.45\n",
      "episode : 17950, reward mean : 0.41264680262572007, total_step : 496596, cur_epsilon : 0.45\n",
      "[EVAL] episode : 17950, reward mean : 0.6695000051520765, step mean : 24.05, eval_episode_rewards : [ 0.75  0.97  0.41  0.85  0.81  0.83  0.66  0.87  0.91  0.87 -1.11  0.85\n",
      "  0.66  0.95  0.53  0.89  0.54  0.6   0.88  0.67]\n",
      "episode : 17960, reward mean : 0.41251503937662, total_step : 496929, cur_epsilon : 0.45\n",
      "episode : 17970, reward mean : 0.4127184255566898, total_step : 497161, cur_epsilon : 0.45\n",
      "episode : 17980, reward mean : 0.4126996722650425, total_step : 497491, cur_epsilon : 0.45\n",
      "episode : 17990, reward mean : 0.4129271877357004, total_step : 497679, cur_epsilon : 0.45\n",
      "episode : 18000, reward mean : 0.4131383393006399, total_step : 497896, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18000, reward mean : 0.5405000035651029, step mean : 16.95, eval_episode_rewards : [ 0.92 -1.04  0.73  0.75  0.86  0.88  0.85  0.9  -1.06  0.1   0.86  0.9\n",
      "  0.65  0.93  0.86  0.82 -1.03  0.95  0.99  0.99]\n",
      "episode : 18010, reward mean : 0.4132815162401475, total_step : 498235, cur_epsilon : 0.45\n",
      "episode : 18020, reward mean : 0.41328136001813837, total_step : 498432, cur_epsilon : 0.45\n",
      "episode : 18030, reward mean : 0.41344260164132085, total_step : 498738, cur_epsilon : 0.45\n",
      "episode : 18040, reward mean : 0.41356707913687496, total_step : 498910, cur_epsilon : 0.45\n",
      "episode : 18050, reward mean : 0.41375956275250114, total_step : 499159, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18050, reward mean : 0.5800000049173832, step mean : 23.0, eval_episode_rewards : [ 0.24  0.84  0.74  0.9   0.74  0.83  0.8   0.8   0.92  0.81  0.78 -1.03\n",
      "  0.7   0.58  0.83  0.75  0.98  0.72 -1.02  0.69]\n",
      "episode : 18060, reward mean : 0.41376910895461905, total_step : 499338, cur_epsilon : 0.45\n",
      "episode : 18070, reward mean : 0.4138605483006181, total_step : 499668, cur_epsilon : 0.45\n",
      "episode : 18080, reward mean : 0.413778213928227, total_step : 499813, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 18090, reward mean : 0.4138501994405508, total_step : 500079, cur_epsilon : 0.45\n",
      "episode : 18100, reward mean : 0.41373702253714123, total_step : 500280, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18100, reward mean : 0.726000003889203, step mean : 18.4, eval_episode_rewards : [ 0.9   0.86  0.97  0.9   0.38  0.95  0.95  0.66  0.97  0.95  0.39  0.79\n",
      "  0.8   0.94  0.92  0.81 -1.06  0.88  0.74  0.82]\n",
      "episode : 18110, reward mean : 0.41381502528862607, total_step : 500535, cur_epsilon : 0.45\n",
      "episode : 18120, reward mean : 0.41384437682156433, total_step : 500678, cur_epsilon : 0.45\n",
      "episode : 18130, reward mean : 0.41400331539290425, total_step : 500986, cur_epsilon : 0.45\n",
      "episode : 18140, reward mean : 0.4140617479678026, total_step : 501276, cur_epsilon : 0.45\n",
      "episode : 18150, reward mean : 0.41407879383983115, total_step : 501540, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18150, reward mean : 0.42900000382214787, step mean : 18.1, eval_episode_rewards : [-1.05  0.88  0.8   0.45 -1.05 -1.15  0.96  0.63  0.87  0.86  0.71  0.95\n",
      "  0.75  0.93  0.87  0.9   0.7  -1.25  0.95  0.87]\n",
      "episode : 18160, reward mean : 0.4142323848161649, total_step : 501857, cur_epsilon : 0.45\n",
      "episode : 18170, reward mean : 0.41447991790269423, total_step : 502003, cur_epsilon : 0.45\n",
      "episode : 18180, reward mean : 0.41450110606847146, total_step : 502160, cur_epsilon : 0.45\n",
      "episode : 18190, reward mean : 0.4144826887506191, total_step : 502389, cur_epsilon : 0.45\n",
      "episode : 18200, reward mean : 0.41455604991374106, total_step : 502651, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18200, reward mean : 0.3820000048726797, step mean : 22.8, eval_episode_rewards : [ 0.82  0.9   0.5   0.87  0.96  0.93  0.22 -1.2   0.46  0.63  0.82  0.98\n",
      "  0.38  0.92  0.84 -1.09 -1.11  0.88 -1.03  0.96]\n",
      "episode : 18210, reward mean : 0.4145733173249237, total_step : 502914, cur_epsilon : 0.45\n",
      "episode : 18220, reward mean : 0.4143199840034067, total_step : 503171, cur_epsilon : 0.45\n",
      "episode : 18230, reward mean : 0.4145474552155947, total_step : 503352, cur_epsilon : 0.45\n",
      "episode : 18240, reward mean : 0.4147697427971779, total_step : 503542, cur_epsilon : 0.45\n",
      "episode : 18250, reward mean : 0.4146109648592872, total_step : 503827, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18250, reward mean : 0.7595000053755939, step mean : 25.05, eval_episode_rewards : [0.86 0.97 0.98 0.68 0.85 0.99 0.81 0.73 0.79 0.61 0.93 0.8  0.57 0.28\n",
      " 0.84 0.73 0.9  0.38 0.57 0.92]\n",
      "episode : 18260, reward mean : 0.41463472665708845, total_step : 504179, cur_epsilon : 0.45\n",
      "episode : 18270, reward mean : 0.41484182314201984, total_step : 504396, cur_epsilon : 0.45\n",
      "episode : 18280, reward mean : 0.41496335387533567, total_step : 504569, cur_epsilon : 0.45\n",
      "episode : 18290, reward mean : 0.4151607495293168, total_step : 504803, cur_epsilon : 0.45\n",
      "episode : 18300, reward mean : 0.4152797873731438, total_step : 504980, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18300, reward mean : 0.5720000050961971, step mean : 23.8, eval_episode_rewards : [ 0.68  0.1   0.65  0.79  0.78  0.46  0.72  0.99  0.84  0.78  0.92  0.93\n",
      " -1.32  0.77  0.98 -1.04  0.87  0.88  0.76  0.9 ]\n",
      "synced target net\n",
      "episode : 18310, reward mean : 0.41518624298241963, total_step : 505344, cur_epsilon : 0.45\n",
      "episode : 18320, reward mean : 0.4152805736384001, total_step : 505566, cur_epsilon : 0.45\n",
      "episode : 18330, reward mean : 0.41551500867922175, total_step : 505731, cur_epsilon : 0.45\n",
      "episode : 18340, reward mean : 0.41562977694269504, total_step : 505915, cur_epsilon : 0.45\n",
      "episode : 18350, reward mean : 0.4157558642595763, total_step : 506078, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18350, reward mean : 0.6480000033974648, step mean : 16.2, eval_episode_rewards : [ 0.83  0.92  0.83  0.89  0.94  0.6  -1.01  0.76  0.93  0.97  0.91  0.81\n",
      "  0.83  0.92  0.78  0.83 -1.04  0.91  0.63  0.72]\n",
      "episode : 18360, reward mean : 0.41594662904224006, total_step : 506322, cur_epsilon : 0.45\n",
      "episode : 18370, reward mean : 0.41595863414544026, total_step : 506494, cur_epsilon : 0.45\n",
      "episode : 18380, reward mean : 0.41617574044032035, total_step : 506689, cur_epsilon : 0.45\n",
      "episode : 18390, reward mean : 0.4161734697843997, total_step : 506887, cur_epsilon : 0.45\n",
      "episode : 18400, reward mean : 0.4160793537701197, total_step : 507054, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18400, reward mean : 0.632500005979091, step mean : 27.75, eval_episode_rewards : [ 0.95  0.69  0.73  0.82  0.18  0.45  0.92 -1.15  0.52  0.85  0.78  0.62\n",
      "  0.8   0.51  0.98  0.86  0.87  0.6   0.88  0.79]\n",
      "episode : 18410, reward mean : 0.41559913685238764, total_step : 507431, cur_epsilon : 0.45\n",
      "episode : 18420, reward mean : 0.41565364329619625, total_step : 507725, cur_epsilon : 0.45\n",
      "episode : 18430, reward mean : 0.4156972387173919, total_step : 507938, cur_epsilon : 0.45\n",
      "episode : 18440, reward mean : 0.4156735417360355, total_step : 508176, cur_epsilon : 0.45\n",
      "episode : 18450, reward mean : 0.4157777837211604, total_step : 508378, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18450, reward mean : 0.6080000042915344, step mean : 20.2, eval_episode_rewards : [ 0.93  0.93  0.88  0.81  0.86  0.87  0.87  0.85  0.8   0.6   0.78 -1.02\n",
      "  0.87  0.75  0.77  0.69  0.96 -1.13  0.53  0.56]\n",
      "episode : 18460, reward mean : 0.4158494100602241, total_step : 508640, cur_epsilon : 0.45\n",
      "episode : 18470, reward mean : 0.415938825651491, total_step : 508968, cur_epsilon : 0.45\n",
      "episode : 18480, reward mean : 0.41613258170094986, total_step : 509204, cur_epsilon : 0.45\n",
      "episode : 18490, reward mean : 0.4162877290348964, total_step : 509311, cur_epsilon : 0.45\n",
      "episode : 18500, reward mean : 0.41627460053643667, total_step : 509628, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18500, reward mean : 0.6650000052526593, step mean : 24.5, eval_episode_rewards : [ 0.81  0.72  0.85  0.92  0.94  0.62  0.78  0.75  0.71  0.62  0.84  0.79\n",
      " -1.05  0.03  0.92  0.6   0.92  0.77  0.94  0.82]\n",
      "episode : 18510, reward mean : 0.41630686709867626, total_step : 509960, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 18520, reward mean : 0.41640551350115684, total_step : 510171, cur_epsilon : 0.45\n",
      "episode : 18530, reward mean : 0.41650729142390774, total_step : 510376, cur_epsilon : 0.45\n",
      "episode : 18540, reward mean : 0.41666127886470433, total_step : 510684, cur_epsilon : 0.45\n",
      "episode : 18550, reward mean : 0.41687224313717164, total_step : 510886, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18550, reward mean : 0.7115000053308904, step mean : 24.8, eval_episode_rewards : [ 0.79  0.99  0.69  0.51  0.69  0.64  0.9   0.75  0.75  0.89  0.76  0.81\n",
      "  0.92  0.84  0.89 -1.    0.77  0.8   0.94  0.9 ]\n",
      "episode : 18560, reward mean : 0.41697360507748493, total_step : 511091, cur_epsilon : 0.45\n",
      "episode : 18570, reward mean : 0.41713247766841643, total_step : 511389, cur_epsilon : 0.45\n",
      "episode : 18580, reward mean : 0.4171383267137211, total_step : 511571, cur_epsilon : 0.45\n",
      "episode : 18590, reward mean : 0.4173652560719163, total_step : 511742, cur_epsilon : 0.45\n",
      "episode : 18600, reward mean : 0.41743333927065296, total_step : 512008, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18600, reward mean : 0.529000006057322, step mean : 28.1, eval_episode_rewards : [ 0.98  0.96  0.9   0.66  0.2   0.43  0.81  0.86  0.82  0.73  0.8   0.83\n",
      "  0.97  0.17  0.91 -1.2   0.55  0.87  0.64 -1.31]\n",
      "episode : 18610, reward mean : 0.41722246697868137, total_step : 512193, cur_epsilon : 0.45\n",
      "episode : 18620, reward mean : 0.41731901775193264, total_step : 512505, cur_epsilon : 0.45\n",
      "episode : 18630, reward mean : 0.41753087013328005, total_step : 512703, cur_epsilon : 0.45\n",
      "episode : 18640, reward mean : 0.41772640078496426, total_step : 512931, cur_epsilon : 0.45\n",
      "episode : 18650, reward mean : 0.41781930888356295, total_step : 513150, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18650, reward mean : 0.6130000041797757, step mean : 19.7, eval_episode_rewards : [ 0.94  0.61  0.85  0.95  0.81 -1.07  0.79  0.96  0.85  0.9   0.88  0.87\n",
      "  0.9   0.48  0.87  0.72  0.25 -1.05  0.76  0.99]\n",
      "episode : 18660, reward mean : 0.4179212277985726, total_step : 513352, cur_epsilon : 0.45\n",
      "episode : 18670, reward mean : 0.4180444622793433, total_step : 513514, cur_epsilon : 0.45\n",
      "episode : 18680, reward mean : 0.4181563228488415, total_step : 513796, cur_epsilon : 0.45\n",
      "episode : 18690, reward mean : 0.41837507281193304, total_step : 513979, cur_epsilon : 0.45\n",
      "episode : 18700, reward mean : 0.4181925193021164, total_step : 514409, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18700, reward mean : 0.6105000053532421, step mean : 24.9, eval_episode_rewards : [ 0.75  0.65  0.7   0.86  0.83  0.96  0.69  0.96  0.83  0.91  0.31  0.97\n",
      "  0.62 -1.11  0.92  0.92 -1.    0.61  0.85  0.98]\n",
      "episode : 18710, reward mean : 0.418295035328276, total_step : 514609, cur_epsilon : 0.45\n",
      "episode : 18720, reward mean : 0.41844979225733503, total_step : 514911, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 18730, reward mean : 0.4185296375391974, total_step : 515153, cur_epsilon : 0.45\n",
      "episode : 18740, reward mean : 0.4185805822397595, total_step : 515449, cur_epsilon : 0.45\n",
      "episode : 18750, reward mean : 0.4186149392663439, total_step : 515776, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18750, reward mean : 0.6035000043921173, step mean : 20.65, eval_episode_rewards : [ 0.97  0.67  0.49  0.42 -1.08  0.94  0.9   0.99  0.96  0.98  0.69  0.78\n",
      "  0.71  0.88 -1.03  0.81  0.75  0.76  0.68  0.8 ]\n",
      "episode : 18760, reward mean : 0.41847068823562233, total_step : 516038, cur_epsilon : 0.45\n",
      "episode : 18770, reward mean : 0.4186867400820252, total_step : 516224, cur_epsilon : 0.45\n",
      "episode : 18780, reward mean : 0.4186932966654626, total_step : 516403, cur_epsilon : 0.45\n",
      "episode : 18790, reward mean : 0.41875945244464247, total_step : 516670, cur_epsilon : 0.45\n",
      "episode : 18800, reward mean : 0.41882872933462423, total_step : 516931, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18800, reward mean : 0.4945000045932829, step mean : 21.55, eval_episode_rewards : [ 0.89  0.95 -1.09  0.95 -1.18  0.86  0.87  0.67  0.76  0.87  0.53  0.91\n",
      " -1.3   0.93  0.85  0.7   0.46  0.73  0.64  0.89]\n",
      "episode : 18810, reward mean : 0.4189516274069358, total_step : 517091, cur_epsilon : 0.45\n",
      "episode : 18820, reward mean : 0.4190154150681616, total_step : 517362, cur_epsilon : 0.45\n",
      "episode : 18830, reward mean : 0.4188874196299746, total_step : 517594, cur_epsilon : 0.45\n",
      "episode : 18840, reward mean : 0.41886837110832936, total_step : 517821, cur_epsilon : 0.45\n",
      "episode : 18850, reward mean : 0.41865730035654825, total_step : 518010, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18850, reward mean : 0.23700000587850809, step mean : 27.3, eval_episode_rewards : [ 0.96 -1.63  0.55 -1.02  0.59  0.57 -1.46  0.81  0.84  0.91  0.81  0.77\n",
      "  0.83  0.6   0.5  -1.03  0.91  0.91 -1.09  0.41]\n",
      "episode : 18860, reward mean : 0.41874231769726045, total_step : 518241, cur_epsilon : 0.45\n",
      "episode : 18870, reward mean : 0.41896714953923814, total_step : 518408, cur_epsilon : 0.45\n",
      "episode : 18880, reward mean : 0.41873093812797474, total_step : 518645, cur_epsilon : 0.45\n",
      "episode : 18890, reward mean : 0.4186733780794591, total_step : 518945, cur_epsilon : 0.45\n",
      "episode : 18900, reward mean : 0.41861799534311683, total_step : 519241, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18900, reward mean : 0.6500000055879355, step mean : 26.0, eval_episode_rewards : [ 0.68  0.71  0.45  0.94  0.98  0.86  0.33  0.58  0.68 -1.05  0.71  0.89\n",
      "  0.77  0.82  0.75  0.66  0.77  0.94  0.89  0.64]\n",
      "episode : 18910, reward mean : 0.41861766853651977, total_step : 519433, cur_epsilon : 0.45\n",
      "episode : 18920, reward mean : 0.4186205133226706, total_step : 519619, cur_epsilon : 0.45\n",
      "episode : 18930, reward mean : 0.4186941422144226, total_step : 519871, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 18940, reward mean : 0.41889493728426325, total_step : 520082, cur_epsilon : 0.45\n",
      "episode : 18950, reward mean : 0.41906333046029065, total_step : 520354, cur_epsilon : 0.45\n",
      "[EVAL] episode : 18950, reward mean : 0.8175000040791929, step mean : 19.25, eval_episode_rewards : [0.86 0.98 0.7  0.86 0.8  0.17 0.73 0.96 0.86 0.76 0.93 0.86 0.7  0.82\n",
      " 0.88 0.93 0.84 0.94 0.85 0.92]\n",
      "episode : 18960, reward mean : 0.4192162506480776, total_step : 520655, cur_epsilon : 0.45\n",
      "episode : 18970, reward mean : 0.4194312130904702, total_step : 520838, cur_epsilon : 0.45\n",
      "episode : 18980, reward mean : 0.41944731888091397, total_step : 520998, cur_epsilon : 0.45\n",
      "episode : 18990, reward mean : 0.4194060090793912, total_step : 521267, cur_epsilon : 0.45\n",
      "episode : 19000, reward mean : 0.4195084269715375, total_step : 521463, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19000, reward mean : 0.6570000054314733, step mean : 25.3, eval_episode_rewards : [ 0.77  0.97  0.98  0.84  0.96 -1.35  0.82  0.27  0.83  0.51  0.9   0.48\n",
      "  0.95  0.86  0.56  0.71  0.91  0.84  0.78  0.55]\n",
      "episode : 19010, reward mean : 0.4196275703573716, total_step : 521627, cur_epsilon : 0.45\n",
      "episode : 19020, reward mean : 0.4196792908810024, total_step : 521919, cur_epsilon : 0.45\n",
      "episode : 19030, reward mean : 0.4195917032387928, total_step : 522276, cur_epsilon : 0.45\n",
      "episode : 19040, reward mean : 0.41973319919662505, total_step : 522597, cur_epsilon : 0.45\n",
      "episode : 19050, reward mean : 0.4199112920082734, total_step : 522848, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19050, reward mean : 0.5395000035874545, step mean : 17.05, eval_episode_rewards : [ 0.96  0.39 -1.02 -1.08  0.9   0.95  0.92  0.72  0.92  0.86  0.61 -1.04\n",
      "  0.96  0.82  0.86  0.93  0.83  0.84  0.95  0.51]\n",
      "episode : 19060, reward mean : 0.4200178443230415, total_step : 523035, cur_epsilon : 0.45\n",
      "episode : 19070, reward mean : 0.42020661315397717, total_step : 523265, cur_epsilon : 0.45\n",
      "episode : 19080, reward mean : 0.42030608558117466, total_step : 523465, cur_epsilon : 0.45\n",
      "episode : 19090, reward mean : 0.42032897396470753, total_step : 523910, cur_epsilon : 0.45\n",
      "episode : 19100, reward mean : 0.4204633567034613, total_step : 524142, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19100, reward mean : 0.5770000049844384, step mean : 23.3, eval_episode_rewards : [ 0.96 -1.06  0.76  0.91  0.2   0.47  0.83  0.96  0.91  0.97  0.74  0.97\n",
      " -1.33  0.43  0.97  0.94  0.77  0.51  0.87  0.76]\n",
      "episode : 19110, reward mean : 0.4205688180573468, total_step : 524330, cur_epsilon : 0.45\n",
      "episode : 19120, reward mean : 0.4207076419009457, total_step : 524654, cur_epsilon : 0.45\n",
      "episode : 19130, reward mean : 0.42060586059513716, total_step : 524838, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 19140, reward mean : 0.4206739871076609, total_step : 525097, cur_epsilon : 0.45\n",
      "episode : 19150, reward mean : 0.42039948372253005, total_step : 525311, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19150, reward mean : 0.5770000049844384, step mean : 23.3, eval_episode_rewards : [ 0.76  0.93  0.8  -1.18  0.68  0.93  0.5   0.99  0.81  0.94  0.75  0.98\n",
      "  0.46  0.78  0.82 -1.05  0.85  0.99  0.05  0.75]\n",
      "episode : 19160, reward mean : 0.42057359672972006, total_step : 525567, cur_epsilon : 0.45\n",
      "episode : 19170, reward mean : 0.42071518588356427, total_step : 525784, cur_epsilon : 0.45\n",
      "episode : 19180, reward mean : 0.4208435929831937, total_step : 525927, cur_epsilon : 0.45\n",
      "episode : 19190, reward mean : 0.4210745238899455, total_step : 526073, cur_epsilon : 0.45\n",
      "episode : 19200, reward mean : 0.42095365174454247, total_step : 526294, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19200, reward mean : 0.660500003118068, step mean : 14.95, eval_episode_rewards : [ 0.78  0.96  0.94  0.92 -1.03 -1.08  0.88  0.91  0.48  0.67  0.97  0.7\n",
      "  0.83  0.85  0.87  0.68  0.96  0.98  0.96  0.98]\n",
      "episode : 19210, reward mean : 0.4211436810798047, total_step : 526518, cur_epsilon : 0.45\n",
      "episode : 19220, reward mean : 0.42120343983477676, total_step : 526891, cur_epsilon : 0.45\n",
      "episode : 19230, reward mean : 0.4211705727341914, total_step : 527143, cur_epsilon : 0.45\n",
      "episode : 19240, reward mean : 0.4213544757655297, total_step : 527378, cur_epsilon : 0.45\n",
      "episode : 19250, reward mean : 0.42143013578082444, total_step : 527621, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19250, reward mean : 0.4395000058226287, step mean : 27.05, eval_episode_rewards : [-1.44  0.96  0.5   0.98  0.83  0.73  0.83 -1.06  0.44  0.5  -1.02  0.95\n",
      "  0.99  0.5   0.65  0.76  0.44  0.79  0.72  0.74]\n",
      "episode : 19260, reward mean : 0.4214974098565175, total_step : 527880, cur_epsilon : 0.45\n",
      "episode : 19270, reward mean : 0.42166425085075093, total_step : 528147, cur_epsilon : 0.45\n",
      "episode : 19280, reward mean : 0.42181276524684846, total_step : 528449, cur_epsilon : 0.45\n",
      "episode : 19290, reward mean : 0.42195957044999854, total_step : 528554, cur_epsilon : 0.45\n",
      "episode : 19300, reward mean : 0.4220606276695013, total_step : 528747, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19300, reward mean : 0.571500005107373, step mean : 23.85, eval_episode_rewards : [ 0.85  0.95  0.8   0.76  0.87  0.49  0.84  0.82  0.86 -1.18  0.97  0.85\n",
      "  0.71  0.9   0.57  0.78  0.05  0.74 -1.03  0.83]\n",
      "episode : 19310, reward mean : 0.4222268313867851, total_step : 529014, cur_epsilon : 0.45\n",
      "episode : 19320, reward mean : 0.4222401715402432, total_step : 529376, cur_epsilon : 0.45\n",
      "episode : 19330, reward mean : 0.42232126819487303, total_step : 529607, cur_epsilon : 0.45\n",
      "episode : 19340, reward mean : 0.42241055399445276, total_step : 529822, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 19350, reward mean : 0.4224744245121183, total_step : 530086, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19350, reward mean : 0.7355000036768615, step mean : 17.45, eval_episode_rewards : [ 0.94  0.98  0.82  0.88  0.64  0.7   0.75  0.96  0.93  0.83  0.93  0.98\n",
      " -1.52  0.68  0.95  0.86  0.91  0.83  0.73  0.93]\n",
      "episode : 19360, reward mean : 0.42265754722930443, total_step : 530319, cur_epsilon : 0.45\n",
      "episode : 19370, reward mean : 0.42274600487379344, total_step : 530535, cur_epsilon : 0.45\n",
      "episode : 19380, reward mean : 0.42283849919876093, total_step : 530842, cur_epsilon : 0.45\n",
      "episode : 19390, reward mean : 0.4229891755820366, total_step : 531137, cur_epsilon : 0.45\n",
      "episode : 19400, reward mean : 0.422919078071304, total_step : 531359, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19400, reward mean : 0.6510000066831708, step mean : 30.85, eval_episode_rewards : [ 0.93  0.79  0.91  0.94  0.65  0.9   0.65  0.85  0.83  0.26  0.91  0.82\n",
      "  0.72  0.58  0.77  0.46 -1.    0.76  0.83  0.46]\n",
      "episode : 19410, reward mean : 0.4231370486665492, total_step : 531523, cur_epsilon : 0.45\n",
      "episode : 19420, reward mean : 0.4230674621351754, total_step : 531744, cur_epsilon : 0.45\n",
      "episode : 19430, reward mean : 0.4231852863972304, total_step : 531902, cur_epsilon : 0.45\n",
      "episode : 19440, reward mean : 0.4232057672185601, total_step : 532049, cur_epsilon : 0.45\n",
      "episode : 19450, reward mean : 0.4232889519165483, total_step : 532274, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19450, reward mean : 0.6050000054761767, step mean : 25.45, eval_episode_rewards : [ 0.75 -1.    0.56  0.84  0.79  0.56  0.87 -1.04  0.74  0.61  0.98  0.94\n",
      "  0.61  0.96  0.92  0.74  0.87  0.62  0.83  0.95]\n",
      "episode : 19460, reward mean : 0.4230267273801985, total_step : 532470, cur_epsilon : 0.45\n",
      "episode : 19470, reward mean : 0.42313200384474103, total_step : 532652, cur_epsilon : 0.45\n",
      "episode : 19480, reward mean : 0.4232212584652198, total_step : 532865, cur_epsilon : 0.45\n",
      "episode : 19490, reward mean : 0.4233242747533002, total_step : 533051, cur_epsilon : 0.45\n",
      "episode : 19500, reward mean : 0.4235276982042079, total_step : 533241, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19500, reward mean : 0.7225000039674342, step mean : 18.75, eval_episode_rewards : [ 0.8   0.98  0.83  0.73  0.79 -1.03  0.98  0.78  0.87  0.97  0.47  0.95\n",
      "  0.89  0.82  0.89  0.7   0.98  0.79  0.34  0.92]\n",
      "episode : 19510, reward mean : 0.4235279402881288, total_step : 533427, cur_epsilon : 0.45\n",
      "episode : 19520, reward mean : 0.42351998540288466, total_step : 533629, cur_epsilon : 0.45\n",
      "episode : 19530, reward mean : 0.42371480364096864, total_step : 533835, cur_epsilon : 0.45\n",
      "episode : 19540, reward mean : 0.42392272851310725, total_step : 534015, cur_epsilon : 0.45\n",
      "episode : 19550, reward mean : 0.4239381133083759, total_step : 534171, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19550, reward mean : 0.5145000041462481, step mean : 19.55, eval_episode_rewards : [ 0.85  0.79 -1.08  0.91  0.71  0.71  0.42 -1.06  0.89  0.8   0.88 -1.32\n",
      "  0.92  0.82  0.97  0.77  0.91  0.95  0.63  0.82]\n",
      "episode : 19560, reward mean : 0.42392843124858565, total_step : 534376, cur_epsilon : 0.45\n",
      "episode : 19570, reward mean : 0.42402248928284364, total_step : 534578, cur_epsilon : 0.45\n",
      "episode : 19580, reward mean : 0.42393105798263, total_step : 534743, cur_epsilon : 0.45\n",
      "episode : 19590, reward mean : 0.42411383947670356, total_step : 534971, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 19600, reward mean : 0.424313271193366, total_step : 535166, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19600, reward mean : 0.1855000059120357, step mean : 27.4, eval_episode_rewards : [ 0.96  0.73  0.87 -1.29  0.67 -1.23  0.97  0.9   0.96  0.06 -1.17 -1.17\n",
      " -1.04  0.89  0.65  0.83  0.46  0.76  0.9  -1.  ]\n",
      "episode : 19610, reward mean : 0.4243085219507229, total_step : 535460, cur_epsilon : 0.45\n",
      "episode : 19620, reward mean : 0.42448522505121616, total_step : 535699, cur_epsilon : 0.45\n",
      "episode : 19630, reward mean : 0.4246515596312031, total_step : 535958, cur_epsilon : 0.45\n",
      "episode : 19640, reward mean : 0.4245509223828506, total_step : 536141, cur_epsilon : 0.45\n",
      "episode : 19650, reward mean : 0.42456540028661793, total_step : 536298, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19650, reward mean : 0.46900000516325235, step mean : 24.1, eval_episode_rewards : [ 0.8   0.98 -1.16  0.97  0.8   0.34  0.91  0.54  0.87  0.64 -1.41  0.49\n",
      "  0.86  0.7   0.92  0.69  0.84  0.83 -1.14  0.91]\n",
      "episode : 19660, reward mean : 0.42466022968841743, total_step : 536497, cur_epsilon : 0.45\n",
      "episode : 19670, reward mean : 0.42472700130802366, total_step : 536751, cur_epsilon : 0.45\n",
      "episode : 19680, reward mean : 0.42481047336260097, total_step : 536972, cur_epsilon : 0.45\n",
      "episode : 19690, reward mean : 0.4249446478336974, total_step : 537293, cur_epsilon : 0.45\n",
      "episode : 19700, reward mean : 0.42500000588350034, total_step : 537569, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19700, reward mean : 0.5840000059455633, step mean : 27.55, eval_episode_rewards : [ 0.44  0.74  0.81  0.92 -1.07  0.95  0.77  0.88  0.91  0.59  0.78  0.87\n",
      "  0.9   0.51  0.8   0.68 -1.    0.8   0.48  0.92]\n",
      "episode : 19710, reward mean : 0.4250192854355951, total_step : 537716, cur_epsilon : 0.45\n",
      "episode : 19720, reward mean : 0.4249883425957571, total_step : 537962, cur_epsilon : 0.45\n",
      "episode : 19730, reward mean : 0.42518044176544034, total_step : 538168, cur_epsilon : 0.45\n",
      "episode : 19740, reward mean : 0.4252639369847557, total_step : 538388, cur_epsilon : 0.45\n",
      "episode : 19750, reward mean : 0.4252263349941637, total_step : 538647, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19750, reward mean : 0.587000004760921, step mean : 22.3, eval_episode_rewards : [ 0.28  0.8   0.84 -1.24  0.89  0.57  0.77 -1.37  0.8   0.89  0.97  0.99\n",
      "  0.92  0.84  0.77  0.88  0.71  0.69  0.95  0.79]\n",
      "episode : 19760, reward mean : 0.4253046617501977, total_step : 538877, cur_epsilon : 0.45\n",
      "episode : 19770, reward mean : 0.4255113867587765, total_step : 539053, cur_epsilon : 0.45\n",
      "episode : 19780, reward mean : 0.425670379993889, total_step : 539323, cur_epsilon : 0.45\n",
      "episode : 19790, reward mean : 0.4254613499920919, total_step : 539620, cur_epsilon : 0.45\n",
      "episode : 19800, reward mean : 0.42545657153457705, total_step : 539814, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19800, reward mean : 0.487000004760921, step mean : 22.3, eval_episode_rewards : [ 0.78  0.73  0.88  0.92  0.8  -1.29  0.8   0.82  0.54  0.89  0.75  0.97\n",
      "  0.7   0.48  0.53 -1.12 -1.02  0.97  0.68  0.93]\n",
      "synced target net\n",
      "episode : 19810, reward mean : 0.42527562425335974, total_step : 540155, cur_epsilon : 0.45\n",
      "episode : 19820, reward mean : 0.4251276547184137, total_step : 540433, cur_epsilon : 0.45\n",
      "episode : 19830, reward mean : 0.4253267834876043, total_step : 540623, cur_epsilon : 0.45\n",
      "episode : 19840, reward mean : 0.4254057518451572, total_step : 540851, cur_epsilon : 0.45\n",
      "episode : 19850, reward mean : 0.4253969832065912, total_step : 541053, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19850, reward mean : 0.2200000062584877, step mean : 29.0, eval_episode_rewards : [-1.06 -1.34 -1.98  0.35  0.81  0.69  0.37  0.81 -1.23 -1.44  0.42  0.88\n",
      "  0.91  0.78  0.81  0.9   0.96  0.86  0.94  0.96]\n",
      "episode : 19860, reward mean : 0.425402825613659, total_step : 541226, cur_epsilon : 0.45\n",
      "episode : 19870, reward mean : 0.4253840018487496, total_step : 541448, cur_epsilon : 0.45\n",
      "episode : 19880, reward mean : 0.42533149480870297, total_step : 541737, cur_epsilon : 0.45\n",
      "episode : 19890, reward mean : 0.42550377661381955, total_step : 541979, cur_epsilon : 0.45\n",
      "episode : 19900, reward mean : 0.42559045813650115, total_step : 542290, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19900, reward mean : 0.6645000052638352, step mean : 24.55, eval_episode_rewards : [ 0.95  0.55  0.31  0.33  0.85  0.76  0.47  0.86  0.88  0.9   0.76 -1.25\n",
      "  0.95  0.85  0.99  0.72  0.91  0.92  0.83  0.75]\n",
      "episode : 19910, reward mean : 0.4255108044674084, total_step : 542433, cur_epsilon : 0.45\n",
      "episode : 19920, reward mean : 0.42559388137514137, total_step : 542652, cur_epsilon : 0.45\n",
      "episode : 19930, reward mean : 0.42561816944522485, total_step : 542887, cur_epsilon : 0.45\n",
      "episode : 19940, reward mean : 0.42584102894044074, total_step : 543027, cur_epsilon : 0.45\n",
      "episode : 19950, reward mean : 0.425451634943784, total_step : 543287, cur_epsilon : 0.45\n",
      "[EVAL] episode : 19950, reward mean : 0.5940000046044588, step mean : 21.6, eval_episode_rewards : [ 0.78  0.77  0.44  0.89  0.6  -1.01  0.48  0.78  0.89  0.9   0.66  0.71\n",
      "  0.93  0.81  0.93  0.88  0.67  0.96 -1.01  0.82]\n",
      "episode : 19960, reward mean : 0.4256648355291481, total_step : 543446, cur_epsilon : 0.45\n",
      "episode : 19970, reward mean : 0.42566300036062, total_step : 543634, cur_epsilon : 0.45\n",
      "episode : 19980, reward mean : 0.4257537596218579, total_step : 543837, cur_epsilon : 0.45\n",
      "episode : 19990, reward mean : 0.4259149633467257, total_step : 544099, cur_epsilon : 0.45\n",
      "episode : 20000, reward mean : 0.4258960058674216, total_step : 544321, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20000, reward mean : 0.2420000046491623, step mean : 21.75, eval_episode_rewards : [ 0.95  0.85  0.91  0.75 -1.01  0.8  -1.    0.62  0.93  0.62  0.73 -1.29\n",
      "  0.92  0.79 -1.34  0.96 -1.15 -1.05  0.94  0.91]\n",
      "episode : 20010, reward mean : 0.4259550283561046, total_step : 544587, cur_epsilon : 0.45\n",
      "episode : 20020, reward mean : 0.4259900158567058, total_step : 544800, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 20030, reward mean : 0.4260768905401051, total_step : 545109, cur_epsilon : 0.45\n",
      "episode : 20040, reward mean : 0.42612824938018357, total_step : 545390, cur_epsilon : 0.45\n",
      "episode : 20050, reward mean : 0.4262020008792715, total_step : 545626, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20050, reward mean : 0.537000005878508, step mean : 27.3, eval_episode_rewards : [-1.52 -1.04  0.85  0.93  0.31  0.91  0.97  0.33  0.92  0.61  0.88  0.92\n",
      "  0.91  0.78  0.77  0.83  0.09  0.87  0.78  0.64]\n",
      "episode : 20060, reward mean : 0.42609322620500045, total_step : 545828, cur_epsilon : 0.45\n",
      "episode : 20070, reward mean : 0.4262556112470225, total_step : 546086, cur_epsilon : 0.45\n",
      "episode : 20080, reward mean : 0.42592879072716644, total_step : 546425, cur_epsilon : 0.45\n",
      "episode : 20090, reward mean : 0.42600846778742335, total_step : 546649, cur_epsilon : 0.45\n",
      "episode : 20100, reward mean : 0.4261333391978521, total_step : 546782, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20100, reward mean : 0.5005000044591725, step mean : 20.95, eval_episode_rewards : [ 0.94  0.81  0.9   0.36  0.88  0.96  0.95  0.54  0.79 -1.03  0.82  0.83\n",
      "  0.82  0.83 -1.01  0.64  0.71  0.92  0.79 -1.44]\n",
      "episode : 20110, reward mean : 0.42598608244342007, total_step : 547062, cur_epsilon : 0.45\n",
      "episode : 20120, reward mean : 0.426049707653686, total_step : 547318, cur_epsilon : 0.45\n",
      "episode : 20130, reward mean : 0.4259220128188671, total_step : 547559, cur_epsilon : 0.45\n",
      "episode : 20140, reward mean : 0.4259836205610563, total_step : 547819, cur_epsilon : 0.45\n",
      "episode : 20150, reward mean : 0.42614541529305744, total_step : 548077, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20150, reward mean : 0.6275000038556755, step mean : 18.25, eval_episode_rewards : [ 0.91 -1.07  0.88  0.9   0.73  0.85  0.84  0.89  0.65  0.47  0.66  0.95\n",
      "  0.82  0.78  0.85  0.91 -1.02  0.71  0.9   0.94]\n",
      "episode : 20160, reward mean : 0.4263521884022916, total_step : 548244, cur_epsilon : 0.45\n",
      "episode : 20170, reward mean : 0.42651611890156244, total_step : 548497, cur_epsilon : 0.45\n",
      "episode : 20180, reward mean : 0.4267319186459544, total_step : 548645, cur_epsilon : 0.45\n",
      "episode : 20190, reward mean : 0.42686726688174415, total_step : 548955, cur_epsilon : 0.45\n",
      "episode : 20200, reward mean : 0.427000995960898, total_step : 549268, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20200, reward mean : 0.4795000060461462, step mean : 28.0, eval_episode_rewards : [-1.06  0.68  0.13  0.96  0.66  0.96  0.45  0.97  0.94  0.6   0.25  0.93\n",
      "  0.65 -1.16  0.96  0.95  0.83  0.94  0.95 -1.  ]\n",
      "episode : 20210, reward mean : 0.4271761562819936, total_step : 549497, cur_epsilon : 0.45\n",
      "episode : 20220, reward mean : 0.42737240942128735, total_step : 549683, cur_epsilon : 0.45\n",
      "episode : 20230, reward mean : 0.42746219073361946, total_step : 549884, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 20240, reward mean : 0.427508405069342, total_step : 550173, cur_epsilon : 0.45\n",
      "episode : 20250, reward mean : 0.4274558083291775, total_step : 550462, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20250, reward mean : 0.529500003810972, step mean : 18.05, eval_episode_rewards : [ 0.91 -1.08  0.99  0.98  0.84  0.82  0.64  0.82 -1.05  0.88  0.83  0.91\n",
      " -1.33  0.86  0.77  0.86  0.68  0.68  0.74  0.84]\n",
      "episode : 20260, reward mean : 0.4276426514663972, total_step : 550666, cur_epsilon : 0.45\n",
      "episode : 20270, reward mean : 0.4276877216957087, total_step : 550957, cur_epsilon : 0.45\n",
      "episode : 20280, reward mean : 0.42760207686413454, total_step : 551113, cur_epsilon : 0.45\n",
      "episode : 20290, reward mean : 0.427708236512555, total_step : 551280, cur_epsilon : 0.45\n",
      "episode : 20300, reward mean : 0.42764926694113337, total_step : 551582, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20300, reward mean : 0.5845000059343874, step mean : 27.5, eval_episode_rewards : [ 0.95  0.79  0.95  0.95  0.94 -1.    0.52  0.63  0.63  0.37  0.74 -1.13\n",
      "  0.99  0.76  0.78  0.86  0.66  0.54  0.83  0.93]\n",
      "episode : 20310, reward mean : 0.42774397434491584, total_step : 551772, cur_epsilon : 0.45\n",
      "episode : 20320, reward mean : 0.42772736806059475, total_step : 551988, cur_epsilon : 0.45\n",
      "episode : 20330, reward mean : 0.42793015833876286, total_step : 552158, cur_epsilon : 0.45\n",
      "episode : 20340, reward mean : 0.4278775869769794, total_step : 552546, cur_epsilon : 0.45\n",
      "episode : 20350, reward mean : 0.4280692933244443, total_step : 552738, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20350, reward mean : 0.5570000065490603, step mean : 30.25, eval_episode_rewards : [ 0.91  0.78  0.88  0.93  0.59  0.66  0.49  0.8   0.93  0.55 -1.19  0.38\n",
      "  0.53  0.8   0.81  0.56  0.93 -1.    0.81  0.99]\n",
      "episode : 20360, reward mean : 0.42807810015762354, total_step : 553001, cur_epsilon : 0.45\n",
      "episode : 20370, reward mean : 0.4282508649611384, total_step : 553231, cur_epsilon : 0.45\n",
      "episode : 20380, reward mean : 0.4282733130181013, total_step : 553466, cur_epsilon : 0.45\n",
      "episode : 20390, reward mean : 0.4283462540146517, total_step : 553699, cur_epsilon : 0.45\n",
      "episode : 20400, reward mean : 0.42854804506835836, total_step : 553869, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20400, reward mean : 0.39000000692903997, step mean : 32.0, eval_episode_rewards : [-1.07 -1.48  0.64  0.68  0.88  0.25  0.77  0.49  0.71  0.95  0.97  0.99\n",
      "  0.77 -1.13  0.85  0.66  0.93  0.25  0.66  0.03]\n",
      "episode : 20410, reward mean : 0.4284537050208658, total_step : 554243, cur_epsilon : 0.45\n",
      "episode : 20420, reward mean : 0.42848678352320374, total_step : 554557, cur_epsilon : 0.45\n",
      "episode : 20430, reward mean : 0.4285604561719637, total_step : 554788, cur_epsilon : 0.45\n",
      "episode : 20440, reward mean : 0.42864384146957657, total_step : 554999, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 20450, reward mean : 0.42875404008170065, total_step : 555155, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20450, reward mean : 0.3020000044256449, step mean : 20.8, eval_episode_rewards : [-1.32  0.94  0.77  0.9   0.88  0.95  0.61  0.84  0.89 -1.06  0.91 -1.05\n",
      "  0.76  0.67  0.83  0.75 -1.02  0.64 -1.19  0.34]\n",
      "episode : 20460, reward mean : 0.42888514759011737, total_step : 555268, cur_epsilon : 0.45\n",
      "episode : 20470, reward mean : 0.42902736295829924, total_step : 555558, cur_epsilon : 0.45\n",
      "episode : 20480, reward mean : 0.4292211972556288, total_step : 555742, cur_epsilon : 0.45\n",
      "episode : 20490, reward mean : 0.4293621337168526, total_step : 556034, cur_epsilon : 0.45\n",
      "episode : 20500, reward mean : 0.4293848838968793, total_step : 556168, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20500, reward mean : 0.6905000069178641, step mean : 31.95, eval_episode_rewards : [0.89 0.88 0.77 0.33 0.43 0.78 0.82 0.52 0.69 0.68 0.7  0.99 0.97 0.83\n",
      " 0.54 0.12 0.75 0.39 0.87 0.86]\n",
      "episode : 20510, reward mean : 0.4293393525082903, total_step : 556442, cur_epsilon : 0.45\n",
      "episode : 20520, reward mean : 0.42937427485437946, total_step : 556751, cur_epsilon : 0.45\n",
      "episode : 20530, reward mean : 0.4294393628867433, total_step : 556998, cur_epsilon : 0.45\n",
      "episode : 20540, reward mean : 0.4292546309687618, total_step : 557158, cur_epsilon : 0.45\n",
      "episode : 20550, reward mean : 0.4291727552396175, total_step : 557507, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20550, reward mean : 0.5620000042021275, step mean : 19.75, eval_episode_rewards : [ 0.63  0.97  0.92 -1.    0.8   0.86  0.95  0.94  0.71  0.72 -1.08  0.83\n",
      "  0.85 -1.09  0.89  0.93  0.48  0.97  0.98  0.98]\n",
      "episode : 20560, reward mean : 0.4291551614893194, total_step : 557724, cur_epsilon : 0.45\n",
      "episode : 20570, reward mean : 0.42911036073306724, total_step : 557997, cur_epsilon : 0.45\n",
      "episode : 20580, reward mean : 0.4291171098317122, total_step : 558263, cur_epsilon : 0.45\n",
      "episode : 20590, reward mean : 0.42913890822558526, total_step : 558399, cur_epsilon : 0.45\n",
      "episode : 20600, reward mean : 0.4292305883691981, total_step : 558591, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20600, reward mean : 0.7455000034533441, step mean : 16.45, eval_episode_rewards : [ 0.88  0.76  0.77  0.97  0.73  0.97  0.95  0.79  0.93  0.87  0.47  0.99\n",
      "  0.75  0.91  0.88  0.85  0.98  0.78 -1.04  0.72]\n",
      "episode : 20610, reward mean : 0.4293318835731048, total_step : 558763, cur_epsilon : 0.45\n",
      "episode : 20620, reward mean : 0.42944035501816424, total_step : 558920, cur_epsilon : 0.45\n",
      "episode : 20630, reward mean : 0.4294091187846376, total_step : 559165, cur_epsilon : 0.45\n",
      "episode : 20640, reward mean : 0.4294990368492454, total_step : 559360, cur_epsilon : 0.45\n",
      "episode : 20650, reward mean : 0.42927990898877816, total_step : 559593, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20650, reward mean : 0.7360000036656856, step mean : 17.4, eval_episode_rewards : [ 0.58  0.91  0.94  0.83  0.97  0.95  0.82  0.94  0.98  0.93  0.61  0.99\n",
      " -1.21  0.53  0.71  0.87  0.76  0.98  0.87  0.76]\n",
      "episode : 20660, reward mean : 0.42948112878281286, total_step : 559758, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 20670, reward mean : 0.4295433053075474, total_step : 560010, cur_epsilon : 0.45\n",
      "episode : 20680, reward mean : 0.4296798897847692, total_step : 560207, cur_epsilon : 0.45\n",
      "episode : 20690, reward mean : 0.4297056607431212, total_step : 560334, cur_epsilon : 0.45\n",
      "episode : 20700, reward mean : 0.4296797159829757, total_step : 560667, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20700, reward mean : 0.7315000037662684, step mean : 17.85, eval_episode_rewards : [ 0.53 -1.02  0.99  0.84  0.76  0.94  0.98  0.67  0.96  0.68  0.84  0.94\n",
      "  0.75  0.71  0.89  0.71  0.98  0.58  0.94  0.96]\n",
      "episode : 20710, reward mean : 0.4297803052092616, total_step : 560839, cur_epsilon : 0.45\n",
      "episode : 20720, reward mean : 0.4297466274583712, total_step : 561089, cur_epsilon : 0.45\n",
      "episode : 20730, reward mean : 0.42983647472159087, total_step : 561283, cur_epsilon : 0.45\n",
      "episode : 20740, reward mean : 0.42993636070467756, total_step : 561456, cur_epsilon : 0.45\n",
      "episode : 20750, reward mean : 0.430129162460386, total_step : 561636, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20750, reward mean : 0.39450000459328294, step mean : 21.55, eval_episode_rewards : [ 0.92  0.59  0.66  0.85 -1.57  0.9  -1.02  0.98  0.68  0.98 -1.01  0.87\n",
      "  0.85  0.9   0.36  0.39  0.96  0.97 -1.07  0.7 ]\n",
      "episode : 20760, reward mean : 0.4302148420566438, total_step : 561838, cur_epsilon : 0.45\n",
      "episode : 20770, reward mean : 0.430203183492503, total_step : 562042, cur_epsilon : 0.45\n",
      "episode : 20780, reward mean : 0.43007507801694117, total_step : 562288, cur_epsilon : 0.45\n",
      "episode : 20790, reward mean : 0.43022030405252243, total_step : 562566, cur_epsilon : 0.45\n",
      "episode : 20800, reward mean : 0.430191351985738, total_step : 562806, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20800, reward mean : 0.6135000041685998, step mean : 19.65, eval_episode_rewards : [ 0.7   0.8   0.79  0.98  0.32 -1.11  0.87  0.98  0.97  0.95  0.92  0.92\n",
      "  0.8  -1.02  0.87  0.81  0.59  0.7   0.96  0.47]\n",
      "episode : 20810, reward mean : 0.4301215819977779, total_step : 563131, cur_epsilon : 0.45\n",
      "episode : 20820, reward mean : 0.43003170612061486, total_step : 563397, cur_epsilon : 0.45\n",
      "episode : 20830, reward mean : 0.43015795110309507, total_step : 563613, cur_epsilon : 0.45\n",
      "episode : 20840, reward mean : 0.4302687198431329, total_step : 563861, cur_epsilon : 0.45\n",
      "episode : 20850, reward mean : 0.43011847105967127, total_step : 564154, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20850, reward mean : 0.6895000047050417, step mean : 22.05, eval_episode_rewards : [ 0.93 -1.16  0.74  0.83  0.84  0.8   0.29  0.74  0.95  0.9   0.67  0.84\n",
      "  0.77  0.79  0.88  0.97  0.85  0.82  0.63  0.71]\n",
      "episode : 20860, reward mean : 0.4300791045853534, total_step : 564416, cur_epsilon : 0.45\n",
      "episode : 20870, reward mean : 0.42998515197460124, total_step : 564691, cur_epsilon : 0.45\n",
      "episode : 20880, reward mean : 0.4300775920378109, total_step : 564878, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 20890, reward mean : 0.4300895223447535, total_step : 565033, cur_epsilon : 0.45\n",
      "episode : 20900, reward mean : 0.43027608238383414, total_step : 565223, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20900, reward mean : 0.3120000075548887, step mean : 34.75, eval_episode_rewards : [ 0.27 -1.36  0.21  0.77  0.43  0.85 -1.33  0.96  0.71  0.95  0.58  0.86\n",
      "  0.47  0.97  0.98 -1.    0.7   0.43 -1.08  0.87]\n",
      "episode : 20910, reward mean : 0.4302448647477121, total_step : 565468, cur_epsilon : 0.45\n",
      "episode : 20920, reward mean : 0.4304015354650455, total_step : 565720, cur_epsilon : 0.45\n",
      "episode : 20930, reward mean : 0.4303545208785788, total_step : 565998, cur_epsilon : 0.45\n",
      "episode : 20940, reward mean : 0.4302679141388304, total_step : 566359, cur_epsilon : 0.45\n",
      "episode : 20950, reward mean : 0.43043007742809153, total_step : 566599, cur_epsilon : 0.45\n",
      "[EVAL] episode : 20950, reward mean : 0.5505000066943466, step mean : 30.9, eval_episode_rewards : [ 0.27  0.86  0.25  0.56  0.66  0.96  0.83  0.84  0.9   0.94 -1.   -1.49\n",
      "  0.62  0.82  0.83  0.91  0.66  0.88  0.75  0.96]\n",
      "episode : 20960, reward mean : 0.43049714323328736, total_step : 566838, cur_epsilon : 0.45\n",
      "episode : 20970, reward mean : 0.43051311979971874, total_step : 566984, cur_epsilon : 0.45\n",
      "episode : 20980, reward mean : 0.43065682184270754, total_step : 567262, cur_epsilon : 0.45\n",
      "episode : 20990, reward mean : 0.4306522211672231, total_step : 567451, cur_epsilon : 0.45\n",
      "episode : 21000, reward mean : 0.4307895296363249, total_step : 567742, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21000, reward mean : 0.8330000037327409, step mean : 17.7, eval_episode_rewards : [0.89 0.76 0.81 0.84 0.85 0.94 0.87 0.87 0.89 0.68 0.9  0.99 0.99 0.71\n",
      " 0.91 0.83 0.67 0.91 0.53 0.82]\n",
      "episode : 21010, reward mean : 0.43070300439880654, total_step : 568002, cur_epsilon : 0.45\n",
      "episode : 21020, reward mean : 0.4307716518787616, total_step : 568336, cur_epsilon : 0.45\n",
      "episode : 21030, reward mean : 0.43088540763399796, total_step : 568575, cur_epsilon : 0.45\n",
      "episode : 21040, reward mean : 0.43091825677804335, total_step : 568885, cur_epsilon : 0.45\n",
      "episode : 21050, reward mean : 0.4310047564205145, total_step : 569082, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21050, reward mean : 0.6655000052414834, step mean : 24.45, eval_episode_rewards : [ 0.87  0.37  0.89  0.87  0.88  0.84  0.59  0.48  0.9  -1.05  0.99  0.88\n",
      "  0.72  0.86  0.95  0.42  0.97  0.77  0.92  0.19]\n",
      "episode : 21060, reward mean : 0.43108595074530326, total_step : 569290, cur_epsilon : 0.45\n",
      "episode : 21070, reward mean : 0.4311461852278205, total_step : 569542, cur_epsilon : 0.45\n",
      "episode : 21080, reward mean : 0.43123767185908524, total_step : 569728, cur_epsilon : 0.45\n",
      "episode : 21090, reward mean : 0.4313456672745867, total_step : 569879, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 21100, reward mean : 0.43143033757646926, total_step : 570079, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21100, reward mean : 0.726000003889203, step mean : 18.4, eval_episode_rewards : [ 0.61  0.96  0.77  0.83  0.87  0.84  0.95  0.73  0.68  0.29  0.89  0.94\n",
      "  0.8   0.87  0.91  0.92 -1.02  0.9   0.85  0.93]\n",
      "episode : 21110, reward mean : 0.4313766045914985, total_step : 570371, cur_epsilon : 0.45\n",
      "episode : 21120, reward mean : 0.43146118006482015, total_step : 570571, cur_epsilon : 0.45\n",
      "episode : 21130, reward mean : 0.4316384345959472, total_step : 570775, cur_epsilon : 0.45\n",
      "episode : 21140, reward mean : 0.4316338752636945, total_step : 571062, cur_epsilon : 0.45\n",
      "episode : 21150, reward mean : 0.4317910223700828, total_step : 571308, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21150, reward mean : 0.39350000573322175, step mean : 26.6, eval_episode_rewards : [ 0.56  0.93  0.85  0.85  0.96  0.87 -1.05 -1.08  0.85  0.51  0.6  -1.\n",
      "  0.7  -1.04  0.76  0.43  0.91  0.94  0.43  0.89]\n",
      "episode : 21160, reward mean : 0.4316328035515252, total_step : 571421, cur_epsilon : 0.45\n",
      "episode : 21170, reward mean : 0.4315862127176337, total_step : 571797, cur_epsilon : 0.45\n",
      "episode : 21180, reward mean : 0.43169216823933876, total_step : 572151, cur_epsilon : 0.45\n",
      "episode : 21190, reward mean : 0.43187353106891563, total_step : 572345, cur_epsilon : 0.45\n",
      "episode : 21200, reward mean : 0.43191981714201283, total_step : 572625, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21200, reward mean : 0.5495000055991113, step mean : 26.05, eval_episode_rewards : [ 0.55  0.68  0.74  0.88  0.52  0.73 -1.19  0.48  0.83  0.72  0.87  0.45\n",
      "  0.91  0.99  0.57  0.85  0.9   0.96 -1.35  0.9 ]\n",
      "episode : 21210, reward mean : 0.43209052915873253, total_step : 572841, cur_epsilon : 0.45\n",
      "episode : 21220, reward mean : 0.4321286580369094, total_step : 573237, cur_epsilon : 0.45\n",
      "episode : 21230, reward mean : 0.432123416089648, total_step : 573426, cur_epsilon : 0.45\n",
      "episode : 21240, reward mean : 0.43227307550089955, total_step : 573686, cur_epsilon : 0.45\n",
      "episode : 21250, reward mean : 0.43233177052674926, total_step : 573939, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21250, reward mean : 0.6790000049397349, step mean : 23.1, eval_episode_rewards : [ 0.94  0.85  0.45  0.88 -1.04  0.95  0.93  0.75  0.76  0.35  0.85  0.89\n",
      "  0.7   0.92  0.38  0.61  0.92  0.85  0.78  0.86]\n",
      "episode : 21260, reward mean : 0.43251835012848744, total_step : 574120, cur_epsilon : 0.45\n",
      "episode : 21270, reward mean : 0.43254537488485817, total_step : 574440, cur_epsilon : 0.45\n",
      "episode : 21280, reward mean : 0.4326842163468858, total_step : 574722, cur_epsilon : 0.45\n",
      "episode : 21290, reward mean : 0.43277924489894837, total_step : 574897, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 21300, reward mean : 0.4329370950211382, total_step : 575138, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21300, reward mean : 0.64500000346452, step mean : 16.5, eval_episode_rewards : [ 0.89  0.92  0.82 -1.01  0.75  0.73  0.87 -1.03  0.96  0.8   0.99  0.81\n",
      "  0.29  0.79  0.96  0.95  0.97  0.79  0.9   0.75]\n",
      "episode : 21310, reward mean : 0.4330689875161645, total_step : 575234, cur_epsilon : 0.45\n",
      "episode : 21320, reward mean : 0.43323171313408254, total_step : 575464, cur_epsilon : 0.45\n",
      "episode : 21330, reward mean : 0.4333098979870297, total_step : 575674, cur_epsilon : 0.45\n",
      "episode : 21340, reward mean : 0.4334217490202415, total_step : 575812, cur_epsilon : 0.45\n",
      "episode : 21350, reward mean : 0.43360609480705486, total_step : 575995, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21350, reward mean : 0.6315000037662685, step mean : 17.85, eval_episode_rewards : [ 0.86  0.91  0.75  0.56  0.83  0.9   0.83  0.76  0.73  0.99  0.78  0.9\n",
      "  0.98  0.96  0.92  0.86  0.89 -1.4   0.95 -1.33]\n",
      "episode : 21360, reward mean : 0.4336956986970851, total_step : 576180, cur_epsilon : 0.45\n",
      "episode : 21370, reward mean : 0.43381048779687104, total_step : 576410, cur_epsilon : 0.45\n",
      "episode : 21380, reward mean : 0.43389289636396644, total_step : 576610, cur_epsilon : 0.45\n",
      "episode : 21390, reward mean : 0.434075742136546, total_step : 576795, cur_epsilon : 0.45\n",
      "episode : 21400, reward mean : 0.43417103384752, total_step : 576967, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21400, reward mean : 0.6380000047385692, step mean : 22.15, eval_episode_rewards : [ 0.91  0.81 -1.09  0.89  0.38  0.9   0.6   0.98  0.97  0.9   0.95  0.98\n",
      "  0.86  0.85  0.92  0.86  0.86  0.92 -1.    0.31]\n",
      "episode : 21410, reward mean : 0.4343517106201279, total_step : 577156, cur_epsilon : 0.45\n",
      "episode : 21420, reward mean : 0.4345037406363484, total_step : 577406, cur_epsilon : 0.45\n",
      "episode : 21430, reward mean : 0.43456323492691457, total_step : 577654, cur_epsilon : 0.45\n",
      "episode : 21440, reward mean : 0.43467118118276277, total_step : 577998, cur_epsilon : 0.45\n",
      "episode : 21450, reward mean : 0.4345412645507213, total_step : 578252, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21450, reward mean : 0.5825000059790909, step mean : 27.7, eval_episode_rewards : [ 0.93  0.76  0.46  0.27  0.86  0.86  0.85  0.58  0.94  0.85  0.85  0.75\n",
      "  0.77  0.66  0.79 -1.    0.96 -1.11  0.99  0.63]\n",
      "episode : 21460, reward mean : 0.4344841623801186, total_step : 578550, cur_epsilon : 0.45\n",
      "episode : 21470, reward mean : 0.4345733639830572, total_step : 578734, cur_epsilon : 0.45\n",
      "episode : 21480, reward mean : 0.4346461883057568, total_step : 579153, cur_epsilon : 0.45\n",
      "episode : 21490, reward mean : 0.4347529141385786, total_step : 579299, cur_epsilon : 0.45\n",
      "episode : 21500, reward mean : 0.43481721511110666, total_step : 579536, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21500, reward mean : 0.28600000478327275, step mean : 22.4, eval_episode_rewards : [ 0.79  0.58  0.88  0.81 -1.12  0.78  0.47  0.7  -1.01  0.72  0.67 -1.14\n",
      "  0.99 -1.17  0.88  0.9   0.37  0.82  0.82 -1.02]\n",
      "episode : 21510, reward mean : 0.4348159053894162, total_step : 579714, cur_epsilon : 0.45\n",
      "episode : 21520, reward mean : 0.43501859316706215, total_step : 579853, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 21530, reward mean : 0.4350631734795775, total_step : 580132, cur_epsilon : 0.45\n",
      "episode : 21540, reward mean : 0.4352641655081155, total_step : 580274, cur_epsilon : 0.45\n",
      "episode : 21550, reward mean : 0.435414390955855, total_step : 580525, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21550, reward mean : 0.7595000053755939, step mean : 25.05, eval_episode_rewards : [0.69 0.51 0.59 0.68 0.84 0.74 0.78 0.87 0.78 0.93 0.85 0.69 0.42 0.56\n",
      " 0.97 0.82 0.88 0.93 0.82 0.84]\n",
      "episode : 21560, reward mean : 0.43536085923753526, total_step : 580815, cur_epsilon : 0.45\n",
      "episode : 21570, reward mean : 0.4353333391382164, total_step : 581049, cur_epsilon : 0.45\n",
      "episode : 21580, reward mean : 0.4354981522362393, total_step : 581268, cur_epsilon : 0.45\n",
      "episode : 21590, reward mean : 0.43565308593374025, total_step : 581508, cur_epsilon : 0.45\n",
      "episode : 21600, reward mean : 0.43573565395143643, total_step : 581704, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21600, reward mean : 0.42100000400096177, step mean : 18.9, eval_episode_rewards : [ 0.3   0.9  -1.04  0.81  0.81  0.66  0.85  0.86  0.87 -1.11  0.91  0.82\n",
      "  0.95  0.96  0.95  0.69 -1.15  0.86  0.75 -1.23]\n",
      "episode : 21610, reward mean : 0.435879691134085, total_step : 581967, cur_epsilon : 0.45\n",
      "episode : 21620, reward mean : 0.4359537523336343, total_step : 582181, cur_epsilon : 0.45\n",
      "episode : 21630, reward mean : 0.43603606682823615, total_step : 582377, cur_epsilon : 0.45\n",
      "episode : 21640, reward mean : 0.43593392447024487, total_step : 582572, cur_epsilon : 0.45\n",
      "episode : 21650, reward mean : 0.4359325693115833, total_step : 582848, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21650, reward mean : 0.5125000053085387, step mean : 24.7, eval_episode_rewards : [ 0.78  0.79  0.7   0.96  0.98  0.71  0.68 -1.12  0.1  -1.    0.81  0.98\n",
      "  0.85  0.99  0.82  0.84 -1.08  0.77  0.76  0.93]\n",
      "episode : 21660, reward mean : 0.4360133945354581, total_step : 583047, cur_epsilon : 0.45\n",
      "episode : 21670, reward mean : 0.43607014885518647, total_step : 583298, cur_epsilon : 0.45\n",
      "episode : 21680, reward mean : 0.4362596921460373, total_step : 583461, cur_epsilon : 0.45\n",
      "episode : 21690, reward mean : 0.4362812413901448, total_step : 583588, cur_epsilon : 0.45\n",
      "episode : 21700, reward mean : 0.43633226386215623, total_step : 583851, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21700, reward mean : 0.694500004593283, step mean : 21.55, eval_episode_rewards : [ 0.98  0.71  0.46  0.68  0.89  0.64 -1.02  0.58  0.97  0.92  0.83  0.82\n",
      "  0.8   0.95  0.85  0.99  0.88  0.81  0.3   0.85]\n",
      "episode : 21710, reward mean : 0.4361515488642865, total_step : 584017, cur_epsilon : 0.45\n",
      "episode : 21720, reward mean : 0.4361399689644685, total_step : 584315, cur_epsilon : 0.45\n",
      "episode : 21730, reward mean : 0.4361638346044934, total_step : 584536, cur_epsilon : 0.45\n",
      "episode : 21740, reward mean : 0.4360970619126037, total_step : 584655, cur_epsilon : 0.45\n",
      "episode : 21750, reward mean : 0.4360818448746684, total_step : 584862, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21750, reward mean : 0.4850000048056245, step mean : 22.5, eval_episode_rewards : [ 0.83 -1.21 -1.52  0.09  0.86  0.81  0.97 -1.02  0.88  0.84  0.91  0.83\n",
      "  0.86  0.88  0.83  0.82  0.89  0.7   0.57  0.88]\n",
      "synced target net\n",
      "episode : 21760, reward mean : 0.4361374138822772, total_step : 585115, cur_epsilon : 0.45\n",
      "episode : 21770, reward mean : 0.4362430007399313, total_step : 585259, cur_epsilon : 0.45\n",
      "episode : 21780, reward mean : 0.43630762746364316, total_step : 585492, cur_epsilon : 0.45\n",
      "episode : 21790, reward mean : 0.43589261708205834, total_step : 585770, cur_epsilon : 0.45\n",
      "episode : 21800, reward mean : 0.4361000057909672, total_step : 585892, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21800, reward mean : 0.5715000062249601, step mean : 28.8, eval_episode_rewards : [ 0.75  0.93  0.78  0.59  0.34 -1.09  0.93 -1.    0.8   0.65  0.79  0.81\n",
      "  0.89  0.36  0.89  0.7   0.8   0.83  0.86  0.82]\n",
      "episode : 21810, reward mean : 0.43597937305336193, total_step : 586129, cur_epsilon : 0.45\n",
      "episode : 21820, reward mean : 0.4361122881005989, total_step : 586413, cur_epsilon : 0.45\n",
      "episode : 21830, reward mean : 0.4362061441314982, total_step : 586582, cur_epsilon : 0.45\n",
      "episode : 21840, reward mean : 0.4361996394885711, total_step : 586770, cur_epsilon : 0.45\n",
      "episode : 21850, reward mean : 0.4363038959478623, total_step : 586916, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21850, reward mean : 0.4855000047944486, step mean : 22.45, eval_episode_rewards : [ 0.92  0.24  0.95 -1.13  0.52  0.95  0.96  0.04  0.73  0.81  0.95  0.9\n",
      "  0.97 -1.15 -1.01  0.71  0.86  0.94  0.72  0.83]\n",
      "episode : 21860, reward mean : 0.43649817596028634, total_step : 587065, cur_epsilon : 0.45\n",
      "episode : 21870, reward mean : 0.43651532357406153, total_step : 587401, cur_epsilon : 0.45\n",
      "episode : 21880, reward mean : 0.4366878485651874, total_step : 587597, cur_epsilon : 0.45\n",
      "episode : 21890, reward mean : 0.4368880825324002, total_step : 587732, cur_epsilon : 0.45\n",
      "episode : 21900, reward mean : 0.4368557135473307, total_step : 587976, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21900, reward mean : 0.7265000038780272, step mean : 18.35, eval_episode_rewards : [ 0.88  0.85  0.88  0.96  0.78  0.53  0.9   0.69  0.82  0.86  0.84  0.96\n",
      "  0.87  0.77  0.77  0.85  0.97  0.67  0.73 -1.05]\n",
      "episode : 21910, reward mean : 0.4368649076548839, total_step : 588129, cur_epsilon : 0.45\n",
      "episode : 21920, reward mean : 0.43694526125731314, total_step : 588326, cur_epsilon : 0.45\n",
      "episode : 21930, reward mean : 0.4369434622342741, total_step : 588503, cur_epsilon : 0.45\n",
      "episode : 21940, reward mean : 0.43689790915582355, total_step : 588875, cur_epsilon : 0.45\n",
      "episode : 21950, reward mean : 0.43688292149988966, total_step : 589081, cur_epsilon : 0.45\n",
      "[EVAL] episode : 21950, reward mean : 0.5955000056885182, step mean : 26.4, eval_episode_rewards : [-1.03  0.86  0.92  0.81  0.88  0.85  0.87  0.83 -1.    0.74  0.97  0.72\n",
      "  0.94  0.72  0.9   0.5   0.71  0.78  0.54  0.4 ]\n",
      "episode : 21960, reward mean : 0.4366092953998901, total_step : 589354, cur_epsilon : 0.45\n",
      "episode : 21970, reward mean : 0.43675876773027367, total_step : 589599, cur_epsilon : 0.45\n",
      "episode : 21980, reward mean : 0.4368935453636848, total_step : 589876, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 21990, reward mean : 0.4369195146503951, total_step : 590192, cur_epsilon : 0.45\n",
      "episode : 22000, reward mean : 0.43689591487319296, total_step : 590417, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22000, reward mean : 0.3490000056102872, step mean : 26.1, eval_episode_rewards : [ 0.77 -1.26  0.72  0.8  -1.14  0.88  0.58 -1.05  0.74  0.19  0.85  0.38\n",
      "  0.95  0.89 -1.28  0.87  0.95  0.57  0.74  0.83]\n",
      "episode : 22010, reward mean : 0.4370072752038446, total_step : 590545, cur_epsilon : 0.45\n",
      "episode : 22020, reward mean : 0.4369337024209474, total_step : 590880, cur_epsilon : 0.45\n",
      "episode : 22030, reward mean : 0.43685883464992614, total_step : 591018, cur_epsilon : 0.45\n",
      "episode : 22040, reward mean : 0.4370054504261124, total_step : 591268, cur_epsilon : 0.45\n",
      "episode : 22050, reward mean : 0.4370802778892574, total_step : 591575, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22050, reward mean : 0.31250000530853866, step mean : 24.7, eval_episode_rewards : [ 0.52  0.93  0.55  0.42  0.79 -1.09  0.87 -1.1  -1.11  0.76  0.96  0.82\n",
      "  0.92  0.72 -1.   -1.05  0.78  0.83  0.87  0.86]\n",
      "episode : 22060, reward mean : 0.43707525509959283, total_step : 591759, cur_epsilon : 0.45\n",
      "episode : 22070, reward mean : 0.4372569156109301, total_step : 591931, cur_epsilon : 0.45\n",
      "episode : 22080, reward mean : 0.4374234659228629, total_step : 592136, cur_epsilon : 0.45\n",
      "episode : 22090, reward mean : 0.43752377218688676, total_step : 592287, cur_epsilon : 0.45\n",
      "episode : 22100, reward mean : 0.43760317319683834, total_step : 592484, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22100, reward mean : 0.6300000049173832, step mean : 22.95, eval_episode_rewards : [ 0.76  0.87  0.99  0.94 -1.    0.8  -1.04  0.84  0.79  0.96  0.84  0.76\n",
      "  0.49  0.91  0.92  0.83  0.83  0.66  0.74  0.71]\n",
      "episode : 22110, reward mean : 0.43773270591188684, total_step : 592770, cur_epsilon : 0.45\n",
      "episode : 22120, reward mean : 0.4378779442932046, total_step : 593021, cur_epsilon : 0.45\n",
      "episode : 22130, reward mean : 0.4380447414283246, total_step : 593224, cur_epsilon : 0.45\n",
      "episode : 22140, reward mean : 0.4381395721719739, total_step : 593586, cur_epsilon : 0.45\n",
      "episode : 22150, reward mean : 0.4379620825247019, total_step : 593751, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22150, reward mean : 0.6855000047944486, step mean : 22.45, eval_episode_rewards : [ 0.87  0.95  0.82  0.73  0.85  0.87  0.87 -1.47  0.74  0.93  0.68  0.3\n",
      "  0.88  0.8   0.86  0.79  0.99  0.89  0.93  0.43]\n",
      "episode : 22160, reward mean : 0.437965258483822, total_step : 594116, cur_epsilon : 0.45\n",
      "episode : 22170, reward mean : 0.4380888645947618, total_step : 594414, cur_epsilon : 0.45\n",
      "episode : 22180, reward mean : 0.4381871112758366, total_step : 594568, cur_epsilon : 0.45\n",
      "episode : 22190, reward mean : 0.438369992254751, total_step : 594734, cur_epsilon : 0.45\n",
      "episode : 22200, reward mean : 0.43844865442228476, total_step : 594931, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22200, reward mean : 0.333500003721565, step mean : 17.65, eval_episode_rewards : [ 0.95  0.76 -1.03  0.89  0.82  0.77  0.81  0.66 -1.05 -1.06  0.7   0.93\n",
      "  0.55  0.9   0.57  0.91 -1.01  0.98 -1.11  0.73]\n",
      "synced target net\n",
      "episode : 22210, reward mean : 0.43842278830363224, total_step : 595160, cur_epsilon : 0.45\n",
      "episode : 22220, reward mean : 0.4384577015410643, total_step : 595254, cur_epsilon : 0.45\n",
      "episode : 22230, reward mean : 0.4382694614612092, total_step : 595444, cur_epsilon : 0.45\n",
      "episode : 22240, reward mean : 0.4383583690791069, total_step : 595618, cur_epsilon : 0.45\n",
      "episode : 22250, reward mean : 0.43839461251155715, total_step : 595909, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22250, reward mean : 0.537500003632158, step mean : 17.25, eval_episode_rewards : [ 0.83  0.99  0.91  0.92  0.82  0.9   0.86  0.98 -1.2   0.75  0.78  0.81\n",
      "  0.43  0.82  0.85 -1.3   0.98  0.85  0.87 -1.1 ]\n",
      "episode : 22260, reward mean : 0.43855526183413046, total_step : 596123, cur_epsilon : 0.45\n",
      "episode : 22270, reward mean : 0.43862102058714864, total_step : 596348, cur_epsilon : 0.45\n",
      "episode : 22280, reward mean : 0.43868896447588596, total_step : 596568, cur_epsilon : 0.45\n",
      "episode : 22290, reward mean : 0.4386043126315059, total_step : 596728, cur_epsilon : 0.45\n",
      "episode : 22300, reward mean : 0.43852153042999553, total_step : 596884, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22300, reward mean : 0.528500008303672, step mean : 38.15, eval_episode_rewards : [ 0.75  0.79  0.82  0.7  -1.26  0.73  0.53  0.9   0.77  0.84  0.7   0.19\n",
      "  0.97  0.08  0.76  0.34  0.47  0.27  0.57  0.65]\n",
      "episode : 22310, reward mean : 0.4386907274150617, total_step : 597078, cur_epsilon : 0.45\n",
      "episode : 22320, reward mean : 0.4388257226114759, total_step : 597348, cur_epsilon : 0.45\n",
      "episode : 22330, reward mean : 0.4388181875829793, total_step : 597536, cur_epsilon : 0.45\n",
      "episode : 22340, reward mean : 0.4389731481099366, total_step : 597761, cur_epsilon : 0.45\n",
      "episode : 22350, reward mean : 0.4389785292542121, total_step : 598019, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22350, reward mean : 0.6895000058226287, step mean : 27.0, eval_episode_rewards : [ 0.59  0.89  0.88  0.74  0.92  0.98  0.69  0.84  0.7   0.44  0.98 -1.\n",
      "  0.93  0.78  0.55  0.69  0.59  0.91  0.94  0.75]\n",
      "episode : 22360, reward mean : 0.43870483581709785, total_step : 598202, cur_epsilon : 0.45\n",
      "episode : 22370, reward mean : 0.4387621872562534, total_step : 598445, cur_epsilon : 0.45\n",
      "episode : 22380, reward mean : 0.4387573784174792, total_step : 598726, cur_epsilon : 0.45\n",
      "episode : 22390, reward mean : 0.4386775403758141, total_step : 598876, cur_epsilon : 0.45\n",
      "episode : 22400, reward mean : 0.43871741647657475, total_step : 599158, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22400, reward mean : 0.36600000523030757, step mean : 24.4, eval_episode_rewards : [ 0.99  0.93  0.75  0.94 -1.03  0.92  0.82  0.9   0.94  0.51  0.29  0.62\n",
      "  0.81 -1.56  0.81  0.82 -1.08  0.4   0.87 -1.33]\n",
      "episode : 22410, reward mean : 0.4386144635939554, total_step : 599459, cur_epsilon : 0.45\n",
      "episode : 22420, reward mean : 0.4387337256559436, total_step : 599763, cur_epsilon : 0.45\n",
      "episode : 22430, reward mean : 0.43871467361800504, total_step : 599977, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 22440, reward mean : 0.43885829453232056, total_step : 600226, cur_epsilon : 0.45\n",
      "episode : 22450, reward mean : 0.43891359150816245, total_step : 600473, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22450, reward mean : 0.6235000039450824, step mean : 18.65, eval_episode_rewards : [ 0.78  0.84  0.71  0.68 -1.19  0.72  0.96  0.67  0.99  0.95  0.85  0.96\n",
      "  0.86  0.97  0.46  0.91  0.66  0.79 -1.07  0.97]\n",
      "episode : 22460, reward mean : 0.43899065580588903, total_step : 600671, cur_epsilon : 0.45\n",
      "episode : 22470, reward mean : 0.43904940496001127, total_step : 600910, cur_epsilon : 0.45\n",
      "episode : 22480, reward mean : 0.4391236712408726, total_step : 601114, cur_epsilon : 0.45\n",
      "episode : 22490, reward mean : 0.4391938696994304, total_step : 601327, cur_epsilon : 0.45\n",
      "episode : 22500, reward mean : 0.4389608946490619, total_step : 601622, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22500, reward mean : 0.7400000058114529, step mean : 27.0, eval_episode_rewards : [0.73 0.43 0.58 0.49 0.47 0.95 0.96 0.81 0.93 0.86 0.83 0.9  0.67 0.65\n",
      " 0.58 0.77 0.87 0.89 0.55 0.88]\n",
      "episode : 22510, reward mean : 0.4390630888331041, total_step : 601763, cur_epsilon : 0.45\n",
      "episode : 22520, reward mean : 0.43911590273922135, total_step : 602015, cur_epsilon : 0.45\n",
      "episode : 22530, reward mean : 0.4392174935515654, total_step : 602157, cur_epsilon : 0.45\n",
      "episode : 22540, reward mean : 0.4393096774512408, total_step : 602320, cur_epsilon : 0.45\n",
      "episode : 22550, reward mean : 0.4392403605222669, total_step : 602447, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22550, reward mean : 0.5040000054985285, step mean : 25.55, eval_episode_rewards : [ 0.82  0.83  0.9   0.57 -1.    0.98  0.83  0.45 -1.14 -1.04  0.52  0.97\n",
      "  0.52  0.97  0.92  0.86  0.8   0.43  0.93  0.96]\n",
      "episode : 22560, reward mean : 0.4393847575279098, total_step : 602692, cur_epsilon : 0.45\n",
      "episode : 22570, reward mean : 0.43944307177141473, total_step : 602931, cur_epsilon : 0.45\n",
      "episode : 22580, reward mean : 0.4394827338326574, total_step : 603212, cur_epsilon : 0.45\n",
      "episode : 22590, reward mean : 0.43945241832636517, total_step : 603451, cur_epsilon : 0.45\n",
      "episode : 22600, reward mean : 0.43960752787785523, total_step : 603671, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22600, reward mean : 0.48600000254809855, step mean : 12.4, eval_episode_rewards : [ 0.83  0.88 -1.04  0.99  0.86  0.68  0.99  0.8   0.76  0.87 -1.03  0.82\n",
      "  0.99  0.97  0.96 -1.08  0.95  0.76 -1.16  0.92]\n",
      "episode : 22610, reward mean : 0.4396665249929353, total_step : 603908, cur_epsilon : 0.45\n",
      "episode : 22620, reward mean : 0.4398342232595368, total_step : 604099, cur_epsilon : 0.45\n",
      "episode : 22630, reward mean : 0.4397618263456979, total_step : 604433, cur_epsilon : 0.45\n",
      "episode : 22640, reward mean : 0.4397323379097276, total_step : 604769, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 22650, reward mean : 0.43976468566624205, total_step : 605066, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22650, reward mean : 0.7520000033080578, step mean : 15.8, eval_episode_rewards : [ 0.96  0.88  0.92  0.81  0.89  0.57  0.95  0.93  0.87  0.96  0.89  0.51\n",
      "  0.75  0.65  0.91 -1.07  0.82  0.94  0.93  0.97]\n",
      "episode : 22660, reward mean : 0.43976390690099704, total_step : 605238, cur_epsilon : 0.45\n",
      "episode : 22670, reward mean : 0.4398557622599435, total_step : 605499, cur_epsilon : 0.45\n",
      "episode : 22680, reward mean : 0.43983774825740063, total_step : 605710, cur_epsilon : 0.45\n",
      "episode : 22690, reward mean : 0.43999251346516355, total_step : 605929, cur_epsilon : 0.45\n",
      "episode : 22700, reward mean : 0.44014185597242106, total_step : 606160, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22700, reward mean : 0.5680000051856041, step mean : 24.2, eval_episode_rewards : [ 0.72  0.94  0.86  0.65  0.85  0.86  0.59  0.7  -1.15  0.91 -1.44  0.65\n",
      "  0.86  0.56  0.75  0.89  0.88  0.6   0.78  0.9 ]\n",
      "episode : 22710, reward mean : 0.44022501675967324, total_step : 606341, cur_epsilon : 0.45\n",
      "episode : 22720, reward mean : 0.4402337205408528, total_step : 606691, cur_epsilon : 0.45\n",
      "episode : 22730, reward mean : 0.4401979819948105, total_step : 606942, cur_epsilon : 0.45\n",
      "episode : 22740, reward mean : 0.44034829071199516, total_step : 607170, cur_epsilon : 0.45\n",
      "episode : 22750, reward mean : 0.44023692882806065, total_step : 607393, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22750, reward mean : 0.6580000054091215, step mean : 25.2, eval_episode_rewards : [ 0.92  0.8   0.42  0.75  0.88  0.74  0.7   0.92 -1.06  0.71  0.97  0.63\n",
      "  0.72  0.45  0.9   0.89  0.84  0.53  0.48  0.97]\n",
      "episode : 22760, reward mean : 0.44014631506581, total_step : 607668, cur_epsilon : 0.45\n",
      "episode : 22770, reward mean : 0.44022530219315187, total_step : 607858, cur_epsilon : 0.45\n",
      "episode : 22780, reward mean : 0.44021949653102443, total_step : 608041, cur_epsilon : 0.45\n",
      "episode : 22790, reward mean : 0.44038526244043075, total_step : 608233, cur_epsilon : 0.45\n",
      "episode : 22800, reward mean : 0.44053772504670186, total_step : 608455, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22800, reward mean : 0.783000004850328, step mean : 22.7, eval_episode_rewards : [0.86 0.86 0.8  0.88 0.69 0.5  0.62 0.99 0.67 0.86 0.94 0.78 0.74 0.9\n",
      " 0.89 0.91 0.85 0.8  0.68 0.44]\n",
      "episode : 22810, reward mean : 0.4405712464326198, total_step : 608748, cur_epsilon : 0.45\n",
      "episode : 22820, reward mean : 0.44066477349520994, total_step : 608904, cur_epsilon : 0.45\n",
      "episode : 22830, reward mean : 0.4408493268152228, total_step : 609052, cur_epsilon : 0.45\n",
      "episode : 22840, reward mean : 0.44077583761908595, total_step : 609189, cur_epsilon : 0.45\n",
      "episode : 22850, reward mean : 0.4409286709525979, total_step : 609409, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22850, reward mean : 0.6965000056661665, step mean : 26.3, eval_episode_rewards : [ 0.8   0.72  0.89  0.31  0.77  0.91  0.96  0.92  0.72  0.88  0.92  0.91\n",
      "  0.83 -1.    0.56  0.73  0.81  0.86  0.85  0.58]\n",
      "episode : 22860, reward mean : 0.44102144056429216, total_step : 609566, cur_epsilon : 0.45\n",
      "episode : 22870, reward mean : 0.4411788426473196, total_step : 609775, cur_epsilon : 0.45\n",
      "episode : 22880, reward mean : 0.4412626805673744, total_step : 609952, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 22890, reward mean : 0.44144605204947995, total_step : 610101, cur_epsilon : 0.45\n",
      "episode : 22900, reward mean : 0.4414314467894263, total_step : 610402, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22900, reward mean : 0.8315000037662685, step mean : 17.85, eval_episode_rewards : [0.78 0.91 0.65 0.91 0.88 0.9  0.67 0.82 0.7  0.99 0.93 0.79 0.79 0.79\n",
      " 0.72 0.83 0.99 0.85 0.89 0.84]\n",
      "episode : 22910, reward mean : 0.4414875657585889, total_step : 610642, cur_epsilon : 0.45\n",
      "episode : 22920, reward mean : 0.44140314710136325, total_step : 610804, cur_epsilon : 0.45\n",
      "episode : 22930, reward mean : 0.4415508125430624, total_step : 611034, cur_epsilon : 0.45\n",
      "episode : 22940, reward mean : 0.4416769891749148, total_step : 611313, cur_epsilon : 0.45\n",
      "episode : 22950, reward mean : 0.4417281103148764, total_step : 611564, cur_epsilon : 0.45\n",
      "[EVAL] episode : 22950, reward mean : 0.533500003721565, step mean : 17.65, eval_episode_rewards : [ 0.91  0.62  0.39  0.95 -1.06  0.82  0.88 -1.02  0.87  0.75 -1.02  0.88\n",
      "  0.85  0.91  0.82  0.93  0.66  0.92  0.7   0.91]\n",
      "episode : 22960, reward mean : 0.44166855974705366, total_step : 611869, cur_epsilon : 0.45\n",
      "episode : 22970, reward mean : 0.44173966616612703, total_step : 612074, cur_epsilon : 0.45\n",
      "episode : 22980, reward mean : 0.441848134545705, total_step : 612193, cur_epsilon : 0.45\n",
      "episode : 22990, reward mean : 0.44186559947310466, total_step : 612321, cur_epsilon : 0.45\n",
      "episode : 23000, reward mean : 0.44191435356268094, total_step : 612577, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23000, reward mean : 0.8035000043921172, step mean : 20.65, eval_episode_rewards : [0.51 0.73 0.99 0.87 0.88 0.76 0.92 0.98 0.87 0.91 0.75 0.95 0.94 0.63\n",
      " 0.97 0.83 0.66 0.73 0.66 0.53]\n",
      "episode : 23010, reward mean : 0.4419543734029169, total_step : 612853, cur_epsilon : 0.45\n",
      "episode : 23020, reward mean : 0.4417489197252433, total_step : 613193, cur_epsilon : 0.45\n",
      "episode : 23030, reward mean : 0.44184281945752674, total_step : 613345, cur_epsilon : 0.45\n",
      "episode : 23040, reward mean : 0.4419084258747009, total_step : 613562, cur_epsilon : 0.45\n",
      "episode : 23050, reward mean : 0.4418958842600827, total_step : 613759, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23050, reward mean : 0.6970000045374036, step mean : 21.3, eval_episode_rewards : [ 0.69  0.95  0.75  0.95  0.88  0.7   0.83  0.82  0.93  0.54  0.98  0.7\n",
      "  0.83  0.95  0.97  0.35  0.9   0.37  0.9  -1.05]\n",
      "episode : 23060, reward mean : 0.4420242902104713, total_step : 614031, cur_epsilon : 0.45\n",
      "episode : 23070, reward mean : 0.4420866984092458, total_step : 614255, cur_epsilon : 0.45\n",
      "episode : 23080, reward mean : 0.4420493991488665, total_step : 614509, cur_epsilon : 0.45\n",
      "episode : 23090, reward mean : 0.44205933877808634, total_step : 614654, cur_epsilon : 0.45\n",
      "episode : 23100, reward mean : 0.44221732175017975, total_step : 614857, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23100, reward mean : 0.2420000035315752, step mean : 16.8, eval_episode_rewards : [ 0.74  0.92  0.97  0.72 -1.06  0.69  0.99  0.93 -1.38  0.6   0.92 -1.27\n",
      "  0.92  0.9   0.99  0.91  0.91 -1.05 -1.11 -1.4 ]\n",
      "synced target net\n",
      "episode : 23110, reward mean : 0.4423595903280112, total_step : 615096, cur_epsilon : 0.45\n",
      "episode : 23120, reward mean : 0.4424204209571298, total_step : 615323, cur_epsilon : 0.45\n",
      "episode : 23130, reward mean : 0.44256809911703937, total_step : 615549, cur_epsilon : 0.45\n",
      "episode : 23140, reward mean : 0.44256828576736895, total_step : 615914, cur_epsilon : 0.45\n",
      "episode : 23150, reward mean : 0.44272700357232764, total_step : 616114, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23150, reward mean : 0.5920000046491622, step mean : 21.8, eval_episode_rewards : [-1.05  0.95  0.79  0.97 -1.09  0.66  0.94  0.61  0.89  0.98  0.85  0.79\n",
      "  0.8   0.15  0.68  0.76  0.86  0.98  0.85  0.47]\n",
      "episode : 23160, reward mean : 0.4427504375115333, total_step : 616427, cur_epsilon : 0.45\n",
      "episode : 23170, reward mean : 0.44282693710864673, total_step : 616617, cur_epsilon : 0.45\n",
      "episode : 23180, reward mean : 0.44287187803554334, total_step : 616880, cur_epsilon : 0.45\n",
      "episode : 23190, reward mean : 0.44281328731907654, total_step : 617183, cur_epsilon : 0.45\n",
      "episode : 23200, reward mean : 0.44258448849111026, total_step : 617481, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23200, reward mean : 0.6740000050514936, step mean : 23.6, eval_episode_rewards : [ 0.58  0.95 -1.75  0.85  0.71  0.84  0.7   0.69  0.86  0.62  0.87  0.92\n",
      "  0.69  0.76  0.9   0.98  0.69  0.97  0.74  0.91]\n",
      "episode : 23210, reward mean : 0.4427647623018021, total_step : 617630, cur_epsilon : 0.45\n",
      "episode : 23220, reward mean : 0.4427911340693793, total_step : 617936, cur_epsilon : 0.45\n",
      "episode : 23230, reward mean : 0.44278089251534414, total_step : 618127, cur_epsilon : 0.45\n",
      "episode : 23240, reward mean : 0.4428605909281364, total_step : 618309, cur_epsilon : 0.45\n",
      "episode : 23250, reward mean : 0.44291742508473897, total_step : 618544, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23250, reward mean : 0.7905000046826899, step mean : 21.95, eval_episode_rewards : [0.73 0.85 0.58 0.65 0.74 0.92 0.94 0.73 0.6  0.92 0.93 0.78 0.77 0.58\n",
      " 0.91 0.85 0.9  0.82 0.72 0.89]\n",
      "episode : 23260, reward mean : 0.4430348294618942, total_step : 618838, cur_epsilon : 0.45\n",
      "episode : 23270, reward mean : 0.44300387337064223, total_step : 619077, cur_epsilon : 0.45\n",
      "episode : 23280, reward mean : 0.44316409507628746, total_step : 619271, cur_epsilon : 0.45\n",
      "episode : 23290, reward mean : 0.44314341491816855, total_step : 619585, cur_epsilon : 0.45\n",
      "episode : 23300, reward mean : 0.4431231816948388, total_step : 619799, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23300, reward mean : 0.648500004503876, step mean : 21.1, eval_episode_rewards : [ 0.89  0.54  0.8   0.96  0.9   0.65  0.96  0.96 -1.    0.9   0.45 -1.03\n",
      "  0.71  0.96  0.82  0.91  0.89  0.97  0.89  0.84]\n",
      "episode : 23310, reward mean : 0.44328700701539503, total_step : 619984, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 23320, reward mean : 0.44330746713350005, total_step : 620103, cur_epsilon : 0.45\n",
      "episode : 23330, reward mean : 0.44337634520347324, total_step : 620309, cur_epsilon : 0.45\n",
      "episode : 23340, reward mean : 0.4434203142096746, total_step : 620573, cur_epsilon : 0.45\n",
      "episode : 23350, reward mean : 0.4434432605448193, total_step : 620886, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23350, reward mean : 0.6960000056773424, step mean : 26.35, eval_episode_rewards : [ 0.92  0.75  0.55  0.85 -1.    0.94  0.79  0.87  0.88  0.89  0.52  0.79\n",
      "  0.9   0.86  0.77  0.96  0.79  0.55  0.9   0.44]\n",
      "episode : 23360, reward mean : 0.44345634134393824, total_step : 621222, cur_epsilon : 0.45\n",
      "episode : 23370, reward mean : 0.4435027870710239, total_step : 621480, cur_epsilon : 0.45\n",
      "episode : 23380, reward mean : 0.44353678930332524, total_step : 621767, cur_epsilon : 0.45\n",
      "episode : 23390, reward mean : 0.44366054441947084, total_step : 622044, cur_epsilon : 0.45\n",
      "episode : 23400, reward mean : 0.4436243647019355, total_step : 622295, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23400, reward mean : 0.5875000047497452, step mean : 22.25, eval_episode_rewards : [ 0.94  0.87 -1.54  0.32  0.91  0.93  0.92  0.69  0.93 -1.29  0.9   0.62\n",
      "  0.77  0.87  0.79  0.76  0.73  0.93  0.82  0.88]\n",
      "episode : 23410, reward mean : 0.44370611422734607, total_step : 622470, cur_epsilon : 0.45\n",
      "episode : 23420, reward mean : 0.4436819015418352, total_step : 622693, cur_epsilon : 0.45\n",
      "episode : 23430, reward mean : 0.44364063739506315, total_step : 622956, cur_epsilon : 0.45\n",
      "episode : 23440, reward mean : 0.4437896814937115, total_step : 623173, cur_epsilon : 0.45\n",
      "episode : 23450, reward mean : 0.44369254303949157, total_step : 623466, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23450, reward mean : 0.3835000048391521, step mean : 22.65, eval_episode_rewards : [-1.24  0.78  0.48 -1.51  0.76  0.91 -1.19  0.97  0.84  0.79  0.87  0.9\n",
      "  0.79 -1.29  0.91  0.9   0.98  0.89  0.42  0.71]\n",
      "episode : 23460, reward mean : 0.44383078151440464, total_step : 623708, cur_epsilon : 0.45\n",
      "episode : 23470, reward mean : 0.4439961710424152, total_step : 623886, cur_epsilon : 0.45\n",
      "episode : 23480, reward mean : 0.4439723225899859, total_step : 624108, cur_epsilon : 0.45\n",
      "episode : 23490, reward mean : 0.4439378516162556, total_step : 624355, cur_epsilon : 0.45\n",
      "episode : 23500, reward mean : 0.44407192061786005, total_step : 624606, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23500, reward mean : 0.4225000050850213, step mean : 23.7, eval_episode_rewards : [ 0.92 -1.04  0.97 -1.15  0.86  0.74  0.48  0.8   0.75  0.93  0.91  0.88\n",
      "  0.65 -1.    0.87  0.99 -1.01  0.51  0.53  0.86]\n",
      "episode : 23510, reward mean : 0.4439766114232218, total_step : 624796, cur_epsilon : 0.45\n",
      "episode : 23520, reward mean : 0.4438044274913236, total_step : 624967, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 23530, reward mean : 0.4439932058912437, total_step : 625089, cur_epsilon : 0.45\n",
      "episode : 23540, reward mean : 0.44389337870278917, total_step : 625290, cur_epsilon : 0.45\n",
      "episode : 23550, reward mean : 0.4440543581614955, total_step : 625477, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23550, reward mean : 0.333500003721565, step mean : 17.65, eval_episode_rewards : [ 0.87  0.6   0.89  0.96 -1.12  0.58  0.9  -1.18  0.79  0.93 -1.03 -1.03\n",
      "  0.75 -1.03  0.98  0.59  0.81  0.87  0.8   0.74]\n",
      "episode : 23560, reward mean : 0.4441791228676477, total_step : 625749, cur_epsilon : 0.45\n",
      "episode : 23570, reward mean : 0.4442439598994275, total_step : 626061, cur_epsilon : 0.45\n",
      "episode : 23580, reward mean : 0.44441603625373344, total_step : 626221, cur_epsilon : 0.45\n",
      "episode : 23590, reward mean : 0.4445638039385037, total_step : 626438, cur_epsilon : 0.45\n",
      "episode : 23600, reward mean : 0.4445466158878437, total_step : 626644, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23600, reward mean : 0.7460000034421682, step mean : 16.4, eval_episode_rewards : [ 0.87  0.91  0.6   0.86  0.91  0.78  0.82  0.86  0.94  0.7   0.94  0.92\n",
      "  0.58  0.98  0.99  0.99  0.95 -1.01  0.52  0.81]\n",
      "episode : 23610, reward mean : 0.4445099591277934, total_step : 626896, cur_epsilon : 0.45\n",
      "episode : 23620, reward mean : 0.4445580074125167, total_step : 627247, cur_epsilon : 0.45\n",
      "episode : 23630, reward mean : 0.44450360284160495, total_step : 627541, cur_epsilon : 0.45\n",
      "episode : 23640, reward mean : 0.4445118500498106, total_step : 627687, cur_epsilon : 0.45\n",
      "episode : 23650, reward mean : 0.44467315582310424, total_step : 627871, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23650, reward mean : 0.7500000033527613, step mean : 16.0, eval_episode_rewards : [-1.06  0.78  0.91  0.94  0.91  0.92  0.94  0.55  0.9   0.96  0.88  0.81\n",
      "  0.71  0.91  0.96  0.54  0.92  0.91  0.79  0.82]\n",
      "episode : 23660, reward mean : 0.4446648408824126, total_step : 628155, cur_epsilon : 0.45\n",
      "episode : 23670, reward mean : 0.4445994987472401, total_step : 628475, cur_epsilon : 0.45\n",
      "episode : 23680, reward mean : 0.4446849719342788, total_step : 628737, cur_epsilon : 0.45\n",
      "episode : 23690, reward mean : 0.4444778444684976, total_step : 628993, cur_epsilon : 0.45\n",
      "episode : 23700, reward mean : 0.44459747407255296, total_step : 629275, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23700, reward mean : 0.8160000041127204, step mean : 19.4, eval_episode_rewards : [0.89 0.81 0.9  0.79 0.79 0.88 0.72 0.99 0.82 0.88 0.82 0.77 0.79 0.91\n",
      " 0.52 0.94 0.71 0.92 0.56 0.91]\n",
      "episode : 23710, reward mean : 0.4445196176960417, total_step : 629524, cur_epsilon : 0.45\n",
      "episode : 23720, reward mean : 0.4446488252794606, total_step : 629783, cur_epsilon : 0.45\n",
      "episode : 23730, reward mean : 0.44473325476879266, total_step : 629948, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 23740, reward mean : 0.44486226350965064, total_step : 630207, cur_epsilon : 0.45\n",
      "episode : 23750, reward mean : 0.44491832150605165, total_step : 630439, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23750, reward mean : 0.7410000035539269, step mean : 16.9, eval_episode_rewards : [ 0.98 -1.05  0.96  0.92  0.61  0.53  0.58  0.84  0.91  0.99  0.85  0.73\n",
      "  0.92  0.65  0.94  0.88  0.92  0.89  0.89  0.88]\n",
      "episode : 23760, reward mean : 0.44505429864564744, total_step : 630681, cur_epsilon : 0.45\n",
      "episode : 23770, reward mean : 0.44503366158461, total_step : 630895, cur_epsilon : 0.45\n",
      "episode : 23780, reward mean : 0.44512195693433937, total_step : 631050, cur_epsilon : 0.45\n",
      "episode : 23790, reward mean : 0.44510341050622404, total_step : 631259, cur_epsilon : 0.45\n",
      "episode : 23800, reward mean : 0.4451983250427146, total_step : 631598, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23800, reward mean : 0.6670000040903687, step mean : 19.25, eval_episode_rewards : [ 0.99  0.89  0.93  0.83  0.52  0.96  0.94  0.99  0.9   0.93  0.42  0.79\n",
      "  0.9   0.85  0.9  -1.07 -1.    0.86  0.93  0.88]\n",
      "episode : 23810, reward mean : 0.4452981997498009, total_step : 631725, cur_epsilon : 0.45\n",
      "episode : 23820, reward mean : 0.4454084859827047, total_step : 632027, cur_epsilon : 0.45\n",
      "episode : 23830, reward mean : 0.44548133177291155, total_step : 632218, cur_epsilon : 0.45\n",
      "episode : 23840, reward mean : 0.44555034128317744, total_step : 632418, cur_epsilon : 0.45\n",
      "episode : 23850, reward mean : 0.44555975414015964, total_step : 632659, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23850, reward mean : 0.4810000048950315, step mean : 22.9, eval_episode_rewards : [ 0.91  0.92  0.74  0.92 -1.2   0.79  0.77  0.86  0.93 -1.57  0.58  0.57\n",
      " -1.43  0.89  0.91  0.82  0.85  0.91  0.9   0.55]\n",
      "episode : 23860, reward mean : 0.44560646002921717, total_step : 632912, cur_epsilon : 0.45\n",
      "episode : 23870, reward mean : 0.4456874795279894, total_step : 633083, cur_epsilon : 0.45\n",
      "episode : 23880, reward mean : 0.4457014294958188, total_step : 633214, cur_epsilon : 0.45\n",
      "episode : 23890, reward mean : 0.44573085543841073, total_step : 633508, cur_epsilon : 0.45\n",
      "episode : 23900, reward mean : 0.44576946177749766, total_step : 633780, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23900, reward mean : 0.7900000046938658, step mean : 22.0, eval_episode_rewards : [0.8  0.3  0.61 0.98 0.92 0.91 0.8  0.69 0.62 0.81 0.86 0.97 0.67 0.94\n",
      " 0.97 0.85 0.77 0.75 0.75 0.83]\n",
      "episode : 23910, reward mean : 0.44586073343841875, total_step : 633926, cur_epsilon : 0.45\n",
      "episode : 23920, reward mean : 0.44602508932059276, total_step : 634097, cur_epsilon : 0.45\n",
      "episode : 23930, reward mean : 0.44591183186790095, total_step : 634332, cur_epsilon : 0.45\n",
      "episode : 23940, reward mean : 0.44608062391948194, total_step : 634492, cur_epsilon : 0.45\n",
      "episode : 23950, reward mean : 0.44624175935991434, total_step : 634670, cur_epsilon : 0.45\n",
      "[EVAL] episode : 23950, reward mean : 0.7860000047832727, step mean : 22.4, eval_episode_rewards : [0.83 0.93 0.99 0.8  0.76 0.48 0.9  0.72 0.98 0.89 0.73 0.89 0.81 0.46\n",
      " 0.93 0.94 0.48 0.85 0.55 0.8 ]\n",
      "episode : 23960, reward mean : 0.4462525098789048, total_step : 634808, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 23970, reward mean : 0.4463462718720107, total_step : 635147, cur_epsilon : 0.45\n",
      "episode : 23980, reward mean : 0.44649875466280725, total_step : 635345, cur_epsilon : 0.45\n",
      "episode : 23990, reward mean : 0.44632597486020725, total_step : 635721, cur_epsilon : 0.45\n",
      "episode : 24000, reward mean : 0.4461570890388296, total_step : 635890, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24000, reward mean : 0.7550000054761767, step mean : 25.5, eval_episode_rewards : [0.69 0.78 0.98 0.84 0.47 0.9  0.94 0.78 0.89 0.61 0.85 0.22 0.88 0.99\n",
      " 0.87 0.91 0.6  0.24 0.98 0.68]\n",
      "episode : 24010, reward mean : 0.44612953506793357, total_step : 636120, cur_epsilon : 0.45\n",
      "episode : 24020, reward mean : 0.4462772746471925, total_step : 636329, cur_epsilon : 0.45\n",
      "episode : 24030, reward mean : 0.44628506604476653, total_step : 636474, cur_epsilon : 0.45\n",
      "episode : 24040, reward mean : 0.44636148656795915, total_step : 636654, cur_epsilon : 0.45\n",
      "episode : 24050, reward mean : 0.44651310341521755, total_step : 636853, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24050, reward mean : 0.5085000065155327, step mean : 30.15, eval_episode_rewards : [ 0.02  0.76 -1.06  0.81  0.96  0.59 -1.32  0.57  0.92  0.9   0.91  0.85\n",
      "  0.93  0.56  0.86  0.43  0.95  0.57  0.22  0.74]\n",
      "episode : 24060, reward mean : 0.4465739874140586, total_step : 637070, cur_epsilon : 0.45\n",
      "episode : 24070, reward mean : 0.4464432961047841, total_step : 637348, cur_epsilon : 0.45\n",
      "episode : 24080, reward mean : 0.44663081965386014, total_step : 637460, cur_epsilon : 0.45\n",
      "episode : 24090, reward mean : 0.44654463002494554, total_step : 637631, cur_epsilon : 0.45\n",
      "episode : 24100, reward mean : 0.44662324221317823, total_step : 637805, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24100, reward mean : 0.6090000042691827, step mean : 20.1, eval_episode_rewards : [ 0.93 -1.04  0.82  0.63  0.5   0.88  0.81  0.4   0.78  0.82  0.88  0.73\n",
      "  0.73  0.97  0.98 -1.08  0.95  0.99  0.52  0.98]\n",
      "episode : 24110, reward mean : 0.44663459715328735, total_step : 637941, cur_epsilon : 0.45\n",
      "episode : 24120, reward mean : 0.44660738546495476, total_step : 638170, cur_epsilon : 0.45\n",
      "episode : 24130, reward mean : 0.4467111536452583, total_step : 638382, cur_epsilon : 0.45\n",
      "episode : 24140, reward mean : 0.4468674456296364, total_step : 638568, cur_epsilon : 0.45\n",
      "episode : 24150, reward mean : 0.4467677075587074, total_step : 638772, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24150, reward mean : 0.5645000052638351, step mean : 24.55, eval_episode_rewards : [ 0.87  0.79  0.85  0.88  0.7   0.93  0.58  0.54  0.46  0.77  0.98  0.98\n",
      " -1.13  0.76  0.93  0.81  0.83  0.17 -1.21  0.8 ]\n",
      "episode : 24160, reward mean : 0.4468555520519164, total_step : 638923, cur_epsilon : 0.45\n",
      "episode : 24170, reward mean : 0.4469073288218975, total_step : 639161, cur_epsilon : 0.45\n",
      "episode : 24180, reward mean : 0.4469532728568798, total_step : 639413, cur_epsilon : 0.45\n",
      "episode : 24190, reward mean : 0.44701323430036566, total_step : 639631, cur_epsilon : 0.45\n",
      "episode : 24200, reward mean : 0.44715868337895753, total_step : 639842, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24200, reward mean : 0.741500003542751, step mean : 16.85, eval_episode_rewards : [ 0.96  0.94  0.78  0.99  0.91  0.71  0.99  0.75  0.82 -1.06  0.94  0.74\n",
      "  0.87  0.94  0.86  0.92  0.44  0.65  0.69  0.99]\n",
      "synced target net\n",
      "episode : 24210, reward mean : 0.44726353316140455, total_step : 640151, cur_epsilon : 0.45\n",
      "episode : 24220, reward mean : 0.4474145391362263, total_step : 640348, cur_epsilon : 0.45\n",
      "episode : 24230, reward mean : 0.44753281625831026, total_step : 640624, cur_epsilon : 0.45\n",
      "episode : 24240, reward mean : 0.4475334215353676, total_step : 640985, cur_epsilon : 0.45\n",
      "episode : 24250, reward mean : 0.44763753146555313, total_step : 641095, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24250, reward mean : 0.45000000558793546, step mean : 26.0, eval_episode_rewards : [-1.02  0.97  0.79  0.7   0.81  0.1   0.69  0.85  0.59 -1.35  0.45  0.98\n",
      "  0.93  0.56  0.77  0.62  0.71 -1.04  0.99  0.9 ]\n",
      "episode : 24260, reward mean : 0.4477230065157491, total_step : 641250, cur_epsilon : 0.45\n",
      "episode : 24270, reward mean : 0.44787804442156626, total_step : 641436, cur_epsilon : 0.45\n",
      "episode : 24280, reward mean : 0.44794563995689596, total_step : 641634, cur_epsilon : 0.45\n",
      "episode : 24290, reward mean : 0.4479975355374207, total_step : 641870, cur_epsilon : 0.45\n",
      "episode : 24300, reward mean : 0.44812840075128607, total_step : 642114, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24300, reward mean : 0.7265000049956143, step mean : 23.3, eval_episode_rewards : [ 0.93  0.87  0.44  0.87  0.39  0.79  0.62  0.99  0.9   0.68  0.75  0.82\n",
      "  0.82  0.99  0.97  0.92  0.84  0.97 -1.    0.97]\n",
      "episode : 24310, reward mean : 0.44822501597396575, total_step : 642441, cur_epsilon : 0.45\n",
      "episode : 24320, reward mean : 0.44828947937379493, total_step : 642646, cur_epsilon : 0.45\n",
      "episode : 24330, reward mean : 0.44823633943417596, total_step : 642937, cur_epsilon : 0.45\n",
      "episode : 24340, reward mean : 0.4482806137423188, total_step : 643191, cur_epsilon : 0.45\n",
      "episode : 24350, reward mean : 0.4482657141080107, total_step : 643389, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24350, reward mean : 0.818000004068017, step mean : 19.2, eval_episode_rewards : [0.81 0.87 0.84 0.49 0.97 0.94 0.98 0.86 0.91 0.99 0.59 0.8  0.8  0.76\n",
      " 0.84 0.72 0.89 0.51 0.92 0.87]\n",
      "episode : 24360, reward mean : 0.44819458697055437, total_step : 643724, cur_epsilon : 0.45\n",
      "episode : 24370, reward mean : 0.4483188403224604, total_step : 643983, cur_epsilon : 0.45\n",
      "episode : 24380, reward mean : 0.4483777743521268, total_step : 644201, cur_epsilon : 0.45\n",
      "episode : 24390, reward mean : 0.44844814016995016, total_step : 644391, cur_epsilon : 0.45\n",
      "episode : 24400, reward mean : 0.44852295650749424, total_step : 644570, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24400, reward mean : 0.2700000051409006, step mean : 24.0, eval_episode_rewards : [ 0.87  0.9  -1.54  0.89  0.8   0.87  0.92 -1.13  0.64 -1.11  0.83  0.82\n",
      "  0.76 -1.39 -1.34  0.75  0.72  0.61  0.69  0.84]\n",
      "episode : 24410, reward mean : 0.4484920171582793, total_step : 644807, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 24420, reward mean : 0.44843489512279444, total_step : 645108, cur_epsilon : 0.45\n",
      "episode : 24430, reward mean : 0.4484568210792161, total_step : 645416, cur_epsilon : 0.45\n",
      "episode : 24440, reward mean : 0.4484398583882319, total_step : 645619, cur_epsilon : 0.45\n",
      "episode : 24450, reward mean : 0.44842618155622654, total_step : 645814, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24450, reward mean : 0.7285000038333237, step mean : 18.15, eval_episode_rewards : [ 0.82  0.67 -1.14  0.97  0.86  0.99  0.95  0.99  0.86  0.86  0.96  0.87\n",
      "  0.89  0.12  0.89  0.58  0.75  0.93  0.97  0.78]\n",
      "episode : 24460, reward mean : 0.44850041451707834, total_step : 645994, cur_epsilon : 0.45\n",
      "episode : 24470, reward mean : 0.44842951120211183, total_step : 646129, cur_epsilon : 0.45\n",
      "episode : 24480, reward mean : 0.4485510677766524, total_step : 646393, cur_epsilon : 0.45\n",
      "episode : 24490, reward mean : 0.44862311715849595, total_step : 646578, cur_epsilon : 0.45\n",
      "episode : 24500, reward mean : 0.44868490364311303, total_step : 646788, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24500, reward mean : 0.5710000051185489, step mean : 23.9, eval_episode_rewards : [ 0.53 -1.06  0.74  0.89  0.32  0.6  -1.01  0.81  0.85  0.75  0.8   0.99\n",
      "  0.75  0.92  0.73  0.69  0.79  0.85  0.83  0.65]\n",
      "episode : 24510, reward mean : 0.4488266070706596, total_step : 647002, cur_epsilon : 0.45\n",
      "episode : 24520, reward mean : 0.4488605277064343, total_step : 647280, cur_epsilon : 0.45\n",
      "episode : 24530, reward mean : 0.44881900282993553, total_step : 647543, cur_epsilon : 0.45\n",
      "episode : 24540, reward mean : 0.4487282045436803, total_step : 647927, cur_epsilon : 0.45\n",
      "episode : 24550, reward mean : 0.44874053521502894, total_step : 648058, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24550, reward mean : 0.6970000045374036, step mean : 21.3, eval_episode_rewards : [ 0.81  0.96  0.81  0.42  0.93 -1.04  0.5   0.94  0.71  0.88  0.89  0.89\n",
      "  0.94  0.73  0.87  0.78  0.76  0.7   0.65  0.81]\n",
      "episode : 24560, reward mean : 0.4487500056823115, total_step : 648196, cur_epsilon : 0.45\n",
      "episode : 24570, reward mean : 0.44885511353777213, total_step : 648499, cur_epsilon : 0.45\n",
      "episode : 24580, reward mean : 0.4488661570240349, total_step : 648633, cur_epsilon : 0.45\n",
      "episode : 24590, reward mean : 0.44893453191100774, total_step : 648826, cur_epsilon : 0.45\n",
      "episode : 24600, reward mean : 0.4490695178756354, total_step : 649055, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24600, reward mean : 0.5190000062808394, step mean : 29.1, eval_episode_rewards : [ 0.83  0.79  0.52  0.86  0.92  0.64  0.91  0.81  0.86  0.72  0.66  0.77\n",
      "  0.71  0.93  0.36  0.05  0.69 -1.07 -1.5   0.92]\n",
      "episode : 24610, reward mean : 0.44911540592417015, total_step : 649303, cur_epsilon : 0.45\n",
      "episode : 24620, reward mean : 0.4492193395556363, total_step : 649608, cur_epsilon : 0.45\n",
      "episode : 24630, reward mean : 0.44926959561237745, total_step : 649944, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 24640, reward mean : 0.4493993563305393, total_step : 650185, cur_epsilon : 0.45\n",
      "episode : 24650, reward mean : 0.4494450361070055, total_step : 650433, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24650, reward mean : 0.49700000453740356, step mean : 21.3, eval_episode_rewards : [ 0.99  0.88  0.77  0.71  0.91 -1.01 -1.09  0.61  0.8   0.59  0.76  0.95\n",
      "  0.93  0.85  0.91  0.88  0.27 -1.58  0.86  0.95]\n",
      "episode : 24660, reward mean : 0.4493163073843304, total_step : 650711, cur_epsilon : 0.45\n",
      "episode : 24670, reward mean : 0.44937779246623705, total_step : 650920, cur_epsilon : 0.45\n",
      "episode : 24680, reward mean : 0.4494517884999846, total_step : 651098, cur_epsilon : 0.45\n",
      "episode : 24690, reward mean : 0.44941798866876964, total_step : 651342, cur_epsilon : 0.45\n",
      "episode : 24700, reward mean : 0.4493336089186159, total_step : 651610, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24700, reward mean : 0.4445000079460442, step mean : 36.55, eval_episode_rewards : [ 0.63  0.78  0.48  0.31  0.68  0.89  0.69  0.44  0.16  0.23  0.54 -1.09\n",
      "  0.55  0.88  0.93  0.92  0.87  0.51 -1.16  0.65]\n",
      "episode : 24710, reward mean : 0.44937637152345505, total_step : 651865, cur_epsilon : 0.45\n",
      "episode : 24720, reward mean : 0.44951214160164366, total_step : 652090, cur_epsilon : 0.45\n",
      "episode : 24730, reward mean : 0.44953134413416923, total_step : 652302, cur_epsilon : 0.45\n",
      "episode : 24740, reward mean : 0.4495784212000933, total_step : 652546, cur_epsilon : 0.45\n",
      "episode : 24750, reward mean : 0.4496775814349001, total_step : 652661, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24750, reward mean : 0.33250000486150383, step mean : 22.7, eval_episode_rewards : [ 0.86  0.76  0.92  0.73  0.89  0.79  0.81  0.77  0.96 -1.06 -1.18 -1.38\n",
      "  0.99  0.79  0.69  0.55 -1.11 -1.    0.95  0.92]\n",
      "episode : 24760, reward mean : 0.4497201187628651, total_step : 652916, cur_epsilon : 0.45\n",
      "episode : 24770, reward mean : 0.4496972200490435, total_step : 653133, cur_epsilon : 0.45\n",
      "episode : 24780, reward mean : 0.4497663495018086, total_step : 653322, cur_epsilon : 0.45\n",
      "episode : 24790, reward mean : 0.44980274871774634, total_step : 653592, cur_epsilon : 0.45\n",
      "episode : 24800, reward mean : 0.4497955701915517, total_step : 653770, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24800, reward mean : 0.5815000048838556, step mean : 22.85, eval_episode_rewards : [ 0.81  0.35  0.96  0.97 -1.24  0.94 -1.22  0.6   0.8   0.9   0.99  0.82\n",
      "  0.59  0.78  0.6   0.54  0.78  0.9   0.88  0.88]\n",
      "episode : 24810, reward mean : 0.44977670861727187, total_step : 653977, cur_epsilon : 0.45\n",
      "episode : 24820, reward mean : 0.4497393288022233, total_step : 654329, cur_epsilon : 0.45\n",
      "episode : 24830, reward mean : 0.4499005292349863, total_step : 654489, cur_epsilon : 0.45\n",
      "episode : 24840, reward mean : 0.44990298473981066, total_step : 654643, cur_epsilon : 0.45\n",
      "episode : 24850, reward mean : 0.44995734973782503, total_step : 654868, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24850, reward mean : 0.6405000035651028, step mean : 16.95, eval_episode_rewards : [ 0.76  0.53  0.87  0.78  0.74 -1.1   0.95  0.97  0.91  0.79  0.99  0.92\n",
      "  0.86  0.66  0.81  0.91 -1.12  0.92  0.79  0.87]\n",
      "synced target net\n",
      "episode : 24860, reward mean : 0.4500000056733504, total_step : 655122, cur_epsilon : 0.45\n",
      "episode : 24870, reward mean : 0.45008042384692476, total_step : 655282, cur_epsilon : 0.45\n",
      "episode : 24880, reward mean : 0.4501611793058149, total_step : 655540, cur_epsilon : 0.45\n",
      "episode : 24890, reward mean : 0.4502141478987446, total_step : 655867, cur_epsilon : 0.45\n",
      "episode : 24900, reward mean : 0.4499441823794929, total_step : 656099, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24900, reward mean : 0.8235000039450824, step mean : 18.65, eval_episode_rewards : [0.85 0.95 0.87 0.94 0.61 0.76 0.82 0.85 0.93 0.95 0.92 0.87 0.3  0.61\n",
      " 0.78 0.88 0.96 0.97 0.93 0.72]\n",
      "episode : 24910, reward mean : 0.4500766817060838, total_step : 656329, cur_epsilon : 0.45\n",
      "episode : 24920, reward mean : 0.45016051931501055, total_step : 656480, cur_epsilon : 0.45\n",
      "episode : 24930, reward mean : 0.4501781043481131, total_step : 656796, cur_epsilon : 0.45\n",
      "episode : 24940, reward mean : 0.45033761593554267, total_step : 656958, cur_epsilon : 0.45\n",
      "episode : 24950, reward mean : 0.4503098253098751, total_step : 657187, cur_epsilon : 0.45\n",
      "[EVAL] episode : 24950, reward mean : 0.7965000045485795, step mean : 21.35, eval_episode_rewards : [0.42 0.76 0.99 0.83 0.5  0.94 0.73 0.87 0.66 0.86 0.97 0.91 0.71 0.82\n",
      " 0.95 0.84 0.83 0.74 0.88 0.72]\n",
      "episode : 24960, reward mean : 0.4502195569519431, total_step : 657372, cur_epsilon : 0.45\n",
      "episode : 24970, reward mean : 0.4501766176042429, total_step : 657639, cur_epsilon : 0.45\n",
      "episode : 24980, reward mean : 0.4502349936598696, total_step : 657853, cur_epsilon : 0.45\n",
      "episode : 24990, reward mean : 0.45021889322393, total_step : 658053, cur_epsilon : 0.45\n",
      "episode : 25000, reward mean : 0.45038320566788315, total_step : 658202, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25000, reward mean : 0.5400000035762786, step mean : 17.0, eval_episode_rewards : [ 0.87  0.83  0.9   0.94  0.46  0.45 -1.13  0.86  0.76 -1.1   0.83  0.84\n",
      "  0.95  0.95  0.85  0.99 -1.22  0.94  0.92  0.91]\n",
      "episode : 25010, reward mean : 0.4504658193414251, total_step : 658355, cur_epsilon : 0.45\n",
      "episode : 25020, reward mean : 0.45042126865656995, total_step : 658626, cur_epsilon : 0.45\n",
      "episode : 25030, reward mean : 0.45031842356506785, total_step : 658843, cur_epsilon : 0.45\n",
      "episode : 25040, reward mean : 0.4501988874574284, total_step : 659302, cur_epsilon : 0.45\n",
      "episode : 25050, reward mean : 0.4501976104578126, total_step : 659465, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25050, reward mean : 0.8005000044591725, step mean : 20.95, eval_episode_rewards : [0.91 0.67 0.7  0.96 0.52 0.99 0.59 0.95 0.96 0.92 0.82 0.82 0.6  0.72\n",
      " 0.76 0.71 0.7  0.83 0.92 0.96]\n",
      "episode : 25060, reward mean : 0.45031285482947103, total_step : 659736, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 25070, reward mean : 0.4503474328713733, total_step : 660009, cur_epsilon : 0.45\n",
      "episode : 25080, reward mean : 0.4503453007233782, total_step : 660273, cur_epsilon : 0.45\n",
      "episode : 25090, reward mean : 0.45041052778732166, total_step : 660469, cur_epsilon : 0.45\n",
      "episode : 25100, reward mean : 0.4503223164231921, total_step : 660650, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25100, reward mean : 0.7545000043697655, step mean : 20.5, eval_episode_rewards : [ 0.99  0.96  0.92  0.88  0.86  0.69  0.99  0.81  0.97  0.83  0.79  0.89\n",
      "  0.57  0.94  0.64  0.83 -1.    0.74  0.96  0.83]\n",
      "episode : 25110, reward mean : 0.4504078113202826, total_step : 660795, cur_epsilon : 0.45\n",
      "episode : 25120, reward mean : 0.4503785884674419, total_step : 661028, cur_epsilon : 0.45\n",
      "episode : 25130, reward mean : 0.45045683017653554, total_step : 661191, cur_epsilon : 0.45\n",
      "episode : 25140, reward mean : 0.4503957892744376, total_step : 661304, cur_epsilon : 0.45\n",
      "episode : 25150, reward mean : 0.4504727690812815, total_step : 661470, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25150, reward mean : 0.5185000051744282, step mean : 24.1, eval_episode_rewards : [ 0.72  0.94  0.99  0.89  0.48  0.92  0.92  0.82  0.77  0.95  0.95 -1.26\n",
      "  0.96 -1.    0.76  0.32  0.84  0.94  0.56 -1.1 ]\n",
      "episode : 25160, reward mean : 0.45051669882542333, total_step : 661719, cur_epsilon : 0.45\n",
      "episode : 25170, reward mean : 0.4506384641439001, total_step : 661972, cur_epsilon : 0.45\n",
      "episode : 25180, reward mean : 0.4505508396560748, total_step : 662152, cur_epsilon : 0.45\n",
      "episode : 25190, reward mean : 0.45055379684683217, total_step : 662304, cur_epsilon : 0.45\n",
      "episode : 25200, reward mean : 0.4506127040720885, total_step : 662515, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25200, reward mean : 0.7885000047273933, step mean : 22.15, eval_episode_rewards : [0.87 0.88 0.93 0.68 0.84 0.67 0.92 0.37 0.94 0.62 0.98 0.95 0.69 0.43\n",
      " 0.65 0.97 0.97 0.66 0.83 0.92]\n",
      "episode : 25210, reward mean : 0.450613254370173, total_step : 662772, cur_epsilon : 0.45\n",
      "episode : 25220, reward mean : 0.45068398662610165, total_step : 662953, cur_epsilon : 0.45\n",
      "episode : 25230, reward mean : 0.4505822490192758, total_step : 663169, cur_epsilon : 0.45\n",
      "episode : 25240, reward mean : 0.4506672005858361, total_step : 663314, cur_epsilon : 0.45\n",
      "episode : 25250, reward mean : 0.45064277793400653, total_step : 663535, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25250, reward mean : 0.21950000403448938, step mean : 19.05, eval_episode_rewards : [ 0.96 -1.09  0.92  0.93  0.81 -1.01 -1.28  0.72  0.67  0.94 -1.37  0.66\n",
      "  0.82  0.81 -1.19  0.81 -1.04  0.87  0.94  0.51]\n",
      "episode : 25260, reward mean : 0.45080562719178585, total_step : 663683, cur_epsilon : 0.45\n",
      "episode : 25270, reward mean : 0.4509184860674127, total_step : 663957, cur_epsilon : 0.45\n",
      "episode : 25280, reward mean : 0.45105182527574783, total_step : 664179, cur_epsilon : 0.45\n",
      "episode : 25290, reward mean : 0.4510877873874485, total_step : 664447, cur_epsilon : 0.45\n",
      "episode : 25300, reward mean : 0.45115731790779606, total_step : 664630, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25300, reward mean : 0.44050000691786406, step mean : 31.9, eval_episode_rewards : [ 0.73  0.9   0.65  0.85  0.82 -1.    0.59  0.22  0.96  0.87 -1.12  0.79\n",
      "  0.9   0.63  0.42 -1.09  0.68  0.47  0.83  0.71]\n",
      "episode : 25310, reward mean : 0.45120901395163837, total_step : 664858, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 25320, reward mean : 0.4512839708985587, total_step : 665027, cur_epsilon : 0.45\n",
      "episode : 25330, reward mean : 0.4512787265372174, total_step : 665199, cur_epsilon : 0.45\n",
      "episode : 25340, reward mean : 0.45131768521087595, total_step : 665459, cur_epsilon : 0.45\n",
      "episode : 25350, reward mean : 0.4512410312929215, total_step : 665612, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25350, reward mean : 0.7310000037774443, step mean : 17.9, eval_episode_rewards : [ 0.88  0.91  0.95 -1.21  0.62  0.59  0.94  0.99  0.87  0.94  0.45  0.88\n",
      "  0.8   0.97  0.81  0.94  0.82  0.91  0.81  0.75]\n",
      "episode : 25360, reward mean : 0.4513868353042417, total_step : 665801, cur_epsilon : 0.45\n",
      "episode : 25370, reward mean : 0.4514592094344678, total_step : 665976, cur_epsilon : 0.45\n",
      "episode : 25380, reward mean : 0.45152679840000043, total_step : 666163, cur_epsilon : 0.45\n",
      "episode : 25390, reward mean : 0.45152856019788096, total_step : 666317, cur_epsilon : 0.45\n",
      "episode : 25400, reward mean : 0.45151654108126804, total_step : 666506, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25400, reward mean : 0.5280000038444996, step mean : 18.2, eval_episode_rewards : [ 0.76  0.96  0.85  0.99  0.88  0.84  0.99  0.86  0.9   0.95 -1.11  0.18\n",
      "  0.85  0.51 -1.03  0.68  0.94  0.87 -1.08  0.77]\n",
      "episode : 25410, reward mean : 0.4515588407523839, total_step : 666757, cur_epsilon : 0.45\n",
      "episode : 25420, reward mean : 0.4515539002184918, total_step : 666928, cur_epsilon : 0.45\n",
      "episode : 25430, reward mean : 0.45152969499020845, total_step : 667148, cur_epsilon : 0.45\n",
      "episode : 25440, reward mean : 0.4516914364635072, total_step : 667295, cur_epsilon : 0.45\n",
      "episode : 25450, reward mean : 0.4516903789277044, total_step : 667654, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25450, reward mean : 0.5375000058673323, step mean : 27.25, eval_episode_rewards : [-1.74  0.93  0.88  0.91  0.75  0.8   0.86  0.66  0.5   0.91  0.2   0.65\n",
      "  0.62 -1.02  0.88  0.93  0.53  0.85  0.77  0.88]\n",
      "episode : 25460, reward mean : 0.45171720910340385, total_step : 667944, cur_epsilon : 0.45\n",
      "episode : 25470, reward mean : 0.451855129321425, total_step : 668151, cur_epsilon : 0.45\n",
      "episode : 25480, reward mean : 0.45186068068471924, total_step : 668295, cur_epsilon : 0.45\n",
      "episode : 25490, reward mean : 0.4520105980338858, total_step : 668471, cur_epsilon : 0.45\n",
      "episode : 25500, reward mean : 0.45198431936987477, total_step : 668696, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25500, reward mean : 0.7405000035651028, step mean : 16.95, eval_episode_rewards : [ 0.88  0.94  0.9   0.83  0.94  0.54 -1.02  0.78  0.92  0.86  0.89  0.75\n",
      "  0.96  0.92  0.87  0.66  0.89  0.64  0.88  0.78]\n",
      "episode : 25510, reward mean : 0.4519905975680639, total_step : 668838, cur_epsilon : 0.45\n",
      "episode : 25520, reward mean : 0.4520822940433696, total_step : 668962, cur_epsilon : 0.45\n",
      "episode : 25530, reward mean : 0.4521551172746965, total_step : 669134, cur_epsilon : 0.45\n",
      "episode : 25540, reward mean : 0.4522967949907821, total_step : 669330, cur_epsilon : 0.45\n",
      "episode : 25550, reward mean : 0.4523405144468621, total_step : 669576, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25550, reward mean : 0.7135000041685998, step mean : 19.65, eval_episode_rewards : [ 0.77  0.81  0.51  0.85  0.97  0.76  0.94  0.75  0.85  0.67  0.81  0.88\n",
      "  0.8   0.59 -1.1   0.9   0.87  0.83  0.88  0.93]\n",
      "episode : 25560, reward mean : 0.45238067856698305, total_step : 669831, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 25570, reward mean : 0.4525299235122875, total_step : 670007, cur_epsilon : 0.45\n",
      "episode : 25580, reward mean : 0.4525031330827784, total_step : 670233, cur_epsilon : 0.45\n",
      "episode : 25590, reward mean : 0.4525287277968153, total_step : 670525, cur_epsilon : 0.45\n",
      "episode : 25600, reward mean : 0.4526652400142484, total_step : 670733, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25600, reward mean : 0.6850000048056245, step mean : 22.5, eval_episode_rewards : [ 0.94  0.83  0.99  0.46  0.9   0.66  0.97  0.79 -1.46  0.73  0.94  0.85\n",
      "  0.89  0.9   0.87  0.12  0.88  0.82  0.74  0.88]\n",
      "episode : 25610, reward mean : 0.4528199978289149, total_step : 670894, cur_epsilon : 0.45\n",
      "episode : 25620, reward mean : 0.4527790844830147, total_step : 671156, cur_epsilon : 0.45\n",
      "episode : 25630, reward mean : 0.4528002397393807, total_step : 671459, cur_epsilon : 0.45\n",
      "episode : 25640, reward mean : 0.45270008364141673, total_step : 671673, cur_epsilon : 0.45\n",
      "episode : 25650, reward mean : 0.4526760290297326, total_step : 671892, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25650, reward mean : 0.6030000066384673, step mean : 30.7, eval_episode_rewards : [ 0.73  0.72  0.55  0.84  0.84  0.25  0.53  0.95  0.85  0.77  0.79 -1.52\n",
      "  0.25  0.89  0.83  0.57  0.85  0.88  0.72  0.77]\n",
      "episode : 25660, reward mean : 0.4527108396208226, total_step : 672160, cur_epsilon : 0.45\n",
      "episode : 25670, reward mean : 0.4528550893926712, total_step : 672347, cur_epsilon : 0.45\n",
      "episode : 25680, reward mean : 0.45293458507561657, total_step : 672500, cur_epsilon : 0.45\n",
      "episode : 25690, reward mean : 0.45291670474055945, total_step : 672703, cur_epsilon : 0.45\n",
      "episode : 25700, reward mean : 0.4530023402651112, total_step : 672840, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25700, reward mean : 0.8265000038780272, step mean : 18.35, eval_episode_rewards : [0.94 0.93 0.95 0.71 0.94 0.98 0.92 0.57 0.65 0.7  0.9  0.87 0.43 0.98\n",
      " 0.91 0.93 0.85 0.72 0.89 0.76]\n",
      "episode : 25710, reward mean : 0.45301711959695534, total_step : 672959, cur_epsilon : 0.45\n",
      "episode : 25720, reward mean : 0.4530003944354662, total_step : 673159, cur_epsilon : 0.45\n",
      "episode : 25730, reward mean : 0.45314186338597334, total_step : 673352, cur_epsilon : 0.45\n",
      "episode : 25740, reward mean : 0.4532031913350171, total_step : 673551, cur_epsilon : 0.45\n",
      "episode : 25750, reward mean : 0.4532757337872409, total_step : 673820, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25750, reward mean : 0.6150000041350723, step mean : 19.5, eval_episode_rewards : [ 0.93  0.59 -1.14  0.82  0.87  0.5   0.9   0.83  0.87  0.65  0.96 -1.11\n",
      "  0.79  0.97  0.96  0.75  0.81  0.77  0.64  0.94]\n",
      "episode : 25760, reward mean : 0.4533536546993046, total_step : 673976, cur_epsilon : 0.45\n",
      "episode : 25770, reward mean : 0.45348273748940315, total_step : 674200, cur_epsilon : 0.45\n",
      "episode : 25780, reward mean : 0.4533103237061525, total_step : 674401, cur_epsilon : 0.45\n",
      "episode : 25790, reward mean : 0.45339434452011124, total_step : 674541, cur_epsilon : 0.45\n",
      "episode : 25800, reward mean : 0.4533600831483022, total_step : 674786, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25800, reward mean : 0.6775000027380884, step mean : 13.25, eval_episode_rewards : [ 0.88  0.91  0.83  0.82 -1.04  0.7   0.8   0.91  0.99  0.84  0.87 -1.02\n",
      "  0.98  0.8   0.87  0.93  0.95  0.73  0.86  0.94]\n",
      "synced target net\n",
      "episode : 25810, reward mean : 0.4534006255435989, total_step : 675038, cur_epsilon : 0.45\n",
      "episode : 25820, reward mean : 0.4535383479985662, total_step : 675239, cur_epsilon : 0.45\n",
      "episode : 25830, reward mean : 0.4535381395801717, total_step : 675396, cur_epsilon : 0.45\n",
      "episode : 25840, reward mean : 0.45361378271631525, total_step : 675557, cur_epsilon : 0.45\n",
      "episode : 25850, reward mean : 0.45365648531691827, total_step : 675803, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25850, reward mean : 0.4935000057332218, step mean : 26.6, eval_episode_rewards : [ 0.92  0.67  0.8   0.74 -1.26  0.72  0.5   0.85 -1.    0.48  0.89  0.83\n",
      "  0.98 -1.13  0.9   0.7   0.64  0.83  0.92  0.89]\n",
      "episode : 25860, reward mean : 0.4537188764688201, total_step : 675998, cur_epsilon : 0.45\n",
      "episode : 25870, reward mean : 0.45387978915781774, total_step : 676138, cur_epsilon : 0.45\n",
      "episode : 25880, reward mean : 0.45396754812748624, total_step : 676267, cur_epsilon : 0.45\n",
      "episode : 25890, reward mean : 0.45403245058243064, total_step : 676455, cur_epsilon : 0.45\n",
      "episode : 25900, reward mean : 0.4540339824560181, total_step : 676607, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25900, reward mean : 0.8350000036880374, step mean : 17.5, eval_episode_rewards : [0.7  0.79 0.96 0.94 0.82 0.54 0.89 0.79 0.88 0.85 0.7  0.91 0.91 0.87\n",
      " 0.75 0.9  0.92 0.97 0.75 0.86]\n",
      "episode : 25910, reward mean : 0.45416982422440105, total_step : 676811, cur_epsilon : 0.45\n",
      "episode : 25920, reward mean : 0.4542392031516719, total_step : 676987, cur_epsilon : 0.45\n",
      "episode : 25930, reward mean : 0.45421404341451976, total_step : 677208, cur_epsilon : 0.45\n",
      "episode : 25940, reward mean : 0.4543261428602735, total_step : 677473, cur_epsilon : 0.45\n",
      "episode : 25950, reward mean : 0.45439306920362, total_step : 677655, cur_epsilon : 0.45\n",
      "[EVAL] episode : 25950, reward mean : 0.6280000038444996, step mean : 18.2, eval_episode_rewards : [ 0.56  0.79  0.61  0.96 -1.09  0.75  0.95  0.92  0.82  0.95  0.77  0.9\n",
      "  0.68  0.94 -1.06  0.66  0.9   0.93  0.71  0.91]\n",
      "episode : 25960, reward mean : 0.4543963846635039, total_step : 677802, cur_epsilon : 0.45\n",
      "episode : 25970, reward mean : 0.45428340954632196, total_step : 678051, cur_epsilon : 0.45\n",
      "episode : 25980, reward mean : 0.45428714957461047, total_step : 678197, cur_epsilon : 0.45\n",
      "episode : 25990, reward mean : 0.45431127918474934, total_step : 678490, cur_epsilon : 0.45\n",
      "episode : 26000, reward mean : 0.4544092363875646, total_step : 678791, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26000, reward mean : 0.29300000462681053, step mean : 21.7, eval_episode_rewards : [-1.07  0.85  0.89  0.45  0.42 -1.23 -1.05  0.83 -1.05  0.78  0.92  0.6\n",
      "  0.84  0.86  0.85 -1.16  0.62  0.83  0.84  0.84]\n",
      "episode : 26010, reward mean : 0.4545767068858618, total_step : 678911, cur_epsilon : 0.45\n",
      "episode : 26020, reward mean : 0.4547336720265906, total_step : 679058, cur_epsilon : 0.45\n",
      "episode : 26030, reward mean : 0.4548920532524631, total_step : 679201, cur_epsilon : 0.45\n",
      "episode : 26040, reward mean : 0.4548406354139506, total_step : 679290, cur_epsilon : 0.45\n",
      "episode : 26050, reward mean : 0.45494933382870767, total_step : 679562, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26050, reward mean : 0.7700000051409006, step mean : 24.0, eval_episode_rewards : [0.87 0.97 0.94 0.7  0.4  0.95 0.83 0.79 0.55 0.97 0.8  0.87 0.79 0.83\n",
      " 0.61 0.91 0.52 0.81 0.53 0.76]\n",
      "episode : 26060, reward mean : 0.45495971397962065, total_step : 679890, cur_epsilon : 0.45\n",
      "episode : 26070, reward mean : 0.45505447435100954, total_step : 679998, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 26080, reward mean : 0.4550494688040965, total_step : 680366, cur_epsilon : 0.45\n",
      "episode : 26090, reward mean : 0.4551759350884973, total_step : 680591, cur_epsilon : 0.45\n",
      "episode : 26100, reward mean : 0.4551816148079675, total_step : 680731, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26100, reward mean : 0.6475000034086407, step mean : 16.25, eval_episode_rewards : [ 0.92  0.97  0.94  0.72  0.98  0.62  0.84  0.85 -1.04  0.92  0.68 -1.08\n",
      "  0.54  0.97  0.76  0.93  0.66  0.88  0.96  0.93]\n",
      "episode : 26110, reward mean : 0.45509958431725356, total_step : 680900, cur_epsilon : 0.45\n",
      "episode : 26120, reward mean : 0.4550911235283621, total_step : 681077, cur_epsilon : 0.45\n",
      "episode : 26130, reward mean : 0.4550099558598646, total_step : 681343, cur_epsilon : 0.45\n",
      "episode : 26140, reward mean : 0.4549051318541397, total_step : 681572, cur_epsilon : 0.45\n",
      "episode : 26150, reward mean : 0.4550439826656938, total_step : 681764, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26150, reward mean : 0.7695000051520765, step mean : 24.05, eval_episode_rewards : [0.98 0.94 0.7  0.42 0.83 0.95 0.8  0.9  0.75 0.86 0.95 0.65 0.79 0.57\n",
      " 0.67 0.54 0.66 0.93 0.89 0.61]\n",
      "episode : 26160, reward mean : 0.4549721004113139, total_step : 682006, cur_epsilon : 0.45\n",
      "episode : 26170, reward mean : 0.4548758176081011, total_step : 682213, cur_epsilon : 0.45\n",
      "episode : 26180, reward mean : 0.454861732117826, total_step : 682405, cur_epsilon : 0.45\n",
      "episode : 26190, reward mean : 0.4548323843793168, total_step : 682637, cur_epsilon : 0.45\n",
      "episode : 26200, reward mean : 0.4548969521730399, total_step : 682823, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26200, reward mean : 0.5760000050067902, step mean : 23.4, eval_episode_rewards : [ 0.9   0.89  0.75 -1.18  0.85  0.97  0.71  0.98  0.5   0.83  0.7   0.88\n",
      "  0.86  0.96  0.57  0.33  0.86  0.71  0.79 -1.34]\n",
      "episode : 26210, reward mean : 0.4549904672644296, total_step : 683133, cur_epsilon : 0.45\n",
      "episode : 26220, reward mean : 0.4550728507639029, total_step : 683272, cur_epsilon : 0.45\n",
      "episode : 26230, reward mean : 0.4549207070934701, total_step : 683426, cur_epsilon : 0.45\n",
      "episode : 26240, reward mean : 0.45504764280141413, total_step : 683648, cur_epsilon : 0.45\n",
      "episode : 26250, reward mean : 0.4551428627478225, total_step : 683753, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26250, reward mean : 0.8550000032410026, step mean : 15.5, eval_episode_rewards : [0.85 0.74 0.96 0.99 0.91 0.73 0.94 0.83 0.77 0.61 0.94 0.91 0.75 0.97\n",
      " 0.66 0.94 0.96 0.88 0.79 0.97]\n",
      "episode : 26260, reward mean : 0.4551866011874238, total_step : 683993, cur_epsilon : 0.45\n",
      "episode : 26270, reward mean : 0.4552124151977032, total_step : 684280, cur_epsilon : 0.45\n",
      "episode : 26280, reward mean : 0.45524696146500904, total_step : 684544, cur_epsilon : 0.45\n",
      "episode : 26290, reward mean : 0.455366304577865, total_step : 684785, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 26300, reward mean : 0.45533688773390457, total_step : 685017, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26300, reward mean : 0.5615000053308904, step mean : 24.85, eval_episode_rewards : [ 0.9   0.94  0.76  0.6   0.6   0.91  0.56  0.89  0.81  0.65 -1.01  0.97\n",
      "  0.5   0.98  0.87  0.76  0.68 -1.11  0.75  0.22]\n",
      "episode : 26310, reward mean : 0.4554500246087761, total_step : 685274, cur_epsilon : 0.45\n",
      "episode : 26320, reward mean : 0.45559005119666557, total_step : 685460, cur_epsilon : 0.45\n",
      "episode : 26330, reward mean : 0.4556479357220473, total_step : 685761, cur_epsilon : 0.45\n",
      "episode : 26340, reward mean : 0.4557623442526887, total_step : 686014, cur_epsilon : 0.45\n",
      "episode : 26350, reward mean : 0.45568122002471106, total_step : 686182, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26350, reward mean : 0.6575000031851232, step mean : 15.25, eval_episode_rewards : [ 0.81  0.83  0.96 -1.04  0.88  0.98  0.99  0.96  0.98  0.29 -1.21  0.87\n",
      "  0.92  0.83  0.94  0.9   0.8   0.94  0.81  0.71]\n",
      "episode : 26360, reward mean : 0.4557727673624491, total_step : 686295, cur_epsilon : 0.45\n",
      "episode : 26370, reward mean : 0.4559260579334642, total_step : 686445, cur_epsilon : 0.45\n",
      "episode : 26380, reward mean : 0.45600720802632705, total_step : 686585, cur_epsilon : 0.45\n",
      "episode : 26390, reward mean : 0.4559215667969877, total_step : 686765, cur_epsilon : 0.45\n",
      "episode : 26400, reward mean : 0.45599735408351105, total_step : 686919, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26400, reward mean : 0.6505000055767596, step mean : 25.95, eval_episode_rewards : [ 0.72  0.83  0.79  0.85  0.58  0.86  0.62  0.65  0.97 -1.69  0.89  0.51\n",
      "  0.88  0.97  0.98  0.59  0.83  0.97  0.24  0.97]\n",
      "episode : 26410, reward mean : 0.456013258153256, total_step : 687031, cur_epsilon : 0.45\n",
      "episode : 26420, reward mean : 0.4561256679743641, total_step : 687288, cur_epsilon : 0.45\n",
      "episode : 26430, reward mean : 0.45611086446930016, total_step : 687481, cur_epsilon : 0.45\n",
      "episode : 26440, reward mean : 0.45610855325102617, total_step : 687641, cur_epsilon : 0.45\n",
      "episode : 26450, reward mean : 0.4561943345190165, total_step : 687968, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26450, reward mean : 0.4480000056326389, step mean : 26.2, eval_episode_rewards : [ 0.99  0.93  0.8  -1.2   0.6  -1.37 -1.05  0.72  0.93  0.81  0.95  0.89\n",
      "  0.11  0.85  0.95  0.73  0.42  0.24  0.72  0.94]\n",
      "episode : 26460, reward mean : 0.45627929509067605, total_step : 688297, cur_epsilon : 0.45\n",
      "episode : 26470, reward mean : 0.45639592550630204, total_step : 688542, cur_epsilon : 0.45\n",
      "episode : 26480, reward mean : 0.4563145826354741, total_step : 688711, cur_epsilon : 0.45\n",
      "episode : 26490, reward mean : 0.45644507920844135, total_step : 688919, cur_epsilon : 0.45\n",
      "episode : 26500, reward mean : 0.456595854651464, total_step : 689073, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26500, reward mean : 0.6940000046044588, step mean : 21.6, eval_episode_rewards : [ 0.94  0.92  0.87  0.59  0.86  0.78  0.84  0.95  0.72  0.93  0.71  0.63\n",
      "  0.84  0.84 -1.27  0.91  0.88  0.25  0.83  0.86]\n",
      "episode : 26510, reward mean : 0.45658242732190896, total_step : 689262, cur_epsilon : 0.45\n",
      "episode : 26520, reward mean : 0.4564879392287771, total_step : 689466, cur_epsilon : 0.45\n",
      "episode : 26530, reward mean : 0.456574449619953, total_step : 689790, cur_epsilon : 0.45\n",
      "episode : 26540, reward mean : 0.45672231154676357, total_step : 689951, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 26550, reward mean : 0.45678004325776167, total_step : 690151, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26550, reward mean : 0.6260000061243772, step mean : 28.4, eval_episode_rewards : [ 0.82  0.76  0.97  0.5   0.87  0.11  0.31  0.72  0.93  0.83  0.96  0.59\n",
      "  0.92  0.83 -1.09  0.35  0.66  0.98  0.8   0.7 ]\n",
      "episode : 26560, reward mean : 0.45675075860478054, total_step : 690382, cur_epsilon : 0.45\n",
      "episode : 26570, reward mean : 0.45686639625877756, total_step : 690628, cur_epsilon : 0.45\n",
      "episode : 26580, reward mean : 0.4569179890384411, total_step : 690844, cur_epsilon : 0.45\n",
      "episode : 26590, reward mean : 0.45692817407634617, total_step : 691069, cur_epsilon : 0.45\n",
      "episode : 26600, reward mean : 0.4569947424333515, total_step : 691245, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26600, reward mean : 0.864500003028661, step mean : 14.55, eval_episode_rewards : [0.76 0.91 0.97 0.97 0.66 0.83 0.97 0.89 0.87 0.78 0.72 0.96 0.93 0.96\n",
      " 0.94 0.84 0.88 0.99 0.59 0.87]\n",
      "episode : 26610, reward mean : 0.45705825436926123, total_step : 691429, cur_epsilon : 0.45\n",
      "episode : 26620, reward mean : 0.4568215683247449, total_step : 691612, cur_epsilon : 0.45\n",
      "episode : 26630, reward mean : 0.4568753341663317, total_step : 691822, cur_epsilon : 0.45\n",
      "episode : 26640, reward mean : 0.45694369928332396, total_step : 692092, cur_epsilon : 0.45\n",
      "episode : 26650, reward mean : 0.45709306375009984, total_step : 692247, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26650, reward mean : 0.6275000049732625, step mean : 23.2, eval_episode_rewards : [ 0.8  -1.01  0.95  0.83  0.98  0.73  0.93  0.95  0.52 -1.    0.88  0.95\n",
      "  0.86  0.88  0.97  0.76  0.72  0.27  0.68  0.9 ]\n",
      "episode : 26660, reward mean : 0.4570296379965321, total_step : 692468, cur_epsilon : 0.45\n",
      "episode : 26670, reward mean : 0.45716610982480527, total_step : 692657, cur_epsilon : 0.45\n",
      "episode : 26680, reward mean : 0.4572455078373891, total_step : 692998, cur_epsilon : 0.45\n",
      "episode : 26690, reward mean : 0.45721057134336046, total_step : 693244, cur_epsilon : 0.45\n",
      "episode : 26700, reward mean : 0.4572071216924444, total_step : 693406, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26700, reward mean : 0.6845000048168004, step mean : 22.55, eval_episode_rewards : [ 0.96  0.84  0.55  0.61  0.94  0.78  0.94 -1.36  0.95  0.51  0.92  0.91\n",
      "  0.89  0.39  0.83  0.87  0.68  0.85  0.83  0.8 ]\n",
      "episode : 26710, reward mean : 0.457200679491727, total_step : 693576, cur_epsilon : 0.45\n",
      "episode : 26720, reward mean : 0.45730951157486355, total_step : 693838, cur_epsilon : 0.45\n",
      "episode : 26730, reward mean : 0.45730004299730287, total_step : 694016, cur_epsilon : 0.45\n",
      "episode : 26740, reward mean : 0.45745625091046543, total_step : 694151, cur_epsilon : 0.45\n",
      "episode : 26750, reward mean : 0.4573424354916282, total_step : 694408, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26750, reward mean : 0.5850000048056245, step mean : 22.5, eval_episode_rewards : [-1.4   0.66 -1.2   0.96  0.97  0.62  0.87  0.86  0.78  0.92  0.95  0.43\n",
      "  0.77  0.87  0.85  0.46  0.78  0.9   0.78  0.87]\n",
      "episode : 26760, reward mean : 0.4574861789774428, total_step : 694576, cur_epsilon : 0.45\n",
      "episode : 26770, reward mean : 0.4576372861213005, total_step : 694724, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 26780, reward mean : 0.4576882057332192, total_step : 695039, cur_epsilon : 0.45\n",
      "episode : 26790, reward mean : 0.4577439398872247, total_step : 695242, cur_epsilon : 0.45\n",
      "episode : 26800, reward mean : 0.45780970707521845, total_step : 695418, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26800, reward mean : 0.652000005543232, step mean : 25.8, eval_episode_rewards : [ 0.82  0.29  0.96  0.82  0.96  0.73  0.8   0.87  0.55  0.92  0.62  0.73\n",
      "  0.81  0.87  0.85  0.86  0.92  0.13 -1.19  0.72]\n",
      "episode : 26810, reward mean : 0.4578735602258141, total_step : 695599, cur_epsilon : 0.45\n",
      "episode : 26820, reward mean : 0.4579485514424158, total_step : 695750, cur_epsilon : 0.45\n",
      "episode : 26830, reward mean : 0.4579973965610419, total_step : 695971, cur_epsilon : 0.45\n",
      "episode : 26840, reward mean : 0.4580525391123739, total_step : 696175, cur_epsilon : 0.45\n",
      "episode : 26850, reward mean : 0.45816015455615144, total_step : 696438, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26850, reward mean : 0.7340000037103891, step mean : 17.6, eval_episode_rewards : [ 0.85  0.93  0.83  0.84  0.38 -1.03  0.97  0.64  0.89  0.8   0.81  0.98\n",
      "  0.97  0.97  0.88  0.9   0.89  0.85  0.5   0.83]\n",
      "episode : 26860, reward mean : 0.4581373845822055, total_step : 696651, cur_epsilon : 0.45\n",
      "episode : 26870, reward mean : 0.45812021399032604, total_step : 696849, cur_epsilon : 0.45\n",
      "episode : 26880, reward mean : 0.4581659281982519, total_step : 697078, cur_epsilon : 0.45\n",
      "episode : 26890, reward mean : 0.45825437523214047, total_step : 697192, cur_epsilon : 0.45\n",
      "episode : 26900, reward mean : 0.45833829554137134, total_step : 697518, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26900, reward mean : 0.7160000041127205, step mean : 19.4, eval_episode_rewards : [ 0.92  0.94  0.89  0.34  0.85  0.74  0.73 -1.17  0.86  0.73  0.61  0.99\n",
      "  0.79  0.87  0.74  0.87  0.82  0.98  0.88  0.94]\n",
      "episode : 26910, reward mean : 0.45824415273528335, total_step : 697723, cur_epsilon : 0.45\n",
      "episode : 26920, reward mean : 0.45824443351181243, total_step : 697874, cur_epsilon : 0.45\n",
      "episode : 26930, reward mean : 0.458317495364664, total_step : 698029, cur_epsilon : 0.45\n",
      "episode : 26940, reward mean : 0.4583778823389165, total_step : 698218, cur_epsilon : 0.45\n",
      "episode : 26950, reward mean : 0.45842449537137997, total_step : 698444, cur_epsilon : 0.45\n",
      "[EVAL] episode : 26950, reward mean : 0.6770000049844385, step mean : 23.3, eval_episode_rewards : [ 0.69  0.96  0.94 -1.08  0.68  0.6   0.89  0.67  0.57  0.66  0.91  0.88\n",
      "  0.67  0.84  0.77  0.85  0.86  0.68  0.61  0.89]\n",
      "episode : 26960, reward mean : 0.4585689966725766, total_step : 698606, cur_epsilon : 0.45\n",
      "episode : 26970, reward mean : 0.4586536948587421, total_step : 698828, cur_epsilon : 0.45\n",
      "episode : 26980, reward mean : 0.458719427367566, total_step : 699002, cur_epsilon : 0.45\n",
      "episode : 26990, reward mean : 0.4586387606673757, total_step : 699171, cur_epsilon : 0.45\n",
      "episode : 27000, reward mean : 0.4585963018692478, total_step : 699437, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27000, reward mean : 0.694500004593283, step mean : 21.55, eval_episode_rewards : [ 0.69  0.95  0.94  0.77  0.77  0.06  0.99  0.98  0.04  0.87  0.94  0.72\n",
      "  0.96  0.9   0.98  0.83  0.95 -1.03  0.73  0.85]\n",
      "episode : 27010, reward mean : 0.4585716457058934, total_step : 699655, cur_epsilon : 0.45\n",
      "episode : 27020, reward mean : 0.45855440971718, total_step : 699853, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 27030, reward mean : 0.4586196874064034, total_step : 700028, cur_epsilon : 0.45\n",
      "episode : 27040, reward mean : 0.4586560706601109, total_step : 700281, cur_epsilon : 0.45\n",
      "episode : 27050, reward mean : 0.4585848484539501, total_step : 700425, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27050, reward mean : 0.791000004671514, step mean : 21.9, eval_episode_rewards : [0.92 0.79 0.84 0.95 0.95 0.77 0.39 0.64 0.9  0.96 0.9  0.68 0.52 0.63\n",
      " 0.56 0.95 0.77 0.87 0.99 0.84]\n",
      "episode : 27060, reward mean : 0.4586692590799929, total_step : 700548, cur_epsilon : 0.45\n",
      "episode : 27070, reward mean : 0.4587262708070338, total_step : 700745, cur_epsilon : 0.45\n",
      "episode : 27080, reward mean : 0.45874889773953614, total_step : 700934, cur_epsilon : 0.45\n",
      "episode : 27090, reward mean : 0.45876892398858576, total_step : 701231, cur_epsilon : 0.45\n",
      "episode : 27100, reward mean : 0.4588830313986215, total_step : 701473, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27100, reward mean : 0.7155000041238964, step mean : 19.45, eval_episode_rewards : [ 0.98  0.94  0.63  0.92  0.89 -1.05  0.56  0.92  0.61  0.5   0.85  0.84\n",
      "  0.57  0.9   0.99  0.63  0.92  0.97  0.83  0.91]\n",
      "episode : 27110, reward mean : 0.4589760291762169, total_step : 701772, cur_epsilon : 0.45\n",
      "episode : 27120, reward mean : 0.45883960733911333, total_step : 702093, cur_epsilon : 0.45\n",
      "episode : 27130, reward mean : 0.45890085333858716, total_step : 702278, cur_epsilon : 0.45\n",
      "episode : 27140, reward mean : 0.45897200262007926, total_step : 702436, cur_epsilon : 0.45\n",
      "episode : 27150, reward mean : 0.45904567775839655, total_step : 702587, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27150, reward mean : 0.5600000053644181, step mean : 25.0, eval_episode_rewards : [ 0.89  0.6   0.7   0.59  0.84  0.97  0.94  0.76  0.73 -1.47  0.91 -1.41\n",
      "  0.25  0.95  0.99  0.98  0.76  0.74  0.8   0.68]\n",
      "episode : 27160, reward mean : 0.45912003502104326, total_step : 702736, cur_epsilon : 0.45\n",
      "episode : 27170, reward mean : 0.4591910250719402, total_step : 702894, cur_epsilon : 0.45\n",
      "episode : 27180, reward mean : 0.45910228665443886, total_step : 703185, cur_epsilon : 0.45\n",
      "episode : 27190, reward mean : 0.45919456238645695, total_step : 703285, cur_epsilon : 0.45\n",
      "episode : 27200, reward mean : 0.4592772114453563, total_step : 703411, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27200, reward mean : 0.5770000049844384, step mean : 23.3, eval_episode_rewards : [ 0.97  0.64  0.99  0.77  0.7   0.9   0.61  0.59 -1.1   0.45  0.93  0.88\n",
      "  0.32 -1.2   0.92  0.99  0.87  0.98  0.62  0.71]\n",
      "episode : 27210, reward mean : 0.45939030324754765, total_step : 703654, cur_epsilon : 0.45\n",
      "episode : 27220, reward mean : 0.45939309887648244, total_step : 703896, cur_epsilon : 0.45\n",
      "episode : 27230, reward mean : 0.4592941664144249, total_step : 704116, cur_epsilon : 0.45\n",
      "episode : 27240, reward mean : 0.4594115327281273, total_step : 704347, cur_epsilon : 0.45\n",
      "episode : 27250, reward mean : 0.4594128495979747, total_step : 704494, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27250, reward mean : 0.5510000055655837, step mean : 25.9, eval_episode_rewards : [ 0.59 -1.07  0.78  0.44  0.68  0.84  0.89  0.74  0.13  0.55  0.96  0.66\n",
      "  0.93  0.89 -1.03  0.81  0.45  0.87  0.94  0.97]\n",
      "episode : 27260, reward mean : 0.4594038940418815, total_step : 704669, cur_epsilon : 0.45\n",
      "episode : 27270, reward mean : 0.45937844340405676, total_step : 704889, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 27280, reward mean : 0.4595058706624548, total_step : 705092, cur_epsilon : 0.45\n",
      "episode : 27290, reward mean : 0.4596009582900174, total_step : 705383, cur_epsilon : 0.45\n",
      "episode : 27300, reward mean : 0.4595395659984877, total_step : 705501, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27300, reward mean : 0.6205000040121377, step mean : 18.95, eval_episode_rewards : [ 0.92  0.87 -1.02  0.74  0.95  0.68  0.95  0.51  0.77  0.74  0.71  0.7\n",
      "  0.97  0.94  0.7   0.74  0.8   0.91  0.85 -1.02]\n",
      "episode : 27310, reward mean : 0.4596151648403027, total_step : 705645, cur_epsilon : 0.45\n",
      "episode : 27320, reward mean : 0.4596233584119708, total_step : 705773, cur_epsilon : 0.45\n",
      "episode : 27330, reward mean : 0.45950677467518963, total_step : 706042, cur_epsilon : 0.45\n",
      "episode : 27340, reward mean : 0.4594883010942072, total_step : 706243, cur_epsilon : 0.45\n",
      "episode : 27350, reward mean : 0.4595780677124994, total_step : 706348, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27350, reward mean : 0.41700000409036875, step mean : 19.3, eval_episode_rewards : [ 0.88  0.82 -1.16  0.89  0.65  0.96  0.92  0.94 -1.01 -1.21  0.62  0.66\n",
      "  0.75  0.82  0.73  0.76  0.66 -1.16  0.93  0.89]\n",
      "episode : 27360, reward mean : 0.459696642981913, total_step : 706574, cur_epsilon : 0.45\n",
      "episode : 27370, reward mean : 0.4597555773483796, total_step : 706763, cur_epsilon : 0.45\n",
      "episode : 27380, reward mean : 0.45967860307008396, total_step : 706924, cur_epsilon : 0.45\n",
      "episode : 27390, reward mean : 0.45983242614404884, total_step : 707053, cur_epsilon : 0.45\n",
      "episode : 27400, reward mean : 0.4598193486176948, total_step : 707239, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27400, reward mean : 0.39450000459328294, step mean : 21.55, eval_episode_rewards : [ 0.2  -1.19  0.91  0.98  0.86  0.83 -1.28  0.56  0.86  0.93  0.88  0.94\n",
      "  0.77 -1.34  0.83  0.96  0.92 -1.15  0.44  0.98]\n",
      "episode : 27410, reward mean : 0.4597110599116312, total_step : 707486, cur_epsilon : 0.45\n",
      "episode : 27420, reward mean : 0.45976623458132954, total_step : 707685, cur_epsilon : 0.45\n",
      "episode : 27430, reward mean : 0.45974517507343465, total_step : 707893, cur_epsilon : 0.45\n",
      "episode : 27440, reward mean : 0.4598247140064244, total_step : 708225, cur_epsilon : 0.45\n",
      "episode : 27450, reward mean : 0.45994244635281556, total_step : 708452, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27450, reward mean : 0.7095000042580069, step mean : 20.05, eval_episode_rewards : [ 0.88  0.94  0.87  0.81  0.47  0.83  0.95 -1.32  0.69  0.83  0.72  0.83\n",
      "  0.29  0.95  0.94  0.94  0.88  0.85  0.96  0.88]\n",
      "episode : 27460, reward mean : 0.4600109305323657, total_step : 708614, cur_epsilon : 0.45\n",
      "episode : 27470, reward mean : 0.46005606670793747, total_step : 708840, cur_epsilon : 0.45\n",
      "episode : 27480, reward mean : 0.4601131787666647, total_step : 709033, cur_epsilon : 0.45\n",
      "episode : 27490, reward mean : 0.46014987823067904, total_step : 709282, cur_epsilon : 0.45\n",
      "episode : 27500, reward mean : 0.4602607328223234, total_step : 709527, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27500, reward mean : 0.6020000044256448, step mean : 20.8, eval_episode_rewards : [ 0.95  0.85  0.89  0.79 -1.06  0.66  0.83 -1.03  0.59  0.78  0.86  0.66\n",
      "  0.44  0.79  0.91  0.98  0.9   0.87  0.73  0.65]\n",
      "episode : 27510, reward mean : 0.46015885687615654, total_step : 709757, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 27520, reward mean : 0.46019368287491297, total_step : 710011, cur_epsilon : 0.45\n",
      "episode : 27530, reward mean : 0.4602513676991668, total_step : 710202, cur_epsilon : 0.45\n",
      "episode : 27540, reward mean : 0.4602977542776125, total_step : 710424, cur_epsilon : 0.45\n",
      "episode : 27550, reward mean : 0.4603626189779105, total_step : 710595, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27550, reward mean : 0.5345000059343874, step mean : 27.55, eval_episode_rewards : [ 0.77  0.99  0.86  0.63 -1.04  0.59  0.86  0.97 -1.12  0.63  0.75  0.46\n",
      "  0.82  0.85  0.89  0.51  0.55  0.49  0.42  0.81]\n",
      "episode : 27560, reward mean : 0.46047678348663734, total_step : 710830, cur_epsilon : 0.45\n",
      "episode : 27570, reward mean : 0.4605426243354023, total_step : 710998, cur_epsilon : 0.45\n",
      "episode : 27580, reward mean : 0.460665342747352, total_step : 711209, cur_epsilon : 0.45\n",
      "episode : 27590, reward mean : 0.4607046086634143, total_step : 711450, cur_epsilon : 0.45\n",
      "episode : 27600, reward mean : 0.4608315272849944, total_step : 711649, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27600, reward mean : 0.7280000038444996, step mean : 18.2, eval_episode_rewards : [ 0.98  0.84  0.83  0.89  0.68  0.84  0.9   0.96  0.97  0.96  0.86  0.94\n",
      "  0.54  0.86  0.04 -1.07  0.95  0.67  0.93  0.99]\n",
      "episode : 27610, reward mean : 0.4608841055092071, total_step : 711853, cur_epsilon : 0.45\n",
      "episode : 27620, reward mean : 0.4610224530464989, total_step : 712020, cur_epsilon : 0.45\n",
      "episode : 27630, reward mean : 0.4610640663479684, total_step : 712254, cur_epsilon : 0.45\n",
      "episode : 27640, reward mean : 0.46119790713575043, total_step : 712433, cur_epsilon : 0.45\n",
      "episode : 27650, reward mean : 0.46132803447638887, total_step : 712622, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27650, reward mean : 0.6425000057555735, step mean : 26.75, eval_episode_rewards : [ 0.91  0.76  0.97  0.16  0.84  0.55 -1.04  0.74  0.85  0.92  0.31  0.88\n",
      "  0.93  0.88  0.58  0.91  0.81  0.62  0.54  0.73]\n",
      "episode : 27660, reward mean : 0.4614027531923954, total_step : 712764, cur_epsilon : 0.45\n",
      "episode : 27670, reward mean : 0.46150163185258, total_step : 713039, cur_epsilon : 0.45\n",
      "episode : 27680, reward mean : 0.4616221153687395, total_step : 713254, cur_epsilon : 0.45\n",
      "episode : 27690, reward mean : 0.46158397087336944, total_step : 713607, cur_epsilon : 0.45\n",
      "episode : 27700, reward mean : 0.46164801998265653, total_step : 713778, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27700, reward mean : 0.4680000029504299, step mean : 14.2, eval_episode_rewards : [ 0.86  0.9   0.84  0.96  0.97 -1.06 -1.03  0.68  0.85  0.72  0.9  -1.07\n",
      "  0.54  0.92  0.99  0.74 -1.08  0.86  0.94  0.93]\n",
      "episode : 27710, reward mean : 0.4617675984686342, total_step : 713995, cur_epsilon : 0.45\n",
      "episode : 27720, reward mean : 0.46187338216524587, total_step : 714250, cur_epsilon : 0.45\n",
      "episode : 27730, reward mean : 0.4618748703083508, total_step : 714394, cur_epsilon : 0.45\n",
      "episode : 27740, reward mean : 0.46188320669344884, total_step : 714519, cur_epsilon : 0.45\n",
      "episode : 27750, reward mean : 0.46192469022436455, total_step : 714752, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27750, reward mean : 0.6855000047944486, step mean : 22.45, eval_episode_rewards : [-1.02  0.86  0.55  0.39  0.98  0.88  0.65  0.91  0.76  0.6   0.95  0.79\n",
      "  0.92  0.76  0.54  0.86  0.86  0.72  0.93  0.82]\n",
      "episode : 27760, reward mean : 0.46190454444414547, total_step : 714956, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 27770, reward mean : 0.4617933076638346, total_step : 715213, cur_epsilon : 0.45\n",
      "episode : 27780, reward mean : 0.46185205737531154, total_step : 715497, cur_epsilon : 0.45\n",
      "episode : 27790, reward mean : 0.4618265618543674, total_step : 715716, cur_epsilon : 0.45\n",
      "episode : 27800, reward mean : 0.4619726674087185, total_step : 715858, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27800, reward mean : 0.6935000046156347, step mean : 21.65, eval_episode_rewards : [ 0.86  0.93  0.64 -1.03  0.75  0.96  0.8   0.75  0.74  0.58  0.8   0.88\n",
      "  0.85  0.81  0.99  0.99  0.91  0.87  0.17  0.62]\n",
      "episode : 27810, reward mean : 0.46204387464917895, total_step : 716008, cur_epsilon : 0.45\n",
      "episode : 27820, reward mean : 0.4621890781460682, total_step : 716152, cur_epsilon : 0.45\n",
      "episode : 27830, reward mean : 0.462167450739054, total_step : 716360, cur_epsilon : 0.45\n",
      "episode : 27840, reward mean : 0.4621397325472676, total_step : 716585, cur_epsilon : 0.45\n",
      "episode : 27850, reward mean : 0.46219497860528036, total_step : 716779, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27850, reward mean : 0.5730000061914324, step mean : 28.65, eval_episode_rewards : [ 0.75  0.98  0.84  0.95  0.95  0.74  0.55 -1.    0.82  0.92 -1.79  0.79\n",
      "  0.83  0.4   0.92  0.7   0.71  0.73  0.8   0.87]\n",
      "episode : 27860, reward mean : 0.4622541333164295, total_step : 716962, cur_epsilon : 0.45\n",
      "episode : 27870, reward mean : 0.4622411250173885, total_step : 717146, cur_epsilon : 0.45\n",
      "episode : 27880, reward mean : 0.46226614613681927, total_step : 717424, cur_epsilon : 0.45\n",
      "episode : 27890, reward mean : 0.4623004716510797, total_step : 717676, cur_epsilon : 0.45\n",
      "episode : 27900, reward mean : 0.4623487510535479, total_step : 717889, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27900, reward mean : 0.8750000027939677, step mean : 13.5, eval_episode_rewards : [0.94 0.96 0.88 0.91 0.94 0.85 0.74 0.81 0.96 0.87 0.86 0.84 0.8  0.95\n",
      " 0.97 0.82 0.83 0.88 0.87 0.82]\n",
      "episode : 27910, reward mean : 0.46242171818152233, total_step : 718132, cur_epsilon : 0.45\n",
      "episode : 27920, reward mean : 0.46240186799747235, total_step : 718335, cur_epsilon : 0.45\n",
      "episode : 27930, reward mean : 0.46251987663930966, total_step : 718553, cur_epsilon : 0.45\n",
      "episode : 27940, reward mean : 0.46253257532422043, total_step : 718665, cur_epsilon : 0.45\n",
      "episode : 27950, reward mean : 0.4626669107189599, total_step : 718837, cur_epsilon : 0.45\n",
      "[EVAL] episode : 27950, reward mean : 0.557000003196299, step mean : 15.3, eval_episode_rewards : [ 0.79 -1.13  0.96  0.95  0.88  0.84  0.99  0.84 -1.1   0.76  0.95  0.72\n",
      "  0.99  0.95  0.86  0.93  0.95 -1.51  0.85  0.67]\n",
      "episode : 27960, reward mean : 0.46274249480053975, total_step : 718973, cur_epsilon : 0.45\n",
      "episode : 27970, reward mean : 0.46285055969523126, total_step : 719218, cur_epsilon : 0.45\n",
      "episode : 27980, reward mean : 0.4628155880889258, total_step : 719463, cur_epsilon : 0.45\n",
      "episode : 27990, reward mean : 0.4629296232503348, total_step : 719691, cur_epsilon : 0.45\n",
      "episode : 28000, reward mean : 0.4630417912438113, total_step : 719924, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28000, reward mean : 0.6110000042244792, step mean : 19.9, eval_episode_rewards : [ 0.54  0.95  0.96  0.85  0.8   0.73  0.75 -1.03  0.73  0.97  0.83  0.83\n",
      "  0.72  0.82  0.81  0.49  0.95 -1.01  0.85  0.68]\n",
      "synced target net\n",
      "episode : 28010, reward mean : 0.4630203554041681, total_step : 720131, cur_epsilon : 0.45\n",
      "episode : 28020, reward mean : 0.46305924892654216, total_step : 720369, cur_epsilon : 0.45\n",
      "episode : 28030, reward mean : 0.46292865340588957, total_step : 720581, cur_epsilon : 0.45\n",
      "episode : 28040, reward mean : 0.46297432792488585, total_step : 720800, cur_epsilon : 0.45\n",
      "episode : 28050, reward mean : 0.4630809324444562, total_step : 721048, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28050, reward mean : 0.418000004068017, step mean : 19.2, eval_episode_rewards : [ 0.82 -1.03  0.71  0.78  0.96  0.99  0.93  0.69  0.76 -1.58  0.32  0.98\n",
      "  0.91  0.95  0.72 -1.01 -1.14  0.82  0.9   0.88]\n",
      "episode : 28060, reward mean : 0.46315289219875877, total_step : 721193, cur_epsilon : 0.45\n",
      "episode : 28070, reward mean : 0.4631991505216758, total_step : 721410, cur_epsilon : 0.45\n",
      "episode : 28080, reward mean : 0.4632514300279882, total_step : 721610, cur_epsilon : 0.45\n",
      "episode : 28090, reward mean : 0.46332930420834045, total_step : 721738, cur_epsilon : 0.45\n",
      "episode : 28100, reward mean : 0.46345160694859155, total_step : 721941, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28100, reward mean : 0.7890000047162176, step mean : 22.1, eval_episode_rewards : [0.77 0.94 0.81 0.77 0.84 0.25 0.73 0.89 0.85 0.97 0.76 0.5  0.77 0.96\n",
      " 0.83 0.99 0.71 0.9  0.96 0.58]\n",
      "episode : 28110, reward mean : 0.46340199769893675, total_step : 722227, cur_epsilon : 0.45\n",
      "episode : 28120, reward mean : 0.463373405240598, total_step : 722454, cur_epsilon : 0.45\n",
      "episode : 28130, reward mean : 0.46334483311106006, total_step : 722681, cur_epsilon : 0.45\n",
      "episode : 28140, reward mean : 0.4634712208761399, total_step : 722872, cur_epsilon : 0.45\n",
      "episode : 28150, reward mean : 0.4634458314565211, total_step : 723090, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28150, reward mean : 0.3965000045485795, step mean : 21.35, eval_episode_rewards : [ 0.71  0.94  0.69  0.75  0.89  0.94  0.86  0.84 -1.11 -1.69  0.93  0.9\n",
      "  0.84  0.99  0.96 -1.69  0.89  0.42  0.9  -1.03]\n",
      "episode : 28160, reward mean : 0.46358842881858103, total_step : 723235, cur_epsilon : 0.45\n",
      "episode : 28170, reward mean : 0.46353568177475846, total_step : 723530, cur_epsilon : 0.45\n",
      "episode : 28180, reward mean : 0.4635908500935203, total_step : 723721, cur_epsilon : 0.45\n",
      "episode : 28190, reward mean : 0.46373111584487114, total_step : 723872, cur_epsilon : 0.45\n",
      "episode : 28200, reward mean : 0.46371844523777883, total_step : 724054, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28200, reward mean : 0.43550000367686154, step mean : 17.45, eval_episode_rewards : [ 0.99  0.77 -1.12 -1.2   0.92  0.94  0.84  0.96  0.74  0.68 -1.04  0.75\n",
      "  0.87  0.56  0.94  0.81  0.81  0.89 -1.07  0.67]\n",
      "episode : 28210, reward mean : 0.4636923132134811, total_step : 724274, cur_epsilon : 0.45\n",
      "episode : 28220, reward mean : 0.4638199913462665, total_step : 724460, cur_epsilon : 0.45\n",
      "episode : 28230, reward mean : 0.4638749612409528, total_step : 724651, cur_epsilon : 0.45\n",
      "episode : 28240, reward mean : 0.46393555792737173, total_step : 724826, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 28250, reward mean : 0.46397416481131737, total_step : 725063, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28250, reward mean : 0.7395000035874546, step mean : 17.05, eval_episode_rewards : [ 0.95  0.95 -1.01  0.91  0.82  0.97  0.95  0.97  0.52  0.84  0.93  0.85\n",
      "  0.92  0.8   0.9   0.73  0.55  0.72  0.64  0.88]\n",
      "episode : 28260, reward mean : 0.46406122278626444, total_step : 725163, cur_epsilon : 0.45\n",
      "episode : 28270, reward mean : 0.46413300870072727, total_step : 725306, cur_epsilon : 0.45\n",
      "episode : 28280, reward mean : 0.46424151895405746, total_step : 725545, cur_epsilon : 0.45\n",
      "episode : 28290, reward mean : 0.4642661773093037, total_step : 725821, cur_epsilon : 0.45\n",
      "episode : 28300, reward mean : 0.4644113129366834, total_step : 725956, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28300, reward mean : 0.837500003632158, step mean : 17.25, eval_episode_rewards : [0.91 0.83 0.8  0.69 0.92 0.96 0.9  0.34 0.81 0.92 0.66 0.85 0.89 0.86\n",
      " 0.98 0.87 0.93 0.97 0.94 0.72]\n",
      "episode : 28310, reward mean : 0.46446697831677597, total_step : 726144, cur_epsilon : 0.45\n",
      "episode : 28320, reward mean : 0.4645674490185039, total_step : 726405, cur_epsilon : 0.45\n",
      "episode : 28330, reward mean : 0.46463961017413735, total_step : 726546, cur_epsilon : 0.45\n",
      "episode : 28340, reward mean : 0.4647092503974649, total_step : 726694, cur_epsilon : 0.45\n",
      "episode : 28350, reward mean : 0.46473439704843617, total_step : 726968, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28350, reward mean : 0.564000004157424, step mean : 19.55, eval_episode_rewards : [ 0.5   0.79  0.9   0.95  0.43  0.91  0.92 -1.07  0.87  0.86  0.82  0.97\n",
      " -1.    0.99  0.95  0.91 -1.06  0.86  0.9   0.88]\n",
      "episode : 28360, reward mean : 0.4647249702524462, total_step : 727140, cur_epsilon : 0.45\n",
      "episode : 28370, reward mean : 0.4647620781251598, total_step : 727380, cur_epsilon : 0.45\n",
      "episode : 28380, reward mean : 0.4648192444132999, total_step : 727563, cur_epsilon : 0.45\n",
      "episode : 28390, reward mean : 0.46488482410999493, total_step : 727722, cur_epsilon : 0.45\n",
      "episode : 28400, reward mean : 0.4649852167795294, total_step : 727982, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28400, reward mean : 0.40500000659376384, step mean : 30.5, eval_episode_rewards : [-1.19  0.49  0.66  0.88 -1.64  0.36  0.6   0.59  0.96  0.99 -1.32  0.95\n",
      "  0.8   0.87  0.58  0.86  0.9   0.78  0.16  0.82]\n",
      "episode : 28410, reward mean : 0.46507497911303136, total_step : 728272, cur_epsilon : 0.45\n",
      "episode : 28420, reward mean : 0.465197401711577, total_step : 728469, cur_epsilon : 0.45\n",
      "episode : 28430, reward mean : 0.4651808004461378, total_step : 728661, cur_epsilon : 0.45\n",
      "episode : 28440, reward mean : 0.46529079313408295, total_step : 728893, cur_epsilon : 0.45\n",
      "episode : 28450, reward mean : 0.465311077567495, total_step : 729180, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28450, reward mean : 0.5960000056773425, step mean : 26.35, eval_episode_rewards : [ 0.9   0.96  0.94  0.59  0.6   0.81  0.75  0.69  0.97 -1.03  0.45  0.93\n",
      "  0.87  0.78  0.61  0.98 -1.    0.87  0.38  0.87]\n",
      "episode : 28460, reward mean : 0.46527969630517574, total_step : 729414, cur_epsilon : 0.45\n",
      "episode : 28470, reward mean : 0.4653368513130926, total_step : 729596, cur_epsilon : 0.45\n",
      "episode : 28480, reward mean : 0.46532830607160414, total_step : 729765, cur_epsilon : 0.45\n",
      "episode : 28490, reward mean : 0.4653948809038854, total_step : 729920, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 28500, reward mean : 0.46553439147306375, total_step : 730067, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28500, reward mean : 0.7735000050626695, step mean : 23.65, eval_episode_rewards : [0.87 0.9  0.65 0.93 0.85 0.9  0.86 0.93 0.81 0.06 0.94 0.99 0.47 0.69\n",
      " 0.87 0.6  0.76 0.68 0.97 0.74]\n",
      "episode : 28510, reward mean : 0.46559944430157746, total_step : 730325, cur_epsilon : 0.45\n",
      "episode : 28520, reward mean : 0.46572861700824253, total_step : 730501, cur_epsilon : 0.45\n",
      "episode : 28530, reward mean : 0.46584262730886056, total_step : 730720, cur_epsilon : 0.45\n",
      "episode : 28540, reward mean : 0.46587947292126763, total_step : 730959, cur_epsilon : 0.45\n",
      "episode : 28550, reward mean : 0.46594221216138787, total_step : 731124, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28550, reward mean : 0.702500004414469, step mean : 20.75, eval_episode_rewards : [ 0.73  0.97  0.93  0.76  0.74  0.91  0.94  0.86  0.95  0.87  0.81  0.6\n",
      "  0.92  0.94  0.86  0.64  0.39  0.58 -1.19  0.84]\n",
      "episode : 28560, reward mean : 0.465987050324008, total_step : 731340, cur_epsilon : 0.45\n",
      "episode : 28570, reward mean : 0.4660812095664298, total_step : 731615, cur_epsilon : 0.45\n",
      "episode : 28580, reward mean : 0.46614381236345503, total_step : 731780, cur_epsilon : 0.45\n",
      "episode : 28590, reward mean : 0.4660038530037599, total_step : 731924, cur_epsilon : 0.45\n",
      "episode : 28600, reward mean : 0.4661283271824225, total_step : 732112, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28600, reward mean : 0.7160000041127205, step mean : 19.4, eval_episode_rewards : [ 0.82  0.81  0.9   0.98  0.84  0.81  0.36  0.73  0.89  0.57  0.97  0.93\n",
      "  0.76  0.96 -1.23  0.95  0.9   0.7   0.71  0.96]\n",
      "episode : 28610, reward mean : 0.46616987617844596, total_step : 732337, cur_epsilon : 0.45\n",
      "episode : 28620, reward mean : 0.46625052961331465, total_step : 732650, cur_epsilon : 0.45\n",
      "episode : 28630, reward mean : 0.4663423037930017, total_step : 732931, cur_epsilon : 0.45\n",
      "episode : 28640, reward mean : 0.46631983790635106, total_step : 733139, cur_epsilon : 0.45\n",
      "episode : 28650, reward mean : 0.46628447321781574, total_step : 733384, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28650, reward mean : 0.4170000052079558, step mean : 24.25, eval_episode_rewards : [ 0.77  0.83  0.83  0.81  0.45  0.92  0.94 -1.    0.94  0.86  0.81 -1.13\n",
      "  0.82 -1.03  0.82  0.62  0.5  -1.03  0.9   0.71]\n",
      "episode : 28660, reward mean : 0.4663388052243617, total_step : 733572, cur_epsilon : 0.45\n",
      "episode : 28670, reward mean : 0.4662602078048743, total_step : 733741, cur_epsilon : 0.45\n",
      "episode : 28680, reward mean : 0.46631451038373545, total_step : 733929, cur_epsilon : 0.45\n",
      "episode : 28690, reward mean : 0.4663551815215767, total_step : 734156, cur_epsilon : 0.45\n",
      "episode : 28700, reward mean : 0.4663686466158631, total_step : 734261, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28700, reward mean : 0.7155000041238964, step mean : 19.45, eval_episode_rewards : [ 0.84  0.97  0.73  0.92  0.73  0.99  0.95  0.84  0.98  0.25  0.94  0.63\n",
      "  0.9   0.72  0.9   0.88  0.68  0.68  0.82 -1.04]\n",
      "episode : 28710, reward mean : 0.46641763002153985, total_step : 734464, cur_epsilon : 0.45\n",
      "episode : 28720, reward mean : 0.46645125898229045, total_step : 734711, cur_epsilon : 0.45\n",
      "episode : 28730, reward mean : 0.46650261601158916, total_step : 734907, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 28740, reward mean : 0.46658768817249663, total_step : 735206, cur_epsilon : 0.45\n",
      "episode : 28750, reward mean : 0.4666918315871254, total_step : 735450, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28750, reward mean : 0.6850000048056245, step mean : 22.5, eval_episode_rewards : [ 0.7   0.88  0.35  0.5   0.99  0.93  0.88 -1.14  0.97  0.92  0.57  0.84\n",
      "  0.48  0.7   0.92  0.79  0.82  0.92  0.82  0.86]\n",
      "episode : 28760, reward mean : 0.46666759938022356, total_step : 735663, cur_epsilon : 0.45\n",
      "episode : 28770, reward mean : 0.46678519840871796, total_step : 735868, cur_epsilon : 0.45\n",
      "episode : 28780, reward mean : 0.46688603746605734, total_step : 736121, cur_epsilon : 0.45\n",
      "episode : 28790, reward mean : 0.46690170747959375, total_step : 736419, cur_epsilon : 0.45\n",
      "episode : 28800, reward mean : 0.4669513943889696, total_step : 736718, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28800, reward mean : 0.6880000047385693, step mean : 22.2, eval_episode_rewards : [ 0.96  0.76  0.88  0.85  0.89  0.92  0.91  0.91  0.71  0.68  0.69  0.88\n",
      "  0.93  0.67  0.47  0.75  0.35  0.83  0.78 -1.06]\n",
      "episode : 28810, reward mean : 0.46708435121262937, total_step : 736878, cur_epsilon : 0.45\n",
      "episode : 28820, reward mean : 0.4671984787814521, total_step : 737092, cur_epsilon : 0.45\n",
      "episode : 28830, reward mean : 0.4670999014405117, total_step : 737319, cur_epsilon : 0.45\n",
      "episode : 28840, reward mean : 0.4671432093819849, total_step : 737537, cur_epsilon : 0.45\n",
      "episode : 28850, reward mean : 0.4672513053249858, total_step : 737768, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28850, reward mean : 0.721500005107373, step mean : 23.8, eval_episode_rewards : [ 0.8   0.67  0.85  0.68  0.97 -1.    0.79  0.82  0.55  0.85  0.96  0.7\n",
      "  0.87  0.63  0.88  0.92  0.76  0.89  0.98  0.86]\n",
      "episode : 28860, reward mean : 0.46720825220662804, total_step : 738035, cur_epsilon : 0.45\n",
      "episode : 28870, reward mean : 0.4673411901183439, total_step : 738194, cur_epsilon : 0.45\n",
      "episode : 28880, reward mean : 0.4672451578519227, total_step : 738414, cur_epsilon : 0.45\n",
      "episode : 28890, reward mean : 0.46713673100790976, total_step : 738670, cur_epsilon : 0.45\n",
      "episode : 28900, reward mean : 0.4672529466734445, total_step : 738877, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28900, reward mean : 0.625500003900379, step mean : 18.45, eval_episode_rewards : [ 0.87  0.91 -1.02  0.77  0.92  0.92  0.97  0.75  0.97  0.95  0.44  0.8\n",
      "  0.95 -1.16  0.36  0.9   0.64  0.89  0.93  0.75]\n",
      "episode : 28910, reward mean : 0.4673946786195102, total_step : 739010, cur_epsilon : 0.45\n",
      "episode : 28920, reward mean : 0.4673516652471467, total_step : 739277, cur_epsilon : 0.45\n",
      "episode : 28930, reward mean : 0.4674745993427945, total_step : 739464, cur_epsilon : 0.45\n",
      "episode : 28940, reward mean : 0.4675846634082543, total_step : 739688, cur_epsilon : 0.45\n",
      "episode : 28950, reward mean : 0.4676390383099825, total_step : 739873, cur_epsilon : 0.45\n",
      "[EVAL] episode : 28950, reward mean : 0.6595000031404197, step mean : 15.05, eval_episode_rewards : [ 0.9  -1.08 -1.04  0.83  0.86  0.98  0.88  0.84  0.71  0.95  0.93  0.9\n",
      "  0.81  0.95  0.62  0.72  0.85  0.85  0.83  0.9 ]\n",
      "synced target net\n",
      "episode : 28960, reward mean : 0.46773377621310985, total_step : 740141, cur_epsilon : 0.45\n",
      "episode : 28970, reward mean : 0.46778633618128734, total_step : 740331, cur_epsilon : 0.45\n",
      "episode : 28980, reward mean : 0.46778537471443826, total_step : 740575, cur_epsilon : 0.45\n",
      "episode : 28990, reward mean : 0.4677533687228255, total_step : 740810, cur_epsilon : 0.45\n",
      "episode : 29000, reward mean : 0.4678855227347592, total_step : 740969, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29000, reward mean : 0.3345000036992133, step mean : 17.55, eval_episode_rewards : [ 0.85  0.92 -1.04  0.86 -1.01  0.78  0.89  0.66  0.89  0.77 -1.14 -1.14\n",
      "  0.84  0.98  0.66  0.76  0.82 -1.54  0.94  0.94]\n",
      "episode : 29010, reward mean : 0.46773940569951344, total_step : 741135, cur_epsilon : 0.45\n",
      "episode : 29020, reward mean : 0.4677074486359589, total_step : 741469, cur_epsilon : 0.45\n",
      "episode : 29030, reward mean : 0.4677657650517383, total_step : 741642, cur_epsilon : 0.45\n",
      "episode : 29040, reward mean : 0.4678863691285345, total_step : 741834, cur_epsilon : 0.45\n",
      "episode : 29050, reward mean : 0.46783615006919366, total_step : 741922, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29050, reward mean : 0.34100000467151403, step mean : 21.85, eval_episode_rewards : [ 0.94  0.87  0.96 -1.26  0.57  0.87 -1.02  0.45  0.89  0.79 -1.07  0.79\n",
      "  0.69  0.95  0.96 -1.    0.79  0.99 -1.09  0.75]\n",
      "episode : 29060, reward mean : 0.46792774121023856, total_step : 742198, cur_epsilon : 0.45\n",
      "episode : 29070, reward mean : 0.4679246700929153, total_step : 742349, cur_epsilon : 0.45\n",
      "episode : 29080, reward mean : 0.4680003493681596, total_step : 742471, cur_epsilon : 0.45\n",
      "episode : 29090, reward mean : 0.46805260088230716, total_step : 742661, cur_epsilon : 0.45\n",
      "episode : 29100, reward mean : 0.4681165003332972, total_step : 742817, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29100, reward mean : 0.6745000050403178, step mean : 23.55, eval_episode_rewards : [ 0.99  0.46  0.95  0.99  0.92  0.85  0.89  0.91  0.66  0.84  0.42  0.96\n",
      "  0.98  0.75 -1.11  0.89  0.83  0.4   0.39  0.52]\n",
      "episode : 29110, reward mean : 0.46817485948935617, total_step : 742989, cur_epsilon : 0.45\n",
      "episode : 29120, reward mean : 0.46828640658590065, total_step : 743206, cur_epsilon : 0.45\n",
      "episode : 29130, reward mean : 0.46821902368039947, total_step : 743344, cur_epsilon : 0.45\n",
      "episode : 29140, reward mean : 0.4681074179776074, total_step : 743611, cur_epsilon : 0.45\n",
      "episode : 29150, reward mean : 0.4681334531706587, total_step : 743877, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29150, reward mean : 0.726000003889203, step mean : 18.4, eval_episode_rewards : [ 0.92  0.72  0.82  0.84 -1.17  0.79  0.9   0.72  0.89  0.61  0.97  0.81\n",
      "  0.69  0.93  0.67  0.9   0.92  0.9   0.87  0.82]\n",
      "episode : 29160, reward mean : 0.46827366803676934, total_step : 744010, cur_epsilon : 0.45\n",
      "episode : 29170, reward mean : 0.46832876791191, total_step : 744191, cur_epsilon : 0.45\n",
      "episode : 29180, reward mean : 0.46832145853408547, total_step : 744354, cur_epsilon : 0.45\n",
      "episode : 29190, reward mean : 0.46824940596287606, total_step : 744506, cur_epsilon : 0.45\n",
      "episode : 29200, reward mean : 0.4682866493187054, total_step : 744739, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29200, reward mean : 0.7410000035539269, step mean : 16.9, eval_episode_rewards : [ 0.82  0.6   0.99  0.93  0.67  0.95 -1.03  0.53  0.8   0.94  0.86  0.92\n",
      "  0.94  0.77  0.87  0.9   0.85  0.95  0.84  0.72]\n",
      "episode : 29210, reward mean : 0.46832626361364177, total_step : 744965, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 29220, reward mean : 0.4683555838538488, total_step : 745221, cur_epsilon : 0.45\n",
      "episode : 29230, reward mean : 0.4684399644294617, total_step : 745516, cur_epsilon : 0.45\n",
      "episode : 29240, reward mean : 0.46855780302034755, total_step : 745713, cur_epsilon : 0.45\n",
      "episode : 29250, reward mean : 0.4685565866784662, total_step : 745858, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29250, reward mean : 0.4670000052079558, step mean : 24.3, eval_episode_rewards : [ 0.82 -1.62  0.85  0.63  0.54  0.89  0.66  0.64  0.73  0.97  0.63  0.94\n",
      "  0.74  0.9   0.55 -1.13 -1.05  0.99  0.86  0.8 ]\n",
      "episode : 29260, reward mean : 0.46860595216633405, total_step : 746055, cur_epsilon : 0.45\n",
      "episode : 29270, reward mean : 0.46874889512853407, total_step : 746178, cur_epsilon : 0.45\n",
      "episode : 29280, reward mean : 0.4688203606707466, total_step : 746310, cur_epsilon : 0.45\n",
      "episode : 29290, reward mean : 0.46886139161784307, total_step : 746531, cur_epsilon : 0.45\n",
      "episode : 29300, reward mean : 0.4689894252737711, total_step : 746697, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29300, reward mean : 0.8000000044703484, step mean : 21.0, eval_episode_rewards : [0.82 0.57 0.94 0.98 0.83 0.96 0.26 0.97 0.8  0.8  0.93 0.86 0.92 0.97\n",
      " 0.71 0.38 0.86 0.59 0.92 0.93]\n",
      "episode : 29310, reward mean : 0.46906448858909255, total_step : 746818, cur_epsilon : 0.45\n",
      "episode : 29320, reward mean : 0.46920157437159404, total_step : 746957, cur_epsilon : 0.45\n",
      "episode : 29330, reward mean : 0.4692499202392561, total_step : 747156, cur_epsilon : 0.45\n",
      "episode : 29340, reward mean : 0.46934867623283055, total_step : 747407, cur_epsilon : 0.45\n",
      "episode : 29350, reward mean : 0.4692272627167086, total_step : 747704, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29350, reward mean : 0.7120000042021275, step mean : 19.8, eval_episode_rewards : [ 0.69  0.95  0.77  0.82  0.98  0.97  0.9   0.86  0.92  0.98  0.4   0.74\n",
      "  0.64  0.89  0.78 -1.56  0.9   0.97  0.98  0.66]\n",
      "episode : 29360, reward mean : 0.4693426485278717, total_step : 747906, cur_epsilon : 0.45\n",
      "episode : 29370, reward mean : 0.46938543278257183, total_step : 748121, cur_epsilon : 0.45\n",
      "episode : 29380, reward mean : 0.4694111695330542, total_step : 748386, cur_epsilon : 0.45\n",
      "episode : 29390, reward mean : 0.4694719347035542, total_step : 748647, cur_epsilon : 0.45\n",
      "episode : 29400, reward mean : 0.4695874204415017, total_step : 748848, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29400, reward mean : 0.7095000042580069, step mean : 20.05, eval_episode_rewards : [ 0.95  0.62  0.89  0.75  0.66  0.72  0.85  0.95  0.85  0.95 -1.04  0.69\n",
      "  0.95  0.99  0.75  0.95  0.85  0.43  0.54  0.89]\n",
      "episode : 29410, reward mean : 0.4696674655234984, total_step : 749153, cur_epsilon : 0.45\n",
      "episode : 29420, reward mean : 0.46974269752108944, total_step : 749272, cur_epsilon : 0.45\n",
      "episode : 29430, reward mean : 0.46979647166527, total_step : 749454, cur_epsilon : 0.45\n",
      "episode : 29440, reward mean : 0.469909652213075, total_step : 749661, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 29450, reward mean : 0.46996978476169204, total_step : 750024, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29450, reward mean : 0.4940000046044588, step mean : 21.6, eval_episode_rewards : [ 0.87  0.92  0.84  0.79  0.91  0.79  0.96  0.19  0.99  0.85  0.88 -1.71\n",
      "  0.9   0.62 -1.2  -1.01  0.9   0.66  0.91  0.82]\n",
      "episode : 29460, reward mean : 0.470076380219963, total_step : 750250, cur_epsilon : 0.45\n",
      "episode : 29470, reward mean : 0.47014184463219316, total_step : 750397, cur_epsilon : 0.45\n",
      "episode : 29480, reward mean : 0.4701119457720305, total_step : 750625, cur_epsilon : 0.45\n",
      "episode : 29490, reward mean : 0.47010580404857716, total_step : 750783, cur_epsilon : 0.45\n",
      "episode : 29500, reward mean : 0.470205090218492, total_step : 751030, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29500, reward mean : 0.6925000057555735, step mean : 26.7, eval_episode_rewards : [ 0.93  0.67  0.2   0.57  0.75  0.75  0.4   0.84  0.77  0.75  0.9  -1.\n",
      "  0.82  0.91  0.96  0.97  0.8   0.96  0.93  0.97]\n",
      "episode : 29510, reward mean : 0.4703259966616124, total_step : 751213, cur_epsilon : 0.45\n",
      "episode : 29520, reward mean : 0.4704705339263958, total_step : 751326, cur_epsilon : 0.45\n",
      "episode : 29530, reward mean : 0.47057569121422804, total_step : 751555, cur_epsilon : 0.45\n",
      "episode : 29540, reward mean : 0.4706343995122215, total_step : 751721, cur_epsilon : 0.45\n",
      "episode : 29550, reward mean : 0.4707637956556305, total_step : 751878, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29550, reward mean : 0.7270000038668514, step mean : 18.3, eval_episode_rewards : [ 0.97  0.92  0.74  0.79  0.56  0.81  0.86  0.88  0.64  0.9   0.93  0.77\n",
      "  0.78  0.84  0.84 -1.02  0.65  0.81  0.9   0.97]\n",
      "episode : 29560, reward mean : 0.47075068205891174, total_step : 752056, cur_epsilon : 0.45\n",
      "episode : 29570, reward mean : 0.4708596605244403, total_step : 752273, cur_epsilon : 0.45\n",
      "episode : 29580, reward mean : 0.4708248871453535, total_step : 752515, cur_epsilon : 0.45\n",
      "episode : 29590, reward mean : 0.4709236283140429, total_step : 752762, cur_epsilon : 0.45\n",
      "episode : 29600, reward mean : 0.4708577757378461, total_step : 752896, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29600, reward mean : 0.549000003375113, step mean : 16.1, eval_episode_rewards : [ 0.93  0.76  0.87  0.96  0.9  -1.06  0.87  0.94  0.73 -1.13  0.61  0.99\n",
      "  0.59  0.86  0.87 -1.16  0.93  0.97  0.62  0.93]\n",
      "episode : 29610, reward mean : 0.47089362248868927, total_step : 753129, cur_epsilon : 0.45\n",
      "episode : 29620, reward mean : 0.4709986550283193, total_step : 753357, cur_epsilon : 0.45\n",
      "episode : 29630, reward mean : 0.47113331629996197, total_step : 753497, cur_epsilon : 0.45\n",
      "episode : 29640, reward mean : 0.47125135499351833, total_step : 753686, cur_epsilon : 0.45\n",
      "episode : 29650, reward mean : 0.4712411521768439, total_step : 753855, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29650, reward mean : 0.7215000039897859, step mean : 18.85, eval_episode_rewards : [ 0.28 -1.1   0.46  0.84  0.93  0.9   0.74  0.87  0.93  0.88  0.66  0.94\n",
      "  0.86  0.97  0.9   0.87  0.92  0.96  0.98  0.64]\n",
      "episode : 29660, reward mean : 0.4712521969677977, total_step : 753961, cur_epsilon : 0.45\n",
      "episode : 29670, reward mean : 0.4711911075879397, total_step : 754281, cur_epsilon : 0.45\n",
      "episode : 29680, reward mean : 0.47125337473606327, total_step : 754435, cur_epsilon : 0.45\n",
      "episode : 29690, reward mean : 0.4713984561195103, total_step : 754543, cur_epsilon : 0.45\n",
      "episode : 29700, reward mean : 0.4714555610176364, total_step : 754712, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29700, reward mean : 0.8040000043809414, step mean : 20.6, eval_episode_rewards : [0.93 0.97 0.88 0.76 0.91 0.63 0.74 0.58 0.84 0.93 0.65 0.93 0.76 0.88\n",
      " 0.91 0.94 0.47 0.64 0.89 0.84]\n",
      "episode : 29710, reward mean : 0.47150757866997794, total_step : 754995, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 29720, reward mean : 0.47151514678099227, total_step : 755210, cur_epsilon : 0.45\n",
      "episode : 29730, reward mean : 0.47154188235407823, total_step : 755469, cur_epsilon : 0.45\n",
      "episode : 29740, reward mean : 0.4716842690790254, total_step : 755584, cur_epsilon : 0.45\n",
      "episode : 29750, reward mean : 0.47179866092276423, total_step : 755782, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29750, reward mean : 0.5410000057891011, step mean : 26.9, eval_episode_rewards : [-1.03  0.89  0.74  0.84  0.85  0.59  0.7   0.55  0.85  0.71 -1.13  0.92\n",
      "  0.55  0.88  0.3   0.63  0.87  0.95  0.74  0.42]\n",
      "episode : 29760, reward mean : 0.471850139868661, total_step : 755967, cur_epsilon : 0.45\n",
      "episode : 29770, reward mean : 0.4719002405956117, total_step : 756156, cur_epsilon : 0.45\n",
      "episode : 29780, reward mean : 0.4718918791996494, total_step : 756319, cur_epsilon : 0.45\n",
      "episode : 29790, reward mean : 0.4718640537970031, total_step : 756540, cur_epsilon : 0.45\n",
      "episode : 29800, reward mean : 0.47193356250469015, total_step : 756671, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29800, reward mean : 0.6230000039562583, step mean : 18.7, eval_episode_rewards : [ 0.6   0.85  0.99  0.6  -1.3   0.81  0.85  0.74 -1.01  0.91  0.29  0.91\n",
      "  0.92  0.84  0.79  0.98  0.94  0.87  0.97  0.91]\n",
      "episode : 29810, reward mean : 0.47195975050975, total_step : 756931, cur_epsilon : 0.45\n",
      "episode : 29820, reward mean : 0.4718967190718167, total_step : 757057, cur_epsilon : 0.45\n",
      "episode : 29830, reward mean : 0.4720107329119543, total_step : 757255, cur_epsilon : 0.45\n",
      "episode : 29840, reward mean : 0.4719393486191391, total_step : 757406, cur_epsilon : 0.45\n",
      "episode : 29850, reward mean : 0.47204322153580697, total_step : 757634, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29850, reward mean : 0.4125000053085387, step mean : 24.7, eval_episode_rewards : [ 0.93 -1.01  0.8  -1.    0.96  0.99 -1.18  0.48 -1.4   0.5   0.9   0.92\n",
      "  0.84  0.71  0.77  0.82  0.59  0.84  0.84  0.95]\n",
      "episode : 29860, reward mean : 0.47208339460449866, total_step : 757852, cur_epsilon : 0.45\n",
      "episode : 29870, reward mean : 0.47207801014136314, total_step : 758006, cur_epsilon : 0.45\n",
      "episode : 29880, reward mean : 0.4721847444099475, total_step : 758225, cur_epsilon : 0.45\n",
      "episode : 29890, reward mean : 0.472152230278338, total_step : 758460, cur_epsilon : 0.45\n",
      "episode : 29900, reward mean : 0.472239135888944, total_step : 758738, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29900, reward mean : 0.8300000037997961, step mean : 18.0, eval_episode_rewards : [0.97 0.83 0.7  0.88 0.99 0.92 0.96 0.97 0.98 0.95 0.9  0.94 0.54 0.49\n",
      " 0.75 0.44 0.82 0.85 0.93 0.79]\n",
      "episode : 29910, reward mean : 0.47202508067927035, total_step : 758916, cur_epsilon : 0.45\n",
      "episode : 29920, reward mean : 0.472148735399475, total_step : 759084, cur_epsilon : 0.45\n",
      "episode : 29930, reward mean : 0.4721470151413973, total_step : 759227, cur_epsilon : 0.45\n",
      "episode : 29940, reward mean : 0.4722087562863722, total_step : 759380, cur_epsilon : 0.45\n",
      "episode : 29950, reward mean : 0.4723365663855223, total_step : 759535, cur_epsilon : 0.45\n",
      "[EVAL] episode : 29950, reward mean : 0.6740000050514936, step mean : 23.6, eval_episode_rewards : [ 0.93  0.31  0.86  0.74  0.8   0.9   0.88  0.64 -1.38  0.85  0.88  0.63\n",
      "  0.89  0.96  0.92  0.39  0.94  0.77  0.79  0.78]\n",
      "episode : 29960, reward mean : 0.47241656085816974, total_step : 759833, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 29970, reward mean : 0.4725221943062372, total_step : 760054, cur_epsilon : 0.45\n",
      "episode : 29980, reward mean : 0.4725193516807314, total_step : 760200, cur_epsilon : 0.45\n",
      "episode : 29990, reward mean : 0.4725665276235379, total_step : 760396, cur_epsilon : 0.45\n",
      "episode : 30000, reward mean : 0.47251433878311266, total_step : 760690, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30000, reward mean : 0.7510000055655837, step mean : 25.9, eval_episode_rewards : [0.78 0.93 0.68 0.86 0.82 0.99 0.55 0.62 0.94 0.8  0.94 0.88 0.88 0.71\n",
      " 0.8  0.72 0.31 0.7  0.8  0.31]\n",
      "episode : 30010, reward mean : 0.47253449395378405, total_step : 760967, cur_epsilon : 0.45\n",
      "episode : 30020, reward mean : 0.47246636121194413, total_step : 761109, cur_epsilon : 0.45\n",
      "episode : 30030, reward mean : 0.47249184694101454, total_step : 761370, cur_epsilon : 0.45\n",
      "episode : 30040, reward mean : 0.4725585939969059, total_step : 761507, cur_epsilon : 0.45\n",
      "episode : 30050, reward mean : 0.4726492566963078, total_step : 761772, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30050, reward mean : 0.7615000053308905, step mean : 24.85, eval_episode_rewards : [0.92 0.94 0.9  0.72 0.88 0.33 0.71 0.29 0.81 0.56 0.38 0.83 0.87 0.98\n",
      " 0.66 0.74 0.89 0.98 0.93 0.91]\n",
      "episode : 30060, reward mean : 0.4727568251420375, total_step : 761986, cur_epsilon : 0.45\n",
      "episode : 30070, reward mean : 0.4728014687001259, total_step : 762189, cur_epsilon : 0.45\n",
      "episode : 30080, reward mean : 0.47289129534143054, total_step : 762456, cur_epsilon : 0.45\n",
      "episode : 30090, reward mean : 0.47288634642412664, total_step : 762608, cur_epsilon : 0.45\n",
      "episode : 30100, reward mean : 0.47291595229086936, total_step : 762856, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30100, reward mean : 0.5205000040121377, step mean : 18.95, eval_episode_rewards : [ 0.91  0.58  0.88  0.84 -1.3   0.82  0.77  0.95 -1.06  0.99  0.86 -1.05\n",
      "  0.87  0.94  0.33  0.72  0.8   0.87  0.72  0.97]\n",
      "episode : 30110, reward mean : 0.4729076773161455, total_step : 763018, cur_epsilon : 0.45\n",
      "episode : 30120, reward mean : 0.47295053665449527, total_step : 763226, cur_epsilon : 0.45\n",
      "episode : 30130, reward mean : 0.4730501216091648, total_step : 763463, cur_epsilon : 0.45\n",
      "episode : 30140, reward mean : 0.4731141394862007, total_step : 763607, cur_epsilon : 0.45\n",
      "episode : 30150, reward mean : 0.4730248810665969, total_step : 763813, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30150, reward mean : 0.4935000057332218, step mean : 26.6, eval_episode_rewards : [ 0.61  0.96 -1.08  0.87  0.89  0.93  0.58  0.72  0.69  0.89 -1.2   0.2\n",
      "  0.96  0.94  0.72  0.95 -1.    0.84  0.61  0.79]\n",
      "episode : 30160, reward mean : 0.47295027069601697, total_step : 763975, cur_epsilon : 0.45\n",
      "episode : 30170, reward mean : 0.47299967398845405, total_step : 764163, cur_epsilon : 0.45\n",
      "episode : 30180, reward mean : 0.473044074362982, total_step : 764366, cur_epsilon : 0.45\n",
      "episode : 30190, reward mean : 0.47316496072577036, total_step : 764538, cur_epsilon : 0.45\n",
      "episode : 30200, reward mean : 0.47312450875386297, total_step : 764797, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30200, reward mean : 0.6525000055320561, step mean : 25.75, eval_episode_rewards : [ 0.78  0.76  0.66  0.47  0.92  0.57  0.51  0.96  0.51  0.99  0.88  0.93\n",
      "  0.77  0.35  0.91  0.87  0.87  0.63  0.76 -1.05]\n",
      "synced target net\n",
      "episode : 30210, reward mean : 0.4731602172928041, total_step : 765026, cur_epsilon : 0.45\n",
      "episode : 30220, reward mean : 0.47328160703015804, total_step : 765196, cur_epsilon : 0.45\n",
      "episode : 30230, reward mean : 0.473392992540319, total_step : 765396, cur_epsilon : 0.45\n",
      "episode : 30240, reward mean : 0.47345172501743216, total_step : 765555, cur_epsilon : 0.45\n",
      "episode : 30250, reward mean : 0.47351603849921836, total_step : 765897, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30250, reward mean : 0.35650000320747494, step mean : 15.35, eval_episode_rewards : [ 0.98  0.86  0.9   0.75  0.88  0.83  0.69  0.86  0.93  0.88 -1.05  0.78\n",
      " -1.43 -1.01  0.88 -1.16  0.98  0.94  0.9  -1.26]\n",
      "episode : 30260, reward mean : 0.47360906029927685, total_step : 766152, cur_epsilon : 0.45\n",
      "episode : 30270, reward mean : 0.47373142268552976, total_step : 766318, cur_epsilon : 0.45\n",
      "episode : 30280, reward mean : 0.4737509961938793, total_step : 766595, cur_epsilon : 0.45\n",
      "episode : 30290, reward mean : 0.4737916858631905, total_step : 766808, cur_epsilon : 0.45\n",
      "episode : 30300, reward mean : 0.4737389493345747, total_step : 767003, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30300, reward mean : 0.41700000409036875, step mean : 19.3, eval_episode_rewards : [ 0.97  0.36  0.25 -1.08  0.93  0.93  0.88  0.98  0.85 -1.02  0.84  0.75\n",
      "  0.83  0.88  0.76  0.88  0.74  0.96 -1.13 -1.22]\n",
      "episode : 30310, reward mean : 0.47371099191379507, total_step : 767323, cur_epsilon : 0.45\n",
      "episode : 30320, reward mean : 0.4737269183696987, total_step : 767611, cur_epsilon : 0.45\n",
      "episode : 30330, reward mean : 0.4738539454335899, total_step : 767762, cur_epsilon : 0.45\n",
      "episode : 30340, reward mean : 0.47377456048246896, total_step : 767939, cur_epsilon : 0.45\n",
      "episode : 30350, reward mean : 0.47357957710335535, total_step : 768166, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30350, reward mean : 0.7655000030063093, step mean : 14.45, eval_episode_rewards : [ 0.91  0.88  0.78  0.86  0.93  0.78  0.91  0.47  0.96  0.95  0.73  0.94\n",
      "  0.72  0.92 -1.01  0.95  0.92  0.92  0.83  0.96]\n",
      "episode : 30360, reward mean : 0.47368972875921445, total_step : 768368, cur_epsilon : 0.45\n",
      "episode : 30370, reward mean : 0.47359796395119935, total_step : 768682, cur_epsilon : 0.45\n",
      "episode : 30380, reward mean : 0.4736863122203947, total_step : 768950, cur_epsilon : 0.45\n",
      "episode : 30390, reward mean : 0.4737571623981705, total_step : 769071, cur_epsilon : 0.45\n",
      "episode : 30400, reward mean : 0.4738164528063498, total_step : 769227, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30400, reward mean : 0.5280000038444996, step mean : 18.2, eval_episode_rewards : [ 0.8   0.65  0.68  0.83  0.74  0.76 -1.25  0.93  0.94 -1.01  0.85  0.97\n",
      "  0.94  0.79  0.75 -1.07  0.62  0.71  0.97  0.96]\n",
      "episode : 30410, reward mean : 0.4738530143163792, total_step : 769452, cur_epsilon : 0.45\n",
      "episode : 30420, reward mean : 0.4738925103684134, total_step : 769668, cur_epsilon : 0.45\n",
      "episode : 30430, reward mean : 0.4739681290001084, total_step : 769974, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 30440, reward mean : 0.4739007938732779, total_step : 770115, cur_epsilon : 0.45\n",
      "episode : 30450, reward mean : 0.47387455387680205, total_step : 770331, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30450, reward mean : 0.07400000505149365, step mean : 23.6, eval_episode_rewards : [ 0.86 -1.39  0.65 -1.05  0.96  0.62  0.78  0.96  0.91  0.8  -1.18  0.68\n",
      "  0.81  0.04  0.94 -1.07 -1.1  -1.15  0.95 -1.54]\n",
      "episode : 30460, reward mean : 0.47399376774737545, total_step : 770504, cur_epsilon : 0.45\n",
      "episode : 30470, reward mean : 0.47405317248498413, total_step : 770659, cur_epsilon : 0.45\n",
      "episode : 30480, reward mean : 0.4740885881124079, total_step : 770887, cur_epsilon : 0.45\n",
      "episode : 30490, reward mean : 0.47413217991831197, total_step : 771090, cur_epsilon : 0.45\n",
      "episode : 30500, reward mean : 0.47419279231937206, total_step : 771241, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30500, reward mean : 0.5130000052973628, step mean : 24.65, eval_episode_rewards : [ 0.83  0.71 -1.23  0.95  0.8   0.58  0.91  0.97  0.88 -1.    0.91 -1.52\n",
      "  0.95  0.87  0.6   0.73  0.86  0.99  0.7   0.77]\n",
      "episode : 30510, reward mean : 0.47429728501432694, total_step : 771458, cur_epsilon : 0.45\n",
      "episode : 30520, reward mean : 0.47426704344154136, total_step : 771686, cur_epsilon : 0.45\n",
      "episode : 30530, reward mean : 0.47430265856155734, total_step : 771913, cur_epsilon : 0.45\n",
      "episode : 30540, reward mean : 0.4743032143404311, total_step : 772247, cur_epsilon : 0.45\n",
      "episode : 30550, reward mean : 0.47429689577705975, total_step : 772402, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30550, reward mean : 0.6750000050291419, step mean : 23.5, eval_episode_rewards : [ 0.88  0.83  0.73  0.61 -1.14  0.54  0.91  0.76  0.74  0.49  0.49  0.74\n",
      "  0.86  0.78  0.82  0.88  0.89  0.94  0.85  0.9 ]\n",
      "episode : 30560, reward mean : 0.47439725674208527, total_step : 772631, cur_epsilon : 0.45\n",
      "episode : 30570, reward mean : 0.47438633189642343, total_step : 772800, cur_epsilon : 0.45\n",
      "episode : 30580, reward mean : 0.47437083604029867, total_step : 772983, cur_epsilon : 0.45\n",
      "episode : 30590, reward mean : 0.4744553830066039, total_step : 773260, cur_epsilon : 0.45\n",
      "episode : 30600, reward mean : 0.47452582242545593, total_step : 773479, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30600, reward mean : 0.7960000045597553, step mean : 21.4, eval_episode_rewards : [0.81 0.99 0.91 0.71 0.92 0.94 0.66 0.91 0.97 0.72 0.56 0.95 0.83 0.77\n",
      " 0.86 0.83 0.36 0.97 0.28 0.97]\n",
      "episode : 30610, reward mean : 0.47461483718639336, total_step : 773742, cur_epsilon : 0.45\n",
      "episode : 30620, reward mean : 0.4745643424669525, total_step : 774032, cur_epsilon : 0.45\n",
      "episode : 30630, reward mean : 0.47466961039449757, total_step : 774245, cur_epsilon : 0.45\n",
      "episode : 30640, reward mean : 0.4747294440736031, total_step : 774397, cur_epsilon : 0.45\n",
      "episode : 30650, reward mean : 0.4747223545333787, total_step : 774554, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30650, reward mean : 0.687000004760921, step mean : 22.3, eval_episode_rewards : [ 0.9   0.91  0.4   0.75  0.87  0.94  0.42  0.55  0.99  0.82  0.73  0.86\n",
      "  0.86  0.65  0.97  0.86 -1.1   0.91  0.7   0.75]\n",
      "episode : 30660, reward mean : 0.4747071156714449, total_step : 774736, cur_epsilon : 0.45\n",
      "episode : 30670, reward mean : 0.4748053526748148, total_step : 774970, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 30680, reward mean : 0.47492731964049284, total_step : 775131, cur_epsilon : 0.45\n",
      "episode : 30690, reward mean : 0.4749136580843157, total_step : 775308, cur_epsilon : 0.45\n",
      "episode : 30700, reward mean : 0.4750120575458467, total_step : 775541, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30700, reward mean : 0.7235000039450824, step mean : 18.65, eval_episode_rewards : [ 0.68  0.91  0.91  0.83  0.76  0.92  0.77  0.63  0.98 -1.42  0.81  0.81\n",
      "  0.88  0.75  0.84  0.89  0.97  0.89  0.8   0.86]\n",
      "episode : 30710, reward mean : 0.47498665472819496, total_step : 775754, cur_epsilon : 0.45\n",
      "episode : 30720, reward mean : 0.4750358127194583, total_step : 775938, cur_epsilon : 0.45\n",
      "episode : 30730, reward mean : 0.4751171547934665, total_step : 776223, cur_epsilon : 0.45\n",
      "episode : 30740, reward mean : 0.4751158154467163, total_step : 776362, cur_epsilon : 0.45\n",
      "episode : 30750, reward mean : 0.475026672093546, total_step : 776571, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30750, reward mean : 0.8285000038333237, step mean : 18.15, eval_episode_rewards : [0.89 0.6  0.91 0.99 0.73 0.9  0.8  0.78 0.85 0.97 0.8  0.85 0.49 0.95\n",
      " 0.95 0.97 0.53 0.87 0.86 0.88]\n",
      "episode : 30760, reward mean : 0.47507835393087267, total_step : 776747, cur_epsilon : 0.45\n",
      "episode : 30770, reward mean : 0.47500553028824505, total_step : 777005, cur_epsilon : 0.45\n",
      "episode : 30780, reward mean : 0.47510884233318934, total_step : 777222, cur_epsilon : 0.45\n",
      "episode : 30790, reward mean : 0.4750058514800149, total_step : 777474, cur_epsilon : 0.45\n",
      "episode : 30800, reward mean : 0.4751055249064411, total_step : 777702, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30800, reward mean : 0.4150000041350722, step mean : 19.5, eval_episode_rewards : [ 0.04  0.86  0.81  0.77  0.88  0.99 -1.07  0.73  0.99  0.96 -1.01  0.64\n",
      "  0.96  0.92  0.73 -1.09  0.86 -1.02  0.91  0.44]\n",
      "episode : 30810, reward mean : 0.4752119495995592, total_step : 777909, cur_epsilon : 0.45\n",
      "episode : 30820, reward mean : 0.47510870107776554, total_step : 778162, cur_epsilon : 0.45\n",
      "episode : 30830, reward mean : 0.4751025029921803, total_step : 778316, cur_epsilon : 0.45\n",
      "episode : 30840, reward mean : 0.4752302259169326, total_step : 778457, cur_epsilon : 0.45\n",
      "episode : 30850, reward mean : 0.4753348514529449, total_step : 778669, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30850, reward mean : 0.4945000045932829, step mean : 21.55, eval_episode_rewards : [-1.21  0.99  0.83  0.92  0.91  0.57  0.95  0.64  0.67  0.97  0.81  0.78\n",
      " -1.31  0.96  0.82  0.93  0.83  0.72 -1.18  0.29]\n",
      "episode : 30860, reward mean : 0.47537848889714257, total_step : 778869, cur_epsilon : 0.45\n",
      "episode : 30870, reward mean : 0.475483322559467, total_step : 779080, cur_epsilon : 0.45\n",
      "episode : 30880, reward mean : 0.4754229328832758, total_step : 779201, cur_epsilon : 0.45\n",
      "episode : 30890, reward mean : 0.47551603002552706, total_step : 779448, cur_epsilon : 0.45\n",
      "episode : 30900, reward mean : 0.47556634846363444, total_step : 779627, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30900, reward mean : 0.6245000061579049, step mean : 28.55, eval_episode_rewards : [ 0.51  0.88  0.9   0.88  0.69  0.8   0.25  0.96  0.79  0.59  0.93  0.43\n",
      "  0.78  0.65  0.88  0.58  0.54  0.86  0.75 -1.16]\n",
      "episode : 30910, reward mean : 0.47552216653460505, total_step : 779898, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 30920, reward mean : 0.4754459950725342, total_step : 780167, cur_epsilon : 0.45\n",
      "episode : 30930, reward mean : 0.47541739953735423, total_step : 780390, cur_epsilon : 0.45\n",
      "episode : 30940, reward mean : 0.4754033667656082, total_step : 780568, cur_epsilon : 0.45\n",
      "episode : 30950, reward mean : 0.475460102350928, total_step : 780727, cur_epsilon : 0.45\n",
      "[EVAL] episode : 30950, reward mean : 0.6625000053085387, step mean : 24.75, eval_episode_rewards : [ 0.93  0.75 -1.03  0.96  0.88  0.4   0.34  0.94  0.63  0.66  0.7   0.56\n",
      "  0.79  0.86  0.93  0.77  0.74  0.98  0.82  0.64]\n",
      "episode : 30960, reward mean : 0.4754389589083879, total_step : 780927, cur_epsilon : 0.45\n",
      "episode : 30970, reward mean : 0.47557507806996163, total_step : 781040, cur_epsilon : 0.45\n",
      "episode : 30980, reward mean : 0.4756287981879286, total_step : 781208, cur_epsilon : 0.45\n",
      "episode : 30990, reward mean : 0.47558632358564856, total_step : 781474, cur_epsilon : 0.45\n",
      "episode : 31000, reward mean : 0.4756600054188842, total_step : 781780, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31000, reward mean : 0.6635000052861869, step mean : 24.65, eval_episode_rewards : [ 0.55  0.95  0.93  0.52 -1.05  0.62  0.56  0.97  0.93  0.22  0.94  0.89\n",
      "  0.8   0.96  0.53  0.69  0.95  0.4   0.94  0.97]\n",
      "episode : 31010, reward mean : 0.4757607277662946, total_step : 782002, cur_epsilon : 0.45\n",
      "episode : 31020, reward mean : 0.4757962658955715, total_step : 782226, cur_epsilon : 0.45\n",
      "episode : 31030, reward mean : 0.4758991352925056, total_step : 782441, cur_epsilon : 0.45\n",
      "episode : 31040, reward mean : 0.4760058043869068, total_step : 782644, cur_epsilon : 0.45\n",
      "episode : 31050, reward mean : 0.476067960328466, total_step : 782785, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31050, reward mean : 0.6705000051297247, step mean : 23.95, eval_episode_rewards : [ 0.87  0.96  0.88  0.87  0.82  0.85  0.79  0.86  0.15  0.75  0.86  0.59\n",
      "  0.78  0.93  0.56  0.73  0.6   0.9   0.68 -1.02]\n",
      "episode : 31060, reward mean : 0.4760985244124293, total_step : 783024, cur_epsilon : 0.45\n",
      "episode : 31070, reward mean : 0.47620921043740866, total_step : 783214, cur_epsilon : 0.45\n",
      "episode : 31080, reward mean : 0.47619144685747417, total_step : 783403, cur_epsilon : 0.45\n",
      "episode : 31090, reward mean : 0.4762836979216234, total_step : 783650, cur_epsilon : 0.45\n",
      "episode : 31100, reward mean : 0.47615820477270426, total_step : 783873, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31100, reward mean : 0.8005000044591725, step mean : 20.95, eval_episode_rewards : [0.88 0.7  0.32 0.86 0.7  0.66 0.85 0.73 0.75 0.92 0.87 0.96 0.93 0.85\n",
      " 0.76 0.85 0.85 0.97 0.8  0.8 ]\n",
      "episode : 31110, reward mean : 0.47616618992135107, total_step : 783982, cur_epsilon : 0.45\n",
      "episode : 31120, reward mean : 0.47622976119798827, total_step : 784118, cur_epsilon : 0.45\n",
      "episode : 31130, reward mean : 0.47628590326099257, total_step : 784277, cur_epsilon : 0.45\n",
      "episode : 31140, reward mean : 0.4763246682260781, total_step : 784490, cur_epsilon : 0.45\n",
      "episode : 31150, reward mean : 0.47636790268386253, total_step : 784689, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31150, reward mean : 0.5840000048279762, step mean : 22.6, eval_episode_rewards : [ 0.76  0.88  0.7   0.81 -1.99  0.86  0.96  0.85  0.9   0.69  0.89  0.93\n",
      " -1.08  0.8   0.87  0.64  0.71  0.86  0.69  0.95]\n",
      "episode : 31160, reward mean : 0.4764245882103715, total_step : 784846, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 31170, reward mean : 0.4765428350551755, total_step : 785011, cur_epsilon : 0.45\n",
      "episode : 31180, reward mean : 0.476331307527605, total_step : 785204, cur_epsilon : 0.45\n",
      "episode : 31190, reward mean : 0.47628599451009795, total_step : 785479, cur_epsilon : 0.45\n",
      "episode : 31200, reward mean : 0.4762785310513722, total_step : 785636, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31200, reward mean : 0.7425000035203994, step mean : 16.75, eval_episode_rewards : [ 0.79  0.95  0.83  0.84  0.93  0.71  0.97  0.95  0.51  0.97  0.92  0.85\n",
      "  0.59 -1.01  0.97  0.92  0.73  0.81  0.67  0.95]\n",
      "episode : 31210, reward mean : 0.47629253985470676, total_step : 785926, cur_epsilon : 0.45\n",
      "episode : 31220, reward mean : 0.47627322770360425, total_step : 786120, cur_epsilon : 0.45\n",
      "episode : 31230, reward mean : 0.47634262468557287, total_step : 786237, cur_epsilon : 0.45\n",
      "episode : 31240, reward mean : 0.4764657544481917, total_step : 786386, cur_epsilon : 0.45\n",
      "episode : 31250, reward mean : 0.47643072540843484, total_step : 786629, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31250, reward mean : 0.5990000044927001, step mean : 21.1, eval_episode_rewards : [ 0.85  0.95  0.85  0.95  0.84  0.64 -1.25 -1.05  0.52  0.8   0.94  0.82\n",
      "  0.86  0.88  0.84  0.73  0.96  0.84  0.66  0.35]\n",
      "episode : 31260, reward mean : 0.4764766528808005, total_step : 786819, cur_epsilon : 0.45\n",
      "episode : 31270, reward mean : 0.47659450492767724, total_step : 786984, cur_epsilon : 0.45\n",
      "episode : 31280, reward mean : 0.47663395681372034, total_step : 787194, cur_epsilon : 0.45\n",
      "episode : 31290, reward mean : 0.47673154903106096, total_step : 787422, cur_epsilon : 0.45\n",
      "episode : 31300, reward mean : 0.4767766827227661, total_step : 787614, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31300, reward mean : 0.48600000478327277, step mean : 22.4, eval_episode_rewards : [ 0.93  0.8   0.92 -1.02  0.94 -1.04  0.8   0.74  0.9   0.95  0.88  0.89\n",
      "  0.92  0.01  0.16  0.82  0.92  0.79  0.47 -1.06]\n",
      "episode : 31310, reward mean : 0.4768700149879978, total_step : 787855, cur_epsilon : 0.45\n",
      "episode : 31320, reward mean : 0.4770079875253688, total_step : 787956, cur_epsilon : 0.45\n",
      "episode : 31330, reward mean : 0.47704628692435125, total_step : 788169, cur_epsilon : 0.45\n",
      "episode : 31340, reward mean : 0.47708807177354307, total_step : 788371, cur_epsilon : 0.45\n",
      "episode : 31350, reward mean : 0.47713365771684446, total_step : 788561, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31350, reward mean : 0.8175000040791929, step mean : 19.25, eval_episode_rewards : [0.95 0.95 0.94 0.58 0.88 0.95 0.57 0.78 0.61 0.92 0.98 0.9  0.86 0.85\n",
      " 0.71 0.74 0.75 0.72 0.81 0.9 ]\n",
      "episode : 31360, reward mean : 0.4771195844853444, total_step : 788738, cur_epsilon : 0.45\n",
      "episode : 31370, reward mean : 0.47709946348428356, total_step : 788934, cur_epsilon : 0.45\n",
      "episode : 31380, reward mean : 0.47721033045064026, total_step : 789119, cur_epsilon : 0.45\n",
      "episode : 31390, reward mean : 0.47715578749942034, total_step : 789423, cur_epsilon : 0.45\n",
      "episode : 31400, reward mean : 0.47714204361923274, total_step : 789599, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31400, reward mean : 0.7140000041574239, step mean : 19.6, eval_episode_rewards : [ 0.82  0.71 -1.01  0.79  0.41  0.89  0.96  0.76  0.89  0.74  0.77  0.93\n",
      "  0.79  0.9   0.61  0.93  0.95  0.93  0.65  0.86]\n",
      "episode : 31410, reward mean : 0.4771862518205997, total_step : 789793, cur_epsilon : 0.45\n",
      "episode : 31420, reward mean : 0.4771699608441734, total_step : 789977, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 31430, reward mean : 0.47724785777238515, total_step : 790265, cur_epsilon : 0.45\n",
      "episode : 31440, reward mean : 0.4773094837727385, total_step : 790404, cur_epsilon : 0.45\n",
      "episode : 31450, reward mean : 0.47740859045666334, total_step : 790625, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31450, reward mean : 0.6390000035986304, step mean : 17.1, eval_episode_rewards : [ 0.88  0.93  0.86 -1.23  0.94 -1.1   0.7   0.99  0.82  0.97  0.89  0.64\n",
      "  0.97  0.68  0.85  0.68  0.72  0.96  0.87  0.76]\n",
      "episode : 31460, reward mean : 0.4774154535896167, total_step : 790936, cur_epsilon : 0.45\n",
      "episode : 31470, reward mean : 0.4774985754683088, total_step : 791207, cur_epsilon : 0.45\n",
      "episode : 31480, reward mean : 0.47762357592174437, total_step : 791346, cur_epsilon : 0.45\n",
      "episode : 31490, reward mean : 0.47765703938035464, total_step : 791672, cur_epsilon : 0.45\n",
      "episode : 31500, reward mean : 0.4776647673064754, total_step : 791980, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31500, reward mean : 0.7395000058226288, step mean : 27.05, eval_episode_rewards : [0.63 0.77 0.12 0.92 0.99 0.79 0.83 0.96 0.75 0.94 0.79 0.94 0.85 0.68\n",
      " 0.41 0.62 0.59 0.38 0.99 0.84]\n",
      "episode : 31510, reward mean : 0.4776340898191979, total_step : 792209, cur_epsilon : 0.45\n",
      "episode : 31520, reward mean : 0.47758725159464704, total_step : 792489, cur_epsilon : 0.45\n",
      "episode : 31530, reward mean : 0.47750523851264615, total_step : 792680, cur_epsilon : 0.45\n",
      "episode : 31540, reward mean : 0.47754851522971803, total_step : 792876, cur_epsilon : 0.45\n",
      "episode : 31550, reward mean : 0.4776684681577652, total_step : 793030, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31550, reward mean : 0.6435000057332217, step mean : 26.65, eval_episode_rewards : [ 0.72  0.88  0.49  0.64  0.9   0.82  0.73  0.78  0.84  0.45  0.78  0.58\n",
      "  0.78  0.79  0.57 -1.1   0.79  0.88  0.87  0.68]\n",
      "episode : 31560, reward mean : 0.4777027937397125, total_step : 793254, cur_epsilon : 0.45\n",
      "episode : 31570, reward mean : 0.47771904245933466, total_step : 793335, cur_epsilon : 0.45\n",
      "episode : 31580, reward mean : 0.47776567987588064, total_step : 793520, cur_epsilon : 0.45\n",
      "episode : 31590, reward mean : 0.4778018414221843, total_step : 793738, cur_epsilon : 0.45\n",
      "episode : 31600, reward mean : 0.47789494210689126, total_step : 793976, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31600, reward mean : 0.5920000057667494, step mean : 26.75, eval_episode_rewards : [ 0.56  0.84  0.74  0.82  0.83  0.96  0.97  0.89  0.98  0.93  0.91 -1.29\n",
      "  0.6   0.61  0.62  0.38  0.85  0.92 -1.    0.72]\n",
      "episode : 31610, reward mean : 0.47800127081997273, total_step : 794172, cur_epsilon : 0.45\n",
      "episode : 31620, reward mean : 0.47805218756025697, total_step : 794343, cur_epsilon : 0.45\n",
      "episode : 31630, reward mean : 0.4780758827287554, total_step : 794600, cur_epsilon : 0.45\n",
      "episode : 31640, reward mean : 0.47799178795046005, total_step : 794798, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 31650, reward mean : 0.47807836242679264, total_step : 795056, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31650, reward mean : 0.6625000030733645, step mean : 14.75, eval_episode_rewards : [ 0.6   0.82  0.83  0.96  0.96  0.98  0.96  0.94  0.91  0.86  0.94  0.77\n",
      "  0.68  0.92 -1.15  0.74  0.81  0.76  0.99 -1.03]\n",
      "episode : 31660, reward mean : 0.4780514899195911, total_step : 795273, cur_epsilon : 0.45\n",
      "episode : 31670, reward mean : 0.478125360622628, total_step : 795571, cur_epsilon : 0.45\n",
      "episode : 31680, reward mean : 0.47823232862877313, total_step : 795764, cur_epsilon : 0.45\n",
      "episode : 31690, reward mean : 0.4783329179869014, total_step : 795977, cur_epsilon : 0.45\n",
      "episode : 31700, reward mean : 0.47832240287261524, total_step : 796241, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31700, reward mean : 0.775500005017966, step mean : 23.45, eval_episode_rewards : [0.84 0.91 0.94 0.81 0.93 0.87 0.43 0.69 0.89 0.78 0.48 0.86 0.9  0.6\n",
      " 0.98 0.52 0.98 0.83 0.53 0.74]\n",
      "episode : 31710, reward mean : 0.4783396458884132, total_step : 796518, cur_epsilon : 0.45\n",
      "episode : 31720, reward mean : 0.478458076013689, total_step : 796674, cur_epsilon : 0.45\n",
      "episode : 31730, reward mean : 0.4784998478158355, total_step : 796873, cur_epsilon : 0.45\n",
      "episode : 31740, reward mean : 0.4785781402412565, total_step : 797156, cur_epsilon : 0.45\n",
      "episode : 31750, reward mean : 0.4786321313792444, total_step : 797316, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31750, reward mean : 0.4195000040344894, step mean : 19.05, eval_episode_rewards : [ 0.87  0.99  0.8   0.96  0.86 -1.17  0.79  0.43  0.88  0.58  0.43  0.79\n",
      " -1.04 -1.33  0.83 -1.02  0.94  0.99  0.92  0.89]\n",
      "episode : 31760, reward mean : 0.47874654191833516, total_step : 797484, cur_epsilon : 0.45\n",
      "episode : 31770, reward mean : 0.47865345204201537, total_step : 797711, cur_epsilon : 0.45\n",
      "episode : 31780, reward mean : 0.4786309053309618, total_step : 797914, cur_epsilon : 0.45\n",
      "episode : 31790, reward mean : 0.4786870138864401, total_step : 798067, cur_epsilon : 0.45\n",
      "episode : 31800, reward mean : 0.47868396765657895, total_step : 798208, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31800, reward mean : 0.521500005107373, step mean : 23.8, eval_episode_rewards : [ 0.66  0.57  0.68  0.83  0.86 -1.22 -1.    0.74  0.82  0.91  0.88 -1.15\n",
      "  0.98  0.86  0.94  0.76  0.8   0.77  0.79  0.95]\n",
      "episode : 31810, reward mean : 0.4786705492460191, total_step : 798382, cur_epsilon : 0.45\n",
      "episode : 31820, reward mean : 0.478706479307421, total_step : 798599, cur_epsilon : 0.45\n",
      "episode : 31830, reward mean : 0.4787995655548987, total_step : 798834, cur_epsilon : 0.45\n",
      "episode : 31840, reward mean : 0.4789177189586946, total_step : 798989, cur_epsilon : 0.45\n",
      "episode : 31850, reward mean : 0.4789051859240447, total_step : 799160, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31850, reward mean : 0.591000004671514, step mean : 21.9, eval_episode_rewards : [-1.61  0.84  0.93  0.83 -1.01  0.89  0.96  0.63  0.98  0.64  0.92  0.8\n",
      "  0.86  0.73  0.91  0.29  0.9   0.8   0.76  0.77]\n",
      "episode : 31860, reward mean : 0.4789993776437608, total_step : 799391, cur_epsilon : 0.45\n",
      "episode : 31870, reward mean : 0.479066839402382, total_step : 799507, cur_epsilon : 0.45\n",
      "episode : 31880, reward mean : 0.4791113604703238, total_step : 799696, cur_epsilon : 0.45\n",
      "episode : 31890, reward mean : 0.47921167048726404, total_step : 799907, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 31900, reward mean : 0.4793141119712292, total_step : 800111, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31900, reward mean : 0.7305000037886202, step mean : 17.95, eval_episode_rewards : [ 0.92  0.95  0.97  0.91 -1.21  0.98  0.86  0.85  0.94  0.94  0.49  0.47\n",
      "  0.88  0.93  0.94  0.89  0.89  0.49  0.62  0.9 ]\n",
      "episode : 31910, reward mean : 0.4792764077698552, total_step : 800362, cur_epsilon : 0.45\n",
      "episode : 31920, reward mean : 0.4792167973671958, total_step : 800483, cur_epsilon : 0.45\n",
      "episode : 31930, reward mean : 0.4793041081119631, total_step : 800735, cur_epsilon : 0.45\n",
      "episode : 31940, reward mean : 0.47928334915644266, total_step : 800932, cur_epsilon : 0.45\n",
      "episode : 31950, reward mean : 0.4790982839462226, total_step : 801054, cur_epsilon : 0.45\n",
      "[EVAL] episode : 31950, reward mean : 0.6865000047720968, step mean : 22.35, eval_episode_rewards : [ 0.69  0.73  0.86  0.52  0.88  0.81  0.85  0.97  0.76  0.67  0.59  0.95\n",
      "  0.96  0.74  0.81  0.71  0.81  0.82  0.7  -1.1 ]\n",
      "episode : 31960, reward mean : 0.47905507422214344, total_step : 801323, cur_epsilon : 0.45\n",
      "episode : 31970, reward mean : 0.47913075296221663, total_step : 801612, cur_epsilon : 0.45\n",
      "episode : 31980, reward mean : 0.47904034309721716, total_step : 801832, cur_epsilon : 0.45\n",
      "episode : 31990, reward mean : 0.4790125092934024, total_step : 802052, cur_epsilon : 0.45\n",
      "episode : 32000, reward mean : 0.47906000538542864, total_step : 802231, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32000, reward mean : 0.6200000040233136, step mean : 19.0, eval_episode_rewards : [ 0.66  0.85  0.55 -1.57  0.91  0.65  0.9   0.95  0.89  0.95  0.98  0.9\n",
      "  0.61  0.84  0.94  0.69  0.86 -1.02  0.92  0.94]\n",
      "episode : 32010, reward mean : 0.4791134074466493, total_step : 802391, cur_epsilon : 0.45\n",
      "episode : 32020, reward mean : 0.47912680113770717, total_step : 802679, cur_epsilon : 0.45\n",
      "episode : 32030, reward mean : 0.4791136488437657, total_step : 802852, cur_epsilon : 0.45\n",
      "episode : 32040, reward mean : 0.47916323884213563, total_step : 803024, cur_epsilon : 0.45\n",
      "episode : 32050, reward mean : 0.479274264353788, total_step : 803199, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32050, reward mean : 0.6320000037550926, step mean : 17.8, eval_episode_rewards : [ 0.92  0.82  0.98  0.83  0.91  0.75  0.91  0.95 -1.1   0.92  0.62  0.77\n",
      "  0.89  0.41  0.97 -1.04  0.89  0.55  0.88  0.81]\n",
      "episode : 32060, reward mean : 0.47931160862706906, total_step : 803410, cur_epsilon : 0.45\n",
      "episode : 32070, reward mean : 0.47927939422065025, total_step : 803743, cur_epsilon : 0.45\n",
      "episode : 32080, reward mean : 0.4793846687249737, total_step : 803936, cur_epsilon : 0.45\n",
      "episode : 32090, reward mean : 0.4794091671159629, total_step : 804188, cur_epsilon : 0.45\n",
      "episode : 32100, reward mean : 0.4793993823297577, total_step : 804350, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32100, reward mean : 0.7280000038444996, step mean : 18.2, eval_episode_rewards : [ 0.85  0.98  0.43  0.79  0.98  0.71  0.77  0.77  0.94  0.74  0.88  0.93\n",
      "  0.87  0.82  0.93  0.88  0.83 -1.11  0.81  0.76]\n",
      "episode : 32110, reward mean : 0.47944566094125407, total_step : 804532, cur_epsilon : 0.45\n",
      "episode : 32120, reward mean : 0.479541723937455, total_step : 804754, cur_epsilon : 0.45\n",
      "episode : 32130, reward mean : 0.47960598110477853, total_step : 804878, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 32140, reward mean : 0.4797084683552964, total_step : 805079, cur_epsilon : 0.45\n",
      "episode : 32150, reward mean : 0.4798183568577466, total_step : 805256, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32150, reward mean : 0.6095000042580068, step mean : 20.05, eval_episode_rewards : [ 0.95  0.89  0.76  0.45  0.76  0.95  0.98  0.92 -1.1   0.88  0.45  0.6\n",
      "  0.91  0.94 -1.02  0.68  0.89  0.87  0.7   0.73]\n",
      "episode : 32160, reward mean : 0.47991418448457523, total_step : 805478, cur_epsilon : 0.45\n",
      "episode : 32170, reward mean : 0.479889654120871, total_step : 805687, cur_epsilon : 0.45\n",
      "episode : 32180, reward mean : 0.4799906828188939, total_step : 805892, cur_epsilon : 0.45\n",
      "episode : 32190, reward mean : 0.4799757742526952, total_step : 806270, cur_epsilon : 0.45\n",
      "episode : 32200, reward mean : 0.4800177072433419, total_step : 806465, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32200, reward mean : 0.6050000043585897, step mean : 20.5, eval_episode_rewards : [ 0.86  0.98  0.59  0.75  0.59 -1.31  0.66  0.88  0.96  0.57 -1.04  0.89\n",
      "  0.96  0.83  0.84  0.66  0.99  0.75  0.72  0.97]\n",
      "episode : 32210, reward mean : 0.47992735713389545, total_step : 806686, cur_epsilon : 0.45\n",
      "episode : 32220, reward mean : 0.4799090680733328, total_step : 806875, cur_epsilon : 0.45\n",
      "episode : 32230, reward mean : 0.47989823684013116, total_step : 807040, cur_epsilon : 0.45\n",
      "episode : 32240, reward mean : 0.47999194086247376, total_step : 807268, cur_epsilon : 0.45\n",
      "episode : 32250, reward mean : 0.48003132320773, total_step : 807471, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32250, reward mean : 0.6090000042691827, step mean : 20.1, eval_episode_rewards : [-1.02  0.9   0.83  0.92  0.67  0.58  0.9   0.82  0.82  0.68  0.9   0.92\n",
      "  0.82  0.91 -1.24  0.67  0.96  0.43  0.88  0.83]\n",
      "episode : 32260, reward mean : 0.4799624976280543, total_step : 807623, cur_epsilon : 0.45\n",
      "episode : 32270, reward mean : 0.47994453590086206, total_step : 807811, cur_epsilon : 0.45\n",
      "episode : 32280, reward mean : 0.47981351219184576, total_step : 807964, cur_epsilon : 0.45\n",
      "episode : 32290, reward mean : 0.479869624452891, total_step : 808113, cur_epsilon : 0.45\n",
      "episode : 32300, reward mean : 0.47992074840924137, total_step : 808278, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32300, reward mean : 0.572500002849847, step mean : 13.75, eval_episode_rewards : [-1.21  0.91  0.81  0.82  0.73 -1.07  0.91  0.96  0.93  0.96  0.85  0.89\n",
      "  0.81  0.95 -1.01  0.93  0.72  0.96  0.87  0.73]\n",
      "episode : 32310, reward mean : 0.47985670608694625, total_step : 808514, cur_epsilon : 0.45\n",
      "episode : 32320, reward mean : 0.4799684459685883, total_step : 808683, cur_epsilon : 0.45\n",
      "episode : 32330, reward mean : 0.48007145603917634, total_step : 808880, cur_epsilon : 0.45\n",
      "episode : 32340, reward mean : 0.4800732892322836, total_step : 809004, cur_epsilon : 0.45\n",
      "episode : 32350, reward mean : 0.4800717209829982, total_step : 809139, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32350, reward mean : 0.7050000043585897, step mean : 20.5, eval_episode_rewards : [ 0.97  0.75  0.92  0.8   0.52  0.8  -1.3   0.64  0.94  0.72  0.75  0.99\n",
      "  0.74  0.88  0.93  0.51  0.74  0.96  0.91  0.93]\n",
      "episode : 32360, reward mean : 0.48016069758503854, total_step : 809381, cur_epsilon : 0.45\n",
      "episode : 32370, reward mean : 0.4801427301171342, total_step : 809569, cur_epsilon : 0.45\n",
      "episode : 32380, reward mean : 0.4801667749828822, total_step : 809821, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 32390, reward mean : 0.48013770219183566, total_step : 810045, cur_epsilon : 0.45\n",
      "episode : 32400, reward mean : 0.4801367337674615, total_step : 810378, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32400, reward mean : 0.5565000054426491, step mean : 25.35, eval_episode_rewards : [ 0.77 -1.56  0.82  0.74  0.73  0.64  0.89  0.93  0.96  0.93  0.67  0.94\n",
      "  0.74 -1.63  0.68  0.81  0.92  0.23  0.98  0.94]\n",
      "episode : 32410, reward mean : 0.48007097112297076, total_step : 810521, cur_epsilon : 0.45\n",
      "episode : 32420, reward mean : 0.48012554516124134, total_step : 810674, cur_epsilon : 0.45\n",
      "episode : 32430, reward mean : 0.4801572671654625, total_step : 810901, cur_epsilon : 0.45\n",
      "episode : 32440, reward mean : 0.4801769474177919, total_step : 811167, cur_epsilon : 0.45\n",
      "episode : 32450, reward mean : 0.4800536263274898, total_step : 811596, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32450, reward mean : 0.6165000041015446, step mean : 19.35, eval_episode_rewards : [ 0.78  0.87  0.73  0.83  0.78  0.92 -1.02  0.73  0.81  0.59  0.88  0.96\n",
      "  0.67 -1.04  0.81  0.83  0.77  0.84  0.65  0.94]\n",
      "episode : 32460, reward mean : 0.4801669801097049, total_step : 811758, cur_epsilon : 0.45\n",
      "episode : 32470, reward mean : 0.48025562594434484, total_step : 812000, cur_epsilon : 0.45\n",
      "episode : 32480, reward mean : 0.4802401531542757, total_step : 812180, cur_epsilon : 0.45\n",
      "episode : 32490, reward mean : 0.4802139173436964, total_step : 812395, cur_epsilon : 0.45\n",
      "episode : 32500, reward mean : 0.48024369767835506, total_step : 812628, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32500, reward mean : 0.671500005107373, step mean : 23.85, eval_episode_rewards : [ 0.73  0.94  0.84  0.78  0.78  0.82  0.81  0.65  0.6   0.73  0.91  0.34\n",
      " -1.01  0.94  0.89  0.59  0.57  0.92  0.81  0.79]\n",
      "episode : 32510, reward mean : 0.4802254744566764, total_step : 812817, cur_epsilon : 0.45\n",
      "episode : 32520, reward mean : 0.4802583079530799, total_step : 813040, cur_epsilon : 0.45\n",
      "episode : 32530, reward mean : 0.4803544474233147, total_step : 813257, cur_epsilon : 0.45\n",
      "episode : 32540, reward mean : 0.48040965503109156, total_step : 813407, cur_epsilon : 0.45\n",
      "episode : 32550, reward mean : 0.4805044600540363, total_step : 813628, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32550, reward mean : 0.7665000029839575, step mean : 14.35, eval_episode_rewards : [ 0.98  0.72  0.92  0.94  0.92  0.87  0.85  0.9   0.95  0.97  0.62  0.86\n",
      "  0.84  0.77 -1.03  0.83  0.85  0.91  0.68  0.98]\n",
      "episode : 32560, reward mean : 0.4804772780959881, total_step : 813846, cur_epsilon : 0.45\n",
      "episode : 32570, reward mean : 0.48047037687556376, total_step : 813998, cur_epsilon : 0.45\n",
      "episode : 32580, reward mean : 0.48045457872545533, total_step : 814179, cur_epsilon : 0.45\n",
      "episode : 32590, reward mean : 0.48038478597447654, total_step : 814336, cur_epsilon : 0.45\n",
      "episode : 32600, reward mean : 0.4803730115020202, total_step : 814603, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32600, reward mean : 0.7995000044815243, step mean : 21.05, eval_episode_rewards : [0.28 0.88 0.82 0.87 0.94 0.62 0.69 0.94 0.6  0.96 0.64 0.78 0.9  0.68\n",
      " 0.96 0.99 0.95 0.66 0.95 0.88]\n",
      "episode : 32610, reward mean : 0.4803992693964478, total_step : 814847, cur_epsilon : 0.45\n",
      "episode : 32620, reward mean : 0.48040374540287034, total_step : 814962, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 32630, reward mean : 0.480496787468278, total_step : 815188, cur_epsilon : 0.45\n",
      "episode : 32640, reward mean : 0.4805242087972218, total_step : 815428, cur_epsilon : 0.45\n",
      "episode : 32650, reward mean : 0.4804980145539706, total_step : 815643, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32650, reward mean : 0.7350000036880374, step mean : 17.5, eval_episode_rewards : [ 0.86  0.99  0.54  0.72  0.78  0.91  0.94 -1.01  0.91  0.97  0.9   0.9\n",
      "  0.97  0.96  0.93  0.73  0.77  0.95  0.41  0.57]\n",
      "episode : 32660, reward mean : 0.480528786749421, total_step : 815872, cur_epsilon : 0.45\n",
      "episode : 32670, reward mean : 0.48058433349454965, total_step : 816020, cur_epsilon : 0.45\n",
      "episode : 32680, reward mean : 0.4805783407373931, total_step : 816169, cur_epsilon : 0.45\n",
      "episode : 32690, reward mean : 0.4806757471808789, total_step : 816380, cur_epsilon : 0.45\n",
      "episode : 32700, reward mean : 0.4807342561275701, total_step : 816518, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32700, reward mean : 0.7180000063031912, step mean : 29.2, eval_episode_rewards : [0.56 0.66 0.8  0.91 0.99 0.99 0.66 0.92 0.27 0.69 0.47 0.73 0.9  0.26\n",
      " 0.33 0.76 0.93 0.9  0.77 0.86]\n",
      "episode : 32710, reward mean : 0.4807597118747873, total_step : 816764, cur_epsilon : 0.45\n",
      "episode : 32720, reward mean : 0.4808037950936249, total_step : 816949, cur_epsilon : 0.45\n",
      "episode : 32730, reward mean : 0.4808823762762594, total_step : 817221, cur_epsilon : 0.45\n",
      "episode : 32740, reward mean : 0.4809392234438515, total_step : 817364, cur_epsilon : 0.45\n",
      "episode : 32750, reward mean : 0.4810497763537951, total_step : 817531, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32750, reward mean : 0.5785000049509108, step mean : 23.15, eval_episode_rewards : [ 0.97  0.9   0.51  0.95  0.93  0.92  0.62 -1.54  0.98  0.68  0.83  0.94\n",
      "  0.86  0.72  0.6  -1.53  0.87  0.52  0.98  0.86]\n",
      "episode : 32760, reward mean : 0.48107265493410645, total_step : 817785, cur_epsilon : 0.45\n",
      "episode : 32770, reward mean : 0.48106134194922784, total_step : 817951, cur_epsilon : 0.45\n",
      "episode : 32780, reward mean : 0.4810311218952082, total_step : 818179, cur_epsilon : 0.45\n",
      "episode : 32790, reward mean : 0.4810137290572313, total_step : 818464, cur_epsilon : 0.45\n",
      "episode : 32800, reward mean : 0.4809594565808841, total_step : 818771, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32800, reward mean : 0.7045000043697656, step mean : 20.55, eval_episode_rewards : [ 0.46  0.75  0.89  0.48  0.93  0.91  0.99  0.96  0.98  0.59  0.99  0.32\n",
      "  0.85  0.71  0.94  0.73 -1.02  0.86  0.93  0.84]\n",
      "episode : 32810, reward mean : 0.48105090447735294, total_step : 819000, cur_epsilon : 0.45\n",
      "episode : 32820, reward mean : 0.48112188226587066, total_step : 819296, cur_epsilon : 0.45\n",
      "episode : 32830, reward mean : 0.4811574832778259, total_step : 819508, cur_epsilon : 0.45\n",
      "episode : 32840, reward mean : 0.4811470820977444, total_step : 819871, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 32850, reward mean : 0.48125053808614726, total_step : 820060, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32850, reward mean : 0.6780000027269125, step mean : 13.2, eval_episode_rewards : [-1.05  0.88  0.86  0.88  0.9   0.84  0.93  0.8   0.83  0.9   0.99  0.95\n",
      "  0.78  0.95 -1.16  0.94  0.79  0.98  0.82  0.75]\n",
      "episode : 32860, reward mean : 0.4813606261766594, total_step : 820227, cur_epsilon : 0.45\n",
      "episode : 32870, reward mean : 0.4814049338668603, total_step : 820410, cur_epsilon : 0.45\n",
      "episode : 32880, reward mean : 0.48150365499534853, total_step : 820614, cur_epsilon : 0.45\n",
      "episode : 32890, reward mean : 0.4815466760804689, total_step : 820801, cur_epsilon : 0.45\n",
      "episode : 32900, reward mean : 0.48149362238016447, total_step : 820904, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32900, reward mean : 0.4230000050738454, step mean : 23.65, eval_episode_rewards : [-1.01  0.6  -1.11  0.95  0.85  0.7  -1.02  0.86  0.86  0.73  0.8   0.81\n",
      "  0.81  0.95 -1.    0.05  0.84  0.91  0.93  0.95]\n",
      "episode : 32910, reward mean : 0.48159526515796225, total_step : 821098, cur_epsilon : 0.45\n",
      "episode : 32920, reward mean : 0.48162971374223446, total_step : 821313, cur_epsilon : 0.45\n",
      "episode : 32930, reward mean : 0.48159217055719555, total_step : 821565, cur_epsilon : 0.45\n",
      "episode : 32940, reward mean : 0.4816050448242301, total_step : 821851, cur_epsilon : 0.45\n",
      "episode : 32950, reward mean : 0.4816758778929846, total_step : 822146, cur_epsilon : 0.45\n",
      "[EVAL] episode : 32950, reward mean : 0.7385000036098063, step mean : 17.15, eval_episode_rewards : [ 0.47  0.8   0.35  0.97  0.92  0.96  0.91  0.82  0.87  0.83  0.95  0.94\n",
      "  0.83  0.97  0.87  0.86  0.92  0.94 -1.13  0.72]\n",
      "episode : 32960, reward mean : 0.48170176506753404, total_step : 822389, cur_epsilon : 0.45\n",
      "episode : 32970, reward mean : 0.4816797141846713, total_step : 822590, cur_epsilon : 0.45\n",
      "episode : 32980, reward mean : 0.4816534316772115, total_step : 822805, cur_epsilon : 0.45\n",
      "episode : 32990, reward mean : 0.48169263948944163, total_step : 823004, cur_epsilon : 0.45\n",
      "episode : 33000, reward mean : 0.48168273263001304, total_step : 823165, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33000, reward mean : 0.7655000030063093, step mean : 14.45, eval_episode_rewards : [ 0.94  0.96  0.97  0.7   0.94  0.78  0.42  0.86  0.91  0.75  0.94  0.97\n",
      "  0.99  0.91  0.99  0.91  0.81 -1.02  0.78  0.8 ]\n",
      "episode : 33010, reward mean : 0.48173614592009, total_step : 823317, cur_epsilon : 0.45\n",
      "episode : 33020, reward mean : 0.48181193751914214, total_step : 823595, cur_epsilon : 0.45\n",
      "episode : 33030, reward mean : 0.48179776496878457, total_step : 823770, cur_epsilon : 0.45\n",
      "episode : 33040, reward mean : 0.48176634918185757, total_step : 824002, cur_epsilon : 0.45\n",
      "episode : 33050, reward mean : 0.48180000535664735, total_step : 824318, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33050, reward mean : 0.6635000030510128, step mean : 14.65, eval_episode_rewards : [ 0.93  0.94  0.91  0.52 -1.21  0.95  0.85  0.86  0.97  0.83  0.98  0.81\n",
      "  0.84  0.92  0.7   0.94  0.66 -1.01  0.97  0.91]\n",
      "episode : 33060, reward mean : 0.48184211061939514, total_step : 824507, cur_epsilon : 0.45\n",
      "episode : 33070, reward mean : 0.4819029385274677, total_step : 824634, cur_epsilon : 0.45\n",
      "episode : 33080, reward mean : 0.481777212126158, total_step : 824778, cur_epsilon : 0.45\n",
      "episode : 33090, reward mean : 0.48169719483754037, total_step : 824971, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 33100, reward mean : 0.48166103254463466, total_step : 825219, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33100, reward mean : 0.6710000051185488, step mean : 23.9, eval_episode_rewards : [ 0.76  0.94  0.65  0.71  0.95  0.77  0.72  0.47  0.7   0.87  0.81  0.84\n",
      "  0.85  0.81  0.23  0.97  0.84  0.79 -1.03  0.77]\n",
      "episode : 33110, reward mean : 0.4816656652762046, total_step : 825532, cur_epsilon : 0.45\n",
      "episode : 33120, reward mean : 0.48174366477515845, total_step : 825802, cur_epsilon : 0.45\n",
      "episode : 33130, reward mean : 0.48176366367067835, total_step : 826064, cur_epsilon : 0.45\n",
      "episode : 33140, reward mean : 0.48172843021913403, total_step : 826309, cur_epsilon : 0.45\n",
      "episode : 33150, reward mean : 0.48178220746586403, total_step : 826459, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33150, reward mean : 0.5960000045597553, step mean : 21.4, eval_episode_rewards : [ 0.96  0.99  0.21  0.07  0.96  0.73 -1.07  0.64  0.87  0.77  0.96 -1.03\n",
      "  0.7   0.99  0.95  0.97  0.68  0.85  0.77  0.95]\n",
      "episode : 33160, reward mean : 0.4817430692867559, total_step : 826717, cur_epsilon : 0.45\n",
      "episode : 33170, reward mean : 0.48172174186286015, total_step : 826916, cur_epsilon : 0.45\n",
      "episode : 33180, reward mean : 0.48171881186316096, total_step : 827054, cur_epsilon : 0.45\n",
      "episode : 33190, reward mean : 0.4817472183690795, total_step : 827288, cur_epsilon : 0.45\n",
      "episode : 33200, reward mean : 0.4817936800513978, total_step : 827462, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33200, reward mean : 0.6460000056773424, step mean : 26.4, eval_episode_rewards : [ 0.88  0.61 -1.68  0.95  0.9   0.81  0.74  0.77  0.77  0.91  0.99  0.85\n",
      "  0.18  0.56  0.76  0.89  0.84  0.76  0.84  0.59]\n",
      "episode : 33210, reward mean : 0.4817422516642879, total_step : 827761, cur_epsilon : 0.45\n",
      "episode : 33220, reward mean : 0.4817071095070298, total_step : 828006, cur_epsilon : 0.45\n",
      "episode : 33230, reward mean : 0.481802894308403, total_step : 828216, cur_epsilon : 0.45\n",
      "episode : 33240, reward mean : 0.4817963350751963, total_step : 828366, cur_epsilon : 0.45\n",
      "episode : 33250, reward mean : 0.4817521858032709, total_step : 828641, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33250, reward mean : 0.6270000038668513, step mean : 18.3, eval_episode_rewards : [ 0.92  0.34 -1.02  0.87  0.93  0.53  0.84  0.7   0.97  0.98  0.94  0.97\n",
      "  0.91  0.68  0.99  0.88  0.94 -1.46  0.65  0.98]\n",
      "episode : 33260, reward mean : 0.4817817251355361, total_step : 828871, cur_epsilon : 0.45\n",
      "episode : 33270, reward mean : 0.4817445199297271, total_step : 829123, cur_epsilon : 0.45\n",
      "episode : 33280, reward mean : 0.4817310750630545, total_step : 829296, cur_epsilon : 0.45\n",
      "episode : 33290, reward mean : 0.4817732105178204, total_step : 829484, cur_epsilon : 0.45\n",
      "episode : 33300, reward mean : 0.48173784318891866, total_step : 829730, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33300, reward mean : 0.5395000035874545, step mean : 17.05, eval_episode_rewards : [ 0.86 -1.08 -1.01  0.88  0.7   0.54  0.95  0.91  0.92  0.58  0.94  0.94\n",
      " -1.24  0.87  0.95  0.92  0.63  0.86  0.84  0.83]\n",
      "episode : 33310, reward mean : 0.4817931605590196, total_step : 829874, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 33320, reward mean : 0.48177551555403675, total_step : 830061, cur_epsilon : 0.45\n",
      "episode : 33330, reward mean : 0.4818163869877343, total_step : 830253, cur_epsilon : 0.45\n",
      "episode : 33340, reward mean : 0.4818347384030783, total_step : 830520, cur_epsilon : 0.45\n",
      "episode : 33350, reward mean : 0.4819640233396724, total_step : 830617, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33350, reward mean : 0.3355000036768615, step mean : 17.45, eval_episode_rewards : [-1.11 -1.72  0.78  0.77  0.8   0.89  0.78  0.91  0.84 -1.13  0.77  0.92\n",
      "  0.84  0.93  0.91  0.94  0.95 -1.14  0.87 -1.09]\n",
      "episode : 33360, reward mean : 0.48189059287813274, total_step : 830790, cur_epsilon : 0.45\n",
      "episode : 33370, reward mean : 0.48196764094914135, total_step : 831061, cur_epsilon : 0.45\n",
      "episode : 33380, reward mean : 0.48200090408984764, total_step : 831278, cur_epsilon : 0.45\n",
      "episode : 33390, reward mean : 0.4820446294865996, total_step : 831460, cur_epsilon : 0.45\n",
      "episode : 33400, reward mean : 0.4820368316943038, total_step : 831614, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33400, reward mean : 0.7050000043585897, step mean : 20.5, eval_episode_rewards : [ 0.81  0.84  0.78  0.77  0.9   0.87  0.57  0.93  0.6   0.17  0.81 -1.07\n",
      "  0.91  0.66  0.84  0.99  0.94  0.95  0.89  0.94]\n",
      "episode : 33410, reward mean : 0.4820287392583704, total_step : 831769, cur_epsilon : 0.45\n",
      "episode : 33420, reward mean : 0.4819781621390277, total_step : 832066, cur_epsilon : 0.45\n",
      "episode : 33430, reward mean : 0.48206701103008387, total_step : 832297, cur_epsilon : 0.45\n",
      "episode : 33440, reward mean : 0.48209928764302157, total_step : 832517, cur_epsilon : 0.45\n",
      "episode : 33450, reward mean : 0.4820923820286655, total_step : 832868, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33450, reward mean : 0.7215000039897859, step mean : 18.85, eval_episode_rewards : [ 0.95  0.98  0.67  0.83  0.97  0.42 -1.07  0.81  0.61  0.92  0.97  0.82\n",
      "  0.96  0.88  0.89  0.49  0.97  0.91  0.54  0.91]\n",
      "episode : 33460, reward mean : 0.48214017271052534, total_step : 833036, cur_epsilon : 0.45\n",
      "episode : 33470, reward mean : 0.48226352491537633, total_step : 833151, cur_epsilon : 0.45\n",
      "episode : 33480, reward mean : 0.48214367320622, total_step : 833280, cur_epsilon : 0.45\n",
      "episode : 33490, reward mean : 0.48218991277935735, total_step : 833453, cur_epsilon : 0.45\n",
      "episode : 33500, reward mean : 0.48221881131433064, total_step : 833684, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33500, reward mean : 0.6995000044815243, step mean : 21.05, eval_episode_rewards : [ 0.77  0.81  0.93  0.94  0.68  0.66  0.96  0.88  0.84  0.9   0.94  0.5\n",
      "  0.86 -1.11  0.3   0.77  0.72  0.99  0.71  0.94]\n",
      "episode : 33510, reward mean : 0.48230170632896807, total_step : 833934, cur_epsilon : 0.45\n",
      "episode : 33520, reward mean : 0.4823078812395616, total_step : 834241, cur_epsilon : 0.45\n",
      "episode : 33530, reward mean : 0.48239159496579415, total_step : 834488, cur_epsilon : 0.45\n",
      "episode : 33540, reward mean : 0.48236613533835576, total_step : 834701, cur_epsilon : 0.45\n",
      "episode : 33550, reward mean : 0.48242563872652244, total_step : 834829, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33550, reward mean : 0.6380000036209822, step mean : 17.2, eval_episode_rewards : [ 0.85  0.93  0.96  0.64  0.91  0.95  0.96  0.93  0.93  0.69  0.73  0.92\n",
      "  0.74  0.82  0.84 -1.02 -1.44  0.89  0.78  0.75]\n",
      "synced target net\n",
      "episode : 33560, reward mean : 0.48243921869295664, total_step : 835111, cur_epsilon : 0.45\n",
      "episode : 33570, reward mean : 0.48248943042505, total_step : 835270, cur_epsilon : 0.45\n",
      "episode : 33580, reward mean : 0.48246694995271194, total_step : 835473, cur_epsilon : 0.45\n",
      "episode : 33590, reward mean : 0.4825046198110973, total_step : 835674, cur_epsilon : 0.45\n",
      "episode : 33600, reward mean : 0.4825514934372179, total_step : 835844, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33600, reward mean : 0.5865000047720969, step mean : 22.35, eval_episode_rewards : [-1.19  0.71  0.88  0.93  0.73  0.98 -1.11  0.83  0.8   0.57  0.94  0.98\n",
      "  0.95  0.5   0.79  0.65  0.91  0.31  0.79  0.78]\n",
      "episode : 33610, reward mean : 0.4825941737438088, total_step : 836028, cur_epsilon : 0.45\n",
      "episode : 33620, reward mean : 0.48267906542478495, total_step : 836270, cur_epsilon : 0.45\n",
      "episode : 33630, reward mean : 0.4827692589839162, total_step : 836494, cur_epsilon : 0.45\n",
      "episode : 33640, reward mean : 0.4827452490984946, total_step : 836702, cur_epsilon : 0.45\n",
      "episode : 33650, reward mean : 0.48277712272571466, total_step : 836922, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33650, reward mean : 0.575500005017966, step mean : 23.45, eval_episode_rewards : [ 0.69  0.81  0.47  0.98  0.99  0.72  0.97  0.91  0.83  0.9   0.46  0.89\n",
      "  0.51  0.93  0.49  0.52 -1.09 -1.15  0.77  0.91]\n",
      "episode : 33660, reward mean : 0.48282086095605015, total_step : 837201, cur_epsilon : 0.45\n",
      "episode : 33670, reward mean : 0.4827968571376568, total_step : 837409, cur_epsilon : 0.45\n",
      "episode : 33680, reward mean : 0.48284353265619556, total_step : 837579, cur_epsilon : 0.45\n",
      "episode : 33690, reward mean : 0.4828198331820926, total_step : 837786, cur_epsilon : 0.45\n",
      "episode : 33700, reward mean : 0.4827623198792139, total_step : 837907, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33700, reward mean : 0.5765000049956143, step mean : 23.35, eval_episode_rewards : [ 0.7   0.93  0.63  0.16  0.85  0.75  0.85 -1.61  0.79 -1.01  0.86  0.94\n",
      "  0.83  0.83  0.98  0.66  0.7   0.88  0.96  0.85]\n",
      "episode : 33710, reward mean : 0.48269356808024816, total_step : 838165, cur_epsilon : 0.45\n",
      "episode : 33720, reward mean : 0.48265510616964336, total_step : 838422, cur_epsilon : 0.45\n",
      "episode : 33730, reward mean : 0.48261340587305196, total_step : 838690, cur_epsilon : 0.45\n",
      "episode : 33740, reward mean : 0.482590106109717, total_step : 838896, cur_epsilon : 0.45\n",
      "episode : 33750, reward mean : 0.4824515608943171, total_step : 839091, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33750, reward mean : 0.5235000050626695, step mean : 23.6, eval_episode_rewards : [ 0.48  0.93 -1.    0.8   0.9   0.92  0.33  0.92  0.97  0.94  0.82  0.7\n",
      "  0.96  0.82  0.72 -1.02 -1.03  0.89  0.99  0.43]\n",
      "episode : 33760, reward mean : 0.48243602429565163, total_step : 839271, cur_epsilon : 0.45\n",
      "episode : 33770, reward mean : 0.4824104287908439, total_step : 839485, cur_epsilon : 0.45\n",
      "episode : 33780, reward mean : 0.4824511598676549, total_step : 839774, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 33790, reward mean : 0.48224919148850065, total_step : 840083, cur_epsilon : 0.45\n",
      "episode : 33800, reward mean : 0.48227929527942953, total_step : 840309, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33800, reward mean : 0.575500005017966, step mean : 23.45, eval_episode_rewards : [ 0.8   0.89  0.9   0.87  0.73  0.84  0.82  0.86  0.97  0.3   0.61  0.72\n",
      "  0.42  0.69  0.89  0.9  -1.23  0.88  0.8  -1.15]\n",
      "episode : 33810, reward mean : 0.4823161839836692, total_step : 840512, cur_epsilon : 0.45\n",
      "episode : 33820, reward mean : 0.48237434005070673, total_step : 840643, cur_epsilon : 0.45\n",
      "episode : 33830, reward mean : 0.4823378711377807, total_step : 840993, cur_epsilon : 0.45\n",
      "episode : 33840, reward mean : 0.4823664355981241, total_step : 841224, cur_epsilon : 0.45\n",
      "episode : 33850, reward mean : 0.482415662649166, total_step : 841385, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33850, reward mean : 0.44150000689551233, step mean : 31.8, eval_episode_rewards : [ 0.75  0.9   0.94  0.76  0.7   0.91  0.62 -1.28  0.86  0.13  0.88  0.36\n",
      "  0.81 -1.15  0.7   0.86  0.81  0.92 -1.    0.35]\n",
      "episode : 33860, reward mean : 0.48246161195246545, total_step : 841557, cur_epsilon : 0.45\n",
      "episode : 33870, reward mean : 0.48250605789171863, total_step : 841934, cur_epsilon : 0.45\n",
      "episode : 33880, reward mean : 0.482546640520447, total_step : 842124, cur_epsilon : 0.45\n",
      "episode : 33890, reward mean : 0.48252257836757817, total_step : 842333, cur_epsilon : 0.45\n",
      "episode : 33900, reward mean : 0.48262684899450975, total_step : 842507, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33900, reward mean : 0.7885000024922192, step mean : 12.15, eval_episode_rewards : [ 0.77  0.97  0.92  0.95  0.97  0.84  0.98  0.92  0.75  0.95  0.98  0.77\n",
      "  0.6   0.87  0.84  0.95  0.92  0.92  0.91 -1.01]\n",
      "episode : 33910, reward mean : 0.48268328460591997, total_step : 842843, cur_epsilon : 0.45\n",
      "episode : 33920, reward mean : 0.48271610203516485, total_step : 843059, cur_epsilon : 0.45\n",
      "episode : 33930, reward mean : 0.48268582909171726, total_step : 843289, cur_epsilon : 0.45\n",
      "episode : 33940, reward mean : 0.4826729576051253, total_step : 843460, cur_epsilon : 0.45\n",
      "episode : 33950, reward mean : 0.4827617137309641, total_step : 843686, cur_epsilon : 0.45\n",
      "[EVAL] episode : 33950, reward mean : 0.5215000039897859, step mean : 18.85, eval_episode_rewards : [-1.28  0.89 -1.03  0.61  0.94  0.87  0.89  0.87  0.64  0.96 -1.24  0.99\n",
      "  0.84  0.98  0.91  0.75  0.71  0.85  0.77  0.51]\n",
      "episode : 33960, reward mean : 0.4828086036867358, total_step : 843854, cur_epsilon : 0.45\n",
      "episode : 33970, reward mean : 0.48284310218564175, total_step : 844064, cur_epsilon : 0.45\n",
      "episode : 33980, reward mean : 0.4828837604851754, total_step : 844253, cur_epsilon : 0.45\n",
      "episode : 33990, reward mean : 0.4829764690006299, total_step : 844465, cur_epsilon : 0.45\n",
      "episode : 34000, reward mean : 0.48286471121758223, total_step : 844772, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34000, reward mean : 0.6710000051185488, step mean : 23.9, eval_episode_rewards : [ 0.88  0.74  0.79  0.78  0.87 -1.08  0.99  0.18  0.76  0.77  0.89  0.65\n",
      "  0.78  0.47  0.98  0.87  0.56  0.7   0.93  0.91]\n",
      "episode : 34010, reward mean : 0.4829103258286976, total_step : 844944, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 34020, reward mean : 0.48293915877440646, total_step : 845272, cur_epsilon : 0.45\n",
      "episode : 34030, reward mean : 0.482972088790806, total_step : 845487, cur_epsilon : 0.45\n",
      "episode : 34040, reward mean : 0.4830149877082653, total_step : 845668, cur_epsilon : 0.45\n",
      "episode : 34050, reward mean : 0.48306843411511086, total_step : 845813, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34050, reward mean : 0.6395000035874545, step mean : 17.05, eval_episode_rewards : [ 0.87  0.6   0.91 -1.08 -1.02  0.95  0.79  0.84  0.82  0.98  0.89  0.75\n",
      "  0.38  0.92  0.83  0.92  0.76  0.82  0.89  0.97]\n",
      "episode : 34060, reward mean : 0.4830601932369916, total_step : 845968, cur_epsilon : 0.45\n",
      "episode : 34070, reward mean : 0.4831640792981617, total_step : 846141, cur_epsilon : 0.45\n",
      "episode : 34080, reward mean : 0.4831992424217254, total_step : 846348, cur_epsilon : 0.45\n",
      "episode : 34090, reward mean : 0.4832540387727034, total_step : 846488, cur_epsilon : 0.45\n",
      "episode : 34100, reward mean : 0.4832873953609042, total_step : 846701, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34100, reward mean : 0.3045000054873526, step mean : 25.5, eval_episode_rewards : [-1.01  0.9   0.79  0.95  0.83  0.88  0.97  0.47  0.46  0.87 -1.49  0.99\n",
      " -1.07 -1.04  0.97  0.85  0.64  0.39 -1.    0.74]\n",
      "episode : 34110, reward mean : 0.4833394952165905, total_step : 846850, cur_epsilon : 0.45\n",
      "episode : 34120, reward mean : 0.48336929020767755, total_step : 847075, cur_epsilon : 0.45\n",
      "episode : 34130, reward mean : 0.4834462403147592, total_step : 847339, cur_epsilon : 0.45\n",
      "episode : 34140, reward mean : 0.48348887469188495, total_step : 847520, cur_epsilon : 0.45\n",
      "episode : 34150, reward mean : 0.48357687209456024, total_step : 847746, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34150, reward mean : 0.6365000036545098, step mean : 17.35, eval_episode_rewards : [ 0.86  0.96  0.69  0.61  0.83  0.35 -1.12 -1.01  0.87  0.76  0.66  0.95\n",
      "  0.91  0.99  0.77  0.96  0.93  0.97  0.87  0.92]\n",
      "episode : 34160, reward mean : 0.4835723121211574, total_step : 847888, cur_epsilon : 0.45\n",
      "episode : 34170, reward mean : 0.4835847287714364, total_step : 848172, cur_epsilon : 0.45\n",
      "episode : 34180, reward mean : 0.4835585190803401, total_step : 848388, cur_epsilon : 0.45\n",
      "episode : 34190, reward mean : 0.48347675291634495, total_step : 848594, cur_epsilon : 0.45\n",
      "episode : 34200, reward mean : 0.48349503456919907, total_step : 848858, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34200, reward mean : 0.691500002425164, step mean : 11.85, eval_episode_rewards : [ 0.74  0.81  0.96 -1.07 -1.16  0.99  0.87  0.93  0.89  0.98  0.95  0.87\n",
      "  0.92  0.65  0.93  0.95  0.92  0.9   0.98  0.82]\n",
      "episode : 34210, reward mean : 0.4835065823526401, total_step : 848945, cur_epsilon : 0.45\n",
      "episode : 34220, reward mean : 0.48348802403051, total_step : 849135, cur_epsilon : 0.45\n",
      "episode : 34230, reward mean : 0.4835395904865716, total_step : 849285, cur_epsilon : 0.45\n",
      "episode : 34240, reward mean : 0.4834956244870729, total_step : 849661, cur_epsilon : 0.45\n",
      "episode : 34250, reward mean : 0.4835243848901052, total_step : 849889, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34250, reward mean : 0.52200000397861, step mean : 18.8, eval_episode_rewards : [ 0.66  0.93  0.94  0.44 -1.08  0.84 -1.09  0.83  0.88  0.92  0.86  0.83\n",
      "  0.86  0.8   0.82  0.98  0.64  0.86 -1.05  0.57]\n",
      "synced target net\n",
      "episode : 34260, reward mean : 0.4835119726363585, total_step : 850058, cur_epsilon : 0.45\n",
      "episode : 34270, reward mean : 0.48359673716288754, total_step : 850294, cur_epsilon : 0.45\n",
      "episode : 34280, reward mean : 0.4836336109280812, total_step : 850494, cur_epsilon : 0.45\n",
      "episode : 34290, reward mean : 0.48368533632809557, total_step : 850843, cur_epsilon : 0.45\n",
      "episode : 34300, reward mean : 0.48368571961271434, total_step : 850968, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34300, reward mean : 0.8090000042691827, step mean : 20.1, eval_episode_rewards : [0.53 0.98 0.95 0.51 0.87 0.93 0.9  0.74 0.53 0.95 0.72 0.94 0.79 0.93\n",
      " 0.87 0.94 0.95 0.62 0.61 0.92]\n",
      "episode : 34310, reward mean : 0.48377703826177426, total_step : 851181, cur_epsilon : 0.45\n",
      "episode : 34320, reward mean : 0.4837677792196069, total_step : 851438, cur_epsilon : 0.45\n",
      "episode : 34330, reward mean : 0.4838456214061335, total_step : 851697, cur_epsilon : 0.45\n",
      "episode : 34340, reward mean : 0.4839545772540566, total_step : 851849, cur_epsilon : 0.45\n",
      "episode : 34350, reward mean : 0.484064343025775, total_step : 851998, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34350, reward mean : 0.7625000030733645, step mean : 14.75, eval_episode_rewards : [ 0.9   0.79  0.72  0.92  0.84  0.91  0.95 -1.27  0.56  0.85  0.81  0.88\n",
      "  0.94  0.99  0.77  0.89  0.99  0.97  0.88  0.96]\n",
      "episode : 34360, reward mean : 0.4841222404820001, total_step : 852125, cur_epsilon : 0.45\n",
      "episode : 34370, reward mean : 0.48415770098937694, total_step : 852329, cur_epsilon : 0.45\n",
      "episode : 34380, reward mean : 0.48421088374155363, total_step : 852472, cur_epsilon : 0.45\n",
      "episode : 34390, reward mean : 0.48417738246834097, total_step : 852713, cur_epsilon : 0.45\n",
      "episode : 34400, reward mean : 0.48415930764901255, total_step : 852901, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34400, reward mean : 0.39950000448152423, step mean : 21.05, eval_episode_rewards : [ 0.92  0.63  0.76 -1.03  0.9   0.97  0.95  0.77  0.9   0.64  0.71 -1.05\n",
      "  0.78 -1.1   0.81  0.6   0.88  0.58 -1.1   0.47]\n",
      "episode : 34410, reward mean : 0.4840819582437289, total_step : 853093, cur_epsilon : 0.45\n",
      "episode : 34420, reward mean : 0.4841635730162475, total_step : 853338, cur_epsilon : 0.45\n",
      "episode : 34430, reward mean : 0.4841376759589039, total_step : 853553, cur_epsilon : 0.45\n",
      "episode : 34440, reward mean : 0.48422677651892493, total_step : 853772, cur_epsilon : 0.45\n",
      "episode : 34450, reward mean : 0.4843021823909723, total_step : 854038, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34450, reward mean : 0.525000006146729, step mean : 28.5, eval_episode_rewards : [ 0.94  0.25  0.93  0.95  0.75  0.84  0.92  0.55  0.67  0.65 -1.03  0.34\n",
      "  0.87  0.7   0.76  0.78  0.69  0.52  0.79 -1.37]\n",
      "episode : 34460, reward mean : 0.4843908933086458, total_step : 854258, cur_epsilon : 0.45\n",
      "episode : 34470, reward mean : 0.48446214631494877, total_step : 854538, cur_epsilon : 0.45\n",
      "episode : 34480, reward mean : 0.484452151493904, total_step : 854698, cur_epsilon : 0.45\n",
      "episode : 34490, reward mean : 0.48455204939252056, total_step : 854879, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 34500, reward mean : 0.48455971546703275, total_step : 855178, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34500, reward mean : 0.726000003889203, step mean : 18.4, eval_episode_rewards : [ 0.83  0.5   0.87  0.93  0.87  0.97  0.73  0.88  0.95  0.55  0.87  0.9\n",
      "  0.74  0.71  0.8  -1.03  0.71  0.92  0.98  0.84]\n",
      "episode : 34510, reward mean : 0.4845540476293106, total_step : 855523, cur_epsilon : 0.45\n",
      "episode : 34520, reward mean : 0.48446698098886987, total_step : 855749, cur_epsilon : 0.45\n",
      "episode : 34530, reward mean : 0.48457197172806216, total_step : 855912, cur_epsilon : 0.45\n",
      "episode : 34540, reward mean : 0.4845888877772852, total_step : 856179, cur_epsilon : 0.45\n",
      "episode : 34550, reward mean : 0.4846865465663449, total_step : 856367, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34550, reward mean : 0.671500005107373, step mean : 23.85, eval_episode_rewards : [ 0.87  0.92  0.88  0.97 -1.13  0.62  0.09  0.67  0.73  0.91  0.79  0.85\n",
      "  0.79  0.86  0.92  0.8   0.81  0.77  0.96  0.35]\n",
      "episode : 34560, reward mean : 0.48477373217346786, total_step : 856591, cur_epsilon : 0.45\n",
      "episode : 34570, reward mean : 0.48473416789040713, total_step : 856853, cur_epsilon : 0.45\n",
      "episode : 34580, reward mean : 0.48481116784346534, total_step : 857112, cur_epsilon : 0.45\n",
      "episode : 34590, reward mean : 0.4848462036447064, total_step : 857316, cur_epsilon : 0.45\n",
      "episode : 34600, reward mean : 0.484828329020519, total_step : 857503, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34600, reward mean : 0.667500006314367, step mean : 29.2, eval_episode_rewards : [ 0.71  0.96 -1.    0.53  0.89  0.79  0.71  0.59  0.86  0.83  0.72  0.96\n",
      "  0.98  0.47  0.83  0.8   0.18  0.93  0.88  0.73]\n",
      "episode : 34610, reward mean : 0.48489252193514726, total_step : 857806, cur_epsilon : 0.45\n",
      "episode : 34620, reward mean : 0.48491826066510574, total_step : 858042, cur_epsilon : 0.45\n",
      "episode : 34630, reward mean : 0.48500693572834114, total_step : 858260, cur_epsilon : 0.45\n",
      "episode : 34640, reward mean : 0.4850975803786773, total_step : 858471, cur_epsilon : 0.45\n",
      "episode : 34650, reward mean : 0.4850507989719854, total_step : 858758, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34650, reward mean : 0.7400000035762787, step mean : 17.0, eval_episode_rewards : [ 0.96  0.88  0.91  0.74  0.77  0.95 -1.09  0.91  0.71  0.81  0.8   0.55\n",
      "  0.89  0.85  0.95  0.95  0.91  0.94  0.71  0.7 ]\n",
      "episode : 34660, reward mean : 0.48495009187711063, total_step : 859131, cur_epsilon : 0.45\n",
      "episode : 34670, reward mean : 0.4850450009951609, total_step : 859327, cur_epsilon : 0.45\n",
      "episode : 34680, reward mean : 0.4851464874434447, total_step : 859500, cur_epsilon : 0.45\n",
      "episode : 34690, reward mean : 0.4852237009107487, total_step : 859757, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 34700, reward mean : 0.48518271425508114, total_step : 860024, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34700, reward mean : 0.6495000055991114, step mean : 26.05, eval_episode_rewards : [ 0.65  0.84  0.93  0.69  0.82  0.8   0.44  0.93  0.9   0.51  0.9   0.9\n",
      "  0.77 -1.25  0.75  0.35  0.66  0.69  0.85  0.86]\n",
      "episode : 34710, reward mean : 0.4852310626542459, total_step : 860381, cur_epsilon : 0.45\n",
      "episode : 34720, reward mean : 0.48532748228021627, total_step : 860571, cur_epsilon : 0.45\n",
      "episode : 34730, reward mean : 0.48532623624519744, total_step : 860700, cur_epsilon : 0.45\n",
      "episode : 34740, reward mean : 0.4853837128618714, total_step : 860825, cur_epsilon : 0.45\n",
      "episode : 34750, reward mean : 0.48548317078731473, total_step : 861004, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34750, reward mean : 0.7135000041685998, step mean : 19.65, eval_episode_rewards : [ 0.84  0.94  0.19  0.95  0.64  0.94  0.61  0.93  0.8   0.93  0.7   0.93\n",
      "  0.49  0.9   0.96  0.92  0.96  0.81  0.88 -1.05]\n",
      "episode : 34760, reward mean : 0.4853532849509865, total_step : 861180, cur_epsilon : 0.45\n",
      "episode : 34770, reward mean : 0.4854155934703163, total_step : 861488, cur_epsilon : 0.45\n",
      "episode : 34780, reward mean : 0.4853910346465671, total_step : 861698, cur_epsilon : 0.45\n",
      "episode : 34790, reward mean : 0.4854400743041043, total_step : 861852, cur_epsilon : 0.45\n",
      "episode : 34800, reward mean : 0.48552730416916523, total_step : 862073, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34800, reward mean : 0.5440000057220459, step mean : 26.6, eval_episode_rewards : [ 0.55  0.96  0.91  0.89  0.91 -1.36  0.78  0.4   0.82  0.41  0.98  0.98\n",
      "  0.43  0.85  0.89  0.98  0.78 -1.15  0.49  0.38]\n",
      "episode : 34810, reward mean : 0.4856345930801053, total_step : 862224, cur_epsilon : 0.45\n",
      "episode : 34820, reward mean : 0.4856174665467363, total_step : 862408, cur_epsilon : 0.45\n",
      "episode : 34830, reward mean : 0.48561212130884546, total_step : 862551, cur_epsilon : 0.45\n",
      "episode : 34840, reward mean : 0.4856501201270886, total_step : 862743, cur_epsilon : 0.45\n",
      "episode : 34850, reward mean : 0.4856453424750946, total_step : 862884, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34850, reward mean : 0.6790000049397349, step mean : 23.1, eval_episode_rewards : [ 0.76  0.71  0.93  0.72  0.64  0.66  0.74  0.6   0.74  0.91  0.72  0.55\n",
      "  0.86  0.73  0.72  0.94  0.84 -1.09  0.92  0.98]\n",
      "episode : 34860, reward mean : 0.48561819234950554, total_step : 863103, cur_epsilon : 0.45\n",
      "episode : 34870, reward mean : 0.4855646740857769, total_step : 863414, cur_epsilon : 0.45\n",
      "episode : 34880, reward mean : 0.4855733998117695, total_step : 863708, cur_epsilon : 0.45\n",
      "episode : 34890, reward mean : 0.4855081738454073, total_step : 863860, cur_epsilon : 0.45\n",
      "episode : 34900, reward mean : 0.4853621829660839, total_step : 864094, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34900, reward mean : 0.8250000039115548, step mean : 18.5, eval_episode_rewards : [0.7  0.98 0.67 0.82 0.99 0.86 0.96 0.91 0.87 0.88 0.92 0.73 0.76 0.78\n",
      " 0.89 0.89 0.78 0.31 0.86 0.94]\n",
      "episode : 34910, reward mean : 0.4853385902480768, total_step : 864301, cur_epsilon : 0.45\n",
      "episode : 34920, reward mean : 0.4854275539978797, total_step : 864515, cur_epsilon : 0.45\n",
      "episode : 34930, reward mean : 0.48552305140699725, total_step : 864706, cur_epsilon : 0.45\n",
      "episode : 34940, reward mean : 0.48561620451312704, total_step : 864905, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 34950, reward mean : 0.48547926139496855, total_step : 865207, cur_epsilon : 0.45\n",
      "[EVAL] episode : 34950, reward mean : 0.6195000040344893, step mean : 19.05, eval_episode_rewards : [ 0.97  0.87  0.77  0.84  0.97  0.99  0.08  0.91  0.99  0.65 -1.03  0.66\n",
      "  0.95  0.86  0.68  0.82 -1.05  0.76  0.91  0.79]\n",
      "episode : 34960, reward mean : 0.4855675110354747, total_step : 865423, cur_epsilon : 0.45\n",
      "episode : 34970, reward mean : 0.48558221864051987, total_step : 865696, cur_epsilon : 0.45\n",
      "episode : 34980, reward mean : 0.48560806706429527, total_step : 865930, cur_epsilon : 0.45\n",
      "episode : 34990, reward mean : 0.4856113228344319, total_step : 866243, cur_epsilon : 0.45\n",
      "episode : 35000, reward mean : 0.48570771960046677, total_step : 866430, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35000, reward mean : 0.7210000040009618, step mean : 18.9, eval_episode_rewards : [ 0.66  0.98 -1.16  0.73  0.97  0.93  0.9   0.84  0.87  0.82  0.93  0.79\n",
      "  0.99  0.55  0.28  0.95  0.87  0.84  0.81  0.87]\n",
      "episode : 35010, reward mean : 0.4857446497017334, total_step : 866625, cur_epsilon : 0.45\n",
      "episode : 35020, reward mean : 0.48583381456604335, total_step : 866837, cur_epsilon : 0.45\n",
      "episode : 35030, reward mean : 0.4857958945520424, total_step : 867094, cur_epsilon : 0.45\n",
      "episode : 35040, reward mean : 0.4858898454965581, total_step : 867289, cur_epsilon : 0.45\n",
      "episode : 35050, reward mean : 0.4858641993222538, total_step : 867503, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35050, reward mean : 0.17200000621378422, step mean : 28.75, eval_episode_rewards : [ 0.74  0.16  0.81  0.91  0.99  0.98  0.84  0.91 -1.04 -1.   -1.37  0.74\n",
      " -1.17  0.46  0.84  0.21 -1.08  0.81 -1.04  0.74]\n",
      "episode : 35060, reward mean : 0.4859503761064569, total_step : 867725, cur_epsilon : 0.45\n",
      "episode : 35070, reward mean : 0.48593128560971877, total_step : 867916, cur_epsilon : 0.45\n",
      "episode : 35080, reward mean : 0.48596294716015426, total_step : 868129, cur_epsilon : 0.45\n",
      "episode : 35090, reward mean : 0.4860017152014248, total_step : 868317, cur_epsilon : 0.45\n",
      "episode : 35100, reward mean : 0.48610199961407996, total_step : 868489, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35100, reward mean : 0.7115000042133033, step mean : 19.85, eval_episode_rewards : [ 0.76  0.66  0.98  0.85  0.77 -1.33  0.96  0.95  0.95  0.88  0.97  0.9\n",
      "  0.78  0.47  0.62  0.78  0.93  0.59  0.97  0.79]\n",
      "episode : 35110, reward mean : 0.48606921636301315, total_step : 868728, cur_epsilon : 0.45\n",
      "episode : 35120, reward mean : 0.48610478891144615, total_step : 869026, cur_epsilon : 0.45\n",
      "episode : 35130, reward mean : 0.4861221231610843, total_step : 869388, cur_epsilon : 0.45\n",
      "episode : 35140, reward mean : 0.4860446837419028, total_step : 869584, cur_epsilon : 0.45\n",
      "episode : 35150, reward mean : 0.48615477060366574, total_step : 869721, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35150, reward mean : 0.6930000023916364, step mean : 11.7, eval_episode_rewards : [ 0.79  0.93  0.77  0.92  0.74  0.96  0.84  0.68  0.98  0.94  0.96 -1.06\n",
      "  0.88  0.93  0.98  0.97 -1.03  0.92  0.96  0.8 ]\n",
      "synced target net\n",
      "episode : 35160, reward mean : 0.4861507447893107, total_step : 870059, cur_epsilon : 0.45\n",
      "episode : 35170, reward mean : 0.4861791352527722, total_step : 870283, cur_epsilon : 0.45\n",
      "episode : 35180, reward mean : 0.48624844192439876, total_step : 870563, cur_epsilon : 0.45\n",
      "episode : 35190, reward mean : 0.486237856974541, total_step : 870724, cur_epsilon : 0.45\n",
      "episode : 35200, reward mean : 0.4862102325846949, total_step : 870945, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35200, reward mean : 0.729500003810972, step mean : 18.05, eval_episode_rewards : [ 0.54  0.93  0.88  0.93  0.8   0.84  0.67  0.95  0.96  0.93  0.94  0.89\n",
      "  0.92  0.88  0.96  0.38  0.63 -1.1   0.79  0.87]\n",
      "episode : 35210, reward mean : 0.48624453811486507, total_step : 871148, cur_epsilon : 0.45\n",
      "episode : 35220, reward mean : 0.4862725777135848, total_step : 871373, cur_epsilon : 0.45\n",
      "episode : 35230, reward mean : 0.48624808933060343, total_step : 871583, cur_epsilon : 0.45\n",
      "episode : 35240, reward mean : 0.48632123119110043, total_step : 871849, cur_epsilon : 0.45\n",
      "episode : 35250, reward mean : 0.4864218492825509, total_step : 872018, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35250, reward mean : 0.6570000054314733, step mean : 25.3, eval_episode_rewards : [ 0.49  0.99  0.52  0.95  0.75  0.8  -1.22  0.79  0.12  0.95  0.65  0.8\n",
      "  0.87  0.56  0.71  0.97  0.66  0.93  0.96  0.89]\n",
      "episode : 35260, reward mean : 0.4865156037223716, total_step : 872211, cur_epsilon : 0.45\n",
      "episode : 35270, reward mean : 0.48647661432681955, total_step : 872472, cur_epsilon : 0.45\n",
      "episode : 35280, reward mean : 0.4863894610928663, total_step : 872703, cur_epsilon : 0.45\n",
      "episode : 35290, reward mean : 0.48641627054137926, total_step : 872932, cur_epsilon : 0.45\n",
      "episode : 35300, reward mean : 0.48651020361036695, total_step : 873124, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35300, reward mean : 0.6400000035762787, step mean : 17.0, eval_episode_rewards : [ 0.85  0.97  0.95  0.98  0.86 -1.77  0.92  0.96  0.85  0.76  0.83  0.77\n",
      "  0.92 -1.03  0.91  0.85  0.84  0.95  0.75  0.68]\n",
      "episode : 35310, reward mean : 0.48654857511995864, total_step : 873312, cur_epsilon : 0.45\n",
      "episode : 35320, reward mean : 0.4865042521955291, total_step : 873592, cur_epsilon : 0.45\n",
      "episode : 35330, reward mean : 0.4864857115088721, total_step : 873880, cur_epsilon : 0.45\n",
      "episode : 35340, reward mean : 0.48642728318279504, total_step : 874210, cur_epsilon : 0.45\n",
      "episode : 35350, reward mean : 0.48651287659765596, total_step : 874431, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35350, reward mean : 0.46000000536441804, step mean : 24.9, eval_episode_rewards : [ 0.96  0.86  0.9   0.5  -1.27  0.93  0.51  0.95  0.85  0.96  0.98 -1.\n",
      "  0.68  0.9   0.99  0.83  0.93 -1.    0.9  -1.16]\n",
      "episode : 35360, reward mean : 0.4864411817806693, total_step : 874608, cur_epsilon : 0.45\n",
      "episode : 35370, reward mean : 0.48653209465104275, total_step : 874810, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 35380, reward mean : 0.48662323877473357, total_step : 875011, cur_epsilon : 0.45\n",
      "episode : 35390, reward mean : 0.48671998270382294, total_step : 875192, cur_epsilon : 0.45\n",
      "episode : 35400, reward mean : 0.48679774542207604, total_step : 875440, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35400, reward mean : 0.8245000039227307, step mean : 18.55, eval_episode_rewards : [0.88 0.65 0.81 0.94 0.91 0.96 0.76 0.72 0.95 0.76 0.98 0.85 0.86 0.81\n",
      " 0.54 0.84 0.54 0.91 0.89 0.93]\n",
      "episode : 35410, reward mean : 0.4869008807674265, total_step : 875598, cur_epsilon : 0.45\n",
      "episode : 35420, reward mean : 0.486980807115358, total_step : 875838, cur_epsilon : 0.45\n",
      "episode : 35430, reward mean : 0.4870716959658107, total_step : 876039, cur_epsilon : 0.45\n",
      "episode : 35440, reward mean : 0.4870555922151995, total_step : 876219, cur_epsilon : 0.45\n",
      "episode : 35450, reward mean : 0.48711481489787567, total_step : 876332, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35450, reward mean : 0.5740000061690808, step mean : 28.55, eval_episode_rewards : [ 0.97  0.48  0.93  0.9   0.87  0.75  0.95  0.86  0.85  0.9   0.82  0.48\n",
      " -1.    0.44  0.5   0.83  0.76 -1.37  0.93  0.63]\n",
      "episode : 35460, reward mean : 0.48709475996032636, total_step : 876625, cur_epsilon : 0.45\n",
      "episode : 35470, reward mean : 0.48706625847874274, total_step : 876849, cur_epsilon : 0.45\n",
      "episode : 35480, reward mean : 0.4870828688922711, total_step : 877113, cur_epsilon : 0.45\n",
      "episode : 35490, reward mean : 0.487127365126301, total_step : 877278, cur_epsilon : 0.45\n",
      "episode : 35500, reward mean : 0.487091836292559, total_step : 877527, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35500, reward mean : 0.8520000033080578, step mean : 15.8, eval_episode_rewards : [0.95 0.89 0.9  0.73 0.79 0.92 0.96 0.87 0.93 0.87 0.8  0.83 0.78 0.5\n",
      " 0.86 0.89 0.93 0.88 0.85 0.91]\n",
      "episode : 35510, reward mean : 0.48708786787987135, total_step : 877664, cur_epsilon : 0.45\n",
      "episode : 35520, reward mean : 0.48718046701732426, total_step : 877858, cur_epsilon : 0.45\n",
      "episode : 35530, reward mean : 0.4872696366028474, total_step : 878064, cur_epsilon : 0.45\n",
      "episode : 35540, reward mean : 0.4871826164476305, total_step : 878296, cur_epsilon : 0.45\n",
      "episode : 35550, reward mean : 0.48721491388447385, total_step : 878504, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35550, reward mean : 0.5110000042244792, step mean : 19.9, eval_episode_rewards : [ 0.94  0.89  0.63  0.81 -1.04  0.77  0.96  0.86  0.2   0.91  0.81  0.92\n",
      " -1.09  0.63  0.99  0.71  0.76  0.99  0.91 -1.34]\n",
      "episode : 35560, reward mean : 0.4872494428750144, total_step : 878704, cur_epsilon : 0.45\n",
      "episode : 35570, reward mean : 0.4871970814922686, total_step : 878912, cur_epsilon : 0.45\n",
      "episode : 35580, reward mean : 0.48717088782317003, total_step : 879227, cur_epsilon : 0.45\n",
      "episode : 35590, reward mean : 0.4871899462996109, total_step : 879482, cur_epsilon : 0.45\n",
      "episode : 35600, reward mean : 0.48729382552907624, total_step : 879635, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35600, reward mean : 0.6930000046268106, step mean : 21.7, eval_episode_rewards : [ 0.81  0.91  0.77  0.88  0.88  0.81 -1.02  0.99  0.58  0.7   0.96  0.92\n",
      "  0.86  0.84  0.87  0.58  0.92  0.59  0.45  0.56]\n",
      "episode : 35610, reward mean : 0.4871788876400557, total_step : 879767, cur_epsilon : 0.45\n",
      "episode : 35620, reward mean : 0.4871687307382282, total_step : 879926, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 35630, reward mean : 0.4872032048549778, total_step : 880326, cur_epsilon : 0.45\n",
      "episode : 35640, reward mean : 0.4872418683788433, total_step : 880511, cur_epsilon : 0.45\n",
      "episode : 35650, reward mean : 0.4873503559340852, total_step : 880647, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35650, reward mean : 0.4060000043362379, step mean : 20.4, eval_episode_rewards : [ 0.76  0.97  0.76  0.96  0.58 -1.1   0.77  0.89  0.91  0.68  0.76  0.81\n",
      "  0.94  0.57  0.77  0.87 -1.05 -1.15  0.69 -1.27]\n",
      "episode : 35660, reward mean : 0.48737801988559515, total_step : 880970, cur_epsilon : 0.45\n",
      "episode : 35670, reward mean : 0.48738716538219534, total_step : 881260, cur_epsilon : 0.45\n",
      "episode : 35680, reward mean : 0.4874335815363163, total_step : 881417, cur_epsilon : 0.45\n",
      "episode : 35690, reward mean : 0.4873656539436932, total_step : 881582, cur_epsilon : 0.45\n",
      "episode : 35700, reward mean : 0.4873843190281623, total_step : 881838, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35700, reward mean : 0.40700000431388617, step mean : 20.3, eval_episode_rewards : [ 0.82 -1.56  0.87  0.9   0.78  0.78  0.85  0.78  0.92  0.79  0.83 -1.07\n",
      "  0.93  0.84  0.64 -1.1   0.99  0.34  0.99 -1.18]\n",
      "episode : 35710, reward mean : 0.48737945083671674, total_step : 882178, cur_epsilon : 0.45\n",
      "episode : 35720, reward mean : 0.4874283367695969, total_step : 882326, cur_epsilon : 0.45\n",
      "episode : 35730, reward mean : 0.48747999410812526, total_step : 882664, cur_epsilon : 0.45\n",
      "episode : 35740, reward mean : 0.487517352812628, total_step : 882853, cur_epsilon : 0.45\n",
      "episode : 35750, reward mean : 0.48751916614242574, total_step : 883169, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35750, reward mean : 0.8115000042133034, step mean : 19.85, eval_episode_rewards : [0.84 0.92 0.87 0.89 0.62 0.77 0.92 0.74 0.9  0.88 0.62 0.66 0.84 0.85\n",
      " 0.72 0.99 0.72 0.75 0.91 0.82]\n",
      "episode : 35760, reward mean : 0.48756236548174037, total_step : 883337, cur_epsilon : 0.45\n",
      "episode : 35770, reward mean : 0.4875801003545623, total_step : 883596, cur_epsilon : 0.45\n",
      "episode : 35780, reward mean : 0.4876696531505201, total_step : 883798, cur_epsilon : 0.45\n",
      "episode : 35790, reward mean : 0.4876418046877059, total_step : 884020, cur_epsilon : 0.45\n",
      "episode : 35800, reward mean : 0.48768240753660114, total_step : 884197, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35800, reward mean : 0.8580000031739473, step mean : 15.2, eval_episode_rewards : [0.97 0.88 0.84 0.84 0.89 0.85 0.85 0.86 0.96 0.9  0.9  0.97 0.32 0.89\n",
      " 0.97 0.94 0.84 0.92 0.95 0.62]\n",
      "episode : 35810, reward mean : 0.48777381150102406, total_step : 884392, cur_epsilon : 0.45\n",
      "episode : 35820, reward mean : 0.48784590144914697, total_step : 884656, cur_epsilon : 0.45\n",
      "episode : 35830, reward mean : 0.4878671557343342, total_step : 884902, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 35840, reward mean : 0.4879006749449348, total_step : 885203, cur_epsilon : 0.45\n",
      "episode : 35850, reward mean : 0.48790851297322013, total_step : 885497, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35850, reward mean : 0.6510000055655837, step mean : 25.9, eval_episode_rewards : [ 0.92  0.6   0.65  0.83  0.94  0.96  0.87  0.98  0.53  0.57 -1.03  0.87\n",
      "  0.62  0.84  0.45  0.63  0.36  0.92  0.74  0.77]\n",
      "episode : 35860, reward mean : 0.48797769632317206, total_step : 885771, cur_epsilon : 0.45\n",
      "episode : 35870, reward mean : 0.4880574349093801, total_step : 886007, cur_epsilon : 0.45\n",
      "episode : 35880, reward mean : 0.488111766729942, total_step : 886334, cur_epsilon : 0.45\n",
      "episode : 35890, reward mean : 0.4881713622272758, total_step : 886642, cur_epsilon : 0.45\n",
      "episode : 35900, reward mean : 0.488202233715372, total_step : 886853, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35900, reward mean : 0.7925000046379864, step mean : 21.75, eval_episode_rewards : [0.85 0.86 0.5  0.88 0.97 0.97 0.52 0.74 0.47 0.88 0.92 0.8  0.88 0.37\n",
      " 0.94 0.98 0.73 0.89 0.96 0.74]\n",
      "episode : 35910, reward mean : 0.488108888622586, total_step : 887110, cur_epsilon : 0.45\n",
      "episode : 35920, reward mean : 0.48808046187318754, total_step : 887334, cur_epsilon : 0.45\n",
      "episode : 35930, reward mean : 0.488111889522109, total_step : 887543, cur_epsilon : 0.45\n",
      "episode : 35940, reward mean : 0.4881541511008664, total_step : 887713, cur_epsilon : 0.45\n",
      "episode : 35950, reward mean : 0.488181646470452, total_step : 887936, cur_epsilon : 0.45\n",
      "[EVAL] episode : 35950, reward mean : 0.6215000039897859, step mean : 18.85, eval_episode_rewards : [ 0.68  0.92  0.53  0.94  0.59  0.68  0.93  0.93  0.78  0.92  0.81  0.81\n",
      " -1.15 -1.11  0.77  0.71  0.93  0.98  0.85  0.93]\n",
      "episode : 35960, reward mean : 0.4882174691505465, total_step : 888129, cur_epsilon : 0.45\n",
      "episode : 35970, reward mean : 0.4881829355214457, total_step : 888375, cur_epsilon : 0.45\n",
      "episode : 35980, reward mean : 0.488216236541115, total_step : 888577, cur_epsilon : 0.45\n",
      "episode : 35990, reward mean : 0.48825702113882186, total_step : 888752, cur_epsilon : 0.45\n",
      "episode : 36000, reward mean : 0.48828167196766786, total_step : 888985, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36000, reward mean : 0.5955000045709312, step mean : 21.45, eval_episode_rewards : [ 0.75  0.96  0.92  0.87  0.82 -1.24  0.77  0.67  0.43  0.66  0.76  0.57\n",
      "  0.83 -1.09  0.89  0.96  0.84  0.96  0.71  0.87]\n",
      "episode : 36010, reward mean : 0.4882815937478777, total_step : 889107, cur_epsilon : 0.45\n",
      "episode : 36020, reward mean : 0.4883875677648566, total_step : 889247, cur_epsilon : 0.45\n",
      "episode : 36030, reward mean : 0.48846045492494555, total_step : 889506, cur_epsilon : 0.45\n",
      "episode : 36040, reward mean : 0.4885086068528478, total_step : 889654, cur_epsilon : 0.45\n",
      "episode : 36050, reward mean : 0.4884934865745836, total_step : 889830, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36050, reward mean : 0.6145000052638352, step mean : 24.5, eval_episode_rewards : [ 0.99  0.76  0.37  0.47  0.98 -1.    0.95  0.94  0.86  0.83  0.37  0.94\n",
      "  0.98  0.89 -1.28  0.9   0.86  0.74  0.87  0.87]\n",
      "synced target net\n",
      "episode : 36060, reward mean : 0.4885235771231035, total_step : 890043, cur_epsilon : 0.45\n",
      "episode : 36070, reward mean : 0.48856889911601176, total_step : 890300, cur_epsilon : 0.45\n",
      "episode : 36080, reward mean : 0.4885828766960985, total_step : 890670, cur_epsilon : 0.45\n",
      "episode : 36090, reward mean : 0.48865060103227803, total_step : 890947, cur_epsilon : 0.45\n",
      "episode : 36100, reward mean : 0.48857867565908963, total_step : 891128, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36100, reward mean : 0.7300000037997961, step mean : 18.0, eval_episode_rewards : [ 0.94  0.93  0.86  0.99  0.87  0.61  0.94  0.89  0.9   0.99  0.53 -1.38\n",
      "  0.99  0.62  0.94  0.69  0.88  0.95  0.79  0.67]\n",
      "episode : 36110, reward mean : 0.48856383803184755, total_step : 891303, cur_epsilon : 0.45\n",
      "episode : 36120, reward mean : 0.4886367716330412, total_step : 891561, cur_epsilon : 0.45\n",
      "episode : 36130, reward mean : 0.4885848378468745, total_step : 891670, cur_epsilon : 0.45\n",
      "episode : 36140, reward mean : 0.48862313756077047, total_step : 891853, cur_epsilon : 0.45\n",
      "episode : 36150, reward mean : 0.48860249492359115, total_step : 892049, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36150, reward mean : 0.6105000042356551, step mean : 19.95, eval_episode_rewards : [-1.28  0.92  0.66  0.89  0.91  0.9   0.92  0.99  0.57  0.84  0.74  0.45\n",
      "  0.82  0.72  0.96 -1.01  0.89  0.8   0.62  0.9 ]\n",
      "episode : 36160, reward mean : 0.48871101193347394, total_step : 892178, cur_epsilon : 0.45\n",
      "episode : 36170, reward mean : 0.48879873352385345, total_step : 892382, cur_epsilon : 0.45\n",
      "episode : 36180, reward mean : 0.4888922109341196, total_step : 892565, cur_epsilon : 0.45\n",
      "episode : 36190, reward mean : 0.488834213639776, total_step : 892696, cur_epsilon : 0.45\n",
      "episode : 36200, reward mean : 0.4888745909298446, total_step : 892871, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36200, reward mean : 0.6020000044256448, step mean : 20.8, eval_episode_rewards : [ 0.7  -1.04  0.91  0.72  0.98  0.56  0.73  0.88  0.63  0.64  0.75  0.81\n",
      "  0.98  0.96  0.91  0.61  0.85 -1.29  0.97  0.78]\n",
      "episode : 36210, reward mean : 0.488922678588539, total_step : 893018, cur_epsilon : 0.45\n",
      "episode : 36220, reward mean : 0.48891027586211666, total_step : 893184, cur_epsilon : 0.45\n",
      "episode : 36230, reward mean : 0.4889624673419221, total_step : 893516, cur_epsilon : 0.45\n",
      "episode : 36240, reward mean : 0.48905629668421796, total_step : 893697, cur_epsilon : 0.45\n",
      "episode : 36250, reward mean : 0.4889983501203615, total_step : 893828, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36250, reward mean : 0.6445000034756958, step mean : 16.55, eval_episode_rewards : [ 0.79  0.9   0.73  0.7   0.96  0.72  0.94  0.92 -1.03  0.69  0.64  0.95\n",
      "  0.93  0.98  0.85  0.78  0.93  0.83  0.77 -1.09]\n",
      "episode : 36260, reward mean : 0.4890893599532146, total_step : 894019, cur_epsilon : 0.45\n",
      "episode : 36270, reward mean : 0.4889252878943274, total_step : 894135, cur_epsilon : 0.45\n",
      "episode : 36280, reward mean : 0.4889379876512262, total_step : 894410, cur_epsilon : 0.45\n",
      "episode : 36290, reward mean : 0.4889468225971024, total_step : 894699, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 36300, reward mean : 0.4889168097009015, total_step : 895129, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36300, reward mean : 0.7595000053755939, step mean : 25.05, eval_episode_rewards : [0.91 0.98 0.58 0.55 0.66 0.88 0.32 0.94 0.85 0.26 0.93 0.53 0.85 0.81\n",
      " 0.9  0.68 0.98 0.86 0.79 0.93]\n",
      "episode : 36310, reward mean : 0.4888512859315899, total_step : 895288, cur_epsilon : 0.45\n",
      "episode : 36320, reward mean : 0.4888345317239668, total_step : 895470, cur_epsilon : 0.45\n",
      "episode : 36330, reward mean : 0.48886458002366157, total_step : 895682, cur_epsilon : 0.45\n",
      "episode : 36340, reward mean : 0.488867369077661, total_step : 895793, cur_epsilon : 0.45\n",
      "episode : 36350, reward mean : 0.4889383821826341, total_step : 896056, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36350, reward mean : 0.6355000036768615, step mean : 17.45, eval_episode_rewards : [ 0.96  0.9   0.86  0.77  0.97 -1.32  0.84  0.49  0.9   0.8   0.9   0.66\n",
      " -1.06  0.98  0.95  0.98  0.84  0.58  0.97  0.74]\n",
      "episode : 36360, reward mean : 0.48900550584154096, total_step : 896333, cur_epsilon : 0.45\n",
      "episode : 36370, reward mean : 0.48898708255321877, total_step : 896620, cur_epsilon : 0.45\n",
      "episode : 36380, reward mean : 0.48906542585242113, total_step : 896856, cur_epsilon : 0.45\n",
      "episode : 36390, reward mean : 0.48912558924364796, total_step : 897158, cur_epsilon : 0.45\n",
      "episode : 36400, reward mean : 0.489199181116227, total_step : 897411, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36400, reward mean : 0.49550000233575703, step mean : 11.45, eval_episode_rewards : [ 0.95 -1.13  0.89  0.97  0.85  0.9   0.92  0.91  0.83 -1.04  0.84  0.76\n",
      "  0.93  0.89  0.91  0.93 -1.02  0.67 -1.03  0.98]\n",
      "episode : 36410, reward mean : 0.4892801481098309, total_step : 897637, cur_epsilon : 0.45\n",
      "episode : 36420, reward mean : 0.4892729322545185, total_step : 897784, cur_epsilon : 0.45\n",
      "episode : 36430, reward mean : 0.4893310511330331, total_step : 898093, cur_epsilon : 0.45\n",
      "episode : 36440, reward mean : 0.48931669025282193, total_step : 898266, cur_epsilon : 0.45\n",
      "episode : 36450, reward mean : 0.48939396962589893, total_step : 898505, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36450, reward mean : 0.6930000046268106, step mean : 21.7, eval_episode_rewards : [ 0.91  0.91  0.98  0.89  0.97  0.34  0.84  0.9   0.98  0.52  0.66  0.77\n",
      "  0.82  0.51  0.78  0.89 -1.04  0.8   0.97  0.46]\n",
      "episode : 36460, reward mean : 0.4893217277263551, total_step : 898689, cur_epsilon : 0.45\n",
      "episode : 36470, reward mean : 0.48940526989166305, total_step : 898905, cur_epsilon : 0.45\n",
      "episode : 36480, reward mean : 0.48934430353559977, total_step : 899048, cur_epsilon : 0.45\n",
      "episode : 36490, reward mean : 0.48932804036768096, total_step : 899228, cur_epsilon : 0.45\n",
      "episode : 36500, reward mean : 0.48933973131716657, total_step : 899506, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36500, reward mean : 0.6685000051744282, step mean : 24.15, eval_episode_rewards : [ 0.29  0.88  0.57  0.7   0.9   0.92  0.96  0.99  0.9   0.64  0.64  0.81\n",
      "  0.96  0.9   0.89  0.86  0.92  0.89 -1.76  0.51]\n",
      "episode : 36510, reward mean : 0.489354154837969, total_step : 899774, cur_epsilon : 0.45\n",
      "episode : 36520, reward mean : 0.48939239302224724, total_step : 899955, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 36530, reward mean : 0.4894774211118854, total_step : 900165, cur_epsilon : 0.45\n",
      "episode : 36540, reward mean : 0.4895021946706718, total_step : 900395, cur_epsilon : 0.45\n",
      "episode : 36550, reward mean : 0.4895132747838317, total_step : 900774, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36550, reward mean : 0.5985000067390501, step mean : 31.15, eval_episode_rewards : [ 0.98  0.67  0.73  0.66  0.56 -1.04  0.78  0.32  0.86  0.38  0.45  0.84\n",
      "  0.52  0.91  0.75  0.52  0.86  0.84  0.47  0.91]\n",
      "episode : 36560, reward mean : 0.4894510446766929, total_step : 900922, cur_epsilon : 0.45\n",
      "episode : 36570, reward mean : 0.489531862002405, total_step : 901147, cur_epsilon : 0.45\n",
      "episode : 36580, reward mean : 0.4894592726480943, total_step : 901333, cur_epsilon : 0.45\n",
      "episode : 36590, reward mean : 0.48950150843132434, total_step : 901499, cur_epsilon : 0.45\n",
      "episode : 36600, reward mean : 0.489566671955272, total_step : 901781, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36600, reward mean : 0.8115000042133034, step mean : 19.85, eval_episode_rewards : [0.88 0.92 0.86 0.82 0.94 0.57 0.52 0.82 0.84 0.62 0.97 0.73 0.88 0.86\n",
      " 0.88 0.84 0.78 0.94 0.87 0.69]\n",
      "episode : 36610, reward mean : 0.48957662369910404, total_step : 902065, cur_epsilon : 0.45\n",
      "episode : 36620, reward mean : 0.4896695847532003, total_step : 902245, cur_epsilon : 0.45\n",
      "episode : 36630, reward mean : 0.48971963400738694, total_step : 902382, cur_epsilon : 0.45\n",
      "episode : 36640, reward mean : 0.4895788808341553, total_step : 902717, cur_epsilon : 0.45\n",
      "episode : 36650, reward mean : 0.4895219698169181, total_step : 902846, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36650, reward mean : 0.6020000044256448, step mean : 20.8, eval_episode_rewards : [ 0.45 -1.01  0.51  0.68  0.83  0.82  0.88 -1.04  0.51  0.95  0.93  0.76\n",
      "  0.78  0.97  0.88  0.94  0.69  0.95  0.81  0.75]\n",
      "episode : 36660, reward mean : 0.48950846137004395, total_step : 903016, cur_epsilon : 0.45\n",
      "episode : 36670, reward mean : 0.4895595907786557, total_step : 903149, cur_epsilon : 0.45\n",
      "episode : 36680, reward mean : 0.48965485806677456, total_step : 903320, cur_epsilon : 0.45\n",
      "episode : 36690, reward mean : 0.48976043046925, total_step : 903453, cur_epsilon : 0.45\n",
      "episode : 36700, reward mean : 0.4898182614152231, total_step : 903561, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36700, reward mean : 0.6655000063590706, step mean : 29.4, eval_episode_rewards : [ 0.86  0.82  0.55  0.84  0.62  0.68  0.58  0.55  0.72  0.88  0.91  0.94\n",
      "  0.75  0.93  0.92  0.82  0.5   0.69 -1.    0.75]\n",
      "episode : 36710, reward mean : 0.4897028110588025, total_step : 903705, cur_epsilon : 0.45\n",
      "episode : 36720, reward mean : 0.4896228266343746, total_step : 903919, cur_epsilon : 0.45\n",
      "episode : 36730, reward mean : 0.489676563954247, total_step : 904042, cur_epsilon : 0.45\n",
      "episode : 36740, reward mean : 0.48977708748151905, total_step : 904193, cur_epsilon : 0.45\n",
      "episode : 36750, reward mean : 0.4897333386158254, total_step : 904474, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36750, reward mean : 0.5740000050514936, step mean : 23.6, eval_episode_rewards : [ 0.93  0.97  0.61  0.59  0.8   0.76  0.93  0.85 -1.07  0.58  0.89  0.8\n",
      "  0.53  0.95 -1.06  0.73  0.61  0.71  0.72  0.65]\n",
      "episode : 36760, reward mean : 0.4896700270447102, total_step : 904627, cur_epsilon : 0.45\n",
      "episode : 36770, reward mean : 0.4897114548327131, total_step : 904795, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 36780, reward mean : 0.4897468785830288, total_step : 905185, cur_epsilon : 0.45\n",
      "episode : 36790, reward mean : 0.4897529272723069, total_step : 905483, cur_epsilon : 0.45\n",
      "episode : 36800, reward mean : 0.4898491900647565, total_step : 905649, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36800, reward mean : 0.641000003553927, step mean : 16.9, eval_episode_rewards : [ 0.88  0.76 -1.01 -1.35  0.92  0.5   0.95  0.77  0.94  0.95  0.96  0.75\n",
      "  0.93  0.92  0.81  0.96  0.95  0.64  0.74  0.85]\n",
      "episode : 36810, reward mean : 0.4899543655096586, total_step : 905782, cur_epsilon : 0.45\n",
      "episode : 36820, reward mean : 0.4900391144610209, total_step : 905990, cur_epsilon : 0.45\n",
      "episode : 36830, reward mean : 0.490119744624013, total_step : 906213, cur_epsilon : 0.45\n",
      "episode : 36840, reward mean : 0.49020521700722, total_step : 906418, cur_epsilon : 0.45\n",
      "episode : 36850, reward mean : 0.4901416606398418, total_step : 906572, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36850, reward mean : 0.7440000034868717, step mean : 16.6, eval_episode_rewards : [ 0.95  0.83  0.77  0.76  0.46  0.4   0.95  0.94  0.98 -1.01  0.88  0.86\n",
      "  0.92  0.76  0.76  0.99  0.82  0.9   0.97  0.99]\n",
      "episode : 36860, reward mean : 0.49017200745041856, total_step : 906780, cur_epsilon : 0.45\n",
      "episode : 36870, reward mean : 0.4901969133352632, total_step : 907008, cur_epsilon : 0.45\n",
      "episode : 36880, reward mean : 0.4902961007240602, total_step : 907162, cur_epsilon : 0.45\n",
      "episode : 36890, reward mean : 0.49023909446394326, total_step : 907492, cur_epsilon : 0.45\n",
      "episode : 36900, reward mean : 0.49031220040185663, total_step : 907742, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36900, reward mean : 0.5330000037327409, step mean : 17.7, eval_episode_rewards : [ 0.89  0.56  0.83 -1.01  0.88  0.72  0.91  0.68  0.79  0.82  0.85  0.75\n",
      "  0.8  -1.07  0.98  0.88 -1.05  0.68  0.86  0.91]\n",
      "episode : 36910, reward mean : 0.49041154686698657, total_step : 907895, cur_epsilon : 0.45\n",
      "episode : 36920, reward mean : 0.4903976217469247, total_step : 908066, cur_epsilon : 0.45\n",
      "episode : 36930, reward mean : 0.49033523408956814, total_step : 908216, cur_epsilon : 0.45\n",
      "episode : 36940, reward mean : 0.4904085055490301, total_step : 908465, cur_epsilon : 0.45\n",
      "episode : 36950, reward mean : 0.49049878741598135, total_step : 908651, cur_epsilon : 0.45\n",
      "[EVAL] episode : 36950, reward mean : 0.7035000043921172, step mean : 20.65, eval_episode_rewards : [ 0.97  0.85  0.67  0.67  0.38  0.71  0.83  0.89  0.92  0.96  0.85  0.79\n",
      "  0.77  0.86  0.79  0.82  0.93  0.89  0.74 -1.22]\n",
      "episode : 36960, reward mean : 0.4905216502995347, total_step : 908886, cur_epsilon : 0.45\n",
      "episode : 36970, reward mean : 0.4905791234809977, total_step : 908993, cur_epsilon : 0.45\n",
      "episode : 36980, reward mean : 0.4906419739089886, total_step : 909280, cur_epsilon : 0.45\n",
      "episode : 36990, reward mean : 0.49067180846772723, total_step : 909588, cur_epsilon : 0.45\n",
      "episode : 37000, reward mean : 0.49071162689945386, total_step : 909859, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37000, reward mean : 0.6980000045150518, step mean : 21.2, eval_episode_rewards : [ 0.99  0.78  0.89  0.82  0.59  0.8   0.88  0.6   0.71  0.88  0.65  0.97\n",
      " -1.46  0.86  0.53  0.95  0.96  0.89  0.96  0.71]\n",
      "synced target net\n",
      "episode : 37010, reward mean : 0.4907449390792047, total_step : 910055, cur_epsilon : 0.45\n",
      "episode : 37020, reward mean : 0.49077445152258425, total_step : 910265, cur_epsilon : 0.45\n",
      "episode : 37030, reward mean : 0.4908574181855582, total_step : 910477, cur_epsilon : 0.45\n",
      "episode : 37040, reward mean : 0.49091091240378054, total_step : 910598, cur_epsilon : 0.45\n",
      "episode : 37050, reward mean : 0.49097760311726296, total_step : 910870, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37050, reward mean : 0.5215000039897859, step mean : 18.85, eval_episode_rewards : [ 0.92  0.92 -1.23  0.96  0.75  0.87  0.61  0.99  0.79 -1.03  0.94  0.81\n",
      "  0.82 -1.67  0.95  0.66  0.82  0.76  0.98  0.81]\n",
      "episode : 37060, reward mean : 0.490958990705191, total_step : 911058, cur_epsilon : 0.45\n",
      "episode : 37070, reward mean : 0.4909835499213139, total_step : 911286, cur_epsilon : 0.45\n",
      "episode : 37080, reward mean : 0.4910585273903651, total_step : 911527, cur_epsilon : 0.45\n",
      "episode : 37090, reward mean : 0.4910318197811062, total_step : 911745, cur_epsilon : 0.45\n",
      "episode : 37100, reward mean : 0.4910714338468312, total_step : 911917, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37100, reward mean : 0.7165000041015446, step mean : 19.35, eval_episode_rewards : [ 0.66  0.91 -1.13  0.79  0.67  0.92  0.71  0.23  0.95  0.65  0.93  0.95\n",
      "  0.95  0.96  0.98  0.82  0.76  0.95  0.96  0.71]\n",
      "episode : 37110, reward mean : 0.491101325673988, total_step : 912125, cur_epsilon : 0.45\n",
      "episode : 37120, reward mean : 0.49105630915472487, total_step : 912411, cur_epsilon : 0.45\n",
      "episode : 37130, reward mean : 0.4910700295147065, total_step : 912679, cur_epsilon : 0.45\n",
      "episode : 37140, reward mean : 0.4910716261698648, total_step : 912992, cur_epsilon : 0.45\n",
      "episode : 37150, reward mean : 0.4911020241182393, total_step : 913198, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37150, reward mean : 0.5240000061690807, step mean : 28.6, eval_episode_rewards : [ 0.77  0.72  0.82  0.86 -1.26  0.96  0.6   0.86  0.58  0.74  0.59  0.97\n",
      "  0.87  0.84  0.45  0.47  0.89  0.37 -1.16  0.54]\n",
      "episode : 37160, reward mean : 0.49111545199274703, total_step : 913467, cur_epsilon : 0.45\n",
      "episode : 37170, reward mean : 0.49103202034165805, total_step : 913696, cur_epsilon : 0.45\n",
      "episode : 37180, reward mean : 0.4910435770887274, total_step : 913972, cur_epsilon : 0.45\n",
      "episode : 37190, reward mean : 0.4910820165688623, total_step : 914148, cur_epsilon : 0.45\n",
      "episode : 37200, reward mean : 0.4911513493616154, total_step : 914409, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37200, reward mean : 0.7155000041238964, step mean : 19.45, eval_episode_rewards : [ 0.6   0.8  -1.19  0.65  0.73  0.83  0.95  0.71  0.99  0.89  0.96  0.84\n",
      "  0.95  0.7   0.78  0.83  0.85  0.94  0.62  0.88]\n",
      "episode : 37210, reward mean : 0.4911776456946857, total_step : 914630, cur_epsilon : 0.45\n",
      "episode : 37220, reward mean : 0.491205808607394, total_step : 914943, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 37230, reward mean : 0.49124738642019816, total_step : 915206, cur_epsilon : 0.45\n",
      "episode : 37240, reward mean : 0.4912999515695283, total_step : 915329, cur_epsilon : 0.45\n",
      "episode : 37250, reward mean : 0.49126013950353503, total_step : 915596, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37250, reward mean : 0.8170000040903688, step mean : 19.3, eval_episode_rewards : [0.84 0.78 0.56 0.54 0.91 0.86 0.79 0.84 0.88 0.93 0.71 0.74 0.81 0.99\n",
      " 0.96 0.74 0.92 0.78 0.87 0.89]\n",
      "episode : 37260, reward mean : 0.4912842242768553, total_step : 915825, cur_epsilon : 0.45\n",
      "episode : 37270, reward mean : 0.4913160771826696, total_step : 916025, cur_epsilon : 0.45\n",
      "episode : 37280, reward mean : 0.4913696404685398, total_step : 916344, cur_epsilon : 0.45\n",
      "episode : 37290, reward mean : 0.4913303887575453, total_step : 916807, cur_epsilon : 0.45\n",
      "episode : 37300, reward mean : 0.49136220366786415, total_step : 917007, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37300, reward mean : 0.818000004068017, step mean : 19.2, eval_episode_rewards : [0.86 0.69 0.64 0.96 0.93 0.94 0.93 0.71 0.74 0.88 0.84 0.76 0.83 0.96\n",
      " 0.92 0.63 0.75 0.71 0.72 0.96]\n",
      "episode : 37310, reward mean : 0.49147333682209443, total_step : 917111, cur_epsilon : 0.45\n",
      "episode : 37320, reward mean : 0.4915822667967616, total_step : 917223, cur_epsilon : 0.45\n",
      "episode : 37330, reward mean : 0.4916150601901076, total_step : 917419, cur_epsilon : 0.45\n",
      "episode : 37340, reward mean : 0.4916858649424546, total_step : 917673, cur_epsilon : 0.45\n",
      "episode : 37350, reward mean : 0.49176814449788975, total_step : 917884, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37350, reward mean : 0.5760000061243773, step mean : 28.35, eval_episode_rewards : [ 0.81  0.6   0.91  0.58  0.61 -1.    0.87  0.6  -1.21  0.92  0.99  0.78\n",
      "  0.59  0.99  0.93  0.81  0.92  0.79  0.96  0.07]\n",
      "episode : 37360, reward mean : 0.4917743628751387, total_step : 917979, cur_epsilon : 0.45\n",
      "episode : 37370, reward mean : 0.4918132244327432, total_step : 918152, cur_epsilon : 0.45\n",
      "episode : 37380, reward mean : 0.49187854994945485, total_step : 918426, cur_epsilon : 0.45\n",
      "episode : 37390, reward mean : 0.4917737415660692, total_step : 918635, cur_epsilon : 0.45\n",
      "episode : 37400, reward mean : 0.4918369036688803, total_step : 918917, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37400, reward mean : 0.5015000055544079, step mean : 25.8, eval_episode_rewards : [ 0.23  0.7  -1.02  0.95  0.76  0.77  0.92  0.74  0.9   0.83  0.73  0.94\n",
      "  0.44  0.89  0.87 -1.03  0.9   0.57  0.94 -1.  ]\n",
      "episode : 37410, reward mean : 0.4918735684911613, total_step : 919098, cur_epsilon : 0.45\n",
      "episode : 37420, reward mean : 0.49186692135980953, total_step : 919241, cur_epsilon : 0.45\n",
      "episode : 37430, reward mean : 0.49183356124328487, total_step : 919484, cur_epsilon : 0.45\n",
      "episode : 37440, reward mean : 0.4919177403146178, total_step : 919687, cur_epsilon : 0.45\n",
      "episode : 37450, reward mean : 0.4919628891164315, total_step : 919836, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37450, reward mean : 0.4380000069737434, step mean : 32.15, eval_episode_rewards : [-1.07 -1.02  0.36  0.88  0.15  0.77  0.14  0.85  0.55  0.91  0.76  0.94\n",
      "  0.43  0.87  0.83  0.89 -1.    0.89  0.87  0.76]\n",
      "synced target net\n",
      "episode : 37460, reward mean : 0.4919778483039692, total_step : 920098, cur_epsilon : 0.45\n",
      "episode : 37470, reward mean : 0.4919591726049232, total_step : 920286, cur_epsilon : 0.45\n",
      "episode : 37480, reward mean : 0.4919530468926295, total_step : 920427, cur_epsilon : 0.45\n",
      "episode : 37490, reward mean : 0.4920386822506682, total_step : 920624, cur_epsilon : 0.45\n",
      "episode : 37500, reward mean : 0.4921248052698374, total_step : 920819, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37500, reward mean : 0.5050000054761767, step mean : 25.45, eval_episode_rewards : [ 0.57  0.89  0.93  0.64  0.91  0.94  0.88  0.8   0.6   0.76  0.82  0.97\n",
      " -1.12 -1.19  0.39  0.6   0.89  0.83 -1.    0.99]\n",
      "episode : 37510, reward mean : 0.4922367421391248, total_step : 920917, cur_epsilon : 0.45\n",
      "episode : 37520, reward mean : 0.4923422227522283, total_step : 921039, cur_epsilon : 0.45\n",
      "episode : 37530, reward mean : 0.4923437302885987, total_step : 921351, cur_epsilon : 0.45\n",
      "episode : 37540, reward mean : 0.49238705907734465, total_step : 921506, cur_epsilon : 0.45\n",
      "episode : 37550, reward mean : 0.49246551791773974, total_step : 921729, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37550, reward mean : 0.5365000058896839, step mean : 27.35, eval_episode_rewards : [ 0.23  0.96  0.96  0.92 -1.37  0.83  0.95 -1.37  0.86  0.78  0.95  0.89\n",
      "  0.74  0.83  0.75  0.71  0.41  0.78  0.06  0.86]\n",
      "episode : 37560, reward mean : 0.4925266293351587, total_step : 921817, cur_epsilon : 0.45\n",
      "episode : 37570, reward mean : 0.49254725040936836, total_step : 922057, cur_epsilon : 0.45\n",
      "episode : 37580, reward mean : 0.492540984510624, total_step : 922198, cur_epsilon : 0.45\n",
      "episode : 37590, reward mean : 0.49258367113499496, total_step : 922454, cur_epsilon : 0.45\n",
      "episode : 37600, reward mean : 0.49260399462808757, total_step : 922695, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37600, reward mean : 0.8325000037439167, step mean : 17.75, eval_episode_rewards : [0.76 0.84 0.9  0.77 0.85 0.9  0.96 0.96 0.64 0.89 0.87 0.56 0.75 0.72\n",
      " 0.84 0.86 0.88 0.94 0.9  0.86]\n",
      "episode : 37610, reward mean : 0.4926495665526852, total_step : 922841, cur_epsilon : 0.45\n",
      "episode : 37620, reward mean : 0.4926725198855898, total_step : 923072, cur_epsilon : 0.45\n",
      "episode : 37630, reward mean : 0.492703167635897, total_step : 923274, cur_epsilon : 0.45\n",
      "episode : 37640, reward mean : 0.4926915568058596, total_step : 923435, cur_epsilon : 0.45\n",
      "episode : 37650, reward mean : 0.49272218321953437, total_step : 923637, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37650, reward mean : 0.6025000055320561, step mean : 25.7, eval_episode_rewards : [ 0.55  0.69  0.89  0.89  0.84  0.88  0.83  0.93  0.97  0.9   0.74  0.77\n",
      "  0.98  0.74  0.36  0.96  0.82 -1.    0.96 -1.65]\n",
      "episode : 37660, reward mean : 0.4927684598578119, total_step : 923780, cur_epsilon : 0.45\n",
      "episode : 37670, reward mean : 0.49279719135358846, total_step : 923989, cur_epsilon : 0.45\n",
      "episode : 37680, reward mean : 0.49288562097474975, total_step : 924173, cur_epsilon : 0.45\n",
      "episode : 37690, reward mean : 0.49293102144757145, total_step : 924319, cur_epsilon : 0.45\n",
      "episode : 37700, reward mean : 0.49298462064793475, total_step : 924634, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37700, reward mean : 0.698500004503876, step mean : 21.15, eval_episode_rewards : [ 0.97  0.59  0.99  0.95  0.73  0.93  0.64  0.68  0.92  0.69  0.23  0.83\n",
      "  0.7   0.56  0.87  0.94  0.96 -1.02  0.93  0.88]\n",
      "episode : 37710, reward mean : 0.49298250327374293, total_step : 924759, cur_epsilon : 0.45\n",
      "episode : 37720, reward mean : 0.49306230642892074, total_step : 924975, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 37730, reward mean : 0.49309542005140655, total_step : 925167, cur_epsilon : 0.45\n",
      "episode : 37740, reward mean : 0.49311526758323865, total_step : 925409, cur_epsilon : 0.45\n",
      "episode : 37750, reward mean : 0.4931022569178449, total_step : 925674, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37750, reward mean : 0.5165000041015446, step mean : 19.35, eval_episode_rewards : [ 0.99  0.91  0.51  0.8   0.85  0.62  0.87 -1.22  0.93  0.79  0.57  0.86\n",
      " -1.04  0.92 -1.1   0.97  0.96  0.27  0.93  0.94]\n",
      "episode : 37760, reward mean : 0.49310646712576767, total_step : 925775, cur_epsilon : 0.45\n",
      "episode : 37770, reward mean : 0.49321022501177364, total_step : 925900, cur_epsilon : 0.45\n",
      "episode : 37780, reward mean : 0.49317496555772067, total_step : 926249, cur_epsilon : 0.45\n",
      "episode : 37790, reward mean : 0.4931979412230712, total_step : 926479, cur_epsilon : 0.45\n",
      "episode : 37800, reward mean : 0.49321773012888137, total_step : 926721, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37800, reward mean : 0.6400000058114529, step mean : 27.0, eval_episode_rewards : [ 0.56  0.45  0.75 -1.08  0.78  0.94  0.82  0.74  0.87  0.66  0.66  0.38\n",
      "  0.91  0.96  0.88  0.72  0.91  0.6   0.88  0.41]\n",
      "episode : 37810, reward mean : 0.49326263419471933, total_step : 926868, cur_epsilon : 0.45\n",
      "episode : 37820, reward mean : 0.49335537279051317, total_step : 927034, cur_epsilon : 0.45\n",
      "episode : 37830, reward mean : 0.49339810200818446, total_step : 927189, cur_epsilon : 0.45\n",
      "episode : 37840, reward mean : 0.49338821879027894, total_step : 927442, cur_epsilon : 0.45\n",
      "episode : 37850, reward mean : 0.4934898335284392, total_step : 927574, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37850, reward mean : 0.5090000042691827, step mean : 20.1, eval_episode_rewards : [ 0.87  0.96  0.95  0.34  0.75 -1.19  0.92  0.67 -1.1   0.6  -1.21  0.93\n",
      "  0.91  0.9   0.88  0.88  0.87  0.98  0.86  0.41]\n",
      "episode : 37860, reward mean : 0.4935657738800944, total_step : 927803, cur_epsilon : 0.45\n",
      "episode : 37870, reward mean : 0.4936472194123219, total_step : 928011, cur_epsilon : 0.45\n",
      "episode : 37880, reward mean : 0.4936620960718192, total_step : 928271, cur_epsilon : 0.45\n",
      "episode : 37890, reward mean : 0.49368752175369357, total_step : 928491, cur_epsilon : 0.45\n",
      "episode : 37900, reward mean : 0.4937155725412798, total_step : 928800, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37900, reward mean : 0.5340000037103891, step mean : 17.6, eval_episode_rewards : [ 0.89 -1.26  0.83  0.95  0.91 -1.17  0.98  0.97  0.9   0.61  0.63  0.96\n",
      "  0.38  0.85  0.84  0.88  0.96  0.95 -1.24  0.86]\n",
      "episode : 37910, reward mean : 0.49374756526921276, total_step : 928995, cur_epsilon : 0.45\n",
      "episode : 37920, reward mean : 0.4937689926003731, total_step : 929230, cur_epsilon : 0.45\n",
      "episode : 37930, reward mean : 0.493867128906831, total_step : 929374, cur_epsilon : 0.45\n",
      "episode : 37940, reward mean : 0.4938521402074995, total_step : 929547, cur_epsilon : 0.45\n",
      "episode : 37950, reward mean : 0.4938495441238066, total_step : 929673, cur_epsilon : 0.45\n",
      "[EVAL] episode : 37950, reward mean : 0.6265000061132013, step mean : 28.35, eval_episode_rewards : [ 0.95  0.42  0.73  0.92  0.99  0.72 -1.36  0.88  0.67  0.3   0.35  0.67\n",
      "  0.82  0.7   0.75  0.73  0.83  0.98  0.89  0.59]\n",
      "episode : 37960, reward mean : 0.4939631243286413, total_step : 929758, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 37970, reward mean : 0.49395523306795697, total_step : 930104, cur_epsilon : 0.45\n",
      "episode : 37980, reward mean : 0.49397472879521276, total_step : 930346, cur_epsilon : 0.45\n",
      "episode : 37990, reward mean : 0.4940544932794829, total_step : 930559, cur_epsilon : 0.45\n",
      "episode : 38000, reward mean : 0.494096321045274, total_step : 930716, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38000, reward mean : 0.6215000039897859, step mean : 18.85, eval_episode_rewards : [ 0.81  0.27  0.93  0.78  0.79  0.74  0.78  0.84 -1.27  0.98  0.81  0.91\n",
      "  0.95  0.75  0.81  0.97  0.87  0.88 -1.01  0.84]\n",
      "episode : 38010, reward mean : 0.4941746961264642, total_step : 930934, cur_epsilon : 0.45\n",
      "episode : 38020, reward mean : 0.4942106838454676, total_step : 931113, cur_epsilon : 0.45\n",
      "episode : 38030, reward mean : 0.4942853063332711, total_step : 931345, cur_epsilon : 0.45\n",
      "episode : 38040, reward mean : 0.4942936435309028, total_step : 931629, cur_epsilon : 0.45\n",
      "episode : 38050, reward mean : 0.4942696504588588, total_step : 931836, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38050, reward mean : 0.46800000630319116, step mean : 29.15, eval_episode_rewards : [-1.37  0.74  0.93  0.32  0.85  0.64  0.79  0.8   0.76  0.73 -1.    0.95\n",
      " -1.36  0.96  0.98  0.53  0.61  0.72  0.82  0.96]\n",
      "episode : 38060, reward mean : 0.4943489280085437, total_step : 932050, cur_epsilon : 0.45\n",
      "episode : 38070, reward mean : 0.49442895193197006, total_step : 932261, cur_epsilon : 0.45\n",
      "episode : 38080, reward mean : 0.4944579884478384, total_step : 932466, cur_epsilon : 0.45\n",
      "episode : 38090, reward mean : 0.4945024993469174, total_step : 932612, cur_epsilon : 0.45\n",
      "episode : 38100, reward mean : 0.4943897690329202, total_step : 932757, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38100, reward mean : 0.7455000056885183, step mean : 26.45, eval_episode_rewards : [0.1  0.97 0.94 0.91 0.48 0.14 0.86 0.21 0.84 0.97 0.72 0.74 0.81 0.78\n",
      " 0.95 0.95 0.74 0.9  0.96 0.94]\n",
      "episode : 38110, reward mean : 0.49442666492236004, total_step : 932932, cur_epsilon : 0.45\n",
      "episode : 38120, reward mean : 0.4944821668471183, total_step : 933036, cur_epsilon : 0.45\n",
      "episode : 38130, reward mean : 0.49450852872431084, total_step : 933251, cur_epsilon : 0.45\n",
      "episode : 38140, reward mean : 0.4944328841190701, total_step : 933455, cur_epsilon : 0.45\n",
      "episode : 38150, reward mean : 0.49448047707284537, total_step : 933589, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38150, reward mean : 0.7105000042356551, step mean : 19.95, eval_episode_rewards : [ 0.84  0.93  0.85  0.53  0.84  0.74 -1.15  0.94  0.89  0.77  0.66  0.79\n",
      "  0.69  0.86  0.69  0.78  0.93  0.99  0.76  0.88]\n",
      "episode : 38160, reward mean : 0.4945120597581342, total_step : 933784, cur_epsilon : 0.45\n",
      "episode : 38170, reward mean : 0.4946091223578878, total_step : 933929, cur_epsilon : 0.45\n",
      "episode : 38180, reward mean : 0.4945950812057652, total_step : 934098, cur_epsilon : 0.45\n",
      "episode : 38190, reward mean : 0.4946381304128733, total_step : 934249, cur_epsilon : 0.45\n",
      "episode : 38200, reward mean : 0.49472801571999586, total_step : 934421, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38200, reward mean : 0.4990000044927001, step mean : 21.1, eval_episode_rewards : [ 0.78  0.95  0.86 -1.13  0.89  0.74  0.92  0.84 -1.25  0.43  0.98  0.58\n",
      " -1.05  0.66  0.84  0.45  0.89  0.84  0.81  0.95]\n",
      "episode : 38210, reward mean : 0.4947948233593227, total_step : 934681, cur_epsilon : 0.45\n",
      "episode : 38220, reward mean : 0.4948738932654394, total_step : 934894, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 38230, reward mean : 0.49491813237341076, total_step : 935040, cur_epsilon : 0.45\n",
      "episode : 38240, reward mean : 0.4950081119422458, total_step : 935211, cur_epsilon : 0.45\n",
      "episode : 38250, reward mean : 0.4950894170121527, total_step : 935415, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38250, reward mean : 0.6665000052191317, step mean : 24.35, eval_episode_rewards : [ 0.84  0.69  0.85  0.86  0.11 -1.25  0.66  0.86  0.95  0.82  0.47  0.92\n",
      "  0.89  0.91  0.95  0.85  0.88  0.85  0.92  0.3 ]\n",
      "episode : 38260, reward mean : 0.4951084736217119, total_step : 935657, cur_epsilon : 0.45\n",
      "episode : 38270, reward mean : 0.4951047870601854, total_step : 935786, cur_epsilon : 0.45\n",
      "episode : 38280, reward mean : 0.4951476019024246, total_step : 935937, cur_epsilon : 0.45\n",
      "episode : 38290, reward mean : 0.4950650352800786, total_step : 936168, cur_epsilon : 0.45\n",
      "episode : 38300, reward mean : 0.49504465276540766, total_step : 936361, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38300, reward mean : 0.7375000036321581, step mean : 17.25, eval_episode_rewards : [ 0.87  0.88  0.57  0.77  0.94  0.52  0.99  0.8   0.8  -1.14  0.87  0.97\n",
      "  0.83  0.94  0.93  0.96  0.73  0.86  0.88  0.78]\n",
      "episode : 38310, reward mean : 0.4951383503249216, total_step : 936517, cur_epsilon : 0.45\n",
      "episode : 38320, reward mean : 0.4951928549313467, total_step : 936623, cur_epsilon : 0.45\n",
      "episode : 38330, reward mean : 0.4951620193326128, total_step : 936856, cur_epsilon : 0.45\n",
      "episode : 38340, reward mean : 0.4951611946020654, total_step : 936974, cur_epsilon : 0.45\n",
      "episode : 38350, reward mean : 0.4951533298857384, total_step : 937319, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38350, reward mean : 0.5270000038668513, step mean : 18.3, eval_episode_rewards : [ 0.88  0.44  0.88  0.83 -1.15  0.99  0.99  0.78  0.89  0.85  0.82  0.83\n",
      "  0.53 -1.05  0.7   0.83  0.69  0.97 -1.05  0.89]\n",
      "episode : 38360, reward mean : 0.4950802972147919, total_step : 937514, cur_epsilon : 0.45\n",
      "episode : 38370, reward mean : 0.4950821006302143, total_step : 937622, cur_epsilon : 0.45\n",
      "episode : 38380, reward mean : 0.49514982285659415, total_step : 937877, cur_epsilon : 0.45\n",
      "episode : 38390, reward mean : 0.49518390730072875, total_step : 938061, cur_epsilon : 0.45\n",
      "episode : 38400, reward mean : 0.49517005732578884, total_step : 938229, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38400, reward mean : 0.7575000031851232, step mean : 15.25, eval_episode_rewards : [ 0.99  0.92  0.86  0.91  0.93  0.82  0.9   0.87  0.84 -1.02  0.66  0.8\n",
      "  0.67  0.97  0.94  0.72  0.77  0.86  0.85  0.89]\n",
      "episode : 38410, reward mean : 0.49514397816600136, total_step : 938444, cur_epsilon : 0.45\n",
      "episode : 38420, reward mean : 0.4951582561533574, total_step : 938704, cur_epsilon : 0.45\n",
      "episode : 38430, reward mean : 0.4951855373785245, total_step : 938914, cur_epsilon : 0.45\n",
      "episode : 38440, reward mean : 0.4952000572713931, total_step : 939173, cur_epsilon : 0.45\n",
      "episode : 38450, reward mean : 0.4952613836559812, total_step : 939452, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38450, reward mean : 0.5710000028833747, step mean : 13.9, eval_episode_rewards : [-1.02  0.88 -1.01  0.91  0.98  0.85  0.89  0.86 -1.08  0.75  0.97  0.78\n",
      "  0.97  0.77  0.82  0.7   0.83  0.81  0.97  0.79]\n",
      "episode : 38460, reward mean : 0.49529979723368617, total_step : 939619, cur_epsilon : 0.45\n",
      "episode : 38470, reward mean : 0.4952903613631163, total_step : 939770, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 38480, reward mean : 0.4953058784223973, total_step : 940025, cur_epsilon : 0.45\n",
      "episode : 38490, reward mean : 0.4953177501104635, total_step : 940294, cur_epsilon : 0.45\n",
      "episode : 38500, reward mean : 0.4953324727742122, total_step : 940552, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38500, reward mean : 0.7215000039897859, step mean : 18.85, eval_episode_rewards : [ 0.88  0.94  0.72  0.97  0.65  0.82  0.9   0.92  0.76  0.92  0.9  -1.04\n",
      "  0.44  0.77  0.81  0.85  0.64  0.97  0.89  0.72]\n",
      "episode : 38510, reward mean : 0.495369000307552, total_step : 940726, cur_epsilon : 0.45\n",
      "episode : 38520, reward mean : 0.49534579963362385, total_step : 940930, cur_epsilon : 0.45\n",
      "episode : 38530, reward mean : 0.49525720742128665, total_step : 941186, cur_epsilon : 0.45\n",
      "episode : 38540, reward mean : 0.49533368453526505, total_step : 941406, cur_epsilon : 0.45\n",
      "episode : 38550, reward mean : 0.49541738526666135, total_step : 941598, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38550, reward mean : 0.656000005453825, step mean : 25.4, eval_episode_rewards : [ 0.73  0.68  0.73  0.3   0.62  0.75  0.91  0.73  0.76  0.61  0.95  0.89\n",
      "  0.69  0.82  0.83 -1.01  0.54  0.72  0.9   0.97]\n",
      "episode : 38560, reward mean : 0.495440876609897, total_step : 941822, cur_epsilon : 0.45\n",
      "episode : 38570, reward mean : 0.49547861555906575, total_step : 941991, cur_epsilon : 0.45\n",
      "episode : 38580, reward mean : 0.495483934737601, total_step : 942285, cur_epsilon : 0.45\n",
      "episode : 38590, reward mean : 0.49551749681823687, total_step : 942470, cur_epsilon : 0.45\n",
      "episode : 38600, reward mean : 0.49561762181974445, total_step : 942598, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38600, reward mean : 0.4335000048391521, step mean : 22.6, eval_episode_rewards : [ 0.93  0.82  0.83  0.98  0.76  0.9   0.82 -1.05  0.91  0.6  -1.21 -1.\n",
      "  0.8   0.61  0.9   0.77  0.86  0.7   0.78 -1.04]\n",
      "episode : 38610, reward mean : 0.49566563590555396, total_step : 942927, cur_epsilon : 0.45\n",
      "episode : 38620, reward mean : 0.4957169912567874, total_step : 943043, cur_epsilon : 0.45\n",
      "episode : 38630, reward mean : 0.49561403578462154, total_step : 943155, cur_epsilon : 0.45\n",
      "episode : 38640, reward mean : 0.4956451915735263, total_step : 943349, cur_epsilon : 0.45\n",
      "episode : 38650, reward mean : 0.49567348518614857, total_step : 943554, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38650, reward mean : 0.7440000034868717, step mean : 16.6, eval_episode_rewards : [ 0.74  0.94  0.6   0.81  0.84  0.89  0.69  0.7   0.84  0.86  0.98  0.84\n",
      "  0.87  0.95  0.95  0.93  0.69  0.88  0.89 -1.01]\n",
      "episode : 38660, reward mean : 0.4957641024955785, total_step : 943718, cur_epsilon : 0.45\n",
      "episode : 38670, reward mean : 0.495704168671627, total_step : 943963, cur_epsilon : 0.45\n",
      "episode : 38680, reward mean : 0.4957810290221841, total_step : 944180, cur_epsilon : 0.45\n",
      "episode : 38690, reward mean : 0.4957989196854642, total_step : 944425, cur_epsilon : 0.45\n",
      "episode : 38700, reward mean : 0.4958979380531694, total_step : 944556, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38700, reward mean : 0.5010000044479966, step mean : 20.9, eval_episode_rewards : [ 0.81  0.67 -1.54  0.92  0.73 -1.35 -1.05  0.8   0.93  0.76  0.94  0.76\n",
      "  0.97  0.78  0.83  0.79  0.84  0.71  0.75  0.97]\n",
      "episode : 38710, reward mean : 0.4959666805143594, total_step : 944804, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 38720, reward mean : 0.49603022217893905, total_step : 945072, cur_epsilon : 0.45\n",
      "episode : 38730, reward mean : 0.49608701788876836, total_step : 945366, cur_epsilon : 0.45\n",
      "episode : 38740, reward mean : 0.49616959738960054, total_step : 945560, cur_epsilon : 0.45\n",
      "episode : 38750, reward mean : 0.49624387620442334, total_step : 945786, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38750, reward mean : 0.7385000036098063, step mean : 17.15, eval_episode_rewards : [ 0.86  0.5   0.91  0.88  0.74  0.8   0.93  0.82  0.73  0.94 -1.07  0.79\n",
      "  0.97  0.93  0.84  0.79  0.89  0.96  0.81  0.75]\n",
      "episode : 38760, reward mean : 0.49627735301755155, total_step : 945970, cur_epsilon : 0.45\n",
      "episode : 38770, reward mean : 0.49628811459825006, total_step : 946042, cur_epsilon : 0.45\n",
      "episode : 38780, reward mean : 0.49624214035681247, total_step : 946334, cur_epsilon : 0.45\n",
      "episode : 38790, reward mean : 0.4962374375628888, total_step : 946466, cur_epsilon : 0.45\n",
      "episode : 38800, reward mean : 0.49633351038901385, total_step : 946607, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38800, reward mean : 0.7805000026710331, step mean : 12.95, eval_episode_rewards : [ 0.98  0.83  0.83  0.98 -1.02  0.92  0.87  0.59  0.84  0.73  0.99  0.8\n",
      "  0.91  0.87  0.84  0.95  0.95  0.87  0.92  0.96]\n",
      "episode : 38810, reward mean : 0.49632955947288554, total_step : 946835, cur_epsilon : 0.45\n",
      "episode : 38820, reward mean : 0.49634286973722347, total_step : 947097, cur_epsilon : 0.45\n",
      "episode : 38830, reward mean : 0.4964360083242532, total_step : 947249, cur_epsilon : 0.45\n",
      "episode : 38840, reward mean : 0.49653399081509386, total_step : 947382, cur_epsilon : 0.45\n",
      "episode : 38850, reward mean : 0.4964921545255522, total_step : 947658, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38850, reward mean : 0.6775000049732626, step mean : 23.25, eval_episode_rewards : [ 0.95  0.9   0.99  0.93  0.6   0.38  0.97  0.41  0.57  0.92 -1.01  0.43\n",
      "  0.84  0.73  0.99  0.74  0.72  0.93  0.63  0.93]\n",
      "episode : 38860, reward mean : 0.49652985597922783, total_step : 947825, cur_epsilon : 0.45\n",
      "episode : 38870, reward mean : 0.49659480842317716, total_step : 948086, cur_epsilon : 0.45\n",
      "episode : 38880, reward mean : 0.4966404373312001, total_step : 948222, cur_epsilon : 0.45\n",
      "episode : 38890, reward mean : 0.4966623863071686, total_step : 948450, cur_epsilon : 0.45\n",
      "episode : 38900, reward mean : 0.49669820574607826, total_step : 948624, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38900, reward mean : 0.8135000041685998, step mean : 19.65, eval_episode_rewards : [0.65 0.93 0.78 0.83 0.75 0.84 0.46 0.88 0.97 0.99 0.94 0.96 0.98 0.46\n",
      " 0.96 0.59 0.91 0.68 0.91 0.8 ]\n",
      "episode : 38910, reward mean : 0.4967740992950125, total_step : 948842, cur_epsilon : 0.45\n",
      "episode : 38920, reward mean : 0.4967744142769473, total_step : 949253, cur_epsilon : 0.45\n",
      "episode : 38930, reward mean : 0.49679450818672183, total_step : 949488, cur_epsilon : 0.45\n",
      "episode : 38940, reward mean : 0.4967609194597171, total_step : 949732, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 38950, reward mean : 0.4968166932946164, total_step : 950028, cur_epsilon : 0.45\n",
      "[EVAL] episode : 38950, reward mean : 0.7110000042244792, step mean : 19.9, eval_episode_rewards : [ 0.93  0.95 -1.5   0.8   0.66  0.93  0.8   0.7   0.8   0.87  0.56  0.93\n",
      "  0.99  0.75  0.94  0.86  0.76  0.79  0.87  0.83]\n",
      "episode : 38960, reward mean : 0.49689374239914486, total_step : 950241, cur_epsilon : 0.45\n",
      "episode : 38970, reward mean : 0.496866056041002, total_step : 950462, cur_epsilon : 0.45\n",
      "episode : 38980, reward mean : 0.49686814273945057, total_step : 950767, cur_epsilon : 0.45\n",
      "episode : 38990, reward mean : 0.49696076440152664, total_step : 950919, cur_epsilon : 0.45\n",
      "episode : 39000, reward mean : 0.4970118001047044, total_step : 951233, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39000, reward mean : 0.6845000048168004, step mean : 22.55, eval_episode_rewards : [ 0.76  0.81  0.85  0.83 -1.03  0.71  0.61  0.68  0.84  0.65  0.65  0.98\n",
      "  0.94  0.92  0.94  0.49  0.68  0.87  0.61  0.9 ]\n",
      "episode : 39010, reward mean : 0.49708972581717387, total_step : 951442, cur_epsilon : 0.45\n",
      "episode : 39020, reward mean : 0.49713403906091774, total_step : 951582, cur_epsilon : 0.45\n",
      "episode : 39030, reward mean : 0.497166800005025, total_step : 951767, cur_epsilon : 0.45\n",
      "episode : 39040, reward mean : 0.4972507736740584, total_step : 951952, cur_epsilon : 0.45\n",
      "episode : 39050, reward mean : 0.4972222843606421, total_step : 952176, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39050, reward mean : 0.6160000041127205, step mean : 19.4, eval_episode_rewards : [ 0.82  0.93 -1.12  0.78  0.94  0.33  0.68  0.99  0.97  0.47  0.72  0.77\n",
      "  0.96  0.8   0.9  -1.05  0.79  0.92  0.91  0.81]\n",
      "episode : 39060, reward mean : 0.4972383564858351, total_step : 952426, cur_epsilon : 0.45\n",
      "episode : 39070, reward mean : 0.4972774559603339, total_step : 952586, cur_epsilon : 0.45\n",
      "episode : 39080, reward mean : 0.4973196060491579, total_step : 952734, cur_epsilon : 0.45\n",
      "episode : 39090, reward mean : 0.4973394782412735, total_step : 952969, cur_epsilon : 0.45\n",
      "episode : 39100, reward mean : 0.49741458323524546, total_step : 953188, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39100, reward mean : 0.6990000044927001, step mean : 21.1, eval_episode_rewards : [ 0.91  0.93  0.93  0.78  0.26 -1.17  0.68  0.94  0.92  0.71  0.63  0.98\n",
      "  0.83  0.83  0.94  0.86  0.99  0.42  0.64  0.97]\n",
      "episode : 39110, reward mean : 0.49745666593016985, total_step : 953336, cur_epsilon : 0.45\n",
      "episode : 39120, reward mean : 0.4974721422439426, total_step : 953588, cur_epsilon : 0.45\n",
      "episode : 39130, reward mean : 0.49751674430390097, total_step : 953726, cur_epsilon : 0.45\n",
      "episode : 39140, reward mean : 0.49749796128387797, total_step : 953912, cur_epsilon : 0.45\n",
      "episode : 39150, reward mean : 0.49758238070726507, total_step : 954094, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39150, reward mean : 0.43900000359863045, step mean : 17.1, eval_episode_rewards : [ 0.81  0.75 -1.07  0.84  0.63 -1.03  0.87 -1.01  0.79  0.98  0.65  0.67\n",
      "  0.87  0.8   0.98  0.97  0.64  0.82 -1.01  0.83]\n",
      "episode : 39160, reward mean : 0.49754699194952234, total_step : 954345, cur_epsilon : 0.45\n",
      "episode : 39170, reward mean : 0.49761450612194263, total_step : 954593, cur_epsilon : 0.45\n",
      "episode : 39180, reward mean : 0.49769551314032273, total_step : 954788, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 39190, reward mean : 0.4976108753486221, total_step : 955131, cur_epsilon : 0.45\n",
      "episode : 39200, reward mean : 0.49758699502439563, total_step : 955337, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39200, reward mean : 0.6240000050514937, step mean : 23.55, eval_episode_rewards : [-1.    0.86  0.73  0.86  0.93  0.86  0.96  0.83  0.94  0.66  0.4   0.96\n",
      "  0.89  0.43  0.89  0.98 -1.01  0.95  0.86  0.5 ]\n",
      "episode : 39210, reward mean : 0.4976470340478446, total_step : 955614, cur_epsilon : 0.45\n",
      "episode : 39220, reward mean : 0.4975754769276286, total_step : 956007, cur_epsilon : 0.45\n",
      "episode : 39230, reward mean : 0.4975998013037946, total_step : 956224, cur_epsilon : 0.45\n",
      "episode : 39240, reward mean : 0.49763889411776674, total_step : 956383, cur_epsilon : 0.45\n",
      "episode : 39250, reward mean : 0.49766318994210024, total_step : 956600, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39250, reward mean : 0.5990000044927001, step mean : 21.1, eval_episode_rewards : [ 0.43  0.75  0.92  0.99  0.94  0.68  0.78  0.98  0.98  0.99  0.83  0.91\n",
      " -1.07  0.85  0.49  0.55  0.59  0.77  0.75 -1.13]\n",
      "episode : 39260, reward mean : 0.49768288857049736, total_step : 956835, cur_epsilon : 0.45\n",
      "episode : 39270, reward mean : 0.497727787249959, total_step : 956971, cur_epsilon : 0.45\n",
      "episode : 39280, reward mean : 0.49770952661378526, total_step : 957355, cur_epsilon : 0.45\n",
      "episode : 39290, reward mean : 0.4976912752717836, total_step : 957539, cur_epsilon : 0.45\n",
      "episode : 39300, reward mean : 0.49772316044447684, total_step : 957726, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39300, reward mean : 0.7815000048838556, step mean : 22.85, eval_episode_rewards : [0.33 0.87 0.93 0.96 0.79 0.76 0.81 0.89 0.8  0.98 0.62 0.93 0.84 0.51\n",
      " 0.68 0.91 0.4  0.93 0.84 0.85]\n",
      "episode : 39310, reward mean : 0.49769601133336694, total_step : 957945, cur_epsilon : 0.45\n",
      "episode : 39320, reward mean : 0.497726607465802, total_step : 958137, cur_epsilon : 0.45\n",
      "episode : 39330, reward mean : 0.497561154477232, total_step : 958300, cur_epsilon : 0.45\n",
      "episode : 39340, reward mean : 0.49760371646211765, total_step : 958445, cur_epsilon : 0.45\n",
      "episode : 39350, reward mean : 0.49760432542927724, total_step : 958555, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39350, reward mean : 0.7220000039786101, step mean : 18.8, eval_episode_rewards : [ 0.76  0.9   0.98 -1.47  0.47  0.84  0.83  0.87  0.94  0.87  0.69  0.8\n",
      "  0.8   0.84  0.98  0.92  0.84  0.69  0.91  0.98]\n",
      "episode : 39360, reward mean : 0.4976826779899752, total_step : 958759, cur_epsilon : 0.45\n",
      "episode : 39370, reward mean : 0.4977091746438831, total_step : 958967, cur_epsilon : 0.45\n",
      "episode : 39380, reward mean : 0.49780066545864443, total_step : 959119, cur_epsilon : 0.45\n",
      "episode : 39390, reward mean : 0.4978469206851546, total_step : 959249, cur_epsilon : 0.45\n",
      "episode : 39400, reward mean : 0.4977675179146268, total_step : 959474, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39400, reward mean : 0.6630000030621886, step mean : 14.7, eval_episode_rewards : [ 0.77  0.82  0.96  0.83  0.86  0.98  0.88  0.86 -1.14  0.96  0.9   0.86\n",
      "  0.91  0.89  0.5  -1.03  0.94  0.81  0.78  0.92]\n",
      "episode : 39410, reward mean : 0.4978414160335818, total_step : 959695, cur_epsilon : 0.45\n",
      "episode : 39420, reward mean : 0.4978627652443477, total_step : 959923, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 39430, reward mean : 0.49790084214978847, total_step : 960085, cur_epsilon : 0.45\n",
      "episode : 39440, reward mean : 0.4979746502538875, total_step : 960306, cur_epsilon : 0.45\n",
      "episode : 39450, reward mean : 0.49794981510920217, total_step : 960516, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39450, reward mean : 0.518000004068017, step mean : 19.2, eval_episode_rewards : [-1.09  0.94  0.83  0.73  0.89  0.86  0.86  0.9   0.4   0.75  0.98  0.9\n",
      "  0.93 -1.11  0.51  0.91  0.96 -1.27  0.58  0.9 ]\n",
      "episode : 39460, reward mean : 0.4980223062875394, total_step : 960742, cur_epsilon : 0.45\n",
      "episode : 39470, reward mean : 0.4980362352713837, total_step : 960999, cur_epsilon : 0.45\n",
      "episode : 39480, reward mean : 0.4980625685462455, total_step : 961207, cur_epsilon : 0.45\n",
      "episode : 39490, reward mean : 0.4980222893463272, total_step : 961577, cur_epsilon : 0.45\n",
      "episode : 39500, reward mean : 0.49792532167975095, total_step : 961872, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39500, reward mean : 0.7965000045485795, step mean : 21.35, eval_episode_rewards : [0.78 0.8  0.68 0.75 0.94 0.71 0.92 0.86 0.91 0.93 0.85 0.52 0.87 0.42\n",
      " 0.85 0.94 0.5  0.88 0.99 0.83]\n",
      "episode : 39510, reward mean : 0.49796128085006136, total_step : 962042, cur_epsilon : 0.45\n",
      "episode : 39520, reward mean : 0.4980055720246536, total_step : 962179, cur_epsilon : 0.45\n",
      "episode : 39530, reward mean : 0.49796939555955155, total_step : 962434, cur_epsilon : 0.45\n",
      "episode : 39540, reward mean : 0.498036676947947, total_step : 962680, cur_epsilon : 0.45\n",
      "episode : 39550, reward mean : 0.4980728244388714, total_step : 962849, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39550, reward mean : 0.7425000035203994, step mean : 16.75, eval_episode_rewards : [ 0.99  0.69  0.8   0.48  0.99  0.91  0.86  0.72  0.93  0.84  0.95  0.92\n",
      "  0.93  0.69  0.84  0.93  0.9  -1.15  0.86  0.77]\n",
      "episode : 39560, reward mean : 0.4980149192778399, total_step : 963188, cur_epsilon : 0.45\n",
      "episode : 39570, reward mean : 0.49805509746432425, total_step : 963341, cur_epsilon : 0.45\n",
      "episode : 39580, reward mean : 0.49805508354549805, total_step : 963653, cur_epsilon : 0.45\n",
      "episode : 39590, reward mean : 0.4981480223986048, total_step : 963797, cur_epsilon : 0.45\n",
      "episode : 39600, reward mean : 0.49816338906097557, total_step : 964048, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39600, reward mean : 0.5835000070743263, step mean : 32.65, eval_episode_rewards : [ 0.89  0.42  0.93  0.54  0.53  0.86  0.41  0.92  0.84  0.9   0.45  0.3\n",
      "  0.82  0.88  0.67 -1.11  0.07  0.92  0.96  0.47]\n",
      "episode : 39610, reward mean : 0.4981479476609711, total_step : 964221, cur_epsilon : 0.45\n",
      "episode : 39620, reward mean : 0.49814589113882773, total_step : 964541, cur_epsilon : 0.45\n",
      "episode : 39630, reward mean : 0.4981710877356928, total_step : 964753, cur_epsilon : 0.45\n",
      "episode : 39640, reward mean : 0.49821393054982366, total_step : 964895, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 39650, reward mean : 0.49809710484317504, total_step : 965070, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39650, reward mean : 0.702500004414469, step mean : 20.75, eval_episode_rewards : [ 0.59  0.97  0.47  0.82  0.91  0.72  0.89  0.98  0.93  0.9   0.97  0.63\n",
      " -1.01  0.57  0.87  0.53  0.95  0.76  0.66  0.94]\n",
      "episode : 39660, reward mean : 0.4981147303853693, total_step : 965312, cur_epsilon : 0.45\n",
      "episode : 39670, reward mean : 0.4981593195641968, total_step : 965447, cur_epsilon : 0.45\n",
      "episode : 39680, reward mean : 0.49824042860765927, total_step : 965637, cur_epsilon : 0.45\n",
      "episode : 39690, reward mean : 0.49832905535866584, total_step : 965797, cur_epsilon : 0.45\n",
      "episode : 39700, reward mean : 0.4983597029533509, total_step : 966086, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39700, reward mean : 0.6075000043027103, step mean : 20.25, eval_episode_rewards : [ 0.93  0.87  0.44  0.84  0.88  0.93  0.94 -1.02 -1.22  0.88  0.95  0.64\n",
      "  0.75  0.67  0.65  0.74  0.95  0.73  0.9   0.7 ]\n",
      "episode : 39710, reward mean : 0.4982591339025587, total_step : 966197, cur_epsilon : 0.45\n",
      "episode : 39720, reward mean : 0.4982875178074736, total_step : 966396, cur_epsilon : 0.45\n",
      "episode : 39730, reward mean : 0.4983783087677918, total_step : 966547, cur_epsilon : 0.45\n",
      "episode : 39740, reward mean : 0.4984053902211306, total_step : 966751, cur_epsilon : 0.45\n",
      "episode : 39750, reward mean : 0.4984329611932238, total_step : 966953, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39750, reward mean : 0.7805000026710331, step mean : 12.95, eval_episode_rewards : [ 0.98  0.91  0.72  0.74  0.87  0.9   0.94  0.98  0.88  0.91  0.96  0.92\n",
      "  0.78  0.91  0.88  0.85  0.91  0.84 -1.13  0.86]\n",
      "episode : 39760, reward mean : 0.49843084022830947, total_step : 967172, cur_epsilon : 0.45\n",
      "episode : 39770, reward mean : 0.4984666886475515, total_step : 967341, cur_epsilon : 0.45\n",
      "episode : 39780, reward mean : 0.4985563149207022, total_step : 967496, cur_epsilon : 0.45\n",
      "episode : 39790, reward mean : 0.49860920350868504, total_step : 967797, cur_epsilon : 0.45\n",
      "episode : 39800, reward mean : 0.498638949940992, total_step : 967990, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39800, reward mean : 0.6795000026933848, step mean : 13.05, eval_episode_rewards : [ 0.93  0.92  0.62  0.78  0.82  0.97  0.87  0.92  0.99  0.91  0.81  0.98\n",
      "  0.76  0.9   0.94  0.77 -1.1  -1.13  0.95  0.98]\n",
      "episode : 39810, reward mean : 0.49853253473196235, total_step : 968125, cur_epsilon : 0.45\n",
      "episode : 39820, reward mean : 0.498619543136968, total_step : 968290, cur_epsilon : 0.45\n",
      "episode : 39830, reward mean : 0.4985061563582291, total_step : 968453, cur_epsilon : 0.45\n",
      "episode : 39840, reward mean : 0.4985466919628993, total_step : 968702, cur_epsilon : 0.45\n",
      "episode : 39850, reward mean : 0.49859849957008395, total_step : 969007, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39850, reward mean : 0.5260000061243773, step mean : 28.4, eval_episode_rewards : [-1.34  0.93  0.13  0.93  0.85  0.87  0.92  0.95  0.36  0.21  0.91  0.88\n",
      "  0.7   0.68  0.96 -1.32  0.94  0.49  0.64  0.83]\n",
      "episode : 39860, reward mean : 0.49867888128219523, total_step : 969198, cur_epsilon : 0.45\n",
      "episode : 39870, reward mean : 0.4986945123642268, total_step : 969447, cur_epsilon : 0.45\n",
      "episode : 39880, reward mean : 0.49873521083231676, total_step : 969596, cur_epsilon : 0.45\n",
      "episode : 39890, reward mean : 0.4987728806223751, total_step : 969757, cur_epsilon : 0.45\n",
      "episode : 39900, reward mean : 0.4988446167437243, total_step : 969982, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39900, reward mean : 0.44300000574439763, step mean : 26.6, eval_episode_rewards : [ 0.89  0.99  0.52  0.92  0.89 -1.    0.99  0.76  0.97  0.73  0.62 -1.02\n",
      "  0.52 -1.    0.85  0.71  0.85 -1.04  0.87  0.84]\n",
      "synced target net\n",
      "episode : 39910, reward mean : 0.49889827632520045, total_step : 970279, cur_epsilon : 0.45\n",
      "episode : 39920, reward mean : 0.49898447415265135, total_step : 970446, cur_epsilon : 0.45\n",
      "episode : 39930, reward mean : 0.49907338362650827, total_step : 970602, cur_epsilon : 0.45\n",
      "episode : 39940, reward mean : 0.49901252399195445, total_step : 970756, cur_epsilon : 0.45\n",
      "episode : 39950, reward mean : 0.49904881622712216, total_step : 970922, cur_epsilon : 0.45\n",
      "[EVAL] episode : 39950, reward mean : 0.7925000046379864, step mean : 21.75, eval_episode_rewards : [0.94 0.76 0.93 0.66 0.82 0.95 0.95 0.77 0.9  0.94 0.94 0.88 0.45 0.66\n",
      " 0.89 0.38 0.94 0.58 0.83 0.68]\n",
      "episode : 39960, reward mean : 0.49899349870678555, total_step : 971153, cur_epsilon : 0.45\n",
      "episode : 39970, reward mean : 0.49893870924067985, total_step : 971283, cur_epsilon : 0.45\n",
      "episode : 39980, reward mean : 0.49892796919415106, total_step : 971437, cur_epsilon : 0.45\n",
      "episode : 39990, reward mean : 0.49895224327151183, total_step : 971651, cur_epsilon : 0.45\n",
      "episode : 40000, reward mean : 0.49899000521153214, total_step : 971811, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40000, reward mean : 0.5690000029280782, step mean : 14.1, eval_episode_rewards : [ 0.78 -1.03  0.95 -1.08  0.96  0.95  0.87  0.93  0.66  0.91  0.92 -1.27\n",
      "  0.97  0.77  0.8   0.82  0.96  0.85  0.83  0.83]\n",
      "episode : 40010, reward mean : 0.4990254988377112, total_step : 971980, cur_epsilon : 0.45\n",
      "episode : 40020, reward mean : 0.49905922560054794, total_step : 972156, cur_epsilon : 0.45\n",
      "episode : 40030, reward mean : 0.49903697748241566, total_step : 972556, cur_epsilon : 0.45\n",
      "episode : 40040, reward mean : 0.49902298223417063, total_step : 972723, cur_epsilon : 0.45\n",
      "episode : 40050, reward mean : 0.4990604296801484, total_step : 972884, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40050, reward mean : 0.8360000036656856, step mean : 17.4, eval_episode_rewards : [0.91 0.92 0.94 0.8  0.8  0.77 0.77 0.76 0.97 0.78 0.95 0.87 0.84 0.85\n",
      " 0.96 0.83 0.89 0.97 0.27 0.87]\n",
      "episode : 40060, reward mean : 0.49913804814609114, total_step : 973084, cur_epsilon : 0.45\n",
      "episode : 40070, reward mean : 0.4991944149924925, total_step : 973169, cur_epsilon : 0.45\n",
      "episode : 40080, reward mean : 0.49922605311347473, total_step : 973353, cur_epsilon : 0.45\n",
      "episode : 40090, reward mean : 0.4992596709609671, total_step : 973529, cur_epsilon : 0.45\n",
      "episode : 40100, reward mean : 0.4992266885006796, total_step : 973772, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40100, reward mean : 0.6970000045374036, step mean : 21.3, eval_episode_rewards : [-1.09  0.49  0.31  0.97  0.8   0.93  0.92  0.79  0.86  0.53  0.94  0.98\n",
      "  0.57  0.85  0.83  0.67  0.87  0.93  0.89  0.9 ]\n",
      "episode : 40110, reward mean : 0.49929818521378916, total_step : 973996, cur_epsilon : 0.45\n",
      "episode : 40120, reward mean : 0.49937064329441727, total_step : 974216, cur_epsilon : 0.45\n",
      "episode : 40130, reward mean : 0.49943932741147057, total_step : 974451, cur_epsilon : 0.45\n",
      "episode : 40140, reward mean : 0.49952815667798955, total_step : 974605, cur_epsilon : 0.45\n",
      "episode : 40150, reward mean : 0.4995076017209316, total_step : 974798, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40150, reward mean : 0.7510000033304095, step mean : 15.9, eval_episode_rewards : [ 0.81  0.79  0.96  0.64  0.84  0.88  0.84  0.76  0.48 -1.01  0.9   0.84\n",
      "  0.97  0.77  0.98  0.99  0.89  0.85  0.97  0.87]\n",
      "episode : 40160, reward mean : 0.4995062303067504, total_step : 974914, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 40170, reward mean : 0.49952128974789, total_step : 975164, cur_epsilon : 0.45\n",
      "episode : 40180, reward mean : 0.49957168265901813, total_step : 975472, cur_epsilon : 0.45\n",
      "episode : 40190, reward mean : 0.49958895768327893, total_step : 975713, cur_epsilon : 0.45\n",
      "episode : 40200, reward mean : 0.4994711494863738, total_step : 975996, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40200, reward mean : 0.6210000040009618, step mean : 18.9, eval_episode_rewards : [ 0.69  0.94 -1.09  0.65  0.89  0.69  0.83  0.77  0.85  0.9   0.78  0.57\n",
      "  0.93  0.6   0.93  0.99  0.86  0.9  -1.02  0.76]\n",
      "episode : 40210, reward mean : 0.49952027379805897, total_step : 976309, cur_epsilon : 0.45\n",
      "episode : 40220, reward mean : 0.49951144230414474, total_step : 976554, cur_epsilon : 0.45\n",
      "episode : 40230, reward mean : 0.49950087520518677, total_step : 976707, cur_epsilon : 0.45\n",
      "episode : 40240, reward mean : 0.4995770429808158, total_step : 976911, cur_epsilon : 0.45\n",
      "episode : 40250, reward mean : 0.4996429865739268, total_step : 977156, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40250, reward mean : 0.7145000041462481, step mean : 19.55, eval_episode_rewards : [ 0.97  0.81  0.9   0.95  0.64  0.93 -1.16  0.99  0.82  0.93  0.68  0.67\n",
      "  0.59  0.29  0.75  0.86  0.86  0.92  0.94  0.95]\n",
      "episode : 40260, reward mean : 0.499638604312641, total_step : 977284, cur_epsilon : 0.45\n",
      "episode : 40270, reward mean : 0.499620814742125, total_step : 977466, cur_epsilon : 0.45\n",
      "episode : 40280, reward mean : 0.4996142058017319, total_step : 977603, cur_epsilon : 0.45\n",
      "episode : 40290, reward mean : 0.49968429411125553, total_step : 977831, cur_epsilon : 0.45\n",
      "episode : 40300, reward mean : 0.4997307744359242, total_step : 977954, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40300, reward mean : 0.5800000049173832, step mean : 23.0, eval_episode_rewards : [ 0.76  0.75  0.85  0.85  0.84  0.94  0.86  0.63  0.8  -1.03  0.67  0.68\n",
      "  0.92  0.92  0.94  0.8   0.13  0.49  0.87 -1.07]\n",
      "episode : 40310, reward mean : 0.49977772785505475, total_step : 978275, cur_epsilon : 0.45\n",
      "episode : 40320, reward mean : 0.4998030806022201, total_step : 978483, cur_epsilon : 0.45\n",
      "episode : 40330, reward mean : 0.4998207341912398, total_step : 978722, cur_epsilon : 0.45\n",
      "episode : 40340, reward mean : 0.49987853767953344, total_step : 978999, cur_epsilon : 0.45\n",
      "episode : 40350, reward mean : 0.49991450334639465, total_step : 979164, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40350, reward mean : 0.7000000044703484, step mean : 21.0, eval_episode_rewards : [ 0.96  0.7   0.72  0.97  0.51  0.93  0.74  0.99  0.46 -1.13  0.8   0.55\n",
      "  0.73  0.89  0.92  0.8   0.87  0.76  0.92  0.91]\n",
      "episode : 40360, reward mean : 0.49998761670150754, total_step : 979379, cur_epsilon : 0.45\n",
      "episode : 40370, reward mean : 0.5000728315607559, total_step : 979545, cur_epsilon : 0.45\n",
      "episode : 40380, reward mean : 0.5001097129802244, total_step : 979706, cur_epsilon : 0.45\n",
      "episode : 40390, reward mean : 0.500150537513534, total_step : 979851, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 40400, reward mean : 0.5002198071836835, total_step : 980081, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40400, reward mean : 0.7085000042803585, step mean : 20.15, eval_episode_rewards : [ 0.88  0.47  0.88  0.86  0.88  0.97  0.74  0.28  0.89  0.92  0.9   0.72\n",
      "  0.97  0.84 -1.09  0.88  0.65  0.83  0.77  0.93]\n",
      "episode : 40410, reward mean : 0.5001821383389542, total_step : 980343, cur_epsilon : 0.45\n",
      "episode : 40420, reward mean : 0.5002612620068623, total_step : 980533, cur_epsilon : 0.45\n",
      "episode : 40430, reward mean : 0.5003049767595739, total_step : 980866, cur_epsilon : 0.45\n",
      "episode : 40440, reward mean : 0.5003733978842585, total_step : 981099, cur_epsilon : 0.45\n",
      "episode : 40450, reward mean : 0.5003629223849482, total_step : 981251, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40450, reward mean : 0.6060000043362379, step mean : 20.4, eval_episode_rewards : [ 0.43  0.69  0.82  0.95  0.82  0.93  0.85  0.51  0.81  0.63  0.87  0.7\n",
      "  0.96 -1.11  0.93  0.84 -1.11  0.9   0.77  0.93]\n",
      "episode : 40460, reward mean : 0.5004567526074636, total_step : 981381, cur_epsilon : 0.45\n",
      "episode : 40470, reward mean : 0.5004245171867898, total_step : 981621, cur_epsilon : 0.45\n",
      "episode : 40480, reward mean : 0.5003599360322473, total_step : 981792, cur_epsilon : 0.45\n",
      "episode : 40490, reward mean : 0.5004351743795741, total_step : 981997, cur_epsilon : 0.45\n",
      "episode : 40500, reward mean : 0.5004980298934252, total_step : 982252, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40500, reward mean : 0.6005000044591725, step mean : 20.95, eval_episode_rewards : [ 0.84  0.84 -1.12  0.89  0.83  0.58  0.83  0.83  0.86  0.93  0.76  0.8\n",
      "  0.81  0.9   0.73  0.96 -1.5   0.89  0.37  0.98]\n",
      "episode : 40510, reward mean : 0.5005423404273153, total_step : 982382, cur_epsilon : 0.45\n",
      "episode : 40520, reward mean : 0.5005730555466351, total_step : 982567, cur_epsilon : 0.45\n",
      "episode : 40530, reward mean : 0.5006015349319216, total_step : 982761, cur_epsilon : 0.45\n",
      "episode : 40540, reward mean : 0.5006334536464919, total_step : 982941, cur_epsilon : 0.45\n",
      "episode : 40550, reward mean : 0.5006500668527901, total_step : 983183, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40550, reward mean : 0.6785000060684979, step mean : 28.1, eval_episode_rewards : [ 0.89  0.99  0.71  0.48  0.98  0.83  0.87  0.74  0.73  0.9   0.4  -1.\n",
      "  0.73  0.96  0.21  0.88  0.73  0.61  0.96  0.97]\n",
      "episode : 40560, reward mean : 0.5006163760092058, total_step : 983429, cur_epsilon : 0.45\n",
      "episode : 40570, reward mean : 0.5007044666247258, total_step : 983581, cur_epsilon : 0.45\n",
      "episode : 40580, reward mean : 0.5007203107692867, total_step : 983826, cur_epsilon : 0.45\n",
      "episode : 40590, reward mean : 0.5007413207949122, total_step : 984050, cur_epsilon : 0.45\n",
      "episode : 40600, reward mean : 0.500780300765936, total_step : 984201, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40600, reward mean : 0.37950000604614614, step mean : 28.0, eval_episode_rewards : [ 0.42  0.79  0.91  0.82  0.58 -1.   -1.37  0.75  0.87  0.73  0.57  0.72\n",
      " -1.19  0.73  0.83  0.95  0.8  -1.05  0.86  0.87]\n",
      "episode : 40610, reward mean : 0.5007187936747182, total_step : 984360, cur_epsilon : 0.45\n",
      "episode : 40620, reward mean : 0.5007336339533167, total_step : 984609, cur_epsilon : 0.45\n",
      "episode : 40630, reward mean : 0.5007184398528111, total_step : 984780, cur_epsilon : 0.45\n",
      "episode : 40640, reward mean : 0.5007947886629419, total_step : 984979, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 40650, reward mean : 0.5007724529226181, total_step : 985179, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40650, reward mean : 0.5190000040456653, step mean : 19.1, eval_episode_rewards : [-1.05  0.71  0.78  0.55  0.9   0.82  0.97  0.98  0.94 -1.22  0.88  0.59\n",
      "  0.82  0.61  0.98  0.82 -1.03  0.84  0.59  0.9 ]\n",
      "episode : 40660, reward mean : 0.5007685738153008, total_step : 985304, cur_epsilon : 0.45\n",
      "episode : 40670, reward mean : 0.5008060047052616, total_step : 985461, cur_epsilon : 0.45\n",
      "episode : 40680, reward mean : 0.5008264555410833, total_step : 985687, cur_epsilon : 0.45\n",
      "episode : 40690, reward mean : 0.5008269897143685, total_step : 985994, cur_epsilon : 0.45\n",
      "episode : 40700, reward mean : 0.5008002508973991, total_step : 986212, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40700, reward mean : 0.5660000052303076, step mean : 24.4, eval_episode_rewards : [ 0.29  0.23  0.94  0.89  0.88  0.88  0.95  0.6   0.69 -1.03  0.92  0.59\n",
      "  0.94  0.91  0.81  0.88  0.73  0.79  0.96 -1.53]\n",
      "episode : 40710, reward mean : 0.5008730093728695, total_step : 986425, cur_epsilon : 0.45\n",
      "episode : 40720, reward mean : 0.5009538362379056, total_step : 986605, cur_epsilon : 0.45\n",
      "episode : 40730, reward mean : 0.5009769263848106, total_step : 986820, cur_epsilon : 0.45\n",
      "episode : 40740, reward mean : 0.5010284784417754, total_step : 987119, cur_epsilon : 0.45\n",
      "episode : 40750, reward mean : 0.5011123978344301, total_step : 987286, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40750, reward mean : 0.8050000043585896, step mean : 20.5, eval_episode_rewards : [0.94 0.6  0.96 0.77 0.76 0.59 0.85 0.88 0.76 0.86 0.78 0.82 0.98 0.87\n",
      " 0.78 0.9  0.79 0.76 0.47 0.98]\n",
      "episode : 40760, reward mean : 0.5011486803676835, total_step : 987447, cur_epsilon : 0.45\n",
      "episode : 40770, reward mean : 0.5010900223654642, total_step : 987694, cur_epsilon : 0.45\n",
      "episode : 40780, reward mean : 0.5011851449697102, total_step : 987815, cur_epsilon : 0.45\n",
      "episode : 40790, reward mean : 0.5012091250774628, total_step : 988026, cur_epsilon : 0.45\n",
      "episode : 40800, reward mean : 0.5012375051954159, total_step : 988318, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40800, reward mean : 0.6525000055320561, step mean : 25.75, eval_episode_rewards : [-1.02  0.95  0.79  0.57  0.71  0.8   0.99  0.09  0.65  0.83  0.92  0.64\n",
      "  0.9   0.99  0.95  0.89  0.76  0.93  0.27  0.44]\n",
      "episode : 40810, reward mean : 0.5013170843423637, total_step : 988502, cur_epsilon : 0.45\n",
      "episode : 40820, reward mean : 0.5012430233232922, total_step : 988713, cur_epsilon : 0.45\n",
      "episode : 40830, reward mean : 0.5013176637790876, total_step : 988917, cur_epsilon : 0.45\n",
      "episode : 40840, reward mean : 0.5013310531869414, total_step : 989171, cur_epsilon : 0.45\n",
      "episode : 40850, reward mean : 0.5013620614979959, total_step : 989353, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40850, reward mean : 0.7240000039339065, step mean : 18.6, eval_episode_rewards : [ 0.96  0.82  0.93  0.79  0.89  0.59  0.85  0.38  0.92  0.74  0.73  0.98\n",
      "  0.9   0.95  0.89 -1.35  0.8   0.81  0.98  0.92]\n",
      "episode : 40860, reward mean : 0.5013830203681042, total_step : 989576, cur_epsilon : 0.45\n",
      "episode : 40870, reward mean : 0.5014472770318936, total_step : 989822, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 40880, reward mean : 0.5013094474645805, total_step : 990094, cur_epsilon : 0.45\n",
      "episode : 40890, reward mean : 0.5014003475759462, total_step : 990231, cur_epsilon : 0.45\n",
      "episode : 40900, reward mean : 0.5014804452914098, total_step : 990412, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40900, reward mean : 0.587000004760921, step mean : 22.3, eval_episode_rewards : [ 0.92  0.73  0.76  0.81  0.87  0.96 -1.08  0.99  0.48 -1.06  0.86  0.85\n",
      "  0.97  0.75  0.92  0.33  0.68  0.98  0.71  0.31]\n",
      "episode : 40910, reward mean : 0.5015732146772246, total_step : 990541, cur_epsilon : 0.45\n",
      "episode : 40920, reward mean : 0.5016116865219153, total_step : 990692, cur_epsilon : 0.45\n",
      "episode : 40930, reward mean : 0.5015959006233376, total_step : 990865, cur_epsilon : 0.45\n",
      "episode : 40940, reward mean : 0.5016592626420769, total_step : 991114, cur_epsilon : 0.45\n",
      "episode : 40950, reward mean : 0.5016520198446872, total_step : 991452, cur_epsilon : 0.45\n",
      "[EVAL] episode : 40950, reward mean : 0.7740000050514937, step mean : 23.6, eval_episode_rewards : [0.8  0.93 0.98 0.63 0.76 0.8  0.77 0.96 0.77 0.96 0.84 0.54 0.67 0.2\n",
      " 0.91 0.6  0.83 0.67 0.93 0.93]\n",
      "episode : 40960, reward mean : 0.5017268118330775, total_step : 991654, cur_epsilon : 0.45\n",
      "episode : 40970, reward mean : 0.5017647110743183, total_step : 991807, cur_epsilon : 0.45\n",
      "episode : 40980, reward mean : 0.5017889266168692, total_step : 992016, cur_epsilon : 0.45\n",
      "episode : 40990, reward mean : 0.5018516763311337, total_step : 992267, cur_epsilon : 0.45\n",
      "episode : 41000, reward mean : 0.5018341515335004, total_step : 992546, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41000, reward mean : 0.6275000038556755, step mean : 18.25, eval_episode_rewards : [ 0.89  0.99  0.9   0.72  0.36  0.84 -1.21  0.99  0.87  0.88  0.69  0.93\n",
      "  0.89 -1.03  0.92  0.88  0.74  0.37  0.96  0.97]\n",
      "episode : 41010, reward mean : 0.5018117584227259, total_step : 992746, cur_epsilon : 0.45\n",
      "episode : 41020, reward mean : 0.5017415946601063, total_step : 992942, cur_epsilon : 0.45\n",
      "episode : 41030, reward mean : 0.5017816283935538, total_step : 993086, cur_epsilon : 0.45\n",
      "episode : 41040, reward mean : 0.5018089720523656, total_step : 993282, cur_epsilon : 0.45\n",
      "episode : 41050, reward mean : 0.5017834400261745, total_step : 993495, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41050, reward mean : 0.4205000062473118, step mean : 28.95, eval_episode_rewards : [ 0.93  0.99  0.72  0.8   0.92  0.59  0.92  0.93  0.93  0.86  0.46  0.89\n",
      " -1.08 -1.55  0.45 -1.61  0.6   0.5   0.4   0.76]\n",
      "episode : 41060, reward mean : 0.5017817879468566, total_step : 993610, cur_epsilon : 0.45\n",
      "episode : 41070, reward mean : 0.501757005433227, total_step : 993820, cur_epsilon : 0.45\n",
      "episode : 41080, reward mean : 0.5018527315765848, total_step : 993935, cur_epsilon : 0.45\n",
      "episode : 41090, reward mean : 0.5017950891505544, total_step : 994080, cur_epsilon : 0.45\n",
      "episode : 41100, reward mean : 0.5017992752617783, total_step : 994371, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41100, reward mean : 0.5395000058226287, step mean : 27.05, eval_episode_rewards : [ 0.95  0.95  0.13  0.96  0.97  0.73  0.77  0.86  0.67 -1.16  0.49  0.81\n",
      "  0.87 -1.12  0.66  0.54  0.96  0.92  0.53  0.3 ]\n",
      "episode : 41110, reward mean : 0.5017891075964578, total_step : 994521, cur_epsilon : 0.45\n",
      "episode : 41120, reward mean : 0.5017976705581022, total_step : 994794, cur_epsilon : 0.45\n",
      "episode : 41130, reward mean : 0.5018944861019089, total_step : 994904, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 41140, reward mean : 0.5019265973118283, total_step : 995080, cur_epsilon : 0.45\n",
      "episode : 41150, reward mean : 0.5019166516024226, total_step : 995229, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41150, reward mean : 0.656000005453825, step mean : 25.4, eval_episode_rewards : [ 0.96  0.93  0.93  0.81 -1.02  0.08  0.45  0.76  0.87  0.85  0.39  0.53\n",
      "  0.83  0.56  0.83  0.91  0.86  0.76  0.94  0.89]\n",
      "episode : 41160, reward mean : 0.5019093832232336, total_step : 995367, cur_epsilon : 0.45\n",
      "episode : 41170, reward mean : 0.5019106197106895, total_step : 995470, cur_epsilon : 0.45\n",
      "episode : 41180, reward mean : 0.5019946627858927, total_step : 995632, cur_epsilon : 0.45\n",
      "episode : 41190, reward mean : 0.5020252540315976, total_step : 995814, cur_epsilon : 0.45\n",
      "episode : 41200, reward mean : 0.5021070440193566, total_step : 995985, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41200, reward mean : 0.5900000024586916, step mean : 12.0, eval_episode_rewards : [-1.05  0.71  0.93  0.95  0.99  0.89 -1.04  0.94  0.93  0.91  0.97  0.76\n",
      " -1.02  0.75  0.86  0.87  0.81  0.93  0.72  0.99]\n",
      "episode : 41210, reward mean : 0.5021827278242919, total_step : 996181, cur_epsilon : 0.45\n",
      "episode : 41220, reward mean : 0.5022176179930492, total_step : 996345, cur_epsilon : 0.45\n",
      "episode : 41230, reward mean : 0.5022866896366989, total_step : 996568, cur_epsilon : 0.45\n",
      "episode : 41240, reward mean : 0.5021743504790758, total_step : 996739, cur_epsilon : 0.45\n",
      "episode : 41250, reward mean : 0.5021190354857029, total_step : 996875, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41250, reward mean : 0.6565000065602362, step mean : 30.3, eval_episode_rewards : [ 0.8   0.89  0.76  0.35  0.66  0.72  0.87  0.98  0.89  0.68  0.42 -1.\n",
      "  0.85  0.52  0.82  0.99  0.72  0.6   0.98  0.63]\n",
      "episode : 41260, reward mean : 0.5020981632047109, total_step : 997069, cur_epsilon : 0.45\n",
      "episode : 41270, reward mean : 0.5021112239854837, total_step : 997323, cur_epsilon : 0.45\n",
      "episode : 41280, reward mean : 0.5021286389033776, total_step : 997559, cur_epsilon : 0.45\n",
      "episode : 41290, reward mean : 0.5021908504234752, total_step : 997810, cur_epsilon : 0.45\n",
      "episode : 41300, reward mean : 0.5020987945289935, total_step : 998098, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41300, reward mean : 0.7150000041350723, step mean : 19.5, eval_episode_rewards : [ 0.58  0.86  0.89  0.33  0.88  0.87  0.87  0.95  0.81  0.83  0.55  0.87\n",
      "  0.86  0.86  0.7   0.85 -1.02  0.94  0.95  0.87]\n",
      "episode : 41310, reward mean : 0.5021174101693737, total_step : 998329, cur_epsilon : 0.45\n",
      "episode : 41320, reward mean : 0.5021306925012378, total_step : 998582, cur_epsilon : 0.45\n",
      "episode : 41330, reward mean : 0.5021180792210749, total_step : 998841, cur_epsilon : 0.45\n",
      "episode : 41340, reward mean : 0.5021052301466649, total_step : 999101, cur_epsilon : 0.45\n",
      "episode : 41350, reward mean : 0.5020935964763182, total_step : 999257, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41350, reward mean : 0.6590000053867697, step mean : 25.1, eval_episode_rewards : [-1.15  0.79  0.96  0.73  0.78  0.71  0.86  0.77  0.93  0.89  0.62  0.78\n",
      "  0.6   0.67  0.78  0.89  0.64  0.65  0.58  0.7 ]\n",
      "episode : 41360, reward mean : 0.5021189606948476, total_step : 999460, cur_epsilon : 0.45\n",
      "episode : 41370, reward mean : 0.5022105442196156, total_step : 999589, cur_epsilon : 0.45\n",
      "episode : 41380, reward mean : 0.502286858733837, total_step : 999781, cur_epsilon : 0.45\n",
      "episode : 41390, reward mean : 0.5023131242920614, total_step : 999980, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 41400, reward mean : 0.502291551074643, total_step : 1000177, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41400, reward mean : 0.7030000044032931, step mean : 20.7, eval_episode_rewards : [ 0.94  0.68  0.8   0.53  0.75  0.74  0.99  0.84  0.9   0.66  0.5   0.84\n",
      " -1.11  0.93  0.84  0.96  0.93  0.71  0.72  0.91]\n",
      "episode : 41410, reward mean : 0.5022603287742323, total_step : 1000414, cur_epsilon : 0.45\n",
      "episode : 41420, reward mean : 0.5023266589712893, total_step : 1000647, cur_epsilon : 0.45\n",
      "episode : 41430, reward mean : 0.5023273042398441, total_step : 1000952, cur_epsilon : 0.45\n",
      "episode : 41440, reward mean : 0.5023204685010836, total_step : 1001088, cur_epsilon : 0.45\n",
      "episode : 41450, reward mean : 0.5022475323215828, total_step : 1001298, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41450, reward mean : 0.602500004414469, step mean : 20.75, eval_episode_rewards : [ 0.74  0.91 -1.11  0.74  0.91  0.95  0.57  0.99  0.8   0.56  0.89  0.99\n",
      "  0.72  0.89  0.82 -1.22  0.91  0.8   0.28  0.91]\n",
      "episode : 41460, reward mean : 0.5022315536605466, total_step : 1001472, cur_epsilon : 0.45\n",
      "episode : 41470, reward mean : 0.5022536825370704, total_step : 1001688, cur_epsilon : 0.45\n",
      "episode : 41480, reward mean : 0.502206610772254, total_step : 1001791, cur_epsilon : 0.45\n",
      "episode : 41490, reward mean : 0.5021007523466792, total_step : 1001938, cur_epsilon : 0.45\n",
      "episode : 41500, reward mean : 0.5021404871058661, total_step : 1002081, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41500, reward mean : 0.625500003900379, step mean : 18.45, eval_episode_rewards : [ 0.99  0.88  0.96  0.61  0.97  0.88  0.79  0.99  0.68  0.95  0.94  0.61\n",
      "  0.73  0.88  0.83 -1.04 -1.5   0.47  0.94  0.95]\n",
      "episode : 41510, reward mean : 0.5021985115622956, total_step : 1002348, cur_epsilon : 0.45\n",
      "episode : 41520, reward mean : 0.5021348799370688, total_step : 1002520, cur_epsilon : 0.45\n",
      "episode : 41530, reward mean : 0.5021485724787134, total_step : 1002771, cur_epsilon : 0.45\n",
      "episode : 41540, reward mean : 0.5021834428285392, total_step : 1003033, cur_epsilon : 0.45\n",
      "episode : 41550, reward mean : 0.50226667184432, total_step : 1003195, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41550, reward mean : 0.6015000044368207, step mean : 20.85, eval_episode_rewards : [ 0.93  0.98  0.97  0.84  0.8  -1.16  0.4   0.67  0.9   0.86  0.59  0.75\n",
      "  0.85  0.89  0.77 -1.07  0.62  0.8   0.72  0.92]\n",
      "episode : 41560, reward mean : 0.5023455297201513, total_step : 1003375, cur_epsilon : 0.45\n",
      "episode : 41570, reward mean : 0.5023661345974711, total_step : 1003597, cur_epsilon : 0.45\n",
      "episode : 41580, reward mean : 0.5023167439932635, total_step : 1003710, cur_epsilon : 0.45\n",
      "episode : 41590, reward mean : 0.5022505461716414, total_step : 1003893, cur_epsilon : 0.45\n",
      "episode : 41600, reward mean : 0.5023021686380246, total_step : 1004186, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41600, reward mean : 0.5690000051632523, step mean : 24.1, eval_episode_rewards : [ 0.7   0.72  0.91  0.66  0.9   0.78  0.44  0.63 -1.35  0.89  0.8   0.75\n",
      "  0.93 -1.04  0.42  0.94  0.77  0.87  0.79  0.87]\n",
      "episode : 41610, reward mean : 0.5022766213743672, total_step : 1004400, cur_epsilon : 0.45\n",
      "episode : 41620, reward mean : 0.5022573333836426, total_step : 1004588, cur_epsilon : 0.45\n",
      "episode : 41630, reward mean : 0.5023391836527095, total_step : 1004755, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 41640, reward mean : 0.5023741646383519, total_step : 1005117, cur_epsilon : 0.45\n",
      "episode : 41650, reward mean : 0.502452106016318, total_step : 1005300, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41650, reward mean : 0.7825000048615038, step mean : 22.75, eval_episode_rewards : [0.9  0.97 0.56 0.95 0.75 0.91 0.8  0.95 0.72 0.97 0.91 0.43 0.7  0.74\n",
      " 0.26 0.78 0.62 0.92 0.94 0.87]\n",
      "episode : 41660, reward mean : 0.5024299139611644, total_step : 1005500, cur_epsilon : 0.45\n",
      "episode : 41670, reward mean : 0.5024012530758487, total_step : 1005727, cur_epsilon : 0.45\n",
      "episode : 41680, reward mean : 0.502438584829711, total_step : 1005879, cur_epsilon : 0.45\n",
      "episode : 41690, reward mean : 0.5024794966594085, total_step : 1006016, cur_epsilon : 0.45\n",
      "episode : 41700, reward mean : 0.5025088780760157, total_step : 1006201, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41700, reward mean : 0.837500003632158, step mean : 17.25, eval_episode_rewards : [0.98 0.74 0.98 0.99 0.88 0.98 0.85 0.92 0.9  0.63 0.96 0.98 0.84 0.68\n",
      " 0.4  0.74 0.97 0.91 0.57 0.85]\n",
      "episode : 41710, reward mean : 0.5024917337762559, total_step : 1006380, cur_epsilon : 0.45\n",
      "episode : 41720, reward mean : 0.5025254126520308, total_step : 1006547, cur_epsilon : 0.45\n",
      "episode : 41730, reward mean : 0.5025813615121432, total_step : 1006821, cur_epsilon : 0.45\n",
      "episode : 41740, reward mean : 0.5026451896490983, total_step : 1007062, cur_epsilon : 0.45\n",
      "episode : 41750, reward mean : 0.5025492267299466, total_step : 1007170, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41750, reward mean : 0.66750000519678, step mean : 24.25, eval_episode_rewards : [-1.18  0.8   0.13  0.92  0.45  0.89  0.97  0.85  0.44  0.88  0.73  0.82\n",
      "  0.93  0.93  0.77  0.88  0.81  0.57  0.91  0.85]\n",
      "episode : 41760, reward mean : 0.5024571411886567, total_step : 1007462, cur_epsilon : 0.45\n",
      "episode : 41770, reward mean : 0.5024730719676785, total_step : 1007703, cur_epsilon : 0.45\n",
      "episode : 41780, reward mean : 0.5025418912431037, total_step : 1007923, cur_epsilon : 0.45\n",
      "episode : 41790, reward mean : 0.5025484617419771, total_step : 1008203, cur_epsilon : 0.45\n",
      "episode : 41800, reward mean : 0.5026038329248009, total_step : 1008479, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41800, reward mean : 0.6495000033639371, step mean : 16.05, eval_episode_rewards : [ 0.62  0.99  0.82  0.84  0.94  0.81  0.97  0.9   0.93  0.91  0.65 -1.07\n",
      "  0.75  0.82  0.8   0.79 -1.01  0.94  0.82  0.77]\n",
      "episode : 41810, reward mean : 0.5026515239493731, total_step : 1008787, cur_epsilon : 0.45\n",
      "episode : 41820, reward mean : 0.5026721716013977, total_step : 1009008, cur_epsilon : 0.45\n",
      "episode : 41830, reward mean : 0.5026588624528961, total_step : 1009171, cur_epsilon : 0.45\n",
      "episode : 41840, reward mean : 0.5026049286910167, total_step : 1009304, cur_epsilon : 0.45\n",
      "episode : 41850, reward mean : 0.502590686176039, total_step : 1009471, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41850, reward mean : 0.7400000035762787, step mean : 17.0, eval_episode_rewards : [ 0.98  0.71  0.92  0.7   0.91  0.82  0.91  0.91 -1.01  0.83  0.93  0.77\n",
      "  0.95  0.95  0.7   0.67  0.73  0.86  0.86  0.7 ]\n",
      "episode : 41860, reward mean : 0.5026325899783683, total_step : 1009603, cur_epsilon : 0.45\n",
      "episode : 41870, reward mean : 0.5026207360049524, total_step : 1009760, cur_epsilon : 0.45\n",
      "episode : 41880, reward mean : 0.5026872066995298, total_step : 1009989, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 41890, reward mean : 0.5026500409795382, total_step : 1010252, cur_epsilon : 0.45\n",
      "episode : 41900, reward mean : 0.5026577617348947, total_step : 1010527, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41900, reward mean : 0.45900000538676977, step mean : 25.1, eval_episode_rewards : [ 0.73  0.01  0.92  0.89  0.54  0.67  0.37 -1.15  0.95  0.76  0.67  0.75\n",
      "  0.81  0.85  0.93  0.91 -1.04  0.89 -1.18  0.9 ]\n",
      "episode : 41910, reward mean : 0.5027232215877341, total_step : 1010760, cur_epsilon : 0.45\n",
      "episode : 41920, reward mean : 0.5027390318891595, total_step : 1011001, cur_epsilon : 0.45\n",
      "episode : 41930, reward mean : 0.5028270979447216, total_step : 1011139, cur_epsilon : 0.45\n",
      "episode : 41940, reward mean : 0.5027515550040537, total_step : 1011363, cur_epsilon : 0.45\n",
      "episode : 41950, reward mean : 0.5027780743006337, total_step : 1011559, cur_epsilon : 0.45\n",
      "[EVAL] episode : 41950, reward mean : 0.6690000040456653, step mean : 19.05, eval_episode_rewards : [ 0.99 -1.06  0.93  0.94  0.96  0.84  0.96 -1.    0.99  0.17  0.69  0.71\n",
      "  0.92  0.96  0.82  0.9   0.93  0.87  0.87  0.99]\n",
      "episode : 41960, reward mean : 0.5028103006898902, total_step : 1011731, cur_epsilon : 0.45\n",
      "episode : 41970, reward mean : 0.5027905698591382, total_step : 1011921, cur_epsilon : 0.45\n",
      "episode : 41980, reward mean : 0.5028692286094654, total_step : 1012098, cur_epsilon : 0.45\n",
      "episode : 41990, reward mean : 0.502953089236908, total_step : 1012253, cur_epsilon : 0.45\n",
      "episode : 42000, reward mean : 0.503020481359657, total_step : 1012477, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42000, reward mean : 0.7305000060237944, step mean : 27.95, eval_episode_rewards : [0.53 0.89 0.81 0.78 0.45 0.84 0.77 0.42 0.8  0.95 0.88 0.4  0.96 0.38\n",
      " 0.86 0.82 0.67 0.99 0.79 0.62]\n",
      "episode : 42010, reward mean : 0.5030485650356012, total_step : 1012666, cur_epsilon : 0.45\n",
      "episode : 42020, reward mean : 0.5031280394379353, total_step : 1012839, cur_epsilon : 0.45\n",
      "episode : 42030, reward mean : 0.5031104025034562, total_step : 1013020, cur_epsilon : 0.45\n",
      "episode : 42040, reward mean : 0.5030811183936592, total_step : 1013250, cur_epsilon : 0.45\n",
      "episode : 42050, reward mean : 0.5031488755604476, total_step : 1013472, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42050, reward mean : 0.7310000037774443, step mean : 17.9, eval_episode_rewards : [ 0.91  0.88 -1.05  0.81  0.5   0.82  0.95  0.88  0.92  0.94  0.87  0.72\n",
      "  0.82  0.79  0.89  0.62  0.84  0.73  0.97  0.81]\n",
      "episode : 42060, reward mean : 0.5031669095902572, total_step : 1013703, cur_epsilon : 0.45\n",
      "episode : 42070, reward mean : 0.5031858858429932, total_step : 1013930, cur_epsilon : 0.45\n",
      "episode : 42080, reward mean : 0.5032271914791401, total_step : 1014063, cur_epsilon : 0.45\n",
      "episode : 42090, reward mean : 0.5033133812656727, total_step : 1014207, cur_epsilon : 0.45\n",
      "episode : 42100, reward mean : 0.5032536868766092, total_step : 1014365, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42100, reward mean : 0.8385000036098063, step mean : 17.15, eval_episode_rewards : [0.88 0.92 0.76 0.97 0.85 0.64 0.87 0.87 0.96 0.78 0.99 0.93 0.8  0.96\n",
      " 0.87 0.97 0.78 0.66 0.54 0.77]\n",
      "episode : 42110, reward mean : 0.503254813999663, total_step : 1014467, cur_epsilon : 0.45\n",
      "episode : 42120, reward mean : 0.5031882767708387, total_step : 1014753, cur_epsilon : 0.45\n",
      "episode : 42130, reward mean : 0.503178500299515, total_step : 1014901, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 42140, reward mean : 0.5030987237226068, total_step : 1015144, cur_epsilon : 0.45\n",
      "episode : 42150, reward mean : 0.5030816184509709, total_step : 1015323, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42150, reward mean : 0.6005000044591725, step mean : 20.95, eval_episode_rewards : [ 0.81  0.88 -1.32  0.92  0.77  0.76  0.32  0.83  0.76  0.99 -1.02  0.98\n",
      "  0.87  0.84  0.71  0.92  0.83  0.28  0.91  0.97]\n",
      "episode : 42160, reward mean : 0.5031088761325688, total_step : 1015515, cur_epsilon : 0.45\n",
      "episode : 42170, reward mean : 0.5031384922406343, total_step : 1015697, cur_epsilon : 0.45\n",
      "episode : 42180, reward mean : 0.503086064908929, total_step : 1016025, cur_epsilon : 0.45\n",
      "episode : 42190, reward mean : 0.503128708649138, total_step : 1016251, cur_epsilon : 0.45\n",
      "episode : 42200, reward mean : 0.5030990572975533, total_step : 1016483, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42200, reward mean : 0.6505000055767596, step mean : 25.95, eval_episode_rewards : [ 0.91  0.74  0.95  0.66  0.67  0.83 -1.36  0.87  0.83  0.83  0.86  0.7\n",
      "  0.93  0.74  0.43  0.84  0.25  0.72  0.89  0.72]\n",
      "episode : 42210, reward mean : 0.5031637104479227, total_step : 1016717, cur_epsilon : 0.45\n",
      "episode : 42220, reward mean : 0.5032323594991257, total_step : 1016934, cur_epsilon : 0.45\n",
      "episode : 42230, reward mean : 0.5032761121980305, total_step : 1017256, cur_epsilon : 0.45\n",
      "episode : 42240, reward mean : 0.5032424294075816, total_step : 1017505, cur_epsilon : 0.45\n",
      "episode : 42250, reward mean : 0.5031739696619603, total_step : 1017701, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42250, reward mean : 0.5595000031404197, step mean : 15.05, eval_episode_rewards : [-1.33  0.91  0.8   0.98  0.99  0.76  0.85  0.66  0.97  0.94  0.94  0.88\n",
      "  0.95  0.74 -1.01  0.82 -1.01  0.99  0.49  0.87]\n",
      "episode : 42260, reward mean : 0.5032056369676823, total_step : 1017874, cur_epsilon : 0.45\n",
      "episode : 42270, reward mean : 0.5031930498766922, total_step : 1018034, cur_epsilon : 0.45\n",
      "episode : 42280, reward mean : 0.5031572899324317, total_step : 1018292, cur_epsilon : 0.45\n",
      "episode : 42290, reward mean : 0.5032345759844221, total_step : 1018472, cur_epsilon : 0.45\n",
      "episode : 42300, reward mean : 0.5032560335325592, total_step : 1018688, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42300, reward mean : 0.756000005453825, step mean : 25.4, eval_episode_rewards : [0.89 0.93 0.73 0.66 0.99 0.92 0.57 0.67 0.73 0.3  0.84 0.94 0.81 0.86\n",
      " 0.4  0.75 0.96 0.95 0.74 0.48]\n",
      "episode : 42310, reward mean : 0.5033051339752269, total_step : 1018987, cur_epsilon : 0.45\n",
      "episode : 42320, reward mean : 0.503334835031429, total_step : 1019168, cur_epsilon : 0.45\n",
      "episode : 42330, reward mean : 0.5033635770987843, total_step : 1019452, cur_epsilon : 0.45\n",
      "episode : 42340, reward mean : 0.503372702377166, total_step : 1019720, cur_epsilon : 0.45\n",
      "episode : 42350, reward mean : 0.5034132283040649, total_step : 1019855, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42350, reward mean : 0.6560000032186508, step mean : 15.4, eval_episode_rewards : [ 0.9   0.94  0.9   0.85  0.95  0.83  0.82  0.9   0.88 -1.08  0.85  0.86\n",
      "  0.95  0.87  0.87  0.95 -1.27  0.57  0.89  0.69]\n",
      "synced target net\n",
      "episode : 42360, reward mean : 0.5034915065796512, total_step : 1020030, cur_epsilon : 0.45\n",
      "episode : 42370, reward mean : 0.5035225446955608, total_step : 1020205, cur_epsilon : 0.45\n",
      "episode : 42380, reward mean : 0.5034327564608972, total_step : 1020492, cur_epsilon : 0.45\n",
      "episode : 42390, reward mean : 0.503468983695324, total_step : 1020645, cur_epsilon : 0.45\n",
      "episode : 42400, reward mean : 0.5034825523325048, total_step : 1020894, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42400, reward mean : 0.5085000042803586, step mean : 20.15, eval_episode_rewards : [ 0.81  0.67  0.89  0.93  0.96  0.96 -1.07  0.97  0.81  0.75 -1.23  0.53\n",
      "  0.92  0.33  0.75  0.77  0.88 -1.26  0.96  0.84]\n",
      "episode : 42410, reward mean : 0.5034510780228629, total_step : 1021134, cur_epsilon : 0.45\n",
      "episode : 42420, reward mean : 0.5035169782884988, total_step : 1021361, cur_epsilon : 0.45\n",
      "episode : 42430, reward mean : 0.5035029511909731, total_step : 1021527, cur_epsilon : 0.45\n",
      "episode : 42440, reward mean : 0.5034877525710103, total_step : 1021898, cur_epsilon : 0.45\n",
      "episode : 42450, reward mean : 0.5035571311933764, total_step : 1022110, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42450, reward mean : 0.5615000053308904, step mean : 24.85, eval_episode_rewards : [ 0.8   0.86  0.82  0.81  0.23 -1.17  0.79 -1.63  0.83  0.52  0.82  0.85\n",
      "  0.32  0.81  0.99  0.89  0.92  0.88  0.94  0.95]\n",
      "episode : 42460, reward mean : 0.5035819646538277, total_step : 1022311, cur_epsilon : 0.45\n",
      "episode : 42470, reward mean : 0.503635512579742, total_step : 1022590, cur_epsilon : 0.45\n",
      "episode : 42480, reward mean : 0.5036525475355865, total_step : 1022824, cur_epsilon : 0.45\n",
      "episode : 42490, reward mean : 0.5036166208370763, total_step : 1023083, cur_epsilon : 0.45\n",
      "episode : 42500, reward mean : 0.5036395345743965, total_step : 1023292, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42500, reward mean : 0.40350000439211725, step mean : 20.65, eval_episode_rewards : [-1.46  0.87  0.98 -1.04  0.77  0.87  0.86  0.96  0.7   0.71 -1.07  0.52\n",
      "  0.76  0.33 -1.17  0.93  0.78  0.93  0.95  0.89]\n",
      "episode : 42510, reward mean : 0.5037297158183276, total_step : 1023415, cur_epsilon : 0.45\n",
      "episode : 42520, reward mean : 0.5037782271744409, total_step : 1023515, cur_epsilon : 0.45\n",
      "episode : 42530, reward mean : 0.5038467016106994, total_step : 1023730, cur_epsilon : 0.45\n",
      "episode : 42540, reward mean : 0.5039057409392996, total_step : 1023985, cur_epsilon : 0.45\n",
      "episode : 42550, reward mean : 0.5039461861242192, total_step : 1024119, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42550, reward mean : 0.5425000046379864, step mean : 21.7, eval_episode_rewards : [-1.29  0.58  0.75 -1.05  0.95  0.91  0.9   0.97  0.22  0.79  0.99  0.83\n",
      "  0.89  0.91  0.95  0.85 -1.    0.86  0.92  0.92]\n",
      "episode : 42560, reward mean : 0.5039363303481289, total_step : 1024267, cur_epsilon : 0.45\n",
      "episode : 42570, reward mean : 0.504002824046605, total_step : 1024490, cur_epsilon : 0.45\n",
      "episode : 42580, reward mean : 0.5040237252163045, total_step : 1024707, cur_epsilon : 0.45\n",
      "episode : 42590, reward mean : 0.5040631655256834, total_step : 1024845, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 42600, reward mean : 0.5040802868495, total_step : 1025078, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42600, reward mean : 0.66750000519678, step mean : 24.25, eval_episode_rewards : [ 0.74  0.84  0.87  0.73 -1.04  0.96  0.8   0.67  0.82  0.84  0.5   0.27\n",
      "  0.89  0.71  0.82  0.67  0.98  0.9   0.63  0.75]\n",
      "episode : 42610, reward mean : 0.504063839939587, total_step : 1025254, cur_epsilon : 0.45\n",
      "episode : 42620, reward mean : 0.5041191980263707, total_step : 1025524, cur_epsilon : 0.45\n",
      "episode : 42630, reward mean : 0.5041449734910977, total_step : 1025720, cur_epsilon : 0.45\n",
      "episode : 42640, reward mean : 0.5042084948399574, total_step : 1025955, cur_epsilon : 0.45\n",
      "episode : 42650, reward mean : 0.5042337683474281, total_step : 1026153, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42650, reward mean : 0.6315000071190298, step mean : 32.8, eval_episode_rewards : [ 0.77  0.98  0.63  0.35  0.77  0.38  0.64  0.83  0.92  0.59  0.69  0.88\n",
      "  0.64  0.86  0.63  0.76 -1.    0.96  0.65  0.7 ]\n",
      "episode : 42660, reward mean : 0.5042737979387739, total_step : 1026387, cur_epsilon : 0.45\n",
      "episode : 42670, reward mean : 0.5042678748557567, total_step : 1026518, cur_epsilon : 0.45\n",
      "episode : 42680, reward mean : 0.5042879620464406, total_step : 1026738, cur_epsilon : 0.45\n",
      "episode : 42690, reward mean : 0.504307805579509, total_step : 1026959, cur_epsilon : 0.45\n",
      "episode : 42700, reward mean : 0.5043452042206005, total_step : 1027105, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42700, reward mean : 0.49950000559911134, step mean : 26.0, eval_episode_rewards : [ 0.84  0.34  0.73  0.99 -1.04  0.81 -1.13  0.74  0.79  0.97  0.24  0.83\n",
      "  0.63  0.94  0.89 -1.    0.73  0.91  0.97  0.81]\n",
      "episode : 42710, reward mean : 0.5043853949952554, total_step : 1027239, cur_epsilon : 0.45\n",
      "episode : 42720, reward mean : 0.5044236942959168, total_step : 1027581, cur_epsilon : 0.45\n",
      "episode : 42730, reward mean : 0.5044563590067034, total_step : 1027747, cur_epsilon : 0.45\n",
      "episode : 42740, reward mean : 0.504492985970695, total_step : 1027896, cur_epsilon : 0.45\n",
      "episode : 42750, reward mean : 0.5045218765011917, total_step : 1028078, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42750, reward mean : 0.5880000047385693, step mean : 22.2, eval_episode_rewards : [ 0.87  0.93 -1.19  0.91  0.94  0.82  0.58  0.94  0.53  0.73  0.86  0.36\n",
      "  0.56  0.91  0.8   0.94  0.9   0.6  -1.1   0.87]\n",
      "episode : 42760, reward mean : 0.504362727326652, total_step : 1028363, cur_epsilon : 0.45\n",
      "episode : 42770, reward mean : 0.5044381627431916, total_step : 1028546, cur_epsilon : 0.45\n",
      "episode : 42780, reward mean : 0.5044247363395327, total_step : 1028909, cur_epsilon : 0.45\n",
      "episode : 42790, reward mean : 0.5044896055305897, total_step : 1029137, cur_epsilon : 0.45\n",
      "episode : 42800, reward mean : 0.5045032761847409, total_step : 1029384, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42800, reward mean : 0.8095000042580068, step mean : 20.05, eval_episode_rewards : [0.91 0.66 0.82 0.8  0.82 0.76 0.73 0.92 0.73 0.78 0.71 0.82 0.85 0.95\n",
      " 0.51 0.84 0.98 0.83 0.88 0.89]\n",
      "episode : 42810, reward mean : 0.5045297879180084, total_step : 1029675, cur_epsilon : 0.45\n",
      "episode : 42820, reward mean : 0.5045499818032848, total_step : 1029894, cur_epsilon : 0.45\n",
      "synced target net\n",
      "episode : 42830, reward mean : 0.5045589591611978, total_step : 1030161, cur_epsilon : 0.45\n",
      "episode : 42840, reward mean : 0.5046169519357916, total_step : 1030418, cur_epsilon : 0.45\n",
      "episode : 42850, reward mean : 0.5046858861370832, total_step : 1030628, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42850, reward mean : 0.8320000037550926, step mean : 17.8, eval_episode_rewards : [0.91 0.97 0.92 0.91 0.51 0.82 0.74 0.93 0.94 0.79 0.78 0.97 0.6  0.82\n",
      " 0.89 0.84 0.97 0.94 0.89 0.5 ]\n",
      "episode : 42860, reward mean : 0.5047538549001309, total_step : 1030842, cur_epsilon : 0.45\n",
      "episode : 42870, reward mean : 0.5047331518797081, total_step : 1031135, cur_epsilon : 0.45\n",
      "episode : 42880, reward mean : 0.5047611991866185, total_step : 1031320, cur_epsilon : 0.45\n",
      "episode : 42890, reward mean : 0.5047815393137957, total_step : 1031538, cur_epsilon : 0.45\n",
      "episode : 42900, reward mean : 0.5047815902385696, total_step : 1031843, cur_epsilon : 0.45\n",
      "[EVAL] episode : 42900, reward mean : 0.528000008314848, step mean : 38.2, eval_episode_rewards : [ 0.72  0.32  0.64  0.43  0.3   0.9   0.93  0.4   0.07  0.91  0.98  0.87\n",
      "  0.8   0.3   0.74 -1.51  0.36  0.97  0.69  0.74]\n",
      "episode : 42910, reward mean : 0.5047977213070415, total_step : 1032079, cur_epsilon : 0.45\n",
      "episode : 42920, reward mean : 0.5048194366572764, total_step : 1032291, cur_epsilon : 0.45\n",
      "episode : 42930, reward mean : 0.5048735201814368, total_step : 1032564, cur_epsilon : 0.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m             ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(eval_episode_steps) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_episode_steps)\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[EVAL] episode : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, reward mean : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step mean : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, eval_episode_rewards : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mround(eval_episode_rewards,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 88\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, n_iter, batch_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     77\u001b[0m loss_v, prios \u001b[38;5;241m=\u001b[39m rainbow_loss(\n\u001b[1;32m     78\u001b[0m     states\u001b[38;5;241m=\u001b[39mstates,\n\u001b[1;32m     79\u001b[0m     actions\u001b[38;5;241m=\u001b[39mactions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     n_step_gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_buffer\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_buffer\u001b[38;5;241m.\u001b[39mn_step\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m \u001b[43mloss_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_buffer\u001b[38;5;241m.\u001b[39mupdate_priorities(sampled_indices, prios)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# exp_buffer = NStepPriorityReplayBuffer(\n",
    "#     max_size=150000,\n",
    "#     prob_alpha=0.6,\n",
    "#     beta_start=0.4,\n",
    "#     beta_frames=500000,\n",
    "#     n_step=4,\n",
    "#     gamma=0.99,\n",
    "# )\n",
    "# agent = Agent(\n",
    "#     env=env,\n",
    "#     exp_buffer=exp_buffer,\n",
    "#     net=net,\n",
    "#     epsilon_start=0.9,\n",
    "#     epsilon_final=0.45,\n",
    "#     epsilon_decay_last_step=200000,\n",
    "#     tgt_sync_steps=5000,\n",
    "#     learning_rate=1e-4,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "\n",
    "episode = 0\n",
    "reward_history = []\n",
    "spisode_steps_history = []\n",
    "spisode_start_steps = 0\n",
    "PRINT_EPISODE_INTERVAL = 10\n",
    "print_episode_rewards = []\n",
    "EVAL_EPISODE_INTERVAL = 50\n",
    "N_EVAL_EPISODES = 20\n",
    "\n",
    "while True:\n",
    "\n",
    "    for stp in range(20):\n",
    "        done_reward = agent.play_step()\n",
    "        if done_reward is not None:\n",
    "            episode += 1\n",
    "            reward_history.append(done_reward)\n",
    "            print_episode_rewards.append(done_reward)\n",
    "            spisode_steps_history.append(agent._total_step-spisode_start_steps)\n",
    "            spisode_start_steps = agent._total_step\n",
    "            if episode % PRINT_EPISODE_INTERVAL == 0:\n",
    "                rm = sum(print_episode_rewards) / len(print_episode_rewards)\n",
    "                # rm = sum(reward_history[-1000:]) / len(reward_history[-1000:])\n",
    "                print(f'episode : {episode}, reward mean : {rm}, total_step : {agent._total_step}, cur_epsilon : {agent._epsilon}')\n",
    "            if episode % EVAL_EPISODE_INTERVAL == 0:\n",
    "                eval_episode_rewards = []\n",
    "                eval_episode_steps = []\n",
    "                for i in range(N_EVAL_EPISODES):\n",
    "                    reward, steps = agent.simulate_episode()\n",
    "                    eval_episode_rewards.append(reward)\n",
    "                    eval_episode_steps.append(steps)\n",
    "                mr = sum(eval_episode_rewards) / len(eval_episode_rewards)\n",
    "                ms = sum(eval_episode_steps) / len(eval_episode_steps)\n",
    "                print(f'[EVAL] episode : {episode}, reward mean : {mr}, step mean : {ms}, eval_episode_rewards : {np.round(eval_episode_rewards, 3)}')\n",
    "\n",
    "    agent.train(n_iter=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6f422e2-1466-4102-bdc8-47490d71be55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(reward_history).rolling(window=1000).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6b2426a-867c-42d4-9994-0be6ef888700",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 10, reward mean : 0.305000008828938, total_step : 404, cur_epsilon : 0.8995960000000001\n",
      "episode : 20, reward mean : -0.0814999925903976, total_step : 680, cur_epsilon : 0.89932\n",
      "episode : 30, reward mean : -0.05499999355524778, total_step : 892, cur_epsilon : 0.899108\n",
      "episode : 40, reward mean : -0.0232499944511801, total_step : 1030, cur_epsilon : 0.89897\n",
      "episode : 50, reward mean : -0.08819999400526285, total_step : 1386, cur_epsilon : 0.898614\n",
      "[EVAL] episode : 50, reward mean : 0.0660000029951334, step mean : 14.4,\n",
      "episode : 60, reward mean : -0.10699999425560236, total_step : 1597, cur_epsilon : 0.8984030000000001\n",
      "episode : 70, reward mean : -0.03757142294198275, total_step : 1828, cur_epsilon : 0.898172\n",
      "episode : 80, reward mean : -0.09024999435059726, total_step : 2097, cur_epsilon : 0.897903\n",
      "episode : 90, reward mean : -0.07344443858083752, total_step : 2446, cur_epsilon : 0.8975540000000001\n",
      "episode : 100, reward mean : -0.07709999380633235, total_step : 2863, cur_epsilon : 0.8971370000000001\n",
      "[EVAL] episode : 100, reward mean : 0.1620000008493662, step mean : 4.8,\n",
      "episode : 110, reward mean : -0.0859999938105995, total_step : 3147, cur_epsilon : 0.896853\n",
      "episode : 120, reward mean : -0.07258332742688557, total_step : 3282, cur_epsilon : 0.896718\n",
      "episode : 130, reward mean : -0.0811538403853774, total_step : 3476, cur_epsilon : 0.896524\n",
      "episode : 140, reward mean : -0.044071422956351726, total_step : 3648, cur_epsilon : 0.896352\n",
      "episode : 150, reward mean : -0.0362666609386603, total_step : 3985, cur_epsilon : 0.896015\n",
      "[EVAL] episode : 150, reward mean : 0.06800000742077827, step mean : 34.0,\n",
      "episode : 160, reward mean : -0.046562494069803506, total_step : 4394, cur_epsilon : 0.895606\n",
      "episode : 170, reward mean : -0.06976469998412273, total_step : 4645, cur_epsilon : 0.895355\n",
      "episode : 180, reward mean : -0.04111110534932878, total_step : 4809, cur_epsilon : 0.8951910000000001\n",
      "synced target net\n",
      "episode : 190, reward mean : -0.04189473108241432, total_step : 5075, cur_epsilon : 0.894925\n",
      "episode : 200, reward mean : -0.038149994341656564, total_step : 5252, cur_epsilon : 0.894748\n",
      "[EVAL] episode : 200, reward mean : 0.39400000460445883, step mean : 21.6,\n",
      "episode : 210, reward mean : -0.0645238037176785, total_step : 5652, cur_epsilon : 0.894348\n",
      "episode : 220, reward mean : -0.06654544879089702, total_step : 5870, cur_epsilon : 0.89413\n",
      "episode : 230, reward mean : -0.08634782026967276, total_step : 6202, cur_epsilon : 0.893798\n",
      "episode : 240, reward mean : -0.10079166068850706, total_step : 6643, cur_epsilon : 0.8933570000000001\n",
      "episode : 250, reward mean : -0.08923999398201704, total_step : 6964, cur_epsilon : 0.893036\n",
      "[EVAL] episode : 250, reward mean : -0.005999995395541191, step mean : 21.6,\n",
      "episode : 260, reward mean : -0.08676922478928016, total_step : 7199, cur_epsilon : 0.8928010000000001\n",
      "episode : 270, reward mean : -0.08396295702981728, total_step : 7420, cur_epsilon : 0.89258\n",
      "episode : 280, reward mean : -0.10449999399217112, total_step : 7788, cur_epsilon : 0.892212\n",
      "episode : 290, reward mean : -0.10755171818851397, total_step : 7991, cur_epsilon : 0.892009\n",
      "episode : 300, reward mean : -0.10966666071365277, total_step : 8271, cur_epsilon : 0.891729\n",
      "[EVAL] episode : 300, reward mean : 0.07000000290572643, step mean : 14.0,\n",
      "episode : 310, reward mean : -0.09506451018274792, total_step : 8538, cur_epsilon : 0.891462\n",
      "episode : 320, reward mean : -0.09684374406351708, total_step : 8799, cur_epsilon : 0.891201\n",
      "episode : 330, reward mean : -0.08160605471806996, total_step : 9003, cur_epsilon : 0.890997\n",
      "episode : 340, reward mean : -0.07094117061180227, total_step : 9231, cur_epsilon : 0.890769\n",
      "episode : 350, reward mean : -0.06257142276636192, total_step : 9419, cur_epsilon : 0.8905810000000001\n",
      "[EVAL] episode : 350, reward mean : 0.4380000036209822, step mean : 17.2,\n",
      "episode : 360, reward mean : -0.06597221640145613, total_step : 9713, cur_epsilon : 0.890287\n",
      "synced target net\n",
      "episode : 370, reward mean : -0.06102702119261832, total_step : 10006, cur_epsilon : 0.8899940000000001\n",
      "episode : 380, reward mean : -0.06505262577220013, total_step : 10230, cur_epsilon : 0.8897700000000001\n",
      "episode : 390, reward mean : -0.07102563519699451, total_step : 10538, cur_epsilon : 0.889462\n",
      "episode : 400, reward mean : -0.07029999412596226, total_step : 10889, cur_epsilon : 0.889111\n",
      "[EVAL] episode : 400, reward mean : 0.4320000037550926, step mean : 17.8,\n",
      "episode : 410, reward mean : -0.07634145745598689, total_step : 11316, cur_epsilon : 0.888684\n",
      "episode : 420, reward mean : -0.07630951784639842, total_step : 11601, cur_epsilon : 0.888399\n",
      "episode : 430, reward mean : -0.08699999410485805, total_step : 11747, cur_epsilon : 0.8882530000000001\n",
      "episode : 440, reward mean : -0.07784090318547732, total_step : 12041, cur_epsilon : 0.887959\n",
      "episode : 450, reward mean : -0.07015554966611995, total_step : 12282, cur_epsilon : 0.887718\n",
      "[EVAL] episode : 450, reward mean : 0.04800000786781311, step mean : 36.0,\n",
      "episode : 460, reward mean : -0.06326086368087841, total_step : 12544, cur_epsilon : 0.887456\n",
      "episode : 470, reward mean : -0.06308510050256835, total_step : 12809, cur_epsilon : 0.8871910000000001\n",
      "episode : 480, reward mean : -0.057729160719706366, total_step : 13225, cur_epsilon : 0.886775\n",
      "episode : 490, reward mean : -0.04704081034858008, total_step : 13569, cur_epsilon : 0.886431\n",
      "episode : 500, reward mean : -0.04531999406963587, total_step : 13740, cur_epsilon : 0.88626\n",
      "[EVAL] episode : 500, reward mean : 0.5600000008940696, step mean : 5.0,\n",
      "episode : 510, reward mean : -0.05162744504929173, total_step : 14016, cur_epsilon : 0.885984\n",
      "episode : 520, reward mean : -0.0496538402297749, total_step : 14274, cur_epsilon : 0.885726\n",
      "episode : 530, reward mean : -0.04986791860593377, total_step : 14545, cur_epsilon : 0.885455\n",
      "episode : 540, reward mean : -0.048018512602343605, total_step : 14804, cur_epsilon : 0.885196\n",
      "episode : 550, reward mean : -0.04667272139340639, total_step : 14988, cur_epsilon : 0.885012\n",
      "[EVAL] episode : 550, reward mean : 0.2820000071078539, step mean : 32.8,\n",
      "synced target net\n",
      "episode : 560, reward mean : -0.04635713699140719, total_step : 15227, cur_epsilon : 0.884773\n",
      "episode : 570, reward mean : -0.05308771340601277, total_step : 15566, cur_epsilon : 0.884434\n",
      "episode : 580, reward mean : -0.04570689067437217, total_step : 15801, cur_epsilon : 0.8841990000000001\n",
      "episode : 590, reward mean : -0.05125423133120698, total_step : 16283, cur_epsilon : 0.883717\n",
      "episode : 600, reward mean : -0.050066660741964975, total_step : 16473, cur_epsilon : 0.8835270000000001\n",
      "[EVAL] episode : 600, reward mean : -0.037999994680285455, step mean : 24.8,\n",
      "episode : 610, reward mean : -0.04604917442517691, total_step : 16688, cur_epsilon : 0.883312\n",
      "episode : 620, reward mean : -0.04067741350661362, total_step : 16811, cur_epsilon : 0.883189\n",
      "episode : 630, reward mean : -0.039301581420595685, total_step : 17175, cur_epsilon : 0.882825\n",
      "episode : 640, reward mean : -0.04434374404954724, total_step : 17646, cur_epsilon : 0.882354\n",
      "episode : 650, reward mean : -0.047599994018673895, total_step : 18011, cur_epsilon : 0.881989\n",
      "[EVAL] episode : 650, reward mean : 0.09200000241398812, step mean : 11.8,\n",
      "episode : 660, reward mean : -0.055181812207129866, total_step : 18268, cur_epsilon : 0.8817320000000001\n",
      "episode : 670, reward mean : -0.05419402390218048, total_step : 18467, cur_epsilon : 0.881533\n",
      "episode : 680, reward mean : -0.053867641121477766, total_step : 18709, cur_epsilon : 0.881291\n",
      "episode : 690, reward mean : -0.06266666069842768, total_step : 19079, cur_epsilon : 0.8809210000000001\n",
      "episode : 700, reward mean : -0.06402856539934873, total_step : 19546, cur_epsilon : 0.8804540000000001\n",
      "[EVAL] episode : 700, reward mean : -0.2719999894499779, step mean : 48.2,\n",
      "episode : 710, reward mean : -0.05711267005757127, total_step : 19729, cur_epsilon : 0.880271\n",
      "synced target net\n",
      "episode : 720, reward mean : -0.048888882891171506, total_step : 20004, cur_epsilon : 0.879996\n",
      "episode : 730, reward mean : -0.04616437756953991, total_step : 20264, cur_epsilon : 0.8797360000000001\n",
      "episode : 740, reward mean : -0.04451350747431452, total_step : 20697, cur_epsilon : 0.8793030000000001\n",
      "episode : 750, reward mean : -0.04335999399423599, total_step : 20865, cur_epsilon : 0.879135\n",
      "[EVAL] episode : 750, reward mean : -0.19399999119341374, step mean : 40.4,\n",
      "episode : 760, reward mean : -0.04573683611264354, total_step : 21099, cur_epsilon : 0.878901\n",
      "episode : 770, reward mean : -0.04916882516405025, total_step : 21419, cur_epsilon : 0.8785810000000001\n",
      "episode : 780, reward mean : -0.047512814493324514, total_step : 21747, cur_epsilon : 0.8782530000000001\n",
      "episode : 790, reward mean : -0.05048100663514077, total_step : 22039, cur_epsilon : 0.877961\n",
      "episode : 800, reward mean : -0.05543749395525083, total_step : 22395, cur_epsilon : 0.877605\n",
      "[EVAL] episode : 800, reward mean : -0.4179999906569719, step mean : 42.6,\n",
      "episode : 810, reward mean : -0.05281480875004221, total_step : 22747, cur_epsilon : 0.8772530000000001\n",
      "episode : 820, reward mean : -0.053292676829165074, total_step : 23148, cur_epsilon : 0.8768520000000001\n",
      "episode : 830, reward mean : -0.05273493365216327, total_step : 23464, cur_epsilon : 0.876536\n",
      "episode : 840, reward mean : -0.04484523201050858, total_step : 23664, cur_epsilon : 0.876336\n",
      "episode : 850, reward mean : -0.045858817402930824, total_step : 24104, cur_epsilon : 0.875896\n",
      "[EVAL] episode : 850, reward mean : 0.4680000074207783, step mean : 34.0,\n",
      "episode : 860, reward mean : -0.03973255205336352, total_step : 24233, cur_epsilon : 0.8757670000000001\n",
      "episode : 870, reward mean : -0.03948275254846647, total_step : 24461, cur_epsilon : 0.8755390000000001\n",
      "episode : 880, reward mean : -0.03792044849092649, total_step : 24672, cur_epsilon : 0.875328\n",
      "episode : 890, reward mean : -0.030831454635671017, total_step : 24889, cur_epsilon : 0.875111\n",
      "synced target net\n",
      "episode : 900, reward mean : -0.028299993978192408, total_step : 25102, cur_epsilon : 0.8748980000000001\n",
      "[EVAL] episode : 900, reward mean : -0.14999999217689036, step mean : 36.0,\n",
      "episode : 910, reward mean : -0.03424175224461398, total_step : 25281, cur_epsilon : 0.874719\n",
      "episode : 920, reward mean : -0.032576080931998465, total_step : 25671, cur_epsilon : 0.874329\n",
      "episode : 930, reward mean : -0.0378279509523543, total_step : 26002, cur_epsilon : 0.873998\n",
      "episode : 940, reward mean : -0.03563829184450368, total_step : 26244, cur_epsilon : 0.873756\n",
      "episode : 950, reward mean : -0.03866315190141138, total_step : 26377, cur_epsilon : 0.873623\n",
      "[EVAL] episode : 950, reward mean : -0.43599999472498896, step mean : 24.6,\n",
      "episode : 960, reward mean : -0.034302077351215605, total_step : 26607, cur_epsilon : 0.873393\n",
      "episode : 970, reward mean : -0.03785566407227024, total_step : 27095, cur_epsilon : 0.872905\n",
      "episode : 980, reward mean : -0.0326734633432055, total_step : 27433, cur_epsilon : 0.872567\n",
      "episode : 990, reward mean : -0.03627272124656222, total_step : 27632, cur_epsilon : 0.872368\n",
      "episode : 1000, reward mean : -0.03379999399185181, total_step : 27831, cur_epsilon : 0.872169\n",
      "[EVAL] episode : 1000, reward mean : -0.44399999007582663, step mean : 45.2,\n",
      "episode : 1010, reward mean : -0.029326726662197915, total_step : 28122, cur_epsilon : 0.871878\n",
      "episode : 1020, reward mean : -0.031745092026597144, total_step : 28408, cur_epsilon : 0.871592\n",
      "episode : 1030, reward mean : -0.03125242116924339, total_step : 28698, cur_epsilon : 0.871302\n",
      "episode : 1040, reward mean : -0.032442301701611054, total_step : 28863, cur_epsilon : 0.871137\n",
      "episode : 1050, reward mean : -0.027009517820108506, total_step : 29135, cur_epsilon : 0.870865\n",
      "[EVAL] episode : 1050, reward mean : -0.5979999866336584, step mean : 60.6,\n",
      "episode : 1060, reward mean : -0.026858484525374083, total_step : 29655, cur_epsilon : 0.870345\n",
      "synced target net\n",
      "episode : 1070, reward mean : -0.029084106089077265, total_step : 30029, cur_epsilon : 0.869971\n",
      "episode : 1080, reward mean : -0.03155554951065116, total_step : 30234, cur_epsilon : 0.869766\n",
      "episode : 1090, reward mean : -0.036981645340971446, total_step : 30467, cur_epsilon : 0.869533\n",
      "episode : 1100, reward mean : -0.032963630343702706, total_step : 30672, cur_epsilon : 0.869328\n",
      "[EVAL] episode : 1100, reward mean : -0.8639999940991402, step mean : 27.4,\n",
      "episode : 1110, reward mean : -0.032495489473211334, total_step : 30962, cur_epsilon : 0.869038\n",
      "episode : 1120, reward mean : -0.03275892253067078, total_step : 31333, cur_epsilon : 0.8686670000000001\n",
      "episode : 1130, reward mean : -0.03608848954658065, total_step : 31552, cur_epsilon : 0.868448\n",
      "episode : 1140, reward mean : -0.03908771328758775, total_step : 31740, cur_epsilon : 0.86826\n",
      "episode : 1150, reward mean : -0.04183477658130552, total_step : 32105, cur_epsilon : 0.867895\n",
      "[EVAL] episode : 1150, reward mean : -0.2719999894499779, step mean : 48.2,\n",
      "episode : 1160, reward mean : -0.03964654569321408, total_step : 32402, cur_epsilon : 0.867598\n",
      "episode : 1170, reward mean : -0.038598284577457315, total_step : 32628, cur_epsilon : 0.867372\n",
      "episode : 1180, reward mean : -0.037279654993533584, total_step : 32921, cur_epsilon : 0.867079\n",
      "episode : 1190, reward mean : -0.035831926750535724, total_step : 33196, cur_epsilon : 0.866804\n",
      "episode : 1200, reward mean : -0.03148332733971377, total_step : 33320, cur_epsilon : 0.86668\n",
      "[EVAL] episode : 1200, reward mean : -0.2379999902099371, step mean : 44.8,\n",
      "episode : 1210, reward mean : -0.030115696467643928, total_step : 33695, cur_epsilon : 0.866305\n",
      "episode : 1220, reward mean : -0.033877043164961165, total_step : 33994, cur_epsilon : 0.866006\n",
      "episode : 1230, reward mean : -0.03236584759249193, total_step : 34551, cur_epsilon : 0.865449\n",
      "episode : 1240, reward mean : -0.034193542305259936, total_step : 34919, cur_epsilon : 0.865081\n",
      "synced target net\n",
      "episode : 1250, reward mean : -0.034071993927657605, total_step : 35148, cur_epsilon : 0.8648520000000001\n",
      "[EVAL] episode : 1250, reward mean : -0.24999998994171618, step mean : 45.6,\n",
      "episode : 1260, reward mean : -0.039277771684444615, total_step : 35547, cur_epsilon : 0.864453\n",
      "episode : 1270, reward mean : -0.03755117501828849, total_step : 35777, cur_epsilon : 0.864223\n",
      "episode : 1280, reward mean : -0.035015618908801116, total_step : 36100, cur_epsilon : 0.8639\n",
      "episode : 1290, reward mean : -0.036480614037592285, total_step : 36532, cur_epsilon : 0.863468\n",
      "episode : 1300, reward mean : -0.038123070809703605, total_step : 36792, cur_epsilon : 0.863208\n",
      "[EVAL] episode : 1300, reward mean : -0.8339999858289957, step mean : 64.0,\n",
      "episode : 1310, reward mean : -0.0415572457754885, total_step : 37189, cur_epsilon : 0.862811\n",
      "episode : 1320, reward mean : -0.0395757514588309, total_step : 37379, cur_epsilon : 0.862621\n",
      "episode : 1330, reward mean : -0.04324811417469405, total_step : 37717, cur_epsilon : 0.862283\n",
      "episode : 1340, reward mean : -0.04437312819897684, total_step : 38020, cur_epsilon : 0.86198\n",
      "episode : 1350, reward mean : -0.04582221609850724, total_step : 38270, cur_epsilon : 0.86173\n",
      "[EVAL] episode : 1350, reward mean : 0.6680000074207783, step mean : 34.2,\n",
      "episode : 1360, reward mean : -0.04405881741686779, total_step : 38486, cur_epsilon : 0.861514\n",
      "episode : 1370, reward mean : -0.045817512117435025, total_step : 38880, cur_epsilon : 0.86112\n",
      "episode : 1380, reward mean : -0.04318839966494968, total_step : 39173, cur_epsilon : 0.860827\n",
      "episode : 1390, reward mean : -0.043956828388057166, total_step : 39533, cur_epsilon : 0.860467\n",
      "episode : 1400, reward mean : -0.04654285100953919, total_step : 39749, cur_epsilon : 0.860251\n",
      "[EVAL] episode : 1400, reward mean : 0.43600000366568564, step mean : 17.4,\n",
      "synced target net\n",
      "episode : 1410, reward mean : -0.047404249186845535, total_step : 40026, cur_epsilon : 0.859974\n",
      "episode : 1420, reward mean : -0.047190134706054356, total_step : 40352, cur_epsilon : 0.859648\n",
      "episode : 1430, reward mean : -0.04619579805204502, total_step : 40667, cur_epsilon : 0.859333\n",
      "episode : 1440, reward mean : -0.045583327192192276, total_step : 40934, cur_epsilon : 0.859066\n",
      "episode : 1450, reward mean : -0.049234476632856086, total_step : 41119, cur_epsilon : 0.858881\n",
      "[EVAL] episode : 1450, reward mean : 0.32400000616908076, step mean : 28.6,\n",
      "episode : 1460, reward mean : -0.049424651392722785, total_step : 41505, cur_epsilon : 0.858495\n",
      "episode : 1470, reward mean : -0.04945577618976434, total_step : 41668, cur_epsilon : 0.858332\n",
      "episode : 1480, reward mean : -0.050871615470512895, total_step : 42135, cur_epsilon : 0.857865\n",
      "episode : 1490, reward mean : -0.05055704083943727, total_step : 42349, cur_epsilon : 0.857651\n",
      "episode : 1500, reward mean : -0.049299993846565485, total_step : 42720, cur_epsilon : 0.85728\n",
      "[EVAL] episode : 1500, reward mean : -0.9179999884217978, step mean : 52.6,\n",
      "episode : 1510, reward mean : -0.051231781931112934, total_step : 42970, cur_epsilon : 0.8570300000000001\n",
      "episode : 1520, reward mean : -0.04836841491296103, total_step : 43196, cur_epsilon : 0.856804\n",
      "episode : 1530, reward mean : -0.04812417684695105, total_step : 43615, cur_epsilon : 0.8563850000000001\n",
      "episode : 1540, reward mean : -0.05340259121609973, total_step : 44084, cur_epsilon : 0.855916\n",
      "episode : 1550, reward mean : -0.05349676799389624, total_step : 44461, cur_epsilon : 0.855539\n",
      "[EVAL] episode : 1550, reward mean : 0.08000000268220901, step mean : 13.0,\n",
      "episode : 1560, reward mean : -0.05495512200495563, total_step : 44752, cur_epsilon : 0.855248\n",
      "synced target net\n",
      "episode : 1570, reward mean : -0.05527387914621526, total_step : 45067, cur_epsilon : 0.854933\n",
      "episode : 1580, reward mean : -0.05817720898061614, total_step : 45389, cur_epsilon : 0.854611\n",
      "episode : 1590, reward mean : -0.05685533970688124, total_step : 45647, cur_epsilon : 0.854353\n",
      "episode : 1600, reward mean : -0.05939999378286302, total_step : 46020, cur_epsilon : 0.8539800000000001\n",
      "[EVAL] episode : 1600, reward mean : 0.03600000813603401, step mean : 37.2,\n",
      "episode : 1610, reward mean : -0.05750309937627789, total_step : 46283, cur_epsilon : 0.8537170000000001\n",
      "episode : 1620, reward mean : -0.057395055506056475, total_step : 46632, cur_epsilon : 0.853368\n",
      "episode : 1630, reward mean : -0.058263797455815815, total_step : 46940, cur_epsilon : 0.85306\n",
      "episode : 1640, reward mean : -0.05885974988909211, total_step : 47106, cur_epsilon : 0.852894\n",
      "episode : 1650, reward mean : -0.05755756954013398, total_step : 47459, cur_epsilon : 0.852541\n",
      "[EVAL] episode : 1650, reward mean : 0.6520000077784062, step mean : 35.8,\n",
      "episode : 1660, reward mean : -0.05631324679826397, total_step : 47720, cur_epsilon : 0.85228\n",
      "episode : 1670, reward mean : -0.05641316742619533, total_step : 48102, cur_epsilon : 0.851898\n",
      "episode : 1680, reward mean : -0.05388689854160129, total_step : 48344, cur_epsilon : 0.851656\n",
      "episode : 1690, reward mean : -0.05524259732936966, total_step : 48635, cur_epsilon : 0.851365\n",
      "episode : 1700, reward mean : -0.05651175847277045, total_step : 49015, cur_epsilon : 0.850985\n",
      "[EVAL] episode : 1700, reward mean : -0.7039999887347221, step mean : 51.0,\n",
      "episode : 1710, reward mean : -0.05563742067686647, total_step : 49231, cur_epsilon : 0.850769\n",
      "episode : 1720, reward mean : -0.054360458872086084, total_step : 49675, cur_epsilon : 0.850325\n",
      "episode : 1730, reward mean : -0.05339883768532662, total_step : 49973, cur_epsilon : 0.850027\n",
      "synced target net\n",
      "episode : 1740, reward mean : -0.049931028241227415, total_step : 50233, cur_epsilon : 0.849767\n",
      "episode : 1750, reward mean : -0.050457136594823426, total_step : 50684, cur_epsilon : 0.8493160000000001\n",
      "[EVAL] episode : 1750, reward mean : 0.33000000603497026, step mean : 28.0,\n",
      "episode : 1760, reward mean : -0.051221584658858114, total_step : 50879, cur_epsilon : 0.849121\n",
      "episode : 1770, reward mean : -0.05344067172036838, total_step : 51133, cur_epsilon : 0.848867\n",
      "episode : 1780, reward mean : -0.05200561174201999, total_step : 51341, cur_epsilon : 0.848659\n",
      "episode : 1790, reward mean : -0.052815636220341286, total_step : 51647, cur_epsilon : 0.848353\n",
      "episode : 1800, reward mean : -0.05442221598906649, total_step : 51898, cur_epsilon : 0.848102\n",
      "[EVAL] episode : 1800, reward mean : -0.28199998922646047, step mean : 48.8,\n",
      "episode : 1810, reward mean : -0.053066292105664534, total_step : 52216, cur_epsilon : 0.847784\n",
      "episode : 1820, reward mean : -0.05164834542179501, total_step : 52421, cur_epsilon : 0.847579\n",
      "episode : 1830, reward mean : -0.051387971924628066, total_step : 52635, cur_epsilon : 0.847365\n",
      "episode : 1840, reward mean : -0.05099999379447621, total_step : 52825, cur_epsilon : 0.847175\n",
      "episode : 1850, reward mean : -0.04708107488179529, total_step : 53060, cur_epsilon : 0.84694\n",
      "[EVAL] episode : 1850, reward mean : -0.0679999940097332, step mean : 27.8,\n",
      "episode : 1860, reward mean : -0.0461666604674231, total_step : 53347, cur_epsilon : 0.846653\n",
      "episode : 1870, reward mean : -0.044983951026265, total_step : 53582, cur_epsilon : 0.846418\n",
      "episode : 1880, reward mean : -0.043898929969625584, total_step : 53932, cur_epsilon : 0.846068\n",
      "episode : 1890, reward mean : -0.04484126364723558, total_step : 54164, cur_epsilon : 0.845836\n",
      "episode : 1900, reward mean : -0.045294730653496165, total_step : 54404, cur_epsilon : 0.845596\n",
      "[EVAL] episode : 1900, reward mean : -0.5439999923110008, step mean : 35.4,\n",
      "episode : 1910, reward mean : -0.04201570062444628, total_step : 54633, cur_epsilon : 0.845367\n",
      "episode : 1920, reward mean : -0.040062493819277734, total_step : 54910, cur_epsilon : 0.84509\n",
      "synced target net\n",
      "episode : 1930, reward mean : -0.04101553786681106, total_step : 55144, cur_epsilon : 0.844856\n",
      "episode : 1940, reward mean : -0.0418453546356141, total_step : 55556, cur_epsilon : 0.844444\n",
      "episode : 1950, reward mean : -0.04339999381452799, total_step : 55810, cur_epsilon : 0.84419\n",
      "[EVAL] episode : 1950, reward mean : 0.29000000692903993, step mean : 32.0,\n",
      "episode : 1960, reward mean : -0.044107136682474186, total_step : 56002, cur_epsilon : 0.843998\n",
      "episode : 1970, reward mean : -0.04460405473157688, total_step : 56352, cur_epsilon : 0.8436480000000001\n",
      "episode : 1980, reward mean : -0.043287872616201636, total_step : 56546, cur_epsilon : 0.843454\n",
      "episode : 1990, reward mean : -0.0442110491102215, total_step : 56783, cur_epsilon : 0.843217\n",
      "episode : 2000, reward mean : -0.044274993835948406, total_step : 57050, cur_epsilon : 0.84295\n",
      "[EVAL] episode : 2000, reward mean : 0.14400001019239425, step mean : 46.6,\n",
      "episode : 2010, reward mean : -0.040875615739258964, total_step : 57221, cur_epsilon : 0.8427790000000001\n",
      "episode : 2020, reward mean : -0.040757419575595914, total_step : 57646, cur_epsilon : 0.842354\n",
      "episode : 2030, reward mean : -0.04460097903872065, total_step : 58077, cur_epsilon : 0.841923\n",
      "episode : 2040, reward mean : -0.04292646440239075, total_step : 58390, cur_epsilon : 0.84161\n",
      "episode : 2050, reward mean : -0.04278048160417778, total_step : 58811, cur_epsilon : 0.841189\n",
      "[EVAL] episode : 2050, reward mean : 0.37400000505149367, step mean : 23.6,\n",
      "episode : 2060, reward mean : -0.04363106175606112, total_step : 59138, cur_epsilon : 0.840862\n",
      "episode : 2070, reward mean : -0.042323665303776514, total_step : 59321, cur_epsilon : 0.8406790000000001\n",
      "episode : 2080, reward mean : -0.041264416889036794, total_step : 59553, cur_epsilon : 0.840447\n",
      "episode : 2090, reward mean : -0.042325352665268635, total_step : 59826, cur_epsilon : 0.840174\n",
      "synced target net\n",
      "episode : 2100, reward mean : -0.04057618428997341, total_step : 60111, cur_epsilon : 0.839889\n",
      "[EVAL] episode : 2100, reward mean : 0.34800000563263894, step mean : 26.2,\n",
      "episode : 2110, reward mean : -0.042947861105556734, total_step : 60460, cur_epsilon : 0.8395400000000001\n",
      "episode : 2120, reward mean : -0.04175471078674748, total_step : 60759, cur_epsilon : 0.839241\n",
      "episode : 2130, reward mean : -0.039924876437360694, total_step : 61021, cur_epsilon : 0.838979\n",
      "episode : 2140, reward mean : -0.03803270408257006, total_step : 61365, cur_epsilon : 0.838635\n",
      "episode : 2150, reward mean : -0.03932092403083347, total_step : 61690, cur_epsilon : 0.83831\n",
      "[EVAL] episode : 2150, reward mean : 0.024000008404254914, step mean : 38.4,\n",
      "episode : 2160, reward mean : -0.040486104897637334, total_step : 62090, cur_epsilon : 0.83791\n",
      "episode : 2170, reward mean : -0.040797228796175825, total_step : 62507, cur_epsilon : 0.837493\n",
      "episode : 2180, reward mean : -0.04075687450160264, total_step : 62848, cur_epsilon : 0.837152\n",
      "episode : 2190, reward mean : -0.040136980056150316, total_step : 63262, cur_epsilon : 0.836738\n",
      "episode : 2200, reward mean : -0.04006363012404605, total_step : 63496, cur_epsilon : 0.836504\n",
      "[EVAL] episode : 2200, reward mean : -0.8239999905228614, step mean : 43.2,\n",
      "episode : 2210, reward mean : -0.0391628896817565, total_step : 63846, cur_epsilon : 0.8361540000000001\n",
      "episode : 2220, reward mean : -0.03775224600171855, total_step : 64182, cur_epsilon : 0.8358180000000001\n",
      "episode : 2230, reward mean : -0.03777577850650244, total_step : 64435, cur_epsilon : 0.835565\n",
      "episode : 2240, reward mean : -0.037227672331001875, total_step : 64659, cur_epsilon : 0.835341\n",
      "synced target net\n",
      "episode : 2250, reward mean : -0.03596888264103068, total_step : 65023, cur_epsilon : 0.8349770000000001\n",
      "[EVAL] episode : 2250, reward mean : 0.2540000077337027, step mean : 35.6,\n",
      "episode : 2260, reward mean : -0.03497344507646244, total_step : 65343, cur_epsilon : 0.834657\n",
      "episode : 2270, reward mean : -0.03585461929424087, total_step : 65687, cur_epsilon : 0.8343130000000001\n",
      "episode : 2280, reward mean : -0.036192976186672846, total_step : 66109, cur_epsilon : 0.833891\n",
      "episode : 2290, reward mean : -0.03587335617765459, total_step : 66381, cur_epsilon : 0.833619\n",
      "episode : 2300, reward mean : -0.03627390675165731, total_step : 66917, cur_epsilon : 0.833083\n",
      "[EVAL] episode : 2300, reward mean : 0.01600000411272049, step mean : 19.4,\n",
      "episode : 2310, reward mean : -0.037229430939986076, total_step : 67184, cur_epsilon : 0.832816\n",
      "episode : 2320, reward mean : -0.0376379247401552, total_step : 67524, cur_epsilon : 0.832476\n",
      "episode : 2330, reward mean : -0.0358283198922233, total_step : 67750, cur_epsilon : 0.83225\n",
      "episode : 2340, reward mean : -0.035324780023696586, total_step : 68177, cur_epsilon : 0.831823\n",
      "episode : 2350, reward mean : -0.0336510575324931, total_step : 68429, cur_epsilon : 0.8315710000000001\n",
      "[EVAL] episode : 2350, reward mean : 0.07600000724196435, step mean : 33.2,\n",
      "episode : 2360, reward mean : -0.03227118014089637, total_step : 68747, cur_epsilon : 0.831253\n",
      "episode : 2370, reward mean : -0.03243037343874008, total_step : 69126, cur_epsilon : 0.830874\n",
      "episode : 2380, reward mean : -0.03142436344818283, total_step : 69329, cur_epsilon : 0.830671\n",
      "episode : 2390, reward mean : -0.031016730087727432, total_step : 69772, cur_epsilon : 0.830228\n",
      "synced target net\n",
      "episode : 2400, reward mean : -0.031704160342148194, total_step : 70176, cur_epsilon : 0.829824\n",
      "[EVAL] episode : 2400, reward mean : 0.6500000078231096, step mean : 36.0,\n",
      "episode : 2410, reward mean : -0.031219910703506708, total_step : 70301, cur_epsilon : 0.829699\n",
      "episode : 2420, reward mean : -0.03022313419544746, total_step : 70501, cur_epsilon : 0.829499\n",
      "episode : 2430, reward mean : -0.02952674266349509, total_step : 70871, cur_epsilon : 0.829129\n",
      "episode : 2440, reward mean : -0.030405731382398087, total_step : 71323, cur_epsilon : 0.828677\n",
      "episode : 2450, reward mean : -0.02988978960304236, total_step : 71536, cur_epsilon : 0.828464\n",
      "[EVAL] episode : 2450, reward mean : -0.07999998927116395, step mean : 48.8,\n",
      "episode : 2460, reward mean : -0.029951213200281306, total_step : 71791, cur_epsilon : 0.828209\n",
      "episode : 2470, reward mean : -0.03059513537884362, total_step : 72190, cur_epsilon : 0.82781\n",
      "episode : 2480, reward mean : -0.03151612271284384, total_step : 72459, cur_epsilon : 0.8275410000000001\n",
      "episode : 2490, reward mean : -0.03348995350735135, total_step : 72891, cur_epsilon : 0.827109\n",
      "episode : 2500, reward mean : -0.03347199367284775, total_step : 73130, cur_epsilon : 0.82687\n",
      "[EVAL] episode : 2500, reward mean : 0.49400000683963297, step mean : 31.4,\n",
      "episode : 2510, reward mean : -0.032561746658942854, total_step : 73444, cur_epsilon : 0.8265560000000001\n",
      "episode : 2520, reward mean : -0.03280554922548906, total_step : 73748, cur_epsilon : 0.826252\n",
      "episode : 2530, reward mean : -0.035747029243604, total_step : 74034, cur_epsilon : 0.825966\n",
      "episode : 2540, reward mean : -0.03527164721098824, total_step : 74359, cur_epsilon : 0.8256410000000001\n",
      "episode : 2550, reward mean : -0.03559999366455218, total_step : 74688, cur_epsilon : 0.825312\n",
      "[EVAL] episode : 2550, reward mean : -0.05399998985230923, step mean : 46.2,\n",
      "synced target net\n",
      "episode : 2560, reward mean : -0.03556249366083648, total_step : 75023, cur_epsilon : 0.8249770000000001\n",
      "episode : 2570, reward mean : -0.03565369016103127, total_step : 75292, cur_epsilon : 0.824708\n",
      "episode : 2580, reward mean : -0.03594185412579844, total_step : 75612, cur_epsilon : 0.824388\n",
      "episode : 2590, reward mean : -0.035532812189497776, total_step : 75952, cur_epsilon : 0.824048\n",
      "episode : 2600, reward mean : -0.03503845519314592, total_step : 76269, cur_epsilon : 0.823731\n",
      "[EVAL] episode : 2600, reward mean : -0.021999990567564964, step mean : 43.0,\n",
      "episode : 2610, reward mean : -0.034348652660989444, total_step : 76534, cur_epsilon : 0.823466\n",
      "episode : 2620, reward mean : -0.035503810437979136, total_step : 76980, cur_epsilon : 0.82302\n",
      "episode : 2630, reward mean : -0.035463871976126965, total_step : 77215, cur_epsilon : 0.822785\n",
      "episode : 2640, reward mean : -0.03700756941018908, total_step : 77468, cur_epsilon : 0.822532\n",
      "episode : 2650, reward mean : -0.03763018233877308, total_step : 77680, cur_epsilon : 0.82232\n",
      "[EVAL] episode : 2650, reward mean : -0.7439999967813492, step mean : 15.4,\n",
      "episode : 2660, reward mean : -0.03848119666869927, total_step : 77954, cur_epsilon : 0.822046\n",
      "episode : 2670, reward mean : -0.03789512474745624, total_step : 78246, cur_epsilon : 0.821754\n",
      "episode : 2680, reward mean : -0.039317157837464964, total_step : 78574, cur_epsilon : 0.821426\n",
      "episode : 2690, reward mean : -0.03915612748633641, total_step : 78879, cur_epsilon : 0.821121\n",
      "episode : 2700, reward mean : -0.037737030696675736, total_step : 79145, cur_epsilon : 0.820855\n",
      "[EVAL] episode : 2700, reward mean : 0.3700000051409006, step mean : 24.0,\n",
      "episode : 2710, reward mean : -0.037236156019749896, total_step : 79457, cur_epsilon : 0.820543\n",
      "episode : 2720, reward mean : -0.03666175835601547, total_step : 79847, cur_epsilon : 0.820153\n",
      "synced target net\n",
      "episode : 2730, reward mean : -0.038941385585391695, total_step : 80215, cur_epsilon : 0.819785\n",
      "episode : 2740, reward mean : -0.041043789253780875, total_step : 80640, cur_epsilon : 0.81936\n",
      "episode : 2750, reward mean : -0.04281090272082524, total_step : 80977, cur_epsilon : 0.8190230000000001\n",
      "[EVAL] episode : 2750, reward mean : 0.23200000822544098, step mean : 37.8,\n",
      "episode : 2760, reward mean : -0.04360506608744786, total_step : 81348, cur_epsilon : 0.818652\n",
      "episode : 2770, reward mean : -0.041891690383964494, total_step : 81527, cur_epsilon : 0.818473\n",
      "episode : 2780, reward mean : -0.04194603680076895, total_step : 81794, cur_epsilon : 0.818206\n",
      "episode : 2790, reward mean : -0.040960567113425994, total_step : 82070, cur_epsilon : 0.81793\n",
      "episode : 2800, reward mean : -0.040446422207169236, total_step : 82377, cur_epsilon : 0.817623\n",
      "[EVAL] episode : 2800, reward mean : 0.418000004068017, step mean : 19.2,\n",
      "episode : 2810, reward mean : -0.04087187975075003, total_step : 82747, cur_epsilon : 0.817253\n",
      "episode : 2820, reward mean : -0.040347511351689805, total_step : 83149, cur_epsilon : 0.816851\n",
      "episode : 2830, reward mean : -0.039650170302986044, total_step : 83402, cur_epsilon : 0.816598\n",
      "episode : 2840, reward mean : -0.03812323306739645, total_step : 83618, cur_epsilon : 0.816382\n",
      "episode : 2850, reward mean : -0.037863151527156955, total_step : 83891, cur_epsilon : 0.816109\n",
      "[EVAL] episode : 2850, reward mean : -0.2679999940097332, step mean : 27.6,\n",
      "episode : 2860, reward mean : -0.03808041320384278, total_step : 84300, cur_epsilon : 0.8157\n",
      "episode : 2870, reward mean : -0.03764110860392088, total_step : 84622, cur_epsilon : 0.815378\n",
      "episode : 2880, reward mean : -0.038354160291297984, total_step : 84875, cur_epsilon : 0.815125\n",
      "synced target net\n",
      "episode : 2890, reward mean : -0.0386851147358599, total_step : 85118, cur_epsilon : 0.814882\n",
      "episode : 2900, reward mean : -0.03967585568669541, total_step : 85553, cur_epsilon : 0.814447\n",
      "[EVAL] episode : 2900, reward mean : 0.548000005632639, step mean : 26.0,\n",
      "episode : 2910, reward mean : -0.0394054919014016, total_step : 85823, cur_epsilon : 0.814177\n",
      "episode : 2920, reward mean : -0.039160952525060265, total_step : 86100, cur_epsilon : 0.8139000000000001\n",
      "episode : 2930, reward mean : -0.03706825301114711, total_step : 86336, cur_epsilon : 0.813664\n",
      "episode : 2940, reward mean : -0.03604080994822541, total_step : 86681, cur_epsilon : 0.813319\n",
      "episode : 2950, reward mean : -0.037216942766839164, total_step : 87072, cur_epsilon : 0.812928\n",
      "[EVAL] episode : 2950, reward mean : -0.2539999898523092, step mean : 46.0,\n",
      "episode : 2960, reward mean : -0.036611480102874336, total_step : 87340, cur_epsilon : 0.81266\n",
      "episode : 2970, reward mean : -0.03666666028550779, total_step : 87603, cur_epsilon : 0.812397\n",
      "episode : 2980, reward mean : -0.0359362352270658, total_step : 87931, cur_epsilon : 0.812069\n",
      "episode : 2990, reward mean : -0.03593310398800317, total_step : 88176, cur_epsilon : 0.811824\n",
      "episode : 3000, reward mean : -0.035979993615299466, total_step : 88535, cur_epsilon : 0.811465\n",
      "[EVAL] episode : 3000, reward mean : 0.31200000643730164, step mean : 29.8,\n",
      "episode : 3010, reward mean : -0.03628570789786866, total_step : 88871, cur_epsilon : 0.811129\n",
      "episode : 3020, reward mean : -0.0386324439314965, total_step : 89325, cur_epsilon : 0.810675\n",
      "episode : 3030, reward mean : -0.039943887986144413, total_step : 89670, cur_epsilon : 0.81033\n",
      "episode : 3040, reward mean : -0.040467098858942716, total_step : 89978, cur_epsilon : 0.810022\n",
      "synced target net\n",
      "episode : 3050, reward mean : -0.042219665727776585, total_step : 90262, cur_epsilon : 0.8097380000000001\n",
      "[EVAL] episode : 3050, reward mean : -1.1419999923557043, step mean : 35.0,\n",
      "episode : 3060, reward mean : -0.041356202754879896, total_step : 90450, cur_epsilon : 0.80955\n",
      "episode : 3070, reward mean : -0.04141693171720454, total_step : 90720, cur_epsilon : 0.80928\n",
      "episode : 3080, reward mean : -0.04085388970440375, total_step : 91097, cur_epsilon : 0.808903\n",
      "episode : 3090, reward mean : -0.04036245314723851, total_step : 91396, cur_epsilon : 0.808604\n",
      "episode : 3100, reward mean : -0.03940321940988783, total_step : 91648, cur_epsilon : 0.8083520000000001\n",
      "[EVAL] episode : 3100, reward mean : -0.7339999970048666, step mean : 14.4,\n",
      "episode : 3110, reward mean : -0.03995497751729473, total_step : 92067, cur_epsilon : 0.807933\n",
      "episode : 3120, reward mean : -0.03941666026217624, total_step : 92349, cur_epsilon : 0.807651\n",
      "episode : 3130, reward mean : -0.03813098401438219, total_step : 92596, cur_epsilon : 0.807404\n",
      "episode : 3140, reward mean : -0.03692992989234863, total_step : 93067, cur_epsilon : 0.806933\n",
      "episode : 3150, reward mean : -0.03660316818645076, total_step : 93411, cur_epsilon : 0.806589\n",
      "[EVAL] episode : 3150, reward mean : -0.439999994635582, step mean : 25.0,\n",
      "episode : 3160, reward mean : -0.03639872776155796, total_step : 93692, cur_epsilon : 0.806308\n",
      "episode : 3170, reward mean : -0.036760245938711925, total_step : 94152, cur_epsilon : 0.805848\n",
      "episode : 3180, reward mean : -0.0355345847706, total_step : 94409, cur_epsilon : 0.8055910000000001\n",
      "episode : 3190, reward mean : -0.03446080862226251, total_step : 94712, cur_epsilon : 0.805288\n",
      "episode : 3200, reward mean : -0.03441561857995112, total_step : 94942, cur_epsilon : 0.805058\n",
      "[EVAL] episode : 3200, reward mean : -0.05999999418854714, step mean : 27.0,\n",
      "synced target net\n",
      "episode : 3210, reward mean : -0.033937688284434636, total_step : 95233, cur_epsilon : 0.804767\n",
      "episode : 3220, reward mean : -0.03364595630963952, total_step : 95583, cur_epsilon : 0.804417\n",
      "episode : 3230, reward mean : -0.033684204098979015, total_step : 95938, cur_epsilon : 0.804062\n",
      "episode : 3240, reward mean : -0.03315122814239635, total_step : 96209, cur_epsilon : 0.803791\n",
      "episode : 3250, reward mean : -0.03431691664285385, total_step : 96629, cur_epsilon : 0.8033710000000001\n",
      "[EVAL] episode : 3250, reward mean : -0.45199999436736105, step mean : 26.2,\n",
      "episode : 3260, reward mean : -0.03285888928028703, total_step : 96798, cur_epsilon : 0.803202\n",
      "episode : 3270, reward mean : -0.03385626268302538, total_step : 97165, cur_epsilon : 0.802835\n",
      "episode : 3280, reward mean : -0.03415243259131363, total_step : 97506, cur_epsilon : 0.802494\n",
      "episode : 3290, reward mean : -0.03350455284197914, total_step : 97737, cur_epsilon : 0.8022630000000001\n",
      "episode : 3300, reward mean : -0.03128787235836639, total_step : 98049, cur_epsilon : 0.8019510000000001\n",
      "[EVAL] episode : 3300, reward mean : 0.47600000277161597, step mean : 13.4,\n",
      "episode : 3310, reward mean : -0.031338362153225614, total_step : 98307, cur_epsilon : 0.801693\n",
      "episode : 3320, reward mean : -0.03235240320630192, total_step : 98683, cur_epsilon : 0.8013170000000001\n",
      "episode : 3330, reward mean : -0.031072065645308646, total_step : 98899, cur_epsilon : 0.8011010000000001\n",
      "episode : 3340, reward mean : -0.031290412733810925, total_step : 99213, cur_epsilon : 0.800787\n",
      "episode : 3350, reward mean : -0.030656709994620352, total_step : 99442, cur_epsilon : 0.800558\n",
      "[EVAL] episode : 3350, reward mean : 0.022000003978610037, step mean : 18.8,\n",
      "episode : 3360, reward mean : -0.030440469769140086, total_step : 99709, cur_epsilon : 0.800291\n",
      "synced target net\n",
      "episode : 3370, reward mean : -0.03012759001533161, total_step : 100044, cur_epsilon : 0.799956\n",
      "episode : 3380, reward mean : -0.02978697581624491, total_step : 100468, cur_epsilon : 0.799532\n",
      "episode : 3390, reward mean : -0.029690259047248195, total_step : 100873, cur_epsilon : 0.799127\n",
      "episode : 3400, reward mean : -0.028155875905581256, total_step : 101290, cur_epsilon : 0.79871\n",
      "[EVAL] episode : 3400, reward mean : -0.7059999886900187, step mean : 51.2,\n",
      "episode : 3410, reward mean : -0.027598234018600405, total_step : 101637, cur_epsilon : 0.798363\n",
      "episode : 3420, reward mean : -0.026473677762898436, total_step : 101890, cur_epsilon : 0.79811\n",
      "episode : 3430, reward mean : -0.02688337547145241, total_step : 102267, cur_epsilon : 0.797733\n",
      "episode : 3440, reward mean : -0.025831388891133112, total_step : 102641, cur_epsilon : 0.797359\n",
      "episode : 3450, reward mean : -0.02473912397966437, total_step : 102900, cur_epsilon : 0.7971\n",
      "[EVAL] episode : 3450, reward mean : 0.5180000107735395, step mean : 49.2,\n",
      "episode : 3460, reward mean : -0.025225427064161768, total_step : 103301, cur_epsilon : 0.796699\n",
      "episode : 3470, reward mean : -0.025760800446724515, total_step : 103720, cur_epsilon : 0.79628\n",
      "episode : 3480, reward mean : -0.024931027997156668, total_step : 104265, cur_epsilon : 0.7957350000000001\n",
      "episode : 3490, reward mean : -0.026306583758518312, total_step : 104778, cur_epsilon : 0.795222\n",
      "episode : 3500, reward mean : -0.02663999351007598, total_step : 104931, cur_epsilon : 0.795069\n",
      "[EVAL] episode : 3500, reward mean : 0.5100000020116567, step mean : 10.0,\n",
      "synced target net\n",
      "episode : 3510, reward mean : -0.026566945076349623, total_step : 105241, cur_epsilon : 0.794759\n",
      "episode : 3520, reward mean : -0.02648010715175505, total_step : 105447, cur_epsilon : 0.7945530000000001\n",
      "episode : 3530, reward mean : -0.02822378955243245, total_step : 105699, cur_epsilon : 0.794301\n",
      "episode : 3540, reward mean : -0.029288129104396043, total_step : 106114, cur_epsilon : 0.793886\n",
      "episode : 3550, reward mean : -0.029771824490121553, total_step : 106525, cur_epsilon : 0.793475\n",
      "[EVAL] episode : 3550, reward mean : -0.425999990478158, step mean : 43.4,\n",
      "episode : 3560, reward mean : -0.03053651035114537, total_step : 106936, cur_epsilon : 0.793064\n",
      "episode : 3570, reward mean : -0.030420161558299505, total_step : 107333, cur_epsilon : 0.792667\n",
      "episode : 3580, reward mean : -0.030335189021249723, total_step : 107642, cur_epsilon : 0.792358\n",
      "episode : 3590, reward mean : -0.030309185695453107, total_step : 107873, cur_epsilon : 0.792127\n",
      "episode : 3600, reward mean : -0.02967221572270824, total_step : 108084, cur_epsilon : 0.7919160000000001\n",
      "[EVAL] episode : 3600, reward mean : 0.356000005453825, step mean : 25.4,\n",
      "episode : 3610, reward mean : -0.028415505966775306, total_step : 108369, cur_epsilon : 0.791631\n",
      "episode : 3620, reward mean : -0.02937292167916894, total_step : 108653, cur_epsilon : 0.791347\n",
      "episode : 3630, reward mean : -0.029454538958870676, total_step : 108922, cur_epsilon : 0.7910780000000001\n",
      "episode : 3640, reward mean : -0.03030493855629894, total_step : 109269, cur_epsilon : 0.7907310000000001\n",
      "episode : 3650, reward mean : -0.029704103083030817, total_step : 109690, cur_epsilon : 0.7903100000000001\n",
      "[EVAL] episode : 3650, reward mean : -0.8979999888688326, step mean : 50.6,\n",
      "synced target net\n",
      "episode : 3660, reward mean : -0.030199447032471335, total_step : 110208, cur_epsilon : 0.789792\n",
      "episode : 3670, reward mean : -0.02871661474649117, total_step : 110304, cur_epsilon : 0.7896960000000001\n",
      "episode : 3680, reward mean : -0.028663036970259702, total_step : 110622, cur_epsilon : 0.789378\n",
      "episode : 3690, reward mean : -0.027523028721331257, total_step : 110939, cur_epsilon : 0.789061\n",
      "episode : 3700, reward mean : -0.02790539889772599, total_step : 111217, cur_epsilon : 0.788783\n",
      "[EVAL] episode : 3700, reward mean : -0.5739999916404486, step mean : 38.4,\n",
      "episode : 3710, reward mean : -0.02683557301076917, total_step : 111458, cur_epsilon : 0.7885420000000001\n",
      "episode : 3720, reward mean : -0.028225799944371948, total_step : 111812, cur_epsilon : 0.788188\n",
      "episode : 3730, reward mean : -0.027187661056084705, total_step : 112063, cur_epsilon : 0.787937\n",
      "episode : 3740, reward mean : -0.02493314858237132, total_step : 112257, cur_epsilon : 0.7877430000000001\n",
      "episode : 3750, reward mean : -0.02421332683165868, total_step : 112620, cur_epsilon : 0.78738\n",
      "[EVAL] episode : 3750, reward mean : -0.5139999929815531, step mean : 32.4,\n",
      "episode : 3760, reward mean : -0.024026589227364736, total_step : 113182, cur_epsilon : 0.786818\n",
      "episode : 3770, reward mean : -0.024450921871024985, total_step : 113376, cur_epsilon : 0.786624\n",
      "episode : 3780, reward mean : -0.023634914128435037, total_step : 113601, cur_epsilon : 0.7863990000000001\n",
      "episode : 3790, reward mean : -0.023081787693311796, total_step : 113825, cur_epsilon : 0.7861750000000001\n",
      "episode : 3800, reward mean : -0.023231572446070218, total_step : 114115, cur_epsilon : 0.7858850000000001\n",
      "[EVAL] episode : 3800, reward mean : -0.4839999847114086, step mean : 69.0,\n",
      "episode : 3810, reward mean : -0.023750649664912794, total_step : 114445, cur_epsilon : 0.785555\n",
      "episode : 3820, reward mean : -0.023049731718873635, total_step : 114710, cur_epsilon : 0.78529\n",
      "episode : 3830, reward mean : -0.023046990892030397, total_step : 114942, cur_epsilon : 0.785058\n",
      "synced target net\n",
      "episode : 3840, reward mean : -0.021455722674242377, total_step : 115164, cur_epsilon : 0.784836\n",
      "episode : 3850, reward mean : -0.01984674675946499, total_step : 115376, cur_epsilon : 0.784624\n",
      "[EVAL] episode : 3850, reward mean : 0.418000004068017, step mean : 19.2,\n",
      "episode : 3860, reward mean : -0.02045854272591095, total_step : 115840, cur_epsilon : 0.78416\n",
      "episode : 3870, reward mean : -0.02103100124053484, total_step : 116389, cur_epsilon : 0.7836110000000001\n",
      "episode : 3880, reward mean : -0.02124226151763932, total_step : 116900, cur_epsilon : 0.7831\n",
      "episode : 3890, reward mean : -0.021259633571407484, total_step : 117336, cur_epsilon : 0.782664\n",
      "episode : 3900, reward mean : -0.022879480633120505, total_step : 117898, cur_epsilon : 0.7821020000000001\n",
      "[EVAL] episode : 3900, reward mean : 0.43600000366568564, step mean : 17.4,\n",
      "episode : 3910, reward mean : -0.021969302918028344, total_step : 118175, cur_epsilon : 0.781825\n",
      "episode : 3920, reward mean : -0.02181631998177998, total_step : 118545, cur_epsilon : 0.781455\n",
      "episode : 3930, reward mean : -0.020900756811708894, total_step : 118817, cur_epsilon : 0.781183\n",
      "episode : 3940, reward mean : -0.022319790407977432, total_step : 119106, cur_epsilon : 0.780894\n",
      "episode : 3950, reward mean : -0.02264556307628562, total_step : 119366, cur_epsilon : 0.780634\n",
      "[EVAL] episode : 3950, reward mean : -0.8779999893158674, step mean : 48.6,\n",
      "episode : 3960, reward mean : -0.022656559116310544, total_step : 119603, cur_epsilon : 0.780397\n",
      "episode : 3970, reward mean : -0.022763217642659355, total_step : 119878, cur_epsilon : 0.780122\n",
      "synced target net\n",
      "episode : 3980, reward mean : -0.0216934607977319, total_step : 120184, cur_epsilon : 0.7798160000000001\n",
      "episode : 3990, reward mean : -0.020338339326357036, total_step : 120475, cur_epsilon : 0.779525\n",
      "episode : 4000, reward mean : -0.019757493461947887, total_step : 120772, cur_epsilon : 0.779228\n",
      "[EVAL] episode : 4000, reward mean : 0.0780000027269125, step mean : 13.2,\n",
      "episode : 4010, reward mean : -0.020234407420430398, total_step : 121193, cur_epsilon : 0.778807\n",
      "episode : 4020, reward mean : -0.01933830191487845, total_step : 121463, cur_epsilon : 0.778537\n",
      "episode : 4030, reward mean : -0.018808926464458257, total_step : 121679, cur_epsilon : 0.778321\n",
      "episode : 4040, reward mean : -0.019304448906055625, total_step : 122007, cur_epsilon : 0.777993\n",
      "episode : 4050, reward mean : -0.0180864132227905, total_step : 122143, cur_epsilon : 0.777857\n",
      "[EVAL] episode : 4050, reward mean : 0.40000000447034834, step mean : 21.0,\n",
      "episode : 4060, reward mean : -0.016793096917933726, total_step : 122446, cur_epsilon : 0.7775540000000001\n",
      "episode : 4070, reward mean : -0.017260435729285276, total_step : 122762, cur_epsilon : 0.777238\n",
      "episode : 4080, reward mean : -0.017987738554731156, total_step : 123284, cur_epsilon : 0.776716\n",
      "episode : 4090, reward mean : -0.018435201280201558, total_step : 123594, cur_epsilon : 0.776406\n",
      "episode : 4100, reward mean : -0.01837316419338671, total_step : 123797, cur_epsilon : 0.776203\n",
      "[EVAL] episode : 4100, reward mean : -0.10799998864531517, step mean : 51.6,\n",
      "episode : 4110, reward mean : -0.017489044558610358, total_step : 124062, cur_epsilon : 0.775938\n",
      "episode : 4120, reward mean : -0.01665290608613309, total_step : 124345, cur_epsilon : 0.775655\n",
      "episode : 4130, reward mean : -0.01775302009490154, total_step : 124723, cur_epsilon : 0.775277\n",
      "synced target net\n",
      "episode : 4140, reward mean : -0.01790337510370978, total_step : 125013, cur_epsilon : 0.774987\n",
      "episode : 4150, reward mean : -0.017898788640657103, total_step : 125338, cur_epsilon : 0.774662\n",
      "[EVAL] episode : 4150, reward mean : 0.2280000127851963, step mean : 58.0,\n",
      "episode : 4160, reward mean : -0.016754801149587505, total_step : 125690, cur_epsilon : 0.77431\n",
      "episode : 4170, reward mean : -0.017565940690787433, total_step : 126154, cur_epsilon : 0.773846\n",
      "episode : 4180, reward mean : -0.017997601109169176, total_step : 126362, cur_epsilon : 0.773638\n",
      "episode : 4190, reward mean : -0.018997606821802748, total_step : 126609, cur_epsilon : 0.773391\n",
      "episode : 4200, reward mean : -0.019452374402788422, total_step : 127029, cur_epsilon : 0.7729710000000001\n",
      "[EVAL] episode : 4200, reward mean : -0.07199999392032623, step mean : 28.2,\n",
      "episode : 4210, reward mean : -0.018527309368242845, total_step : 127269, cur_epsilon : 0.7727310000000001\n",
      "episode : 4220, reward mean : -0.017409946068115002, total_step : 127426, cur_epsilon : 0.772574\n",
      "episode : 4230, reward mean : -0.017976352790850556, total_step : 127891, cur_epsilon : 0.772109\n",
      "episode : 4240, reward mean : -0.0186509368537029, total_step : 128104, cur_epsilon : 0.771896\n",
      "episode : 4250, reward mean : -0.019023522864808056, total_step : 128491, cur_epsilon : 0.771509\n",
      "[EVAL] episode : 4250, reward mean : 0.028000008314847946, step mean : 38.0,\n",
      "episode : 4260, reward mean : -0.017941308008865432, total_step : 128758, cur_epsilon : 0.771242\n",
      "episode : 4270, reward mean : -0.018625286194022563, total_step : 129078, cur_epsilon : 0.770922\n",
      "episode : 4280, reward mean : -0.01748831121938599, total_step : 129220, cur_epsilon : 0.77078\n",
      "episode : 4290, reward mean : -0.016393932853423773, total_step : 129578, cur_epsilon : 0.770422\n",
      "episode : 4300, reward mean : -0.015248830672353506, total_step : 129811, cur_epsilon : 0.770189\n",
      "[EVAL] episode : 4300, reward mean : -0.041999994590878484, step mean : 25.2,\n",
      "episode : 4310, reward mean : -0.015037116441625038, total_step : 129945, cur_epsilon : 0.770055\n",
      "synced target net\n",
      "episode : 4320, reward mean : -0.013956011987412004, total_step : 130303, cur_epsilon : 0.7696970000000001\n",
      "episode : 4330, reward mean : -0.014443411486219039, total_step : 130538, cur_epsilon : 0.7694620000000001\n",
      "episode : 4340, reward mean : -0.014248841391524412, total_step : 130977, cur_epsilon : 0.769023\n",
      "episode : 4350, reward mean : -0.013664361288150151, total_step : 131147, cur_epsilon : 0.768853\n",
      "[EVAL] episode : 4350, reward mean : -0.3819999869912863, step mean : 58.8,\n",
      "episode : 4360, reward mean : -0.013857791631764501, total_step : 131554, cur_epsilon : 0.768446\n",
      "episode : 4370, reward mean : -0.01406635501692647, total_step : 131968, cur_epsilon : 0.768032\n",
      "episode : 4380, reward mean : -0.013264833639915947, total_step : 132340, cur_epsilon : 0.76766\n",
      "episode : 4390, reward mean : -0.0131503351495213, total_step : 132513, cur_epsilon : 0.767487\n",
      "episode : 4400, reward mean : -0.01381817527623339, total_step : 132929, cur_epsilon : 0.7670710000000001\n",
      "[EVAL] episode : 4400, reward mean : -0.07599999383091927, step mean : 28.6,\n",
      "episode : 4410, reward mean : -0.014455775765790825, total_step : 133333, cur_epsilon : 0.766667\n",
      "episode : 4420, reward mean : -0.014649314714418682, total_step : 133742, cur_epsilon : 0.766258\n",
      "episode : 4430, reward mean : -0.01374039976704134, total_step : 134063, cur_epsilon : 0.7659370000000001\n",
      "episode : 4440, reward mean : -0.013378371826901629, total_step : 134326, cur_epsilon : 0.765674\n",
      "episode : 4450, reward mean : -0.01162021817074398, total_step : 134567, cur_epsilon : 0.765433\n",
      "[EVAL] episode : 4450, reward mean : -0.0919999934732914, step mean : 30.2,\n",
      "episode : 4460, reward mean : -0.011968603323426748, total_step : 134744, cur_epsilon : 0.765256\n",
      "episode : 4470, reward mean : -0.012563751852952394, total_step : 134931, cur_epsilon : 0.765069\n",
      "synced target net\n",
      "episode : 4480, reward mean : -0.011667404182039069, total_step : 135152, cur_epsilon : 0.764848\n",
      "episode : 4490, reward mean : -0.011409793022132264, total_step : 135458, cur_epsilon : 0.764542\n",
      "episode : 4500, reward mean : -0.0113577712505228, total_step : 135656, cur_epsilon : 0.764344\n",
      "[EVAL] episode : 4500, reward mean : -0.8599999897181988, step mean : 46.8,\n",
      "episode : 4510, reward mean : -0.010964516757217189, total_step : 135900, cur_epsilon : 0.7641\n",
      "episode : 4520, reward mean : -0.012298666035954272, total_step : 136324, cur_epsilon : 0.763676\n",
      "episode : 4530, reward mean : -0.012485644683472104, total_step : 136631, cur_epsilon : 0.763369\n",
      "episode : 4540, reward mean : -0.013425103604120699, total_step : 136880, cur_epsilon : 0.76312\n",
      "episode : 4550, reward mean : -0.01366592753637623, total_step : 137213, cur_epsilon : 0.762787\n",
      "[EVAL] episode : 4550, reward mean : 0.5000000022351742, step mean : 11.0,\n",
      "episode : 4560, reward mean : -0.01329166013951691, total_step : 137466, cur_epsilon : 0.762534\n",
      "episode : 4570, reward mean : -0.013592991281781729, total_step : 137827, cur_epsilon : 0.762173\n",
      "episode : 4580, reward mean : -0.013179032769721148, total_step : 138160, cur_epsilon : 0.7618400000000001\n",
      "episode : 4590, reward mean : -0.013518511983190616, total_step : 138537, cur_epsilon : 0.761463\n",
      "episode : 4600, reward mean : -0.01205868912415336, total_step : 138689, cur_epsilon : 0.7613110000000001\n",
      "[EVAL] episode : 4600, reward mean : 0.30600001104176044, step mean : 50.2,\n",
      "episode : 4610, reward mean : -0.011275481543384508, total_step : 138950, cur_epsilon : 0.76105\n",
      "episode : 4620, reward mean : -0.010651508633660161, total_step : 139083, cur_epsilon : 0.7609170000000001\n",
      "episode : 4630, reward mean : -0.011371483760393078, total_step : 139437, cur_epsilon : 0.760563\n",
      "episode : 4640, reward mean : -0.010997838304972212, total_step : 139784, cur_epsilon : 0.760216\n",
      "synced target net\n",
      "episode : 4650, reward mean : -0.010544079498498029, total_step : 140093, cur_epsilon : 0.759907\n",
      "[EVAL] episode : 4650, reward mean : 0.8540000032633543, step mean : 15.6,\n",
      "episode : 4660, reward mean : -0.010864800345472384, total_step : 140362, cur_epsilon : 0.759638\n",
      "episode : 4670, reward mean : -0.010182006321687897, total_step : 140763, cur_epsilon : 0.759237\n",
      "episode : 4680, reward mean : -0.011032044755008357, total_step : 141080, cur_epsilon : 0.75892\n",
      "episode : 4690, reward mean : -0.010445622463962799, total_step : 141525, cur_epsilon : 0.758475\n",
      "episode : 4700, reward mean : -0.009663823255674636, total_step : 141778, cur_epsilon : 0.7582220000000001\n",
      "[EVAL] episode : 4700, reward mean : 0.7960000045597553, step mean : 21.4,\n",
      "episode : 4710, reward mean : -0.009539271606776999, total_step : 141939, cur_epsilon : 0.758061\n",
      "episode : 4720, reward mean : -0.009389823986122668, total_step : 142187, cur_epsilon : 0.7578130000000001\n",
      "episode : 4730, reward mean : -0.00932134654861235, total_step : 142374, cur_epsilon : 0.757626\n",
      "episode : 4740, reward mean : -0.00903374875821016, total_step : 142657, cur_epsilon : 0.757343\n",
      "episode : 4750, reward mean : -0.008774730321608092, total_step : 143052, cur_epsilon : 0.756948\n",
      "[EVAL] episode : 4750, reward mean : 0.37400000505149367, step mean : 23.6,\n",
      "episode : 4760, reward mean : -0.008172262390339826, total_step : 143283, cur_epsilon : 0.7567170000000001\n",
      "episode : 4770, reward mean : -0.008144647565980751, total_step : 143688, cur_epsilon : 0.756312\n",
      "episode : 4780, reward mean : -0.007786604354477577, total_step : 144034, cur_epsilon : 0.755966\n",
      "episode : 4790, reward mean : -0.007388302456346459, total_step : 144261, cur_epsilon : 0.755739\n",
      "episode : 4800, reward mean : -0.006827076815146332, total_step : 144508, cur_epsilon : 0.755492\n",
      "[EVAL] episode : 4800, reward mean : 0.03800000362098217, step mean : 17.2,\n",
      "episode : 4810, reward mean : -0.006417872898838253, total_step : 144827, cur_epsilon : 0.755173\n",
      "synced target net\n",
      "episode : 4820, reward mean : -0.0070580847615461136, total_step : 145251, cur_epsilon : 0.754749\n",
      "episode : 4830, reward mean : -0.005859206728714346, total_step : 145489, cur_epsilon : 0.754511\n",
      "episode : 4840, reward mean : -0.006293381897594072, total_step : 146012, cur_epsilon : 0.753988\n",
      "episode : 4850, reward mean : -0.005585560474890409, total_step : 146384, cur_epsilon : 0.7536160000000001\n",
      "[EVAL] episode : 4850, reward mean : 0.13400001041591167, step mean : 47.6,\n",
      "episode : 4860, reward mean : -0.0066111045751582696, total_step : 146698, cur_epsilon : 0.753302\n",
      "episode : 4870, reward mean : -0.0066365437708351405, total_step : 147026, cur_epsilon : 0.752974\n",
      "episode : 4880, reward mean : -0.006084009858810144, total_step : 147272, cur_epsilon : 0.7527280000000001\n",
      "episode : 4890, reward mean : -0.006110422912002341, total_step : 147600, cur_epsilon : 0.7524\n",
      "episode : 4900, reward mean : -0.0049020342840528, total_step : 147824, cur_epsilon : 0.752176\n",
      "[EVAL] episode : 4900, reward mean : 0.0660000029951334, step mean : 14.4,\n",
      "episode : 4910, reward mean : -0.004971480231861111, total_step : 148073, cur_epsilon : 0.751927\n",
      "episode : 4920, reward mean : -0.004806904046894695, total_step : 148207, cur_epsilon : 0.751793\n",
      "episode : 4930, reward mean : -0.005158208488556603, total_step : 148494, cur_epsilon : 0.751506\n",
      "episode : 4940, reward mean : -0.004331977288370673, total_step : 148701, cur_epsilon : 0.751299\n",
      "episode : 4950, reward mean : -0.0033595894499108044, total_step : 148834, cur_epsilon : 0.751166\n",
      "[EVAL] episode : 4950, reward mean : -0.10399998426437378, step mean : 71.0,\n",
      "episode : 4960, reward mean : -0.0034778160722776044, total_step : 149106, cur_epsilon : 0.7508940000000001\n",
      "episode : 4970, reward mean : -0.002985908986037406, total_step : 149374, cur_epsilon : 0.750626\n",
      "episode : 4980, reward mean : -0.0034397525275238305, total_step : 149712, cur_epsilon : 0.7502880000000001\n",
      "episode : 4990, reward mean : -0.002336666839783321, total_step : 149975, cur_epsilon : 0.750025\n",
      "synced target net\n",
      "episode : 5000, reward mean : -0.0033259934928268195, total_step : 150282, cur_epsilon : 0.749718\n",
      "[EVAL] episode : 5000, reward mean : 0.04600000791251659, step mean : 36.2,\n",
      "episode : 5010, reward mean : -0.005001989502957957, total_step : 150535, cur_epsilon : 0.749465\n",
      "episode : 5020, reward mean : -0.003904375966742219, total_step : 150799, cur_epsilon : 0.749201\n",
      "episode : 5030, reward mean : -0.003377727088453637, total_step : 151247, cur_epsilon : 0.748753\n",
      "episode : 5040, reward mean : -0.0024583268167805814, total_step : 151696, cur_epsilon : 0.7483040000000001\n",
      "episode : 5050, reward mean : -0.003073260804804245, total_step : 152118, cur_epsilon : 0.747882\n",
      "[EVAL] episode : 5050, reward mean : 0.13600000590085984, step mean : 27.2,\n",
      "episode : 5060, reward mean : -0.002252957909533629, total_step : 152316, cur_epsilon : 0.747684\n",
      "episode : 5070, reward mean : -0.0018185339214122271, total_step : 152508, cur_epsilon : 0.747492\n",
      "episode : 5080, reward mean : -0.0011338517568477496, total_step : 152772, cur_epsilon : 0.747228\n",
      "episode : 5090, reward mean : -0.002043215494258933, total_step : 153046, cur_epsilon : 0.746954\n",
      "episode : 5100, reward mean : -0.001392150349622848, total_step : 153425, cur_epsilon : 0.746575\n",
      "[EVAL] episode : 5100, reward mean : 0.036000003665685655, step mean : 17.4,\n",
      "episode : 5110, reward mean : -0.0007866862417725434, total_step : 153826, cur_epsilon : 0.746174\n",
      "episode : 5120, reward mean : -0.0010683528595109236, total_step : 154080, cur_epsilon : 0.74592\n",
      "episode : 5130, reward mean : -0.0005691942662639692, total_step : 154334, cur_epsilon : 0.745666\n",
      "episode : 5140, reward mean : -0.00042217247340291855, total_step : 154669, cur_epsilon : 0.745331\n",
      "synced target net\n",
      "episode : 5150, reward mean : 0.000566996810505691, total_step : 155069, cur_epsilon : 0.744931\n",
      "[EVAL] episode : 5150, reward mean : 0.24200001247227193, step mean : 56.6,\n",
      "episode : 5160, reward mean : 0.0009399289961962043, total_step : 155286, cur_epsilon : 0.744714\n",
      "episode : 5170, reward mean : 0.0006383043906617211, total_step : 155649, cur_epsilon : 0.744351\n",
      "episode : 5180, reward mean : 0.0009420914582649253, total_step : 155901, cur_epsilon : 0.7440990000000001\n",
      "episode : 5190, reward mean : 0.002146441963912205, total_step : 156085, cur_epsilon : 0.743915\n",
      "episode : 5200, reward mean : 0.002863468051028366, total_step : 156419, cur_epsilon : 0.743581\n",
      "[EVAL] episode : 5200, reward mean : -0.443999994546175, step mean : 25.4,\n",
      "episode : 5210, reward mean : 0.002971215719167651, total_step : 156570, cur_epsilon : 0.74343\n",
      "episode : 5220, reward mean : 0.004203071642909699, total_step : 156934, cur_epsilon : 0.743066\n",
      "episode : 5230, reward mean : 0.0037017273529651746, total_step : 157301, cur_epsilon : 0.742699\n",
      "episode : 5240, reward mean : 0.0034828309399850725, total_step : 157622, cur_epsilon : 0.742378\n",
      "episode : 5250, reward mean : 0.003297149369759219, total_step : 157926, cur_epsilon : 0.742074\n",
      "[EVAL] episode : 5250, reward mean : 0.360000005364418, step mean : 25.0,\n",
      "episode : 5260, reward mean : 0.003564645294383338, total_step : 158192, cur_epsilon : 0.741808\n",
      "episode : 5270, reward mean : 0.0037096839359266934, total_step : 158621, cur_epsilon : 0.741379\n",
      "episode : 5280, reward mean : 0.003585233792281625, total_step : 158992, cur_epsilon : 0.741008\n",
      "episode : 5290, reward mean : 0.003790176655949289, total_step : 159389, cur_epsilon : 0.740611\n",
      "episode : 5300, reward mean : 0.004243402748526829, total_step : 159654, cur_epsilon : 0.7403460000000001\n",
      "[EVAL] episode : 5300, reward mean : 0.27000000737607477, step mean : 34.0,\n",
      "episode : 5310, reward mean : 0.0034406844894848973, total_step : 159985, cur_epsilon : 0.7400150000000001\n",
      "synced target net\n",
      "episode : 5320, reward mean : 0.0034060215577483175, total_step : 160210, cur_epsilon : 0.7397900000000001\n",
      "episode : 5330, reward mean : 0.004290813267384752, total_step : 160345, cur_epsilon : 0.7396550000000001\n",
      "episode : 5340, reward mean : 0.003151691902763723, total_step : 160559, cur_epsilon : 0.739441\n",
      "episode : 5350, reward mean : 0.004130847630238979, total_step : 160842, cur_epsilon : 0.739158\n",
      "[EVAL] episode : 5350, reward mean : -0.03799999020993709, step mean : 44.6,\n",
      "episode : 5360, reward mean : 0.004171648303241428, total_step : 161224, cur_epsilon : 0.738776\n",
      "episode : 5370, reward mean : 0.004225332395726853, total_step : 161500, cur_epsilon : 0.7385\n",
      "episode : 5380, reward mean : 0.004672868959051747, total_step : 161665, cur_epsilon : 0.738335\n",
      "episode : 5390, reward mean : 0.004426722643907304, total_step : 161902, cur_epsilon : 0.738098\n",
      "episode : 5400, reward mean : 0.0031425990954179455, total_step : 162201, cur_epsilon : 0.737799\n",
      "[EVAL] episode : 5400, reward mean : -0.4759999893605709, step mean : 48.4,\n",
      "episode : 5410, reward mean : 0.002924220921564675, total_step : 162526, cur_epsilon : 0.737474\n",
      "episode : 5420, reward mean : 0.0033505600042008825, total_step : 162702, cur_epsilon : 0.737298\n",
      "episode : 5430, reward mean : 0.0026261575115264888, total_step : 163001, cur_epsilon : 0.736999\n",
      "episode : 5440, reward mean : 0.002358462379026391, total_step : 163253, cur_epsilon : 0.736747\n",
      "episode : 5450, reward mean : 0.0024440431963959966, total_step : 163614, cur_epsilon : 0.736386\n",
      "[EVAL] episode : 5450, reward mean : 0.34000000581145284, step mean : 27.0,\n",
      "episode : 5460, reward mean : 0.002589750090006725, total_step : 163942, cur_epsilon : 0.736058\n",
      "episode : 5470, reward mean : 0.002926880354487373, total_step : 164165, cur_epsilon : 0.735835\n",
      "episode : 5480, reward mean : 0.0024251889821522645, total_step : 164546, cur_epsilon : 0.735454\n",
      "episode : 5490, reward mean : 0.0028761449281825176, total_step : 164706, cur_epsilon : 0.735294\n",
      "episode : 5500, reward mean : 0.003169097401540388, total_step : 164952, cur_epsilon : 0.735048\n",
      "[EVAL] episode : 5500, reward mean : 0.13800000585615635, step mean : 27.0,\n",
      "synced target net\n",
      "episode : 5510, reward mean : 0.0038620754549053317, total_step : 165177, cur_epsilon : 0.734823\n",
      "episode : 5520, reward mean : 0.004574281848170727, total_step : 165390, cur_epsilon : 0.73461\n",
      "episode : 5530, reward mean : 0.005560585146072652, total_step : 165650, cur_epsilon : 0.7343500000000001\n",
      "episode : 5540, reward mean : 0.0058140859051543665, total_step : 165914, cur_epsilon : 0.734086\n",
      "episode : 5550, reward mean : 0.006048655130371854, total_step : 166188, cur_epsilon : 0.733812\n",
      "[EVAL] episode : 5550, reward mean : -0.8959999933838845, step mean : 30.6,\n",
      "episode : 5560, reward mean : 0.005933459721867451, total_step : 166555, cur_epsilon : 0.733445\n",
      "episode : 5570, reward mean : 0.006693004684345512, total_step : 166736, cur_epsilon : 0.733264\n",
      "episode : 5580, reward mean : 0.00686022153384583, total_step : 167046, cur_epsilon : 0.732954\n",
      "episode : 5590, reward mean : 0.007273709524109666, total_step : 167418, cur_epsilon : 0.7325820000000001\n",
      "episode : 5600, reward mean : 0.007341077918120261, total_step : 167882, cur_epsilon : 0.732118\n",
      "[EVAL] episode : 5600, reward mean : 0.4280000038444996, step mean : 18.2,\n",
      "episode : 5610, reward mean : 0.007188954795652638, total_step : 168170, cur_epsilon : 0.73183\n",
      "episode : 5620, reward mean : 0.0075676221509871746, total_step : 168560, cur_epsilon : 0.7314400000000001\n",
      "episode : 5630, reward mean : 0.0073783368659644105, total_step : 168869, cur_epsilon : 0.731131\n",
      "episode : 5640, reward mean : 0.00725355260305318, total_step : 169441, cur_epsilon : 0.730559\n",
      "episode : 5650, reward mean : 0.007509741019292743, total_step : 169798, cur_epsilon : 0.730202\n",
      "[EVAL] episode : 5650, reward mean : -0.4679999940097332, step mean : 27.8,\n",
      "episode : 5660, reward mean : 0.008275624875361001, total_step : 169967, cur_epsilon : 0.730033\n",
      "synced target net\n",
      "episode : 5670, reward mean : 0.009049389211418242, total_step : 170130, cur_epsilon : 0.72987\n",
      "episode : 5680, reward mean : 0.010429583956867876, total_step : 170347, cur_epsilon : 0.729653\n",
      "episode : 5690, reward mean : 0.011054488040362805, total_step : 170690, cur_epsilon : 0.72931\n",
      "episode : 5700, reward mean : 0.010984217025535672, total_step : 171129, cur_epsilon : 0.728871\n",
      "[EVAL] episode : 5700, reward mean : 0.356000005453825, step mean : 25.4,\n",
      "episode : 5710, reward mean : 0.011199656235553234, total_step : 171405, cur_epsilon : 0.728595\n",
      "episode : 5720, reward mean : 0.01149476174805704, total_step : 171833, cur_epsilon : 0.728167\n",
      "episode : 5730, reward mean : 0.012141367757000537, total_step : 172061, cur_epsilon : 0.727939\n",
      "episode : 5740, reward mean : 0.013566208586343385, total_step : 172241, cur_epsilon : 0.727759\n",
      "episode : 5750, reward mean : 0.012895658676715, total_step : 172722, cur_epsilon : 0.7272780000000001\n",
      "[EVAL] episode : 5750, reward mean : -0.7579999964684248, step mean : 16.8,\n",
      "episode : 5760, reward mean : 0.012032992613644131, total_step : 173014, cur_epsilon : 0.726986\n",
      "episode : 5770, reward mean : 0.012320630416674362, total_step : 173246, cur_epsilon : 0.726754\n",
      "episode : 5780, reward mean : 0.011505196810052264, total_step : 173515, cur_epsilon : 0.726485\n",
      "episode : 5790, reward mean : 0.010841111863588526, total_step : 174096, cur_epsilon : 0.725904\n",
      "episode : 5800, reward mean : 0.01129655823496909, total_step : 174431, cur_epsilon : 0.725569\n",
      "[EVAL] episode : 5800, reward mean : 0.048000003397464755, step mean : 16.2,\n",
      "episode : 5810, reward mean : 0.012197941105344723, total_step : 174706, cur_epsilon : 0.725294\n",
      "episode : 5820, reward mean : 0.012202755646632913, total_step : 174901, cur_epsilon : 0.725099\n",
      "synced target net\n",
      "episode : 5830, reward mean : 0.012214414738629838, total_step : 175191, cur_epsilon : 0.724809\n",
      "episode : 5840, reward mean : 0.012779116096957395, total_step : 175558, cur_epsilon : 0.724442\n",
      "episode : 5850, reward mean : 0.013153852660877582, total_step : 175835, cur_epsilon : 0.7241650000000001\n",
      "[EVAL] episode : 5850, reward mean : 0.3000000067055225, step mean : 31.0,\n",
      "episode : 5860, reward mean : 0.014054614014338714, total_step : 176104, cur_epsilon : 0.723896\n",
      "episode : 5870, reward mean : 0.014129478394909044, total_step : 176355, cur_epsilon : 0.723645\n",
      "episode : 5880, reward mean : 0.014666673169807106, total_step : 176635, cur_epsilon : 0.723365\n",
      "episode : 5890, reward mean : 0.015336169491257955, total_step : 176935, cur_epsilon : 0.7230650000000001\n",
      "episode : 5900, reward mean : 0.015218650569539454, total_step : 177199, cur_epsilon : 0.722801\n",
      "[EVAL] episode : 5900, reward mean : -0.7619999963790178, step mean : 17.2,\n",
      "episode : 5910, reward mean : 0.015563458278018812, total_step : 177489, cur_epsilon : 0.722511\n",
      "episode : 5920, reward mean : 0.016089533527721526, total_step : 177772, cur_epsilon : 0.722228\n",
      "episode : 5930, reward mean : 0.0166998378640358, total_step : 178004, cur_epsilon : 0.7219960000000001\n",
      "episode : 5940, reward mean : 0.01692256541916417, total_step : 178265, cur_epsilon : 0.721735\n",
      "episode : 5950, reward mean : 0.01735630901890392, total_step : 178600, cur_epsilon : 0.7214\n",
      "[EVAL] episode : 5950, reward mean : 0.7460000056773424, step mean : 26.4,\n",
      "episode : 5960, reward mean : 0.01791946958350455, total_step : 178857, cur_epsilon : 0.7211430000000001\n",
      "episode : 5970, reward mean : 0.01824958773932016, total_step : 179250, cur_epsilon : 0.72075\n",
      "episode : 5980, reward mean : 0.01881104328741248, total_step : 179506, cur_epsilon : 0.720494\n",
      "episode : 5990, reward mean : 0.01909015674566934, total_step : 179730, cur_epsilon : 0.72027\n",
      "episode : 6000, reward mean : 0.01938500649202615, total_step : 179944, cur_epsilon : 0.720056\n",
      "[EVAL] episode : 6000, reward mean : 0.6160000085830688, step mean : 39.4,\n",
      "synced target net\n",
      "episode : 6010, reward mean : 0.01972213627897612, total_step : 180332, cur_epsilon : 0.719668\n",
      "episode : 6020, reward mean : 0.01999502310752126, total_step : 180657, cur_epsilon : 0.7193430000000001\n",
      "episode : 6030, reward mean : 0.020462693063662718, total_step : 180965, cur_epsilon : 0.7190350000000001\n",
      "episode : 6040, reward mean : 0.021342721726708362, total_step : 181223, cur_epsilon : 0.718777\n",
      "episode : 6050, reward mean : 0.021404965170774577, total_step : 181473, cur_epsilon : 0.718527\n",
      "[EVAL] episode : 6050, reward mean : 0.4200000040233135, step mean : 19.0,\n",
      "episode : 6060, reward mean : 0.021143570847551127, total_step : 181719, cur_epsilon : 0.7182810000000001\n",
      "episode : 6070, reward mean : 0.021589792320041424, total_step : 181936, cur_epsilon : 0.718064\n",
      "episode : 6080, reward mean : 0.022154611752883187, total_step : 182280, cur_epsilon : 0.71772\n",
      "episode : 6090, reward mean : 0.022397379237051276, total_step : 182718, cur_epsilon : 0.717282\n",
      "episode : 6100, reward mean : 0.02206066223594253, total_step : 183109, cur_epsilon : 0.7168910000000001\n",
      "[EVAL] episode : 6100, reward mean : 0.48600000254809855, step mean : 12.4,\n",
      "episode : 6110, reward mean : 0.022397715171856986, total_step : 183390, cur_epsilon : 0.71661\n",
      "episode : 6120, reward mean : 0.022148699309445578, total_step : 183730, cur_epsilon : 0.71627\n",
      "episode : 6130, reward mean : 0.022649272407297208, total_step : 184110, cur_epsilon : 0.71589\n",
      "episode : 6140, reward mean : 0.022737791512088688, total_step : 184243, cur_epsilon : 0.715757\n",
      "episode : 6150, reward mean : 0.02231707967181758, total_step : 184689, cur_epsilon : 0.715311\n",
      "[EVAL] episode : 6150, reward mean : 0.4680000029504299, step mean : 14.2,\n",
      "synced target net\n",
      "episode : 6160, reward mean : 0.02233604546485154, total_step : 185063, cur_epsilon : 0.714937\n",
      "episode : 6170, reward mean : 0.02169044410818931, total_step : 185449, cur_epsilon : 0.714551\n",
      "episode : 6180, reward mean : 0.022537223334065534, total_step : 185714, cur_epsilon : 0.714286\n",
      "episode : 6190, reward mean : 0.02301616159768742, total_step : 186104, cur_epsilon : 0.7138960000000001\n",
      "episode : 6200, reward mean : 0.023954845215043714, total_step : 186309, cur_epsilon : 0.7136910000000001\n",
      "[EVAL] episode : 6200, reward mean : 0.3260000105947256, step mean : 48.2,\n",
      "episode : 6210, reward mean : 0.02430757494214412, total_step : 186575, cur_epsilon : 0.713425\n",
      "episode : 6220, reward mean : 0.024517691389948033, total_step : 186830, cur_epsilon : 0.7131700000000001\n",
      "episode : 6230, reward mean : 0.025434998473687694, total_step : 187044, cur_epsilon : 0.712956\n",
      "episode : 6240, reward mean : 0.02555128854388992, total_step : 187156, cur_epsilon : 0.712844\n",
      "episode : 6250, reward mean : 0.02592480648726225, total_step : 187307, cur_epsilon : 0.712693\n",
      "[EVAL] episode : 6250, reward mean : 0.12400001510977746, step mean : 68.4,\n",
      "episode : 6260, reward mean : 0.025913744502531262, total_step : 187498, cur_epsilon : 0.712502\n",
      "episode : 6270, reward mean : 0.02572089962904866, total_step : 187902, cur_epsilon : 0.712098\n",
      "episode : 6280, reward mean : 0.02525956062985596, total_step : 188275, cur_epsilon : 0.711725\n",
      "episode : 6290, reward mean : 0.025545316502238683, total_step : 188480, cur_epsilon : 0.71152\n",
      "episode : 6300, reward mean : 0.025928577909334786, total_step : 188623, cur_epsilon : 0.711377\n",
      "[EVAL] episode : 6300, reward mean : -0.047999989986419675, step mean : 45.6,\n",
      "episode : 6310, reward mean : 0.02610460236290744, total_step : 188995, cur_epsilon : 0.711005\n",
      "episode : 6320, reward mean : 0.02631962673821121, total_step : 189342, cur_epsilon : 0.710658\n",
      "episode : 6330, reward mean : 0.026578205534349687, total_step : 189562, cur_epsilon : 0.710438\n",
      "episode : 6340, reward mean : 0.026807577461474327, total_step : 189899, cur_epsilon : 0.710101\n",
      "synced target net\n",
      "episode : 6350, reward mean : 0.02703780175692688, total_step : 190136, cur_epsilon : 0.709864\n",
      "[EVAL] episode : 6350, reward mean : 0.2720000073313713, step mean : 33.8,\n",
      "episode : 6360, reward mean : 0.028028308369992476, total_step : 190489, cur_epsilon : 0.709511\n",
      "episode : 6370, reward mean : 0.02803454337059703, total_step : 190667, cur_epsilon : 0.709333\n",
      "episode : 6380, reward mean : 0.02834326666998779, total_step : 191052, cur_epsilon : 0.708948\n",
      "episode : 6390, reward mean : 0.02785133668870107, total_step : 191447, cur_epsilon : 0.708553\n",
      "episode : 6400, reward mean : 0.027440631490317172, total_step : 191890, cur_epsilon : 0.70811\n",
      "[EVAL] episode : 6400, reward mean : 0.6740000072866679, step mean : 33.6,\n",
      "episode : 6410, reward mean : 0.02787052131462813, total_step : 192296, cur_epsilon : 0.707704\n",
      "episode : 6420, reward mean : 0.028529601508165445, total_step : 192554, cur_epsilon : 0.707446\n",
      "episode : 6430, reward mean : 0.02851011535358466, total_step : 192748, cur_epsilon : 0.707252\n",
      "episode : 6440, reward mean : 0.029071435057529196, total_step : 192968, cur_epsilon : 0.707032\n",
      "episode : 6450, reward mean : 0.029961246792948985, total_step : 193175, cur_epsilon : 0.706825\n",
      "[EVAL] episode : 6450, reward mean : 0.15000001452863215, step mean : 65.8,\n",
      "episode : 6460, reward mean : 0.030984526607366537, total_step : 193494, cur_epsilon : 0.7065060000000001\n",
      "episode : 6470, reward mean : 0.031038646361397627, total_step : 193838, cur_epsilon : 0.706162\n",
      "episode : 6480, reward mean : 0.03146142623829943, total_step : 194143, cur_epsilon : 0.705857\n",
      "episode : 6490, reward mean : 0.031919883221433784, total_step : 194523, cur_epsilon : 0.705477\n",
      "episode : 6500, reward mean : 0.03208769879461481, total_step : 194792, cur_epsilon : 0.7052080000000001\n",
      "[EVAL] episode : 6500, reward mean : 0.6720000073313713, step mean : 33.8,\n",
      "episode : 6510, reward mean : 0.03276344734156782, total_step : 194930, cur_epsilon : 0.7050700000000001\n",
      "synced target net\n",
      "episode : 6520, reward mean : 0.03363190831879348, total_step : 195141, cur_epsilon : 0.704859\n",
      "episode : 6530, reward mean : 0.034079638943304326, total_step : 195425, cur_epsilon : 0.7045750000000001\n",
      "episode : 6540, reward mean : 0.03400153552726002, total_step : 195652, cur_epsilon : 0.704348\n",
      "episode : 6550, reward mean : 0.03419237289264912, total_step : 196103, cur_epsilon : 0.703897\n",
      "[EVAL] episode : 6550, reward mean : -0.2639999896287918, step mean : 47.4,\n",
      "episode : 6560, reward mean : 0.03328201867804704, total_step : 196476, cur_epsilon : 0.703524\n",
      "episode : 6570, reward mean : 0.03380822565865843, total_step : 196707, cur_epsilon : 0.7032930000000001\n",
      "episode : 6580, reward mean : 0.03370061438491053, total_step : 197053, cur_epsilon : 0.702947\n",
      "episode : 6590, reward mean : 0.03354325382595766, total_step : 197333, cur_epsilon : 0.702667\n",
      "episode : 6600, reward mean : 0.034027279207313604, total_step : 197590, cur_epsilon : 0.70241\n",
      "[EVAL] episode : 6600, reward mean : 0.016000013053417205, step mean : 59.0,\n",
      "episode : 6610, reward mean : 0.034314681211438554, total_step : 197776, cur_epsilon : 0.702224\n",
      "episode : 6620, reward mean : 0.03448187959017485, total_step : 198140, cur_epsilon : 0.70186\n",
      "episode : 6630, reward mean : 0.03470438053328572, total_step : 198368, cur_epsilon : 0.701632\n",
      "episode : 6640, reward mean : 0.03430271732749934, total_step : 198907, cur_epsilon : 0.701093\n",
      "episode : 6650, reward mean : 0.03466917941416789, total_step : 199138, cur_epsilon : 0.700862\n",
      "[EVAL] episode : 6650, reward mean : 0.3040000066161156, step mean : 30.6,\n",
      "episode : 6660, reward mean : 0.03472973621274168, total_step : 199473, cur_epsilon : 0.700527\n",
      "episode : 6670, reward mean : 0.03467167064804455, total_step : 199687, cur_epsilon : 0.700313\n",
      "episode : 6680, reward mean : 0.03527096455978107, total_step : 199862, cur_epsilon : 0.700138\n",
      "synced target net\n",
      "episode : 6690, reward mean : 0.03521525310978895, total_step : 200074, cur_epsilon : 0.699926\n",
      "episode : 6700, reward mean : 0.035813439304648496, total_step : 200248, cur_epsilon : 0.699752\n",
      "[EVAL] episode : 6700, reward mean : 0.43600000366568564, step mean : 17.4,\n",
      "episode : 6710, reward mean : 0.03529955937892154, total_step : 200666, cur_epsilon : 0.699334\n",
      "episode : 6720, reward mean : 0.03498661361518316, total_step : 200950, cur_epsilon : 0.6990500000000001\n",
      "episode : 6730, reward mean : 0.03388113574820943, total_step : 201368, cur_epsilon : 0.698632\n",
      "episode : 6740, reward mean : 0.034332350689022406, total_step : 201640, cur_epsilon : 0.69836\n",
      "episode : 6750, reward mean : 0.035037043512419416, total_step : 201940, cur_epsilon : 0.69806\n",
      "[EVAL] episode : 6750, reward mean : 0.3720000095665455, step mean : 43.6,\n",
      "episode : 6760, reward mean : 0.035406811203333666, total_step : 202065, cur_epsilon : 0.697935\n",
      "episode : 6770, reward mean : 0.035725264961623544, total_step : 202323, cur_epsilon : 0.697677\n",
      "episode : 6780, reward mean : 0.035706496143378424, total_step : 202609, cur_epsilon : 0.697391\n",
      "episode : 6790, reward mean : 0.034911641225339476, total_step : 202921, cur_epsilon : 0.697079\n",
      "episode : 6800, reward mean : 0.0349264770521618, total_step : 203086, cur_epsilon : 0.696914\n",
      "[EVAL] episode : 6800, reward mean : 0.024000003933906555, step mean : 18.6,\n",
      "episode : 6810, reward mean : 0.035177686343327845, total_step : 203290, cur_epsilon : 0.69671\n",
      "episode : 6820, reward mean : 0.035322587105207956, total_step : 203566, cur_epsilon : 0.696434\n",
      "episode : 6830, reward mean : 0.035380679960492156, total_step : 203901, cur_epsilon : 0.696099\n",
      "episode : 6840, reward mean : 0.0351666731286075, total_step : 204222, cur_epsilon : 0.695778\n",
      "episode : 6850, reward mean : 0.035722634195737596, total_step : 204416, cur_epsilon : 0.695584\n",
      "[EVAL] episode : 6850, reward mean : 0.4160000041127205, step mean : 19.4,\n",
      "episode : 6860, reward mean : 0.036094758639237015, total_step : 204535, cur_epsilon : 0.695465\n",
      "episode : 6870, reward mean : 0.03688938054059574, total_step : 204763, cur_epsilon : 0.695237\n",
      "synced target net\n",
      "episode : 6880, reward mean : 0.03750000644886736, total_step : 205015, cur_epsilon : 0.694985\n",
      "episode : 6890, reward mean : 0.03816546363369474, total_step : 205329, cur_epsilon : 0.694671\n",
      "episode : 6900, reward mean : 0.03839565862051171, total_step : 205542, cur_epsilon : 0.694458\n",
      "[EVAL] episode : 6900, reward mean : 0.4460000034421682, step mean : 16.4,\n",
      "episode : 6910, reward mean : 0.03901447822810211, total_step : 205886, cur_epsilon : 0.694114\n",
      "episode : 6920, reward mean : 0.03879046887670767, total_step : 206212, cur_epsilon : 0.6937880000000001\n",
      "episode : 6930, reward mean : 0.03937230082349731, total_step : 206580, cur_epsilon : 0.69342\n",
      "episode : 6940, reward mean : 0.03888905544279503, total_step : 206886, cur_epsilon : 0.693114\n",
      "episode : 6950, reward mean : 0.03918705681003898, total_step : 207149, cur_epsilon : 0.692851\n",
      "[EVAL] episode : 6950, reward mean : 0.11400000639259815, step mean : 29.4,\n",
      "episode : 6960, reward mean : 0.039271558174895575, total_step : 207461, cur_epsilon : 0.692539\n",
      "episode : 6970, reward mean : 0.038892402431501376, total_step : 207696, cur_epsilon : 0.692304\n",
      "episode : 6980, reward mean : 0.039181954875044035, total_step : 208065, cur_epsilon : 0.691935\n",
      "episode : 6990, reward mean : 0.039512166681197156, total_step : 208405, cur_epsilon : 0.691595\n",
      "episode : 7000, reward mean : 0.03911572073958814, total_step : 208752, cur_epsilon : 0.6912480000000001\n",
      "[EVAL] episode : 7000, reward mean : 0.3000000067055225, step mean : 31.0,\n",
      "episode : 7010, reward mean : 0.039768908023515376, total_step : 209065, cur_epsilon : 0.6909350000000001\n",
      "episode : 7020, reward mean : 0.03979345375587076, total_step : 209616, cur_epsilon : 0.690384\n",
      "episode : 7030, reward mean : 0.03952347729726036, total_step : 209776, cur_epsilon : 0.6902240000000001\n",
      "synced target net\n",
      "episode : 7040, reward mean : 0.04025284736893479, total_step : 210132, cur_epsilon : 0.689868\n",
      "episode : 7050, reward mean : 0.04045958092555087, total_step : 210356, cur_epsilon : 0.689644\n",
      "[EVAL] episode : 7050, reward mean : 0.39800000451505185, step mean : 21.2,\n",
      "episode : 7060, reward mean : 0.04002266934632242, total_step : 210634, cur_epsilon : 0.689366\n",
      "episode : 7070, reward mean : 0.04019944068423803, total_step : 210879, cur_epsilon : 0.689121\n",
      "episode : 7080, reward mean : 0.039745769166719104, total_step : 211170, cur_epsilon : 0.68883\n",
      "episode : 7090, reward mean : 0.039644576276012804, total_step : 211612, cur_epsilon : 0.688388\n",
      "episode : 7100, reward mean : 0.039307048713237466, total_step : 211921, cur_epsilon : 0.688079\n",
      "[EVAL] episode : 7100, reward mean : 0.38400000482797625, step mean : 22.6,\n",
      "episode : 7110, reward mean : 0.03989170828988742, total_step : 212276, cur_epsilon : 0.687724\n",
      "episode : 7120, reward mean : 0.039698040172761156, total_step : 212683, cur_epsilon : 0.687317\n",
      "episode : 7130, reward mean : 0.03998457869073326, total_step : 212849, cur_epsilon : 0.6871510000000001\n",
      "episode : 7140, reward mean : 0.03994258348843088, total_step : 213049, cur_epsilon : 0.6869510000000001\n",
      "episode : 7150, reward mean : 0.03991049597417558, total_step : 213541, cur_epsilon : 0.686459\n",
      "[EVAL] episode : 7150, reward mean : 0.3700000051409006, step mean : 24.0,\n",
      "episode : 7160, reward mean : 0.03995112378342733, total_step : 213880, cur_epsilon : 0.6861200000000001\n",
      "episode : 7170, reward mean : 0.04045607341087139, total_step : 214187, cur_epsilon : 0.685813\n",
      "episode : 7180, reward mean : 0.04082173348500453, total_step : 214494, cur_epsilon : 0.6855060000000001\n",
      "episode : 7190, reward mean : 0.040566070438705915, total_step : 214647, cur_epsilon : 0.685353\n",
      "episode : 7200, reward mean : 0.04138333979062736, total_step : 214828, cur_epsilon : 0.685172\n",
      "[EVAL] episode : 7200, reward mean : 0.8160000041127204, step mean : 19.4,\n",
      "synced target net\n",
      "episode : 7210, reward mean : 0.041346747098276175, total_step : 215221, cur_epsilon : 0.684779\n",
      "episode : 7220, reward mean : 0.04156371837512814, total_step : 215631, cur_epsilon : 0.684369\n",
      "episode : 7230, reward mean : 0.04156847119300445, total_step : 215895, cur_epsilon : 0.684105\n",
      "episode : 7240, reward mean : 0.041814923586416475, total_step : 216085, cur_epsilon : 0.683915\n",
      "episode : 7250, reward mean : 0.04233242024918055, total_step : 216278, cur_epsilon : 0.683722\n",
      "[EVAL] episode : 7250, reward mean : 0.7440000057220459, step mean : 26.6,\n",
      "episode : 7260, reward mean : 0.042410474776010704, total_step : 216589, cur_epsilon : 0.683411\n",
      "episode : 7270, reward mean : 0.042511698340758824, total_step : 216883, cur_epsilon : 0.683117\n",
      "episode : 7280, reward mean : 0.04294231414723282, total_step : 217137, cur_epsilon : 0.682863\n",
      "episode : 7290, reward mean : 0.04275995158834386, total_step : 217536, cur_epsilon : 0.682464\n",
      "episode : 7300, reward mean : 0.04358219824093458, total_step : 217903, cur_epsilon : 0.682097\n",
      "[EVAL] episode : 7300, reward mean : 0.5080000020563602, step mean : 10.2,\n",
      "episode : 7310, reward mean : 0.04337483545900329, total_step : 218120, cur_epsilon : 0.68188\n",
      "episode : 7320, reward mean : 0.0436516457977781, total_step : 218284, cur_epsilon : 0.681716\n",
      "episode : 7330, reward mean : 0.04326330795274262, total_step : 218535, cur_epsilon : 0.681465\n",
      "episode : 7340, reward mean : 0.04355177756789717, total_step : 218789, cur_epsilon : 0.681211\n",
      "episode : 7350, reward mean : 0.04281633298159862, total_step : 219096, cur_epsilon : 0.680904\n",
      "[EVAL] episode : 7350, reward mean : 0.24000000804662705, step mean : 37.0,\n",
      "episode : 7360, reward mean : 0.042639952106148245, total_step : 219492, cur_epsilon : 0.680508\n",
      "episode : 7370, reward mean : 0.04340027782161249, total_step : 219699, cur_epsilon : 0.680301\n",
      "episode : 7380, reward mean : 0.042967486125606344, total_step : 219985, cur_epsilon : 0.680015\n",
      "synced target net\n",
      "episode : 7390, reward mean : 0.04394858561012596, total_step : 220227, cur_epsilon : 0.679773\n",
      "episode : 7400, reward mean : 0.04385946590956804, total_step : 220558, cur_epsilon : 0.679442\n",
      "[EVAL] episode : 7400, reward mean : 0.48800000697374346, step mean : 32.0,\n",
      "episode : 7410, reward mean : 0.04375169336112083, total_step : 220903, cur_epsilon : 0.6790970000000001\n",
      "episode : 7420, reward mean : 0.04382615199844752, total_step : 221014, cur_epsilon : 0.6789860000000001\n",
      "episode : 7430, reward mean : 0.043915215059663615, total_step : 221314, cur_epsilon : 0.678686\n",
      "episode : 7440, reward mean : 0.04427285591435809, total_step : 221812, cur_epsilon : 0.678188\n",
      "episode : 7450, reward mean : 0.04489128161996803, total_step : 222117, cur_epsilon : 0.677883\n",
      "[EVAL] episode : 7450, reward mean : 0.7220000062137842, step mean : 28.8,\n",
      "episode : 7460, reward mean : 0.04410054264690018, total_step : 222470, cur_epsilon : 0.67753\n",
      "episode : 7470, reward mean : 0.04406694085792286, total_step : 222760, cur_epsilon : 0.6772400000000001\n",
      "episode : 7480, reward mean : 0.04422995297606776, total_step : 223004, cur_epsilon : 0.676996\n",
      "episode : 7490, reward mean : 0.04472096773070869, total_step : 223202, cur_epsilon : 0.676798\n",
      "episode : 7500, reward mean : 0.044921339782824116, total_step : 223516, cur_epsilon : 0.6764840000000001\n",
      "[EVAL] episode : 7500, reward mean : -0.22599999494850637, step mean : 23.4,\n",
      "episode : 7510, reward mean : 0.04512650444954832, total_step : 223727, cur_epsilon : 0.676273\n",
      "episode : 7520, reward mean : 0.04506915538333991, total_step : 224034, cur_epsilon : 0.6759660000000001\n",
      "episode : 7530, reward mean : 0.04463878467030932, total_step : 224422, cur_epsilon : 0.675578\n",
      "episode : 7540, reward mean : 0.044595497168985934, total_step : 224820, cur_epsilon : 0.67518\n",
      "synced target net\n",
      "episode : 7550, reward mean : 0.04533510578783932, total_step : 225027, cur_epsilon : 0.674973\n",
      "[EVAL] episode : 7550, reward mean : 0.636000008136034, step mean : 37.4,\n",
      "episode : 7560, reward mean : 0.045712969415431855, total_step : 225405, cur_epsilon : 0.6745950000000001\n",
      "episode : 7570, reward mean : 0.04645311080898642, total_step : 225609, cur_epsilon : 0.674391\n",
      "episode : 7580, reward mean : 0.04646438639856737, total_step : 225962, cur_epsilon : 0.674038\n",
      "episode : 7590, reward mean : 0.047202904999246315, total_step : 226165, cur_epsilon : 0.673835\n",
      "episode : 7600, reward mean : 0.047730269605342886, total_step : 226426, cur_epsilon : 0.673574\n",
      "[EVAL] episode : 7600, reward mean : 0.4660000029951334, step mean : 14.4,\n",
      "episode : 7610, reward mean : 0.047685945997658366, total_step : 226622, cur_epsilon : 0.673378\n",
      "episode : 7620, reward mean : 0.04751575448161622, total_step : 227110, cur_epsilon : 0.67289\n",
      "episode : 7630, reward mean : 0.04722936425069994, total_step : 227489, cur_epsilon : 0.6725110000000001\n",
      "episode : 7640, reward mean : 0.04692539911965854, total_step : 227684, cur_epsilon : 0.672316\n",
      "episode : 7650, reward mean : 0.04711111756126865, total_step : 228004, cur_epsilon : 0.671996\n",
      "[EVAL] episode : 7650, reward mean : 0.6920000068843365, step mean : 31.8,\n",
      "episode : 7660, reward mean : 0.04725457564234617, total_step : 228356, cur_epsilon : 0.671644\n",
      "episode : 7670, reward mean : 0.04744589954512643, total_step : 228772, cur_epsilon : 0.671228\n",
      "episode : 7680, reward mean : 0.04740886062451561, total_step : 229161, cur_epsilon : 0.670839\n",
      "episode : 7690, reward mean : 0.04783615730412217, total_step : 229494, cur_epsilon : 0.670506\n",
      "episode : 7700, reward mean : 0.04803117529041581, total_step : 229805, cur_epsilon : 0.670195\n",
      "[EVAL] episode : 7700, reward mean : -0.12399999275803567, step mean : 33.4,\n",
      "synced target net\n",
      "episode : 7710, reward mean : 0.04800130347570286, total_step : 230089, cur_epsilon : 0.669911\n",
      "episode : 7720, reward mean : 0.04823057640951918, total_step : 230472, cur_epsilon : 0.669528\n",
      "episode : 7730, reward mean : 0.048491597660416004, total_step : 230632, cur_epsilon : 0.669368\n",
      "episode : 7740, reward mean : 0.048828171832049, total_step : 230933, cur_epsilon : 0.6690670000000001\n",
      "episode : 7750, reward mean : 0.04930581291164121, total_step : 231324, cur_epsilon : 0.668676\n",
      "[EVAL] episode : 7750, reward mean : 0.3540000054985285, step mean : 25.6,\n",
      "episode : 7760, reward mean : 0.04955284150782972, total_step : 231493, cur_epsilon : 0.668507\n",
      "episode : 7770, reward mean : 0.04995882241355023, total_step : 231738, cur_epsilon : 0.668262\n",
      "episode : 7780, reward mean : 0.049629826506642875, total_step : 232053, cur_epsilon : 0.6679470000000001\n",
      "episode : 7790, reward mean : 0.04925289477230037, total_step : 232307, cur_epsilon : 0.667693\n",
      "episode : 7800, reward mean : 0.0494115449137126, total_step : 232544, cur_epsilon : 0.667456\n",
      "[EVAL] episode : 7800, reward mean : -0.1899999912828207, step mean : 40.0,\n",
      "episode : 7810, reward mean : 0.04955698468659202, total_step : 232890, cur_epsilon : 0.66711\n",
      "episode : 7820, reward mean : 0.04981330568518709, total_step : 233149, cur_epsilon : 0.6668510000000001\n",
      "episode : 7830, reward mean : 0.0501379374871423, total_step : 233455, cur_epsilon : 0.666545\n",
      "episode : 7840, reward mean : 0.050827812572836646, total_step : 233674, cur_epsilon : 0.666326\n",
      "episode : 7850, reward mean : 0.0510267580400227, total_step : 233877, cur_epsilon : 0.666123\n",
      "[EVAL] episode : 7850, reward mean : 0.7180000063031912, step mean : 29.2,\n",
      "episode : 7860, reward mean : 0.050970744358437285, total_step : 234080, cur_epsilon : 0.6659200000000001\n",
      "episode : 7870, reward mean : 0.05128590225217227, total_step : 234391, cur_epsilon : 0.665609\n",
      "episode : 7880, reward mean : 0.051986047051856436, total_step : 234598, cur_epsilon : 0.665402\n",
      "episode : 7890, reward mean : 0.052177446240179136, total_step : 234904, cur_epsilon : 0.665096\n",
      "synced target net\n",
      "episode : 7900, reward mean : 0.052250639354409295, total_step : 235204, cur_epsilon : 0.664796\n",
      "[EVAL] episode : 7900, reward mean : 0.020000004023313523, step mean : 19.0,\n",
      "episode : 7910, reward mean : 0.05270544259977326, total_step : 235501, cur_epsilon : 0.6644990000000001\n",
      "episode : 7920, reward mean : 0.053306824624340866, total_step : 235782, cur_epsilon : 0.664218\n",
      "episode : 7930, reward mean : 0.05357251589598715, total_step : 235928, cur_epsilon : 0.664072\n",
      "episode : 7940, reward mean : 0.053777084526575404, total_step : 236320, cur_epsilon : 0.66368\n",
      "episode : 7950, reward mean : 0.05390944040187125, total_step : 236571, cur_epsilon : 0.663429\n",
      "[EVAL] episode : 7950, reward mean : 0.174000009521842, step mean : 43.6,\n",
      "episode : 7960, reward mean : 0.05352638834811298, total_step : 236832, cur_epsilon : 0.663168\n",
      "episode : 7970, reward mean : 0.052525727898396615, total_step : 237283, cur_epsilon : 0.662717\n",
      "episode : 7980, reward mean : 0.05316541997533768, total_step : 237530, cur_epsilon : 0.66247\n",
      "episode : 7990, reward mean : 0.053588241733405484, total_step : 237749, cur_epsilon : 0.662251\n",
      "episode : 8000, reward mean : 0.053807506438810375, total_step : 238029, cur_epsilon : 0.6619710000000001\n",
      "[EVAL] episode : 8000, reward mean : 0.5460000101476907, step mean : 46.4,\n",
      "episode : 8010, reward mean : 0.05419476299445504, total_step : 238374, cur_epsilon : 0.661626\n",
      "episode : 8020, reward mean : 0.0548603555679953, total_step : 238695, cur_epsilon : 0.661305\n",
      "episode : 8030, reward mean : 0.05483562287852484, total_step : 238969, cur_epsilon : 0.661031\n",
      "episode : 8040, reward mean : 0.05528731987283412, total_step : 239260, cur_epsilon : 0.66074\n",
      "episode : 8050, reward mean : 0.05589441637699463, total_step : 239526, cur_epsilon : 0.660474\n",
      "[EVAL] episode : 8050, reward mean : 0.4860000070184469, step mean : 32.2,\n",
      "episode : 8060, reward mean : 0.05583623472425541, total_step : 239727, cur_epsilon : 0.660273\n",
      "synced target net\n",
      "episode : 8070, reward mean : 0.056188358359103456, total_step : 240096, cur_epsilon : 0.659904\n",
      "episode : 8080, reward mean : 0.05678713515050488, total_step : 240366, cur_epsilon : 0.659634\n",
      "episode : 8090, reward mean : 0.05702349221879531, total_step : 240528, cur_epsilon : 0.6594720000000001\n",
      "episode : 8100, reward mean : 0.05625185828517985, total_step : 240805, cur_epsilon : 0.659195\n",
      "[EVAL] episode : 8100, reward mean : 0.03400000371038914, step mean : 17.6,\n",
      "episode : 8110, reward mean : 0.05685943923132671, total_step : 241066, cur_epsilon : 0.658934\n",
      "episode : 8120, reward mean : 0.056613306923691424, total_step : 241318, cur_epsilon : 0.658682\n",
      "episode : 8130, reward mean : 0.057007386503101935, total_step : 241551, cur_epsilon : 0.6584490000000001\n",
      "episode : 8140, reward mean : 0.057608114536485525, total_step : 241815, cur_epsilon : 0.658185\n",
      "episode : 8150, reward mean : 0.05726503710142491, total_step : 242047, cur_epsilon : 0.657953\n",
      "[EVAL] episode : 8150, reward mean : 0.38000000491738317, step mean : 23.0,\n",
      "episode : 8160, reward mean : 0.057488977011320566, total_step : 242217, cur_epsilon : 0.657783\n",
      "episode : 8170, reward mean : 0.057858023557679744, total_step : 242468, cur_epsilon : 0.657532\n",
      "episode : 8180, reward mean : 0.05860514089187339, total_step : 242609, cur_epsilon : 0.6573910000000001\n",
      "episode : 8190, reward mean : 0.059073266487282054, total_step : 242777, cur_epsilon : 0.657223\n",
      "episode : 8200, reward mean : 0.059700006412387624, total_step : 243014, cur_epsilon : 0.6569860000000001\n",
      "[EVAL] episode : 8200, reward mean : 0.34000000581145284, step mean : 27.0,\n",
      "episode : 8210, reward mean : 0.060300859030059356, total_step : 243271, cur_epsilon : 0.656729\n",
      "episode : 8220, reward mean : 0.060580298382734515, total_step : 243591, cur_epsilon : 0.656409\n",
      "episode : 8230, reward mean : 0.06109964189528152, total_step : 244012, cur_epsilon : 0.655988\n",
      "episode : 8240, reward mean : 0.06115655981352354, total_step : 244314, cur_epsilon : 0.655686\n",
      "episode : 8250, reward mean : 0.061058188234540546, total_step : 244643, cur_epsilon : 0.655357\n",
      "[EVAL] episode : 8250, reward mean : 0.06000000759959221, step mean : 34.8,\n",
      "episode : 8260, reward mean : 0.061233662588174566, total_step : 244847, cur_epsilon : 0.655153\n",
      "synced target net\n",
      "episode : 8270, reward mean : 0.061259982230855735, total_step : 245174, cur_epsilon : 0.654826\n",
      "episode : 8280, reward mean : 0.0612669146284551, total_step : 245517, cur_epsilon : 0.654483\n",
      "episode : 8290, reward mean : 0.06185645997340019, total_step : 245777, cur_epsilon : 0.654223\n",
      "episode : 8300, reward mean : 0.06128313894305064, total_step : 246001, cur_epsilon : 0.653999\n",
      "[EVAL] episode : 8300, reward mean : 0.48400000259280207, step mean : 12.6,\n",
      "episode : 8310, reward mean : 0.06194465141631349, total_step : 246200, cur_epsilon : 0.6538\n",
      "episode : 8320, reward mean : 0.062074525639540155, total_step : 246440, cur_epsilon : 0.65356\n",
      "episode : 8330, reward mean : 0.06220288755973669, total_step : 246681, cur_epsilon : 0.653319\n",
      "episode : 8340, reward mean : 0.06264988650261856, total_step : 246955, cur_epsilon : 0.653045\n",
      "episode : 8350, reward mean : 0.06276168305493757, total_step : 247308, cur_epsilon : 0.652692\n",
      "[EVAL] episode : 8350, reward mean : -0.31399999745190144, step mean : 12.4,\n",
      "episode : 8360, reward mean : 0.06296292507339216, total_step : 247786, cur_epsilon : 0.6522140000000001\n",
      "episode : 8370, reward mean : 0.06310275432318806, total_step : 248115, cur_epsilon : 0.651885\n",
      "episode : 8380, reward mean : 0.06358592526427068, total_step : 248257, cur_epsilon : 0.651743\n",
      "episode : 8390, reward mean : 0.06375566790917171, total_step : 248461, cur_epsilon : 0.6515390000000001\n",
      "episode : 8400, reward mean : 0.0632547683113565, total_step : 248727, cur_epsilon : 0.651273\n",
      "[EVAL] episode : 8400, reward mean : 0.436000008136034, step mean : 37.2,\n",
      "episode : 8410, reward mean : 0.06384899570638763, total_step : 249073, cur_epsilon : 0.650927\n",
      "episode : 8420, reward mean : 0.06466746483918394, total_step : 249330, cur_epsilon : 0.6506700000000001\n",
      "episode : 8430, reward mean : 0.06478173831546256, total_step : 249579, cur_epsilon : 0.650421\n",
      "episode : 8440, reward mean : 0.06499763673383266, total_step : 249742, cur_epsilon : 0.650258\n",
      "episode : 8450, reward mean : 0.06558462178544004, total_step : 249991, cur_epsilon : 0.6500090000000001\n",
      "[EVAL] episode : 8450, reward mean : 0.31200000643730164, step mean : 29.8,\n",
      "synced target net\n",
      "episode : 8460, reward mean : 0.06550000640398539, total_step : 250405, cur_epsilon : 0.649595\n",
      "episode : 8470, reward mean : 0.06556198988021862, total_step : 250895, cur_epsilon : 0.649105\n",
      "episode : 8480, reward mean : 0.06580189320012786, total_step : 251135, cur_epsilon : 0.648865\n",
      "episode : 8490, reward mean : 0.06622851053241846, total_step : 251515, cur_epsilon : 0.648485\n",
      "episode : 8500, reward mean : 0.06692235934734345, total_step : 251669, cur_epsilon : 0.648331\n",
      "[EVAL] episode : 8500, reward mean : 0.4780000116676092, step mean : 53.2,\n",
      "episode : 8510, reward mean : 0.06722914859144724, total_step : 251951, cur_epsilon : 0.6480490000000001\n",
      "episode : 8520, reward mean : 0.06710564020732548, total_step : 252199, cur_epsilon : 0.6478010000000001\n",
      "episode : 8530, reward mean : 0.06710786103638891, total_step : 252538, cur_epsilon : 0.647462\n",
      "episode : 8540, reward mean : 0.06759602513657206, total_step : 252664, cur_epsilon : 0.647336\n",
      "episode : 8550, reward mean : 0.06795088359530557, total_step : 253002, cur_epsilon : 0.646998\n",
      "[EVAL] episode : 8550, reward mean : 0.21200000867247581, step mean : 39.8,\n",
      "episode : 8560, reward mean : 0.06840888490317233, total_step : 253152, cur_epsilon : 0.6468480000000001\n",
      "episode : 8570, reward mean : 0.06831505890531397, total_step : 253374, cur_epsilon : 0.646626\n",
      "episode : 8580, reward mean : 0.06908741898352762, total_step : 253653, cur_epsilon : 0.646347\n",
      "episode : 8590, reward mean : 0.06928056518745644, total_step : 254028, cur_epsilon : 0.645972\n",
      "episode : 8600, reward mean : 0.06943256453805885, total_step : 254337, cur_epsilon : 0.6456630000000001\n",
      "[EVAL] episode : 8600, reward mean : 0.29000000692903993, step mean : 32.0,\n",
      "episode : 8610, reward mean : 0.06999768351705508, total_step : 254591, cur_epsilon : 0.645409\n",
      "episode : 8620, reward mean : 0.07037123609377448, total_step : 254809, cur_epsilon : 0.6451910000000001\n",
      "synced target net\n",
      "episode : 8630, reward mean : 0.07026072482050742, total_step : 255143, cur_epsilon : 0.644857\n",
      "episode : 8640, reward mean : 0.07078125639624583, total_step : 255433, cur_epsilon : 0.644567\n",
      "episode : 8650, reward mean : 0.07073526651399328, total_step : 255812, cur_epsilon : 0.644188\n",
      "[EVAL] episode : 8650, reward mean : 0.4000000089406967, step mean : 40.8,\n",
      "episode : 8660, reward mean : 0.07105427891484895, total_step : 256075, cur_epsilon : 0.6439250000000001\n",
      "episode : 8670, reward mean : 0.07114764192134074, total_step : 256333, cur_epsilon : 0.643667\n",
      "episode : 8680, reward mean : 0.07153802482982076, total_step : 256632, cur_epsilon : 0.6433679999999999\n",
      "episode : 8690, reward mean : 0.07238320547367164, total_step : 256836, cur_epsilon : 0.6431640000000001\n",
      "episode : 8700, reward mean : 0.07225287995633038, total_step : 257087, cur_epsilon : 0.6429130000000001\n",
      "[EVAL] episode : 8700, reward mean : 0.3260000105947256, step mean : 48.2,\n",
      "episode : 8710, reward mean : 0.0717657928445925, total_step : 257348, cur_epsilon : 0.642652\n",
      "episode : 8720, reward mean : 0.07222936419137847, total_step : 257682, cur_epsilon : 0.642318\n",
      "episode : 8730, reward mean : 0.07297595141013873, total_step : 257968, cur_epsilon : 0.642032\n",
      "episode : 8740, reward mean : 0.07357666543482438, total_step : 258380, cur_epsilon : 0.6416200000000001\n",
      "episode : 8750, reward mean : 0.07422057782134839, total_step : 258553, cur_epsilon : 0.6414470000000001\n",
      "[EVAL] episode : 8750, reward mean : -0.44199999459087846, step mean : 25.2,\n",
      "episode : 8760, reward mean : 0.07410845388233084, total_step : 258886, cur_epsilon : 0.641114\n",
      "episode : 8770, reward mean : 0.07465565063445877, total_step : 259142, cur_epsilon : 0.640858\n",
      "episode : 8780, reward mean : 0.07474146425312465, total_step : 259501, cur_epsilon : 0.640499\n",
      "episode : 8790, reward mean : 0.07527645690550983, total_step : 259766, cur_epsilon : 0.640234\n",
      "episode : 8800, reward mean : 0.0753988700281744, total_step : 259993, cur_epsilon : 0.640007\n",
      "[EVAL] episode : 8800, reward mean : -0.3799999915063381, step mean : 38.8,\n",
      "synced target net\n",
      "episode : 8810, reward mean : 0.07557321864769083, total_step : 260174, cur_epsilon : 0.639826\n",
      "episode : 8820, reward mean : 0.07554875922488585, total_step : 260530, cur_epsilon : 0.63947\n",
      "episode : 8830, reward mean : 0.07577463832798827, total_step : 260865, cur_epsilon : 0.639135\n",
      "episode : 8840, reward mean : 0.0761199158945066, total_step : 261193, cur_epsilon : 0.638807\n",
      "episode : 8850, reward mean : 0.07638757701373874, total_step : 261490, cur_epsilon : 0.63851\n",
      "[EVAL] episode : 8850, reward mean : 0.3460000056773424, step mean : 26.4,\n",
      "episode : 8860, reward mean : 0.07721219600617683, total_step : 261693, cur_epsilon : 0.638307\n",
      "episode : 8870, reward mean : 0.07780045734607145, total_step : 261904, cur_epsilon : 0.638096\n",
      "episode : 8880, reward mean : 0.07775113251593031, total_step : 262278, cur_epsilon : 0.637722\n",
      "episode : 8890, reward mean : 0.07753431459698151, total_step : 262403, cur_epsilon : 0.637597\n",
      "episode : 8900, reward mean : 0.07836742211321598, total_step : 262594, cur_epsilon : 0.637406\n",
      "[EVAL] episode : 8900, reward mean : -0.037999994680285455, step mean : 24.8,\n",
      "episode : 8910, reward mean : 0.07845679650524282, total_step : 262846, cur_epsilon : 0.637154\n",
      "episode : 8920, reward mean : 0.07810987185127198, total_step : 263087, cur_epsilon : 0.6369130000000001\n",
      "episode : 8930, reward mean : 0.07811534792656227, total_step : 263414, cur_epsilon : 0.6365860000000001\n",
      "episode : 8940, reward mean : 0.0786051517950425, total_step : 263708, cur_epsilon : 0.6362920000000001\n",
      "episode : 8950, reward mean : 0.07878212928813597, total_step : 264079, cur_epsilon : 0.635921\n",
      "[EVAL] episode : 8950, reward mean : 0.7800000049173832, step mean : 23.0,\n",
      "episode : 8960, reward mean : 0.07855692602440416, total_step : 264311, cur_epsilon : 0.635689\n",
      "episode : 8970, reward mean : 0.07809811117648746, total_step : 264654, cur_epsilon : 0.635346\n",
      "episode : 8980, reward mean : 0.07885969457753783, total_step : 264902, cur_epsilon : 0.6350979999999999\n",
      "synced target net\n",
      "episode : 8990, reward mean : 0.07893104086359662, total_step : 265169, cur_epsilon : 0.634831\n",
      "episode : 9000, reward mean : 0.07921333971619605, total_step : 265545, cur_epsilon : 0.634455\n",
      "[EVAL] episode : 9000, reward mean : -0.2379999902099371, step mean : 44.4,\n",
      "episode : 9010, reward mean : 0.07915094978135745, total_step : 265932, cur_epsilon : 0.6340680000000001\n",
      "episode : 9020, reward mean : 0.07915854296624297, total_step : 266056, cur_epsilon : 0.6339440000000001\n",
      "episode : 9030, reward mean : 0.07933555455055348, total_step : 266227, cur_epsilon : 0.633773\n",
      "episode : 9040, reward mean : 0.07882190903759999, total_step : 266719, cur_epsilon : 0.633281\n",
      "episode : 9050, reward mean : 0.07885194008459867, total_step : 267023, cur_epsilon : 0.632977\n",
      "[EVAL] episode : 9050, reward mean : 0.4660000029951334, step mean : 14.4,\n",
      "episode : 9060, reward mean : 0.07878698210024314, total_step : 267312, cur_epsilon : 0.632688\n",
      "episode : 9070, reward mean : 0.07924807694552687, total_step : 267625, cur_epsilon : 0.632375\n",
      "episode : 9080, reward mean : 0.07908150418517938, total_step : 268105, cur_epsilon : 0.6318950000000001\n",
      "episode : 9090, reward mean : 0.07938064445072439, total_step : 268364, cur_epsilon : 0.6316360000000001\n",
      "episode : 9100, reward mean : 0.07943627012525123, total_step : 268743, cur_epsilon : 0.631257\n",
      "[EVAL] episode : 9100, reward mean : 0.040000008046627046, step mean : 36.8,\n",
      "episode : 9110, reward mean : 0.07982986368561976, total_step : 268915, cur_epsilon : 0.631085\n",
      "episode : 9120, reward mean : 0.0804298309445904, total_step : 269098, cur_epsilon : 0.6309020000000001\n",
      "episode : 9130, reward mean : 0.08078532949404185, total_step : 269402, cur_epsilon : 0.630598\n",
      "episode : 9140, reward mean : 0.08022757750289837, total_step : 269839, cur_epsilon : 0.630161\n",
      "synced target net\n",
      "episode : 9150, reward mean : 0.08069618125044094, total_step : 270140, cur_epsilon : 0.6298600000000001\n",
      "[EVAL] episode : 9150, reward mean : -0.04599999450147152, step mean : 25.6,\n",
      "episode : 9160, reward mean : 0.08068559590775894, total_step : 270479, cur_epsilon : 0.629521\n",
      "episode : 9170, reward mean : 0.08113522994369593, total_step : 270796, cur_epsilon : 0.6292040000000001\n",
      "episode : 9180, reward mean : 0.0809629693513732, total_step : 271083, cur_epsilon : 0.628917\n",
      "episode : 9190, reward mean : 0.08084331433044521, total_step : 271322, cur_epsilon : 0.6286780000000001\n",
      "episode : 9200, reward mean : 0.0810043542103275, total_step : 271503, cur_epsilon : 0.6284970000000001\n",
      "[EVAL] episode : 9200, reward mean : 0.2580000076442957, step mean : 35.2,\n",
      "episode : 9210, reward mean : 0.08127145046888003, total_step : 271885, cur_epsilon : 0.628115\n",
      "episode : 9220, reward mean : 0.08115727319614456, total_step : 272119, cur_epsilon : 0.627881\n",
      "episode : 9230, reward mean : 0.08149079728472013, total_step : 272439, cur_epsilon : 0.627561\n",
      "episode : 9240, reward mean : 0.08205628343958617, total_step : 272645, cur_epsilon : 0.627355\n",
      "episode : 9250, reward mean : 0.08211892530160982, total_step : 272915, cur_epsilon : 0.627085\n",
      "[EVAL] episode : 9250, reward mean : 0.32400000616908076, step mean : 28.6,\n",
      "episode : 9260, reward mean : 0.08203024396403305, total_step : 273224, cur_epsilon : 0.626776\n",
      "episode : 9270, reward mean : 0.08198706140096712, total_step : 273592, cur_epsilon : 0.6264080000000001\n",
      "episode : 9280, reward mean : 0.08235237707428088, total_step : 273880, cur_epsilon : 0.62612\n",
      "episode : 9290, reward mean : 0.08273305266802836, total_step : 274054, cur_epsilon : 0.625946\n",
      "episode : 9300, reward mean : 0.08321075906957029, total_step : 274337, cur_epsilon : 0.6256630000000001\n",
      "[EVAL] episode : 9300, reward mean : 0.4740000028163195, step mean : 13.6,\n",
      "episode : 9310, reward mean : 0.08357787963331034, total_step : 274522, cur_epsilon : 0.625478\n",
      "episode : 9320, reward mean : 0.08369850423326916, total_step : 274835, cur_epsilon : 0.625165\n",
      "synced target net\n",
      "episode : 9330, reward mean : 0.08365166768379757, total_step : 275005, cur_epsilon : 0.624995\n",
      "episode : 9340, reward mean : 0.08407067018391552, total_step : 275140, cur_epsilon : 0.62486\n",
      "episode : 9350, reward mean : 0.08412300102432622, total_step : 275417, cur_epsilon : 0.624583\n",
      "[EVAL] episode : 9350, reward mean : -0.019999990612268446, step mean : 42.8,\n",
      "episode : 9360, reward mean : 0.08401923714181743, total_step : 275739, cur_epsilon : 0.624261\n",
      "episode : 9370, reward mean : 0.084132343619712, total_step : 276058, cur_epsilon : 0.623942\n",
      "episode : 9380, reward mean : 0.08417591255630798, total_step : 276343, cur_epsilon : 0.623657\n",
      "episode : 9390, reward mean : 0.08456230669404372, total_step : 276706, cur_epsilon : 0.623294\n",
      "episode : 9400, reward mean : 0.08509255956442274, total_step : 276933, cur_epsilon : 0.623067\n",
      "[EVAL] episode : 9400, reward mean : 0.8340000037103892, step mean : 17.6,\n",
      "episode : 9410, reward mean : 0.08542721147186792, total_step : 277143, cur_epsilon : 0.622857\n",
      "episode : 9420, reward mean : 0.08607856263456008, total_step : 277454, cur_epsilon : 0.622546\n",
      "episode : 9430, reward mean : 0.08644963521727639, total_step : 277826, cur_epsilon : 0.622174\n",
      "episode : 9440, reward mean : 0.08644809958947254, total_step : 277951, cur_epsilon : 0.6220490000000001\n",
      "episode : 9450, reward mean : 0.08665397462390718, total_step : 278278, cur_epsilon : 0.621722\n",
      "[EVAL] episode : 9450, reward mean : 0.38000000491738317, step mean : 23.0,\n",
      "episode : 9460, reward mean : 0.08657294506163792, total_step : 278676, cur_epsilon : 0.621324\n",
      "episode : 9470, reward mean : 0.08688912992114879, total_step : 278999, cur_epsilon : 0.621001\n",
      "episode : 9480, reward mean : 0.08675317093140528, total_step : 279350, cur_epsilon : 0.62065\n",
      "episode : 9490, reward mean : 0.08729926875377324, total_step : 279555, cur_epsilon : 0.620445\n",
      "episode : 9500, reward mean : 0.08774842742497199, total_step : 279851, cur_epsilon : 0.6201490000000001\n",
      "[EVAL] episode : 9500, reward mean : 0.8380000036209821, step mean : 17.2,\n",
      "synced target net\n",
      "episode : 9510, reward mean : 0.08794637861300247, total_step : 280185, cur_epsilon : 0.619815\n",
      "episode : 9520, reward mean : 0.08858614082745582, total_step : 280498, cur_epsilon : 0.619502\n",
      "episode : 9530, reward mean : 0.08888877867256825, total_step : 280830, cur_epsilon : 0.61917\n",
      "episode : 9540, reward mean : 0.08921279463257085, total_step : 281042, cur_epsilon : 0.618958\n",
      "episode : 9550, reward mean : 0.0897047184134653, total_step : 281293, cur_epsilon : 0.618707\n",
      "[EVAL] episode : 9550, reward mean : -0.723999997228384, step mean : 13.4,\n",
      "episode : 9560, reward mean : 0.08982532017700623, total_step : 281498, cur_epsilon : 0.618502\n",
      "episode : 9570, reward mean : 0.09035110354633727, total_step : 281715, cur_epsilon : 0.618285\n",
      "episode : 9580, reward mean : 0.0907473967606999, total_step : 281855, cur_epsilon : 0.618145\n",
      "episode : 9590, reward mean : 0.09138582492532739, total_step : 282162, cur_epsilon : 0.617838\n",
      "episode : 9600, reward mean : 0.0909822980312553, total_step : 282468, cur_epsilon : 0.617532\n",
      "[EVAL] episode : 9600, reward mean : 0.4520000033080578, step mean : 15.8,\n",
      "episode : 9610, reward mean : 0.09094485548149472, total_step : 282823, cur_epsilon : 0.6171770000000001\n",
      "episode : 9620, reward mean : 0.09107900844488037, total_step : 283112, cur_epsilon : 0.6168880000000001\n",
      "episode : 9630, reward mean : 0.09143406659159674, total_step : 283289, cur_epsilon : 0.616711\n",
      "episode : 9640, reward mean : 0.09171681134358697, total_step : 283634, cur_epsilon : 0.616366\n",
      "episode : 9650, reward mean : 0.09227254522182626, total_step : 283816, cur_epsilon : 0.6161840000000001\n",
      "[EVAL] episode : 9650, reward mean : 0.8300000037997961, step mean : 18.0,\n",
      "episode : 9660, reward mean : 0.09244617613063312, total_step : 283966, cur_epsilon : 0.616034\n",
      "episode : 9670, reward mean : 0.09250569405147831, total_step : 284226, cur_epsilon : 0.615774\n",
      "episode : 9680, reward mean : 0.09252376669059577, total_step : 284625, cur_epsilon : 0.615375\n",
      "episode : 9690, reward mean : 0.0928988711681636, total_step : 284878, cur_epsilon : 0.615122\n",
      "synced target net\n",
      "episode : 9700, reward mean : 0.09244845996870055, total_step : 285230, cur_epsilon : 0.61477\n",
      "[EVAL] episode : 9700, reward mean : 0.06800000295042992, step mean : 14.2,\n",
      "episode : 9710, reward mean : 0.0923058765987366, total_step : 285585, cur_epsilon : 0.614415\n",
      "episode : 9720, reward mean : 0.09236214627883561, total_step : 285848, cur_epsilon : 0.614152\n",
      "episode : 9730, reward mean : 0.09241007830304422, total_step : 286119, cur_epsilon : 0.613881\n",
      "episode : 9740, reward mean : 0.09230493449030379, total_step : 286339, cur_epsilon : 0.613661\n",
      "episode : 9750, reward mean : 0.09262257045946824, total_step : 286547, cur_epsilon : 0.613453\n",
      "[EVAL] episode : 9750, reward mean : -0.20599999092519283, step mean : 41.6,\n",
      "episode : 9760, reward mean : 0.09292111291271467, total_step : 286773, cur_epsilon : 0.613227\n",
      "episode : 9770, reward mean : 0.0935250831171184, total_step : 286900, cur_epsilon : 0.6131\n",
      "episode : 9780, reward mean : 0.09365951555375399, total_step : 287184, cur_epsilon : 0.612816\n",
      "episode : 9790, reward mean : 0.09331052729249213, total_step : 287541, cur_epsilon : 0.6124590000000001\n",
      "episode : 9800, reward mean : 0.09362551655715369, total_step : 287848, cur_epsilon : 0.612152\n",
      "[EVAL] episode : 9800, reward mean : 0.3960000045597553, step mean : 21.4,\n",
      "episode : 9810, reward mean : 0.09376860982306476, total_step : 288321, cur_epsilon : 0.6116790000000001\n",
      "episode : 9820, reward mean : 0.09373727722962191, total_step : 288468, cur_epsilon : 0.611532\n",
      "episode : 9830, reward mean : 0.09399390258918217, total_step : 288732, cur_epsilon : 0.611268\n",
      "episode : 9840, reward mean : 0.09381707952380544, total_step : 289022, cur_epsilon : 0.610978\n",
      "episode : 9850, reward mean : 0.09390559010793867, total_step : 289251, cur_epsilon : 0.610749\n",
      "[EVAL] episode : 9850, reward mean : 0.2820000071078539, step mean : 32.8,\n",
      "episode : 9860, reward mean : 0.09429818079457382, total_step : 289580, cur_epsilon : 0.61042\n",
      "episode : 9870, reward mean : 0.09442047240890057, total_step : 289775, cur_epsilon : 0.610225\n",
      "synced target net\n",
      "episode : 9880, reward mean : 0.0947550670786757, total_step : 290059, cur_epsilon : 0.6099410000000001\n",
      "episode : 9890, reward mean : 0.09540951089939165, total_step : 290327, cur_epsilon : 0.609673\n",
      "episode : 9900, reward mean : 0.09490808715780426, total_step : 290637, cur_epsilon : 0.6093630000000001\n",
      "[EVAL] episode : 9900, reward mean : 0.7440000057220459, step mean : 26.6,\n",
      "episode : 9910, reward mean : 0.09541070261438246, total_step : 290854, cur_epsilon : 0.609146\n",
      "episode : 9920, reward mean : 0.09586492570214755, total_step : 291118, cur_epsilon : 0.608882\n",
      "episode : 9930, reward mean : 0.09601309799044304, total_step : 291485, cur_epsilon : 0.608515\n",
      "episode : 9940, reward mean : 0.0959195234496109, total_step : 291692, cur_epsilon : 0.6083080000000001\n",
      "episode : 9950, reward mean : 0.0964251319732723, total_step : 291903, cur_epsilon : 0.608097\n",
      "[EVAL] episode : 9950, reward mean : -0.021999995037913323, step mean : 23.2,\n",
      "episode : 9960, reward mean : 0.09696084971831416, total_step : 292182, cur_epsilon : 0.607818\n",
      "episode : 9970, reward mean : 0.0970792440557432, total_step : 292377, cur_epsilon : 0.607623\n",
      "episode : 9980, reward mean : 0.09745291215526257, total_step : 292717, cur_epsilon : 0.607283\n",
      "episode : 9990, reward mean : 0.09762763397225567, total_step : 293055, cur_epsilon : 0.6069450000000001\n",
      "episode : 10000, reward mean : 0.09746500634867698, total_step : 293528, cur_epsilon : 0.606472\n",
      "[EVAL] episode : 10000, reward mean : -0.0799999937415123, step mean : 29.0,\n",
      "episode : 10010, reward mean : 0.09791109525913988, total_step : 293794, cur_epsilon : 0.606206\n",
      "episode : 10020, reward mean : 0.09866168299176825, total_step : 293954, cur_epsilon : 0.6060460000000001\n",
      "episode : 10030, reward mean : 0.0986919305765498, total_step : 294435, cur_epsilon : 0.605565\n",
      "episode : 10040, reward mean : 0.09888944858072912, total_step : 294748, cur_epsilon : 0.605252\n",
      "synced target net\n",
      "episode : 10050, reward mean : 0.09894926008226267, total_step : 295098, cur_epsilon : 0.604902\n",
      "[EVAL] episode : 10050, reward mean : 0.48800000697374346, step mean : 32.0,\n",
      "episode : 10060, reward mean : 0.0989930481020413, total_step : 295464, cur_epsilon : 0.604536\n",
      "episode : 10070, reward mean : 0.09907845719509352, total_step : 295689, cur_epsilon : 0.604311\n",
      "episode : 10080, reward mean : 0.0988849269861858, total_step : 295995, cur_epsilon : 0.604005\n",
      "episode : 10090, reward mean : 0.09910902518158773, total_step : 296280, cur_epsilon : 0.60372\n",
      "episode : 10100, reward mean : 0.0990821845676418, total_step : 296517, cur_epsilon : 0.603483\n",
      "[EVAL] episode : 10100, reward mean : 0.516000010818243, step mean : 49.4,\n",
      "episode : 10110, reward mean : 0.09947874027697699, total_step : 296827, cur_epsilon : 0.603173\n",
      "episode : 10120, reward mean : 0.10003261504705863, total_step : 297177, cur_epsilon : 0.602823\n",
      "episode : 10130, reward mean : 0.10065351079336689, total_step : 297458, cur_epsilon : 0.602542\n",
      "episode : 10140, reward mean : 0.1005552331736969, total_step : 297667, cur_epsilon : 0.602333\n",
      "episode : 10150, reward mean : 0.1006975432966143, total_step : 298030, cur_epsilon : 0.60197\n",
      "[EVAL] episode : 10150, reward mean : 0.8920000024139881, step mean : 11.8,\n",
      "episode : 10160, reward mean : 0.10081988824036442, total_step : 298314, cur_epsilon : 0.6016859999999999\n",
      "episode : 10170, reward mean : 0.10072861991789753, total_step : 298516, cur_epsilon : 0.601484\n",
      "episode : 10180, reward mean : 0.10099804171073765, total_step : 298751, cur_epsilon : 0.601249\n",
      "episode : 10190, reward mean : 0.10135133117236292, total_step : 298900, cur_epsilon : 0.6011\n",
      "episode : 10200, reward mean : 0.10167353575556155, total_step : 299179, cur_epsilon : 0.600821\n",
      "[EVAL] episode : 10200, reward mean : 0.7900000046938658, step mean : 22.0,\n",
      "episode : 10210, reward mean : 0.10164643141379932, total_step : 299315, cur_epsilon : 0.600685\n",
      "episode : 10220, reward mean : 0.10170939968563543, total_step : 299559, cur_epsilon : 0.600441\n",
      "episode : 10230, reward mean : 0.10221897016879342, total_step : 299746, cur_epsilon : 0.6002540000000001\n",
      "episode : 10240, reward mean : 0.10216992820896849, total_step : 299904, cur_epsilon : 0.600096\n",
      "synced target net\n",
      "episode : 10250, reward mean : 0.10251317706268008, total_step : 300060, cur_epsilon : 0.59994\n",
      "[EVAL] episode : 10250, reward mean : 0.5420000102370978, step mean : 46.8,\n",
      "episode : 10260, reward mean : 0.10323782309228972, total_step : 300224, cur_epsilon : 0.5997760000000001\n",
      "episode : 10270, reward mean : 0.10324343378912247, total_step : 300622, cur_epsilon : 0.599378\n",
      "episode : 10280, reward mean : 0.10358269115246774, total_step : 300780, cur_epsilon : 0.5992200000000001\n",
      "episode : 10290, reward mean : 0.10424101701616315, total_step : 301009, cur_epsilon : 0.598991\n",
      "episode : 10300, reward mean : 0.1045805888474711, total_step : 301165, cur_epsilon : 0.598835\n",
      "[EVAL] episode : 10300, reward mean : 0.4020000044256449, step mean : 20.8,\n",
      "episode : 10310, reward mean : 0.10445587441115617, total_step : 301399, cur_epsilon : 0.598601\n",
      "episode : 10320, reward mean : 0.1045881846138139, total_step : 301568, cur_epsilon : 0.5984320000000001\n",
      "episode : 10330, reward mean : 0.10466505956077823, total_step : 301794, cur_epsilon : 0.598206\n",
      "episode : 10340, reward mean : 0.10485397149760067, total_step : 301904, cur_epsilon : 0.598096\n",
      "episode : 10350, reward mean : 0.10518164882967293, total_step : 302369, cur_epsilon : 0.597631\n",
      "[EVAL] episode : 10350, reward mean : 0.3680000051856041, step mean : 24.2,\n",
      "episode : 10360, reward mean : 0.1054700835358035, total_step : 302575, cur_epsilon : 0.5974250000000001\n",
      "episode : 10370, reward mean : 0.10583125028522145, total_step : 302705, cur_epsilon : 0.597295\n",
      "episode : 10380, reward mean : 0.10599807953077992, total_step : 303036, cur_epsilon : 0.596964\n",
      "episode : 10390, reward mean : 0.10630510737590627, total_step : 303520, cur_epsilon : 0.59648\n",
      "episode : 10400, reward mean : 0.10629616016378769, total_step : 303833, cur_epsilon : 0.596167\n",
      "[EVAL] episode : 10400, reward mean : 0.11600001081824303, step mean : 49.4,\n",
      "episode : 10410, reward mean : 0.10623151448239482, total_step : 304103, cur_epsilon : 0.595897\n",
      "episode : 10420, reward mean : 0.10604127311199589, total_step : 304405, cur_epsilon : 0.5955950000000001\n",
      "episode : 10430, reward mean : 0.10596549049741298, total_step : 304687, cur_epsilon : 0.595313\n",
      "episode : 10440, reward mean : 0.10587452739009712, total_step : 304985, cur_epsilon : 0.5950150000000001\n",
      "synced target net\n",
      "episode : 10450, reward mean : 0.10602010201234995, total_step : 305337, cur_epsilon : 0.5946629999999999\n",
      "[EVAL] episode : 10450, reward mean : -0.40999999530613424, step mean : 22.0,\n",
      "episode : 10460, reward mean : 0.10610899293250324, total_step : 305548, cur_epsilon : 0.594452\n",
      "episode : 10470, reward mean : 0.10637154404443992, total_step : 305876, cur_epsilon : 0.5941240000000001\n",
      "episode : 10480, reward mean : 0.10671851776899301, total_step : 306216, cur_epsilon : 0.5937840000000001\n",
      "episode : 10490, reward mean : 0.10685605970419965, total_step : 306575, cur_epsilon : 0.5934250000000001\n",
      "episode : 10500, reward mean : 0.10693429203278253, total_step : 306796, cur_epsilon : 0.5932040000000001\n",
      "[EVAL] episode : 10500, reward mean : -0.24799999445676804, step mean : 25.6,\n",
      "episode : 10510, reward mean : 0.10735680936249975, total_step : 307055, cur_epsilon : 0.592945\n",
      "episode : 10520, reward mean : 0.10760742076530967, total_step : 307294, cur_epsilon : 0.592706\n",
      "episode : 10530, reward mean : 0.10776069007835157, total_step : 307635, cur_epsilon : 0.592365\n",
      "episode : 10540, reward mean : 0.10815845034023362, total_step : 307918, cur_epsilon : 0.592082\n",
      "episode : 10550, reward mean : 0.10817536176751716, total_step : 308202, cur_epsilon : 0.591798\n",
      "[EVAL] episode : 10550, reward mean : 0.4280000038444996, step mean : 18.2,\n",
      "episode : 10560, reward mean : 0.10829167298365835, total_step : 308480, cur_epsilon : 0.59152\n",
      "episode : 10570, reward mean : 0.10835667613597698, total_step : 308713, cur_epsilon : 0.591287\n",
      "episode : 10580, reward mean : 0.10906994960188077, total_step : 308860, cur_epsilon : 0.59114\n",
      "episode : 10590, reward mean : 0.10857224427585731, total_step : 309286, cur_epsilon : 0.590714\n",
      "episode : 10600, reward mean : 0.1088622704664913, total_step : 309579, cur_epsilon : 0.5904210000000001\n",
      "[EVAL] episode : 10600, reward mean : 0.8100000042468309, step mean : 20.0,\n",
      "episode : 10610, reward mean : 0.10906126927707299, total_step : 309968, cur_epsilon : 0.590032\n",
      "synced target net\n",
      "episode : 10620, reward mean : 0.10932109859489648, total_step : 310193, cur_epsilon : 0.589807\n",
      "episode : 10630, reward mean : 0.10941110697705457, total_step : 310598, cur_epsilon : 0.589402\n",
      "episode : 10640, reward mean : 0.10950658526394125, total_step : 310797, cur_epsilon : 0.589203\n",
      "episode : 10650, reward mean : 0.10980188424860787, total_step : 310983, cur_epsilon : 0.589017\n",
      "[EVAL] episode : 10650, reward mean : 0.11200000196695328, step mean : 9.8,\n",
      "episode : 10660, reward mean : 0.10979362732781478, total_step : 311292, cur_epsilon : 0.588708\n",
      "episode : 10670, reward mean : 0.1101640175614286, total_step : 311597, cur_epsilon : 0.588403\n",
      "episode : 10680, reward mean : 0.11031742204828293, total_step : 312032, cur_epsilon : 0.587968\n",
      "episode : 10690, reward mean : 0.11074649836550927, total_step : 312273, cur_epsilon : 0.587727\n",
      "episode : 10700, reward mean : 0.11082991285719604, total_step : 312483, cur_epsilon : 0.5875170000000001\n",
      "[EVAL] episode : 10700, reward mean : 0.47600000277161597, step mean : 13.4,\n",
      "episode : 10710, reward mean : 0.11086275141258367, total_step : 312747, cur_epsilon : 0.587253\n",
      "episode : 10720, reward mean : 0.11129291676325417, total_step : 313084, cur_epsilon : 0.586916\n",
      "episode : 10730, reward mean : 0.11146878544189112, total_step : 313493, cur_epsilon : 0.586507\n",
      "episode : 10740, reward mean : 0.11189199886813732, total_step : 313737, cur_epsilon : 0.586263\n",
      "episode : 10750, reward mean : 0.11230605282773112, total_step : 313990, cur_epsilon : 0.58601\n",
      "[EVAL] episode : 10750, reward mean : 0.6760000072419643, step mean : 33.4,\n",
      "episode : 10760, reward mean : 0.11240985761708182, total_step : 314275, cur_epsilon : 0.585725\n",
      "episode : 10770, reward mean : 0.11255896638909367, total_step : 314511, cur_epsilon : 0.585489\n",
      "synced target net\n",
      "episode : 10780, reward mean : 0.11241188016072165, total_step : 315066, cur_epsilon : 0.5849340000000001\n",
      "episode : 10790, reward mean : 0.11235959853448726, total_step : 315319, cur_epsilon : 0.584681\n",
      "episode : 10800, reward mean : 0.11233796928302144, total_step : 315640, cur_epsilon : 0.58436\n",
      "[EVAL] episode : 10800, reward mean : 0.6300000082701445, step mean : 38.0,\n",
      "episode : 10810, reward mean : 0.11290287403501811, total_step : 315927, cur_epsilon : 0.5840730000000001\n",
      "episode : 10820, reward mean : 0.11304806547063831, total_step : 316267, cur_epsilon : 0.5837330000000001\n",
      "episode : 10830, reward mean : 0.11303140059859937, total_step : 316681, cur_epsilon : 0.583319\n",
      "episode : 10840, reward mean : 0.11351384395936236, total_step : 316855, cur_epsilon : 0.583145\n",
      "episode : 10850, reward mean : 0.11364424594948369, total_step : 317010, cur_epsilon : 0.58299\n",
      "[EVAL] episode : 10850, reward mean : 0.44400000795722006, step mean : 36.4,\n",
      "episode : 10860, reward mean : 0.11391989582305702, total_step : 317407, cur_epsilon : 0.582593\n",
      "episode : 10870, reward mean : 0.11430911395558273, total_step : 317680, cur_epsilon : 0.58232\n",
      "episode : 10880, reward mean : 0.11480974896403495, total_step : 317831, cur_epsilon : 0.582169\n",
      "episode : 10890, reward mean : 0.11506704029150792, total_step : 318046, cur_epsilon : 0.5819540000000001\n",
      "episode : 10900, reward mean : 0.11533395126769576, total_step : 318250, cur_epsilon : 0.58175\n",
      "[EVAL] episode : 10900, reward mean : -0.11399998851120471, step mean : 52.2,\n",
      "episode : 10910, reward mean : 0.11557379183432004, total_step : 318681, cur_epsilon : 0.581319\n",
      "episode : 10920, reward mean : 0.1159771125427628, total_step : 318935, cur_epsilon : 0.5810649999999999\n",
      "episode : 10930, reward mean : 0.11623696880662185, total_step : 319343, cur_epsilon : 0.580657\n",
      "episode : 10940, reward mean : 0.11652651454238612, total_step : 319520, cur_epsilon : 0.58048\n",
      "episode : 10950, reward mean : 0.11682101087942379, total_step : 319691, cur_epsilon : 0.580309\n",
      "[EVAL] episode : 10950, reward mean : 0.7140000063925982, step mean : 29.6,\n",
      "synced target net\n",
      "episode : 10960, reward mean : 0.11698449536474816, total_step : 320005, cur_epsilon : 0.579995\n",
      "episode : 10970, reward mean : 0.11705105462983427, total_step : 320425, cur_epsilon : 0.579575\n",
      "episode : 10980, reward mean : 0.11655465112388379, total_step : 320663, cur_epsilon : 0.579337\n",
      "episode : 10990, reward mean : 0.11699363688673076, total_step : 320874, cur_epsilon : 0.579126\n",
      "episode : 11000, reward mean : 0.11723818813027306, total_step : 321098, cur_epsilon : 0.578902\n",
      "[EVAL] episode : 11000, reward mean : 0.7420000057667494, step mean : 26.8,\n",
      "episode : 11010, reward mean : 0.11792734509180017, total_step : 321232, cur_epsilon : 0.578768\n",
      "episode : 11020, reward mean : 0.1182858502270887, total_step : 321428, cur_epsilon : 0.5785720000000001\n",
      "episode : 11030, reward mean : 0.11838078599858856, total_step : 321714, cur_epsilon : 0.5782860000000001\n",
      "episode : 11040, reward mean : 0.11830073094629831, total_step : 322092, cur_epsilon : 0.5779080000000001\n",
      "episode : 11050, reward mean : 0.11824525517660163, total_step : 322344, cur_epsilon : 0.577656\n",
      "[EVAL] episode : 11050, reward mean : 0.34800000563263894, step mean : 26.2,\n",
      "episode : 11060, reward mean : 0.11832550359556408, total_step : 322646, cur_epsilon : 0.577354\n",
      "episode : 11070, reward mean : 0.11844264406916141, total_step : 323006, cur_epsilon : 0.576994\n",
      "episode : 11080, reward mean : 0.11826264168848603, total_step : 323297, cur_epsilon : 0.576703\n",
      "episode : 11090, reward mean : 0.11832822993485051, total_step : 323615, cur_epsilon : 0.576385\n",
      "episode : 11100, reward mean : 0.11892072702950991, total_step : 323849, cur_epsilon : 0.5761510000000001\n",
      "[EVAL] episode : 11100, reward mean : -0.043999994546175, step mean : 25.4,\n",
      "episode : 11110, reward mean : 0.11868497480686106, total_step : 324202, cur_epsilon : 0.575798\n",
      "episode : 11120, reward mean : 0.11918975451220792, total_step : 324532, cur_epsilon : 0.5754680000000001\n",
      "episode : 11130, reward mean : 0.1194079128687286, total_step : 324780, cur_epsilon : 0.5752200000000001\n",
      "synced target net\n",
      "episode : 11140, reward mean : 0.11978725945322877, total_step : 325147, cur_epsilon : 0.5748530000000001\n",
      "episode : 11150, reward mean : 0.12041256236321723, total_step : 325340, cur_epsilon : 0.57466\n",
      "[EVAL] episode : 11150, reward mean : 0.11200001090765, step mean : 49.8,\n",
      "episode : 11160, reward mean : 0.12021326795854609, total_step : 325652, cur_epsilon : 0.5743480000000001\n",
      "episode : 11170, reward mean : 0.12044136709647858, total_step : 325887, cur_epsilon : 0.5741130000000001\n",
      "episode : 11180, reward mean : 0.1206243354677221, total_step : 326172, cur_epsilon : 0.573828\n",
      "episode : 11190, reward mean : 0.12069705724716187, total_step : 326479, cur_epsilon : 0.573521\n",
      "episode : 11200, reward mean : 0.12115000630662377, total_step : 326661, cur_epsilon : 0.573339\n",
      "[EVAL] episode : 11200, reward mean : 0.38400000482797625, step mean : 22.6,\n",
      "episode : 11210, reward mean : 0.1215218617943889, total_step : 327032, cur_epsilon : 0.572968\n",
      "episode : 11220, reward mean : 0.12198752859132547, total_step : 327398, cur_epsilon : 0.572602\n",
      "episode : 11230, reward mean : 0.1216776554626168, total_step : 327634, cur_epsilon : 0.572366\n",
      "episode : 11240, reward mean : 0.12209876075542773, total_step : 327849, cur_epsilon : 0.5721510000000001\n",
      "episode : 11250, reward mean : 0.12217956186268064, total_step : 328145, cur_epsilon : 0.571855\n",
      "[EVAL] episode : 11250, reward mean : 0.42600000388920306, step mean : 18.4,\n",
      "episode : 11260, reward mean : 0.12241830115668065, total_step : 328463, cur_epsilon : 0.571537\n",
      "episode : 11270, reward mean : 0.12250666114143835, total_step : 328651, cur_epsilon : 0.571349\n",
      "episode : 11280, reward mean : 0.12252926162612486, total_step : 329012, cur_epsilon : 0.570988\n",
      "episode : 11290, reward mean : 0.12220549789167433, total_step : 329265, cur_epsilon : 0.570735\n",
      "episode : 11300, reward mean : 0.12247345763151493, total_step : 329450, cur_epsilon : 0.57055\n",
      "[EVAL] episode : 11300, reward mean : 0.45200000777840615, step mean : 35.6,\n",
      "episode : 11310, reward mean : 0.12304421496799069, total_step : 329692, cur_epsilon : 0.570308\n",
      "synced target net\n",
      "episode : 11320, reward mean : 0.12337456460739403, total_step : 330005, cur_epsilon : 0.569995\n",
      "episode : 11330, reward mean : 0.12370609633036003, total_step : 330316, cur_epsilon : 0.5696840000000001\n",
      "episode : 11340, reward mean : 0.12369841900235289, total_step : 330611, cur_epsilon : 0.569389\n",
      "episode : 11350, reward mean : 0.12365198868371972, total_step : 330950, cur_epsilon : 0.5690500000000001\n",
      "[EVAL] episode : 11350, reward mean : -0.04599999450147152, step mean : 25.6,\n",
      "episode : 11360, reward mean : 0.12408011193685627, total_step : 331150, cur_epsilon : 0.5688500000000001\n",
      "episode : 11370, reward mean : 0.12439842318629447, total_step : 331274, cur_epsilon : 0.5687260000000001\n",
      "episode : 11380, reward mean : 0.12460896939199634, total_step : 331520, cur_epsilon : 0.5684800000000001\n",
      "episode : 11390, reward mean : 0.1244556691623485, total_step : 331879, cur_epsilon : 0.5681210000000001\n",
      "episode : 11400, reward mean : 0.12475965541999852, total_step : 332018, cur_epsilon : 0.567982\n",
      "[EVAL] episode : 11400, reward mean : 0.7820000048726797, step mean : 22.8,\n",
      "episode : 11410, reward mean : 0.12480193443129478, total_step : 332354, cur_epsilon : 0.5676460000000001\n",
      "episode : 11420, reward mean : 0.12500788720794376, total_step : 332604, cur_epsilon : 0.567396\n",
      "episode : 11430, reward mean : 0.12503500192206962, total_step : 332858, cur_epsilon : 0.567142\n",
      "episode : 11440, reward mean : 0.12552797832115964, total_step : 332979, cur_epsilon : 0.567021\n",
      "episode : 11450, reward mean : 0.12561310672782786, total_step : 333166, cur_epsilon : 0.5668340000000001\n",
      "[EVAL] episode : 11450, reward mean : -0.13199999257922174, step mean : 34.2,\n",
      "episode : 11460, reward mean : 0.12624258918567427, total_step : 333329, cur_epsilon : 0.566671\n",
      "episode : 11470, reward mean : 0.1258988728988737, total_step : 333706, cur_epsilon : 0.5662940000000001\n",
      "episode : 11480, reward mean : 0.12561063346848905, total_step : 334020, cur_epsilon : 0.56598\n",
      "episode : 11490, reward mean : 0.12589556764587564, total_step : 334177, cur_epsilon : 0.565823\n",
      "episode : 11500, reward mean : 0.12597304976844917, total_step : 334570, cur_epsilon : 0.5654300000000001\n",
      "[EVAL] episode : 11500, reward mean : 0.6280000083148479, step mean : 38.2,\n",
      "episode : 11510, reward mean : 0.12585752149485538, total_step : 334886, cur_epsilon : 0.565114\n",
      "synced target net\n",
      "episode : 11520, reward mean : 0.12581250628766913, total_step : 335022, cur_epsilon : 0.564978\n",
      "episode : 11530, reward mean : 0.12558890481423732, total_step : 335364, cur_epsilon : 0.564636\n",
      "episode : 11540, reward mean : 0.12576257127984403, total_step : 335648, cur_epsilon : 0.564352\n",
      "episode : 11550, reward mean : 0.12596883745646323, total_step : 335894, cur_epsilon : 0.564106\n",
      "[EVAL] episode : 11550, reward mean : 0.3820000048726797, step mean : 22.8,\n",
      "episode : 11560, reward mean : 0.12601038690940009, total_step : 336130, cur_epsilon : 0.5638700000000001\n",
      "episode : 11570, reward mean : 0.12637684293245846, total_step : 336390, cur_epsilon : 0.5636099999999999\n",
      "episode : 11580, reward mean : 0.12672884911813473, total_step : 336666, cur_epsilon : 0.563334\n",
      "episode : 11590, reward mean : 0.12702675347856862, total_step : 336804, cur_epsilon : 0.563196\n",
      "episode : 11600, reward mean : 0.1272836269703212, total_step : 336989, cur_epsilon : 0.563011\n",
      "[EVAL] episode : 11600, reward mean : 0.32600000612437724, step mean : 28.4,\n",
      "episode : 11610, reward mean : 0.12745650929513547, total_step : 337271, cur_epsilon : 0.562729\n",
      "episode : 11620, reward mean : 0.12710499767464123, total_step : 337562, cur_epsilon : 0.562438\n",
      "episode : 11630, reward mean : 0.12723044480052187, total_step : 337798, cur_epsilon : 0.5622020000000001\n",
      "episode : 11640, reward mean : 0.1272826523281621, total_step : 338119, cur_epsilon : 0.5618810000000001\n",
      "episode : 11650, reward mean : 0.12767039254499607, total_step : 338350, cur_epsilon : 0.56165\n",
      "[EVAL] episode : 11650, reward mean : 0.556000005453825, step mean : 25.2,\n",
      "episode : 11660, reward mean : 0.1277778793466605, total_step : 338507, cur_epsilon : 0.561493\n",
      "episode : 11670, reward mean : 0.12749872092770503, total_step : 338715, cur_epsilon : 0.561285\n",
      "episode : 11680, reward mean : 0.12765069120652872, total_step : 339020, cur_epsilon : 0.56098\n",
      "episode : 11690, reward mean : 0.1273584322807904, total_step : 339343, cur_epsilon : 0.560657\n",
      "episode : 11700, reward mean : 0.12721026268716043, total_step : 339698, cur_epsilon : 0.5603020000000001\n",
      "[EVAL] episode : 11700, reward mean : 0.5200000062584877, step mean : 28.8,\n",
      "episode : 11710, reward mean : 0.12747310618929608, total_step : 339873, cur_epsilon : 0.560127\n",
      "synced target net\n",
      "episode : 11720, reward mean : 0.12768260013032387, total_step : 340110, cur_epsilon : 0.55989\n",
      "episode : 11730, reward mean : 0.1277459568262278, total_step : 340318, cur_epsilon : 0.559682\n",
      "episode : 11740, reward mean : 0.1278705343800824, total_step : 340553, cur_epsilon : 0.559447\n",
      "episode : 11750, reward mean : 0.12788936797640424, total_step : 341011, cur_epsilon : 0.558989\n",
      "[EVAL] episode : 11750, reward mean : 0.8460000034421682, step mean : 16.4,\n",
      "episode : 11760, reward mean : 0.12800340763590976, total_step : 341359, cur_epsilon : 0.5586409999999999\n",
      "episode : 11770, reward mean : 0.12783772930164228, total_step : 341735, cur_epsilon : 0.558265\n",
      "episode : 11780, reward mean : 0.12826231527324064, total_step : 341917, cur_epsilon : 0.558083\n",
      "episode : 11790, reward mean : 0.12853605377221633, total_step : 342175, cur_epsilon : 0.557825\n",
      "episode : 11800, reward mean : 0.12882881983451672, total_step : 342511, cur_epsilon : 0.557489\n",
      "[EVAL] episode : 11800, reward mean : 0.5100000020116567, step mean : 10.0,\n",
      "episode : 11810, reward mean : 0.12886622134804296, total_step : 342847, cur_epsilon : 0.557153\n",
      "episode : 11820, reward mean : 0.12903046312879032, total_step : 343132, cur_epsilon : 0.556868\n",
      "episode : 11830, reward mean : 0.12906340441539796, total_step : 343374, cur_epsilon : 0.5566260000000001\n",
      "episode : 11840, reward mean : 0.12914274275960477, total_step : 343561, cur_epsilon : 0.556439\n",
      "episode : 11850, reward mean : 0.12923122990153385, total_step : 343836, cur_epsilon : 0.5561640000000001\n",
      "[EVAL] episode : 11850, reward mean : 0.34400001019239423, step mean : 46.4,\n",
      "episode : 11860, reward mean : 0.1296028730509768, total_step : 344076, cur_epsilon : 0.5559240000000001\n",
      "episode : 11870, reward mean : 0.12988711663136587, total_step : 344219, cur_epsilon : 0.5557810000000001\n",
      "episode : 11880, reward mean : 0.12993519145302374, total_step : 344442, cur_epsilon : 0.555558\n",
      "episode : 11890, reward mean : 0.1297106875136457, total_step : 344787, cur_epsilon : 0.555213\n",
      "synced target net\n",
      "episode : 11900, reward mean : 0.1295798382006523, total_step : 345023, cur_epsilon : 0.554977\n",
      "[EVAL] episode : 11900, reward mean : 0.9400000013411045, step mean : 7.0,\n",
      "episode : 11910, reward mean : 0.12961041768598366, total_step : 345267, cur_epsilon : 0.554733\n",
      "episode : 11920, reward mean : 0.1302239995529475, total_step : 345416, cur_epsilon : 0.554584\n",
      "episode : 11930, reward mean : 0.13065801129129162, total_step : 345578, cur_epsilon : 0.554422\n",
      "episode : 11940, reward mean : 0.13078476338346226, total_step : 346005, cur_epsilon : 0.553995\n",
      "episode : 11950, reward mean : 0.131118834716225, total_step : 346285, cur_epsilon : 0.553715\n",
      "[EVAL] episode : 11950, reward mean : -0.20999999083578585, step mean : 42.0,\n",
      "episode : 11960, reward mean : 0.13142977215094262, total_step : 346592, cur_epsilon : 0.553408\n",
      "episode : 11970, reward mean : 0.13163910400972031, total_step : 346919, cur_epsilon : 0.553081\n",
      "episode : 11980, reward mean : 0.13194992279319812, total_step : 347225, cur_epsilon : 0.552775\n",
      "episode : 11990, reward mean : 0.1322335342046268, total_step : 347462, cur_epsilon : 0.552538\n",
      "episode : 12000, reward mean : 0.13264833959781874, total_step : 347741, cur_epsilon : 0.552259\n",
      "[EVAL] episode : 12000, reward mean : 0.6740000072866679, step mean : 33.6,\n",
      "episode : 12010, reward mean : 0.13253039760556967, total_step : 348059, cur_epsilon : 0.551941\n",
      "episode : 12020, reward mean : 0.13258070510079134, total_step : 348375, cur_epsilon : 0.551625\n",
      "episode : 12030, reward mean : 0.1330872880606134, total_step : 348643, cur_epsilon : 0.5513570000000001\n",
      "episode : 12040, reward mean : 0.13320764746003322, total_step : 348874, cur_epsilon : 0.551126\n",
      "episode : 12050, reward mean : 0.13304481954356323, total_step : 349246, cur_epsilon : 0.550754\n",
      "[EVAL] episode : 12050, reward mean : 0.4300000037997961, step mean : 18.0,\n",
      "episode : 12060, reward mean : 0.13317745236886872, total_step : 349563, cur_epsilon : 0.5504370000000001\n",
      "episode : 12070, reward mean : 0.13333803443677952, total_step : 349945, cur_epsilon : 0.550055\n",
      "synced target net\n",
      "episode : 12080, reward mean : 0.13337003937937839, total_step : 350183, cur_epsilon : 0.549817\n",
      "episode : 12090, reward mean : 0.13364434042457118, total_step : 350328, cur_epsilon : 0.549672\n",
      "episode : 12100, reward mean : 0.13380248560285396, total_step : 350613, cur_epsilon : 0.5493870000000001\n",
      "[EVAL] episode : 12100, reward mean : 0.6540000077337027, step mean : 35.6,\n",
      "episode : 12110, reward mean : 0.13408753723090686, total_step : 350944, cur_epsilon : 0.549056\n",
      "episode : 12120, reward mean : 0.13455858712312044, total_step : 351249, cur_epsilon : 0.548751\n",
      "episode : 12130, reward mean : 0.13477412003299002, total_step : 351562, cur_epsilon : 0.548438\n",
      "episode : 12140, reward mean : 0.13489127480003008, total_step : 351895, cur_epsilon : 0.5481050000000001\n",
      "episode : 12150, reward mean : 0.13498436840565362, total_step : 352156, cur_epsilon : 0.547844\n",
      "[EVAL] episode : 12150, reward mean : 0.17200001403689386, step mean : 63.6,\n",
      "episode : 12160, reward mean : 0.1347204010008442, total_step : 352352, cur_epsilon : 0.547648\n",
      "episode : 12170, reward mean : 0.13487757405353407, total_step : 352636, cur_epsilon : 0.547364\n",
      "episode : 12180, reward mean : 0.13534975995864773, total_step : 352936, cur_epsilon : 0.547064\n",
      "episode : 12190, reward mean : 0.13557260675467336, total_step : 353139, cur_epsilon : 0.546861\n",
      "episode : 12200, reward mean : 0.13594098986782988, total_step : 353364, cur_epsilon : 0.546636\n",
      "[EVAL] episode : 12200, reward mean : 0.3040000066161156, step mean : 30.6,\n",
      "episode : 12210, reward mean : 0.13586569013017596, total_step : 353829, cur_epsilon : 0.546171\n",
      "episode : 12220, reward mean : 0.13592144652481652, total_step : 354035, cur_epsilon : 0.545965\n",
      "episode : 12230, reward mean : 0.13607441346014096, total_step : 354421, cur_epsilon : 0.545579\n",
      "episode : 12240, reward mean : 0.13610376443369984, total_step : 354659, cur_epsilon : 0.5453410000000001\n",
      "episode : 12250, reward mean : 0.13614449606029963, total_step : 354982, cur_epsilon : 0.545018\n",
      "[EVAL] episode : 12250, reward mean : 0.48000000715255736, step mean : 32.8,\n",
      "synced target net\n",
      "episode : 12260, reward mean : 0.13651795079810075, total_step : 355198, cur_epsilon : 0.544802\n",
      "episode : 12270, reward mean : 0.1366112532062242, total_step : 355456, cur_epsilon : 0.544544\n",
      "episode : 12280, reward mean : 0.13680538085428845, total_step : 355691, cur_epsilon : 0.544309\n",
      "episode : 12290, reward mean : 0.13691538461689376, total_step : 355928, cur_epsilon : 0.544072\n",
      "episode : 12300, reward mean : 0.13717561601774722, total_step : 356281, cur_epsilon : 0.5437190000000001\n",
      "[EVAL] episode : 12300, reward mean : 0.4380000036209822, step mean : 17.2,\n",
      "episode : 12310, reward mean : 0.13719415735759016, total_step : 356531, cur_epsilon : 0.543469\n",
      "episode : 12320, reward mean : 0.13761283093383053, total_step : 356688, cur_epsilon : 0.543312\n",
      "episode : 12330, reward mean : 0.13764234202576306, total_step : 357023, cur_epsilon : 0.542977\n",
      "episode : 12340, reward mean : 0.13760130285274846, total_step : 357146, cur_epsilon : 0.542854\n",
      "episode : 12350, reward mean : 0.13773118034578286, total_step : 357458, cur_epsilon : 0.5425420000000001\n",
      "[EVAL] episode : 12350, reward mean : 0.6960000067949295, step mean : 31.4,\n",
      "episode : 12360, reward mean : 0.13800890593379617, total_step : 357787, cur_epsilon : 0.5422130000000001\n",
      "episode : 12370, reward mean : 0.1384478639748035, total_step : 357916, cur_epsilon : 0.542084\n",
      "episode : 12380, reward mean : 0.1386324779822616, total_step : 358159, cur_epsilon : 0.541841\n",
      "episode : 12390, reward mean : 0.13896368664055703, total_step : 358420, cur_epsilon : 0.54158\n",
      "episode : 12400, reward mean : 0.13920807076888458, total_step : 358687, cur_epsilon : 0.541313\n",
      "[EVAL] episode : 12400, reward mean : 0.8360000036656856, step mean : 17.4,\n",
      "episode : 12410, reward mean : 0.13941338256085525, total_step : 358903, cur_epsilon : 0.5410969999999999\n",
      "episode : 12420, reward mean : 0.13985829932762134, total_step : 359221, cur_epsilon : 0.540779\n",
      "episode : 12430, reward mean : 0.13972325645213346, total_step : 359459, cur_epsilon : 0.540541\n",
      "episode : 12440, reward mean : 0.13941560110565382, total_step : 359712, cur_epsilon : 0.5402880000000001\n",
      "episode : 12450, reward mean : 0.1395831387799727, total_step : 359974, cur_epsilon : 0.540026\n",
      "[EVAL] episode : 12450, reward mean : 0.46200000308454037, step mean : 14.8,\n",
      "synced target net\n",
      "episode : 12460, reward mean : 0.1397207125100527, total_step : 360273, cur_epsilon : 0.5397270000000001\n",
      "episode : 12470, reward mean : 0.1399767504335824, total_step : 360424, cur_epsilon : 0.5395760000000001\n",
      "episode : 12480, reward mean : 0.1400625062470611, total_step : 360686, cur_epsilon : 0.5393140000000001\n",
      "episode : 12490, reward mean : 0.1403546899925467, total_step : 360890, cur_epsilon : 0.53911\n",
      "episode : 12500, reward mean : 0.1407248062440753, total_step : 361097, cur_epsilon : 0.538903\n",
      "[EVAL] episode : 12500, reward mean : 0.7620000053197146, step mean : 24.8,\n",
      "episode : 12510, reward mean : 0.1406946505289629, total_step : 361404, cur_epsilon : 0.5385960000000001\n",
      "episode : 12520, reward mean : 0.1410415397897458, total_step : 361639, cur_epsilon : 0.5383610000000001\n",
      "episode : 12530, reward mean : 0.14121469099933828, total_step : 361891, cur_epsilon : 0.538109\n",
      "episode : 12540, reward mean : 0.14105582761398533, total_step : 362159, cur_epsilon : 0.537841\n",
      "episode : 12550, reward mean : 0.14160717755511582, total_step : 362336, cur_epsilon : 0.537664\n",
      "[EVAL] episode : 12550, reward mean : 0.04400000348687172, step mean : 16.6,\n",
      "episode : 12560, reward mean : 0.14187261770437856, total_step : 362570, cur_epsilon : 0.5374300000000001\n",
      "episode : 12570, reward mean : 0.14192681610436078, total_step : 362869, cur_epsilon : 0.537131\n",
      "episode : 12580, reward mean : 0.14229730353535217, total_step : 363071, cur_epsilon : 0.536929\n",
      "episode : 12590, reward mean : 0.1421016742289658, total_step : 363385, cur_epsilon : 0.5366150000000001\n",
      "episode : 12600, reward mean : 0.142636514173318, total_step : 363579, cur_epsilon : 0.536421\n",
      "[EVAL] episode : 12600, reward mean : 0.052000003308057784, step mean : 15.8,\n",
      "episode : 12610, reward mean : 0.14280809505453407, total_step : 363830, cur_epsilon : 0.53617\n",
      "episode : 12620, reward mean : 0.1432995307995759, total_step : 364077, cur_epsilon : 0.535923\n",
      "episode : 12630, reward mean : 0.1437482247593638, total_step : 364177, cur_epsilon : 0.535823\n",
      "episode : 12640, reward mean : 0.14387025939701478, total_step : 364489, cur_epsilon : 0.5355110000000001\n",
      "episode : 12650, reward mean : 0.14423795089493746, total_step : 364690, cur_epsilon : 0.53531\n",
      "[EVAL] episode : 12650, reward mean : 0.39400000460445883, step mean : 21.6,\n",
      "synced target net\n",
      "episode : 12660, reward mean : 0.14427646752682297, total_step : 365006, cur_epsilon : 0.534994\n",
      "episode : 12670, reward mean : 0.1443157126231747, total_step : 365222, cur_epsilon : 0.534778\n",
      "episode : 12680, reward mean : 0.144621457334032, total_step : 365500, cur_epsilon : 0.5345\n",
      "episode : 12690, reward mean : 0.1447769959852792, total_step : 365768, cur_epsilon : 0.534232\n",
      "episode : 12700, reward mean : 0.14527559677974916, total_step : 366000, cur_epsilon : 0.534\n",
      "[EVAL] episode : 12700, reward mean : 0.05600001215934754, step mean : 55.4,\n",
      "episode : 12710, reward mean : 0.1454830904144656, total_step : 366300, cur_epsilon : 0.5337000000000001\n",
      "episode : 12720, reward mean : 0.14573742761159514, total_step : 366540, cur_epsilon : 0.53346\n",
      "episode : 12730, reward mean : 0.1456009488857153, total_step : 366978, cur_epsilon : 0.533022\n",
      "episode : 12740, reward mean : 0.1453273217708422, total_step : 367191, cur_epsilon : 0.5328090000000001\n",
      "episode : 12750, reward mean : 0.14550039838532022, total_step : 367435, cur_epsilon : 0.532565\n",
      "[EVAL] episode : 12750, reward mean : 0.38600000478327273, step mean : 22.4,\n",
      "episode : 12760, reward mean : 0.14574452033441596, total_step : 367687, cur_epsilon : 0.532313\n",
      "episode : 12770, reward mean : 0.1459451902532307, total_step : 367994, cur_epsilon : 0.532006\n",
      "episode : 12780, reward mean : 0.1459968763396307, total_step : 368390, cur_epsilon : 0.53161\n",
      "episode : 12790, reward mean : 0.14585145267194094, total_step : 368640, cur_epsilon : 0.53136\n",
      "episode : 12800, reward mean : 0.14602578747857478, total_step : 368881, cur_epsilon : 0.531119\n",
      "[EVAL] episode : 12800, reward mean : 0.3960000045597553, step mean : 21.4,\n",
      "episode : 12810, reward mean : 0.14629508819282752, total_step : 369000, cur_epsilon : 0.531\n",
      "episode : 12820, reward mean : 0.14675585645935302, total_step : 369273, cur_epsilon : 0.530727\n",
      "episode : 12830, reward mean : 0.14698130006590512, total_step : 369447, cur_epsilon : 0.530553\n",
      "episode : 12840, reward mean : 0.14712539563126675, total_step : 369725, cur_epsilon : 0.530275\n",
      "episode : 12850, reward mean : 0.14738755485847418, total_step : 369851, cur_epsilon : 0.530149\n",
      "[EVAL] episode : 12850, reward mean : 0.08800000250339508, step mean : 12.2,\n",
      "synced target net\n",
      "episode : 12860, reward mean : 0.1474354650057609, total_step : 370052, cur_epsilon : 0.5299480000000001\n",
      "episode : 12870, reward mean : 0.14800233721845982, total_step : 370185, cur_epsilon : 0.529815\n",
      "episode : 12880, reward mean : 0.1483175527994226, total_step : 370441, cur_epsilon : 0.529559\n",
      "episode : 12890, reward mean : 0.14888363693435472, total_step : 370573, cur_epsilon : 0.5294270000000001\n",
      "episode : 12900, reward mean : 0.14895426978017817, total_step : 370943, cur_epsilon : 0.529057\n",
      "[EVAL] episode : 12900, reward mean : 0.8340000037103892, step mean : 17.6,\n",
      "episode : 12910, reward mean : 0.1489504322402416, total_step : 371209, cur_epsilon : 0.528791\n",
      "episode : 12920, reward mean : 0.14891951085809307, total_step : 371510, cur_epsilon : 0.52849\n",
      "episode : 12930, reward mean : 0.14897293738375453, total_step : 371902, cur_epsilon : 0.528098\n",
      "episode : 12940, reward mean : 0.14937249462147584, total_step : 372046, cur_epsilon : 0.527954\n",
      "episode : 12950, reward mean : 0.14976139617256792, total_step : 372203, cur_epsilon : 0.5277970000000001\n",
      "[EVAL] episode : 12950, reward mean : 0.7000000067055225, step mean : 31.0,\n",
      "episode : 12960, reward mean : 0.15018210497535306, total_step : 372417, cur_epsilon : 0.527583\n",
      "episode : 12970, reward mean : 0.15020740790681808, total_step : 372743, cur_epsilon : 0.5272570000000001\n",
      "episode : 12980, reward mean : 0.1500462311717505, total_step : 373012, cur_epsilon : 0.526988\n",
      "episode : 12990, reward mean : 0.15033872830405476, total_step : 373292, cur_epsilon : 0.526708\n",
      "episode : 13000, reward mean : 0.1506692369783727, total_step : 373522, cur_epsilon : 0.526478\n",
      "[EVAL] episode : 13000, reward mean : 0.8660000029951334, step mean : 14.4,\n",
      "episode : 13010, reward mean : 0.15087779252736225, total_step : 373809, cur_epsilon : 0.5261910000000001\n",
      "episode : 13020, reward mean : 0.15106605843522125, total_step : 374023, cur_epsilon : 0.525977\n",
      "episode : 13030, reward mean : 0.1513192694472665, total_step : 374352, cur_epsilon : 0.525648\n",
      "episode : 13040, reward mean : 0.1513182577418529, total_step : 374612, cur_epsilon : 0.525388\n",
      "episode : 13050, reward mean : 0.15155019777673528, total_step : 374768, cur_epsilon : 0.525232\n",
      "[EVAL] episode : 13050, reward mean : 0.3420000057667494, step mean : 26.8,\n",
      "episode : 13060, reward mean : 0.15175728032359737, total_step : 374956, cur_epsilon : 0.5250440000000001\n",
      "synced target net\n",
      "episode : 13070, reward mean : 0.15185693045885693, total_step : 375282, cur_epsilon : 0.524718\n",
      "episode : 13080, reward mean : 0.1520382325033962, total_step : 375503, cur_epsilon : 0.524497\n",
      "episode : 13090, reward mean : 0.15215279459384126, total_step : 375910, cur_epsilon : 0.52409\n",
      "episode : 13100, reward mean : 0.15227557872500241, total_step : 376207, cur_epsilon : 0.523793\n",
      "[EVAL] episode : 13100, reward mean : 0.2860000070184469, step mean : 32.4,\n",
      "episode : 13110, reward mean : 0.1523211351136196, total_step : 376405, cur_epsilon : 0.523595\n",
      "episode : 13120, reward mean : 0.15251067693475862, total_step : 376614, cur_epsilon : 0.523386\n",
      "episode : 13130, reward mean : 0.15276009759582687, total_step : 376843, cur_epsilon : 0.5231570000000001\n",
      "episode : 13140, reward mean : 0.15297336997832858, total_step : 377220, cur_epsilon : 0.52278\n",
      "episode : 13150, reward mean : 0.1534684472668477, total_step : 377426, cur_epsilon : 0.5225740000000001\n",
      "[EVAL] episode : 13150, reward mean : -0.003999995440244675, step mean : 21.4,\n",
      "episode : 13160, reward mean : 0.15363602444011504, total_step : 377761, cur_epsilon : 0.522239\n",
      "episode : 13170, reward mean : 0.15408277005995302, total_step : 378029, cur_epsilon : 0.521971\n",
      "episode : 13180, reward mean : 0.15426100772072043, total_step : 378349, cur_epsilon : 0.5216510000000001\n",
      "episode : 13190, reward mean : 0.15455497208787827, total_step : 378716, cur_epsilon : 0.5212840000000001\n",
      "episode : 13200, reward mean : 0.1550507637789247, total_step : 378917, cur_epsilon : 0.521083\n",
      "[EVAL] episode : 13200, reward mean : 0.48000000715255736, step mean : 32.8,\n",
      "episode : 13210, reward mean : 0.15513096759539946, total_step : 379165, cur_epsilon : 0.520835\n",
      "episode : 13220, reward mean : 0.1551452406965275, total_step : 379500, cur_epsilon : 0.5205\n",
      "episode : 13230, reward mean : 0.1554187514793159, total_step : 379793, cur_epsilon : 0.5202070000000001\n",
      "synced target net\n",
      "episode : 13240, reward mean : 0.15558761949552466, total_step : 380024, cur_epsilon : 0.519976\n",
      "episode : 13250, reward mean : 0.15580830808714877, total_step : 380186, cur_epsilon : 0.519814\n",
      "[EVAL] episode : 13250, reward mean : 0.07200000286102295, step mean : 13.8,\n",
      "episode : 13260, reward mean : 0.15604374677442903, total_step : 380528, cur_epsilon : 0.519472\n",
      "episode : 13270, reward mean : 0.15621100846276575, total_step : 380859, cur_epsilon : 0.5191410000000001\n",
      "episode : 13280, reward mean : 0.15618901222941453, total_step : 381342, cur_epsilon : 0.5186580000000001\n",
      "episode : 13290, reward mean : 0.15649285797293222, total_step : 381592, cur_epsilon : 0.518408\n",
      "episode : 13300, reward mean : 0.15699323928571846, total_step : 381780, cur_epsilon : 0.51822\n",
      "[EVAL] episode : 13300, reward mean : 0.3040000066161156, step mean : 30.6,\n",
      "episode : 13310, reward mean : 0.15732081762181244, total_step : 381997, cur_epsilon : 0.518003\n",
      "episode : 13320, reward mean : 0.15763363983627554, total_step : 382332, cur_epsilon : 0.517668\n",
      "episode : 13330, reward mean : 0.15791298444707727, total_step : 382612, cur_epsilon : 0.517388\n",
      "episode : 13340, reward mean : 0.1579197963067181, total_step : 382855, cur_epsilon : 0.517145\n",
      "episode : 13350, reward mean : 0.15833783391723583, total_step : 383149, cur_epsilon : 0.516851\n",
      "[EVAL] episode : 13350, reward mean : 0.8340000037103892, step mean : 17.6,\n",
      "episode : 13360, reward mean : 0.15863323973597745, total_step : 383505, cur_epsilon : 0.516495\n",
      "episode : 13370, reward mean : 0.15873448638135404, total_step : 383720, cur_epsilon : 0.5162800000000001\n",
      "episode : 13380, reward mean : 0.15883483430565584, total_step : 384136, cur_epsilon : 0.5158640000000001\n",
      "episode : 13390, reward mean : 0.15931591359624406, total_step : 384343, cur_epsilon : 0.515657\n",
      "episode : 13400, reward mean : 0.15975373754555833, total_step : 384607, cur_epsilon : 0.515393\n",
      "[EVAL] episode : 13400, reward mean : 0.42600000388920306, step mean : 18.4,\n",
      "episode : 13410, reward mean : 0.1600745774166607, total_step : 384827, cur_epsilon : 0.5151730000000001\n",
      "synced target net\n",
      "episode : 13420, reward mean : 0.16038227147601777, total_step : 385064, cur_epsilon : 0.5149360000000001\n",
      "episode : 13430, reward mean : 0.16072078058447575, total_step : 385259, cur_epsilon : 0.514741\n",
      "episode : 13440, reward mean : 0.16072322048379906, total_step : 385505, cur_epsilon : 0.514495\n",
      "episode : 13450, reward mean : 0.16090558240487896, total_step : 385709, cur_epsilon : 0.514291\n",
      "[EVAL] episode : 13450, reward mean : 0.33400000594556334, step mean : 27.6,\n",
      "episode : 13460, reward mean : 0.16122586057890886, total_step : 385927, cur_epsilon : 0.514073\n",
      "episode : 13470, reward mean : 0.16149963500184222, total_step : 386306, cur_epsilon : 0.5136940000000001\n",
      "episode : 13480, reward mean : 0.16156232073676471, total_step : 386569, cur_epsilon : 0.513431\n",
      "episode : 13490, reward mean : 0.16146108847984103, total_step : 386853, cur_epsilon : 0.513147\n",
      "episode : 13500, reward mean : 0.1616311173066497, total_step : 387072, cur_epsilon : 0.512928\n",
      "[EVAL] episode : 13500, reward mean : 0.8080000042915344, step mean : 20.2,\n",
      "episode : 13510, reward mean : 0.16164545401266267, total_step : 387400, cur_epsilon : 0.5126\n",
      "episode : 13520, reward mean : 0.16178624880095094, total_step : 387757, cur_epsilon : 0.512243\n",
      "episode : 13530, reward mean : 0.16172653982541074, total_step : 387985, cur_epsilon : 0.512015\n",
      "episode : 13540, reward mean : 0.16187519083388338, total_step : 388232, cur_epsilon : 0.511768\n",
      "episode : 13550, reward mean : 0.16207454494265944, total_step : 388610, cur_epsilon : 0.51139\n",
      "[EVAL] episode : 13550, reward mean : 0.6220000084489584, step mean : 38.8,\n",
      "episode : 13560, reward mean : 0.16215044867445147, total_step : 388854, cur_epsilon : 0.5111460000000001\n",
      "episode : 13570, reward mean : 0.16233088312960212, total_step : 389057, cur_epsilon : 0.510943\n",
      "episode : 13580, reward mean : 0.16216863653525185, total_step : 389424, cur_epsilon : 0.510576\n",
      "episode : 13590, reward mean : 0.16188889508607787, total_step : 389751, cur_epsilon : 0.510249\n",
      "episode : 13600, reward mean : 0.16223897678353003, total_step : 389923, cur_epsilon : 0.510077\n",
      "[EVAL] episode : 13600, reward mean : -0.013999995216727257, step mean : 22.4,\n",
      "synced target net\n",
      "episode : 13610, reward mean : 0.16239456901755142, total_step : 390258, cur_epsilon : 0.509742\n",
      "episode : 13620, reward mean : 0.16262849371185562, total_step : 390387, cur_epsilon : 0.5096130000000001\n",
      "episode : 13630, reward mean : 0.16295231727067372, total_step : 390593, cur_epsilon : 0.5094069999999999\n",
      "episode : 13640, reward mean : 0.1631209739329583, total_step : 390810, cur_epsilon : 0.50919\n",
      "episode : 13650, reward mean : 0.16333846772887878, total_step : 391059, cur_epsilon : 0.5089410000000001\n",
      "[EVAL] episode : 13650, reward mean : 0.8200000040233135, step mean : 19.0,\n",
      "episode : 13660, reward mean : 0.16340337368628313, total_step : 391316, cur_epsilon : 0.508684\n",
      "episode : 13670, reward mean : 0.1635888869524678, total_step : 391709, cur_epsilon : 0.508291\n",
      "episode : 13680, reward mean : 0.16363524011137476, total_step : 392090, cur_epsilon : 0.5079100000000001\n",
      "episode : 13690, reward mean : 0.16380643424169156, total_step : 392302, cur_epsilon : 0.507698\n",
      "episode : 13700, reward mean : 0.16411533465812894, total_step : 392525, cur_epsilon : 0.507475\n",
      "[EVAL] episode : 13700, reward mean : 0.4420000035315752, step mean : 16.8,\n",
      "episode : 13710, reward mean : 0.16434282165164715, total_step : 392659, cur_epsilon : 0.507341\n",
      "episode : 13720, reward mean : 0.16450437936529996, total_step : 392883, cur_epsilon : 0.507117\n",
      "episode : 13730, reward mean : 0.16445958375352865, total_step : 393089, cur_epsilon : 0.506911\n",
      "episode : 13740, reward mean : 0.16448253893606593, total_step : 393303, cur_epsilon : 0.506697\n",
      "episode : 13750, reward mean : 0.1642661880008199, total_step : 393446, cur_epsilon : 0.506554\n",
      "[EVAL] episode : 13750, reward mean : 0.7260000061243772, step mean : 28.4,\n",
      "episode : 13760, reward mean : 0.1642340178073514, total_step : 393536, cur_epsilon : 0.506464\n",
      "episode : 13770, reward mean : 0.16434205410989866, total_step : 393833, cur_epsilon : 0.506167\n",
      "episode : 13780, reward mean : 0.16441364914141002, total_step : 394180, cur_epsilon : 0.50582\n",
      "episode : 13790, reward mean : 0.16456418311962231, total_step : 394418, cur_epsilon : 0.505582\n",
      "episode : 13800, reward mean : 0.1649427598019711, total_step : 394640, cur_epsilon : 0.50536\n",
      "[EVAL] episode : 13800, reward mean : 0.7300000060349703, step mean : 28.0,\n",
      "episode : 13810, reward mean : 0.16507748626523222, total_step : 394899, cur_epsilon : 0.505101\n",
      "synced target net\n",
      "episode : 13820, reward mean : 0.1651657080570271, total_step : 395022, cur_epsilon : 0.504978\n",
      "episode : 13830, reward mean : 0.16505640530817356, total_step : 395317, cur_epsilon : 0.504683\n",
      "episode : 13840, reward mean : 0.16534827207107464, total_step : 395558, cur_epsilon : 0.5044420000000001\n",
      "episode : 13850, reward mean : 0.16560289426185595, total_step : 395850, cur_epsilon : 0.5041500000000001\n",
      "[EVAL] episode : 13850, reward mean : 0.6120000086724758, step mean : 39.8,\n",
      "episode : 13860, reward mean : 0.16524459492391, total_step : 396389, cur_epsilon : 0.503611\n",
      "episode : 13870, reward mean : 0.16557174374086872, total_step : 396580, cur_epsilon : 0.50342\n",
      "episode : 13880, reward mean : 0.16573991972123747, total_step : 396791, cur_epsilon : 0.503209\n",
      "episode : 13890, reward mean : 0.16594529055498802, total_step : 397148, cur_epsilon : 0.5028520000000001\n",
      "episode : 13900, reward mean : 0.16593309970250983, total_step : 397409, cur_epsilon : 0.502591\n",
      "[EVAL] episode : 13900, reward mean : 0.5080000065267086, step mean : 30.0,\n",
      "episode : 13910, reward mean : 0.16592164528706813, total_step : 397768, cur_epsilon : 0.502232\n",
      "episode : 13920, reward mean : 0.1663218452589822, total_step : 398055, cur_epsilon : 0.5019450000000001\n",
      "episode : 13930, reward mean : 0.16643001335737886, total_step : 398348, cur_epsilon : 0.501652\n",
      "episode : 13940, reward mean : 0.16675682109814013, total_step : 398536, cur_epsilon : 0.501464\n",
      "episode : 13950, reward mean : 0.16679427140849679, total_step : 398727, cur_epsilon : 0.5012730000000001\n",
      "[EVAL] episode : 13950, reward mean : 0.6340000081807375, step mean : 37.6,\n",
      "episode : 13960, reward mean : 0.16728152479818972, total_step : 398890, cur_epsilon : 0.5011099999999999\n",
      "episode : 13970, reward mean : 0.1674452459735632, total_step : 399203, cur_epsilon : 0.500797\n",
      "episode : 13980, reward mean : 0.16790701618687234, total_step : 399400, cur_epsilon : 0.5006\n",
      "episode : 13990, reward mean : 0.16823231496297628, total_step : 399587, cur_epsilon : 0.500413\n",
      "episode : 14000, reward mean : 0.16837000617198647, total_step : 399935, cur_epsilon : 0.500065\n",
      "[EVAL] episode : 14000, reward mean : 0.2620000075548887, step mean : 34.8,\n",
      "synced target net\n",
      "episode : 14010, reward mean : 0.16853176919726134, total_step : 400150, cur_epsilon : 0.49985\n",
      "episode : 14020, reward mean : 0.16884879361593647, total_step : 400347, cur_epsilon : 0.499653\n",
      "episode : 14030, reward mean : 0.16886173104528798, total_step : 400669, cur_epsilon : 0.499331\n",
      "episode : 14040, reward mean : 0.16877279819249363, total_step : 400934, cur_epsilon : 0.499066\n",
      "episode : 14050, reward mean : 0.16886335136587616, total_step : 401248, cur_epsilon : 0.49875200000000003\n",
      "[EVAL] episode : 14050, reward mean : 0.8020000044256449, step mean : 20.8,\n",
      "episode : 14060, reward mean : 0.1691074030408914, total_step : 401546, cur_epsilon : 0.498454\n",
      "episode : 14070, reward mean : 0.16925942336911418, total_step : 401773, cur_epsilon : 0.49822700000000003\n",
      "episode : 14080, reward mean : 0.16920171071542428, total_step : 402095, cur_epsilon : 0.49790500000000004\n",
      "episode : 14090, reward mean : 0.16899929644580933, total_step : 402320, cur_epsilon : 0.49768\n",
      "episode : 14100, reward mean : 0.1692815664520236, total_step : 402563, cur_epsilon : 0.497437\n",
      "[EVAL] episode : 14100, reward mean : 0.6820000071078539, step mean : 32.8,\n",
      "episode : 14110, reward mean : 0.16929908483483896, total_step : 402779, cur_epsilon : 0.497221\n",
      "episode : 14120, reward mean : 0.16942210248579131, total_step : 403145, cur_epsilon : 0.49685500000000005\n",
      "episode : 14130, reward mean : 0.16959023971442774, total_step : 403447, cur_epsilon : 0.496553\n",
      "episode : 14140, reward mean : 0.17004880390421104, total_step : 403639, cur_epsilon : 0.496361\n",
      "episode : 14150, reward mean : 0.17041131358527883, total_step : 403766, cur_epsilon : 0.496234\n",
      "[EVAL] episode : 14150, reward mean : 0.24400000795722007, step mean : 36.6,\n",
      "episode : 14160, reward mean : 0.1707591869535535, total_step : 403913, cur_epsilon : 0.496087\n",
      "episode : 14170, reward mean : 0.171115743635313, total_step : 404146, cur_epsilon : 0.495854\n",
      "episode : 14180, reward mean : 0.17111213592292243, total_step : 404489, cur_epsilon : 0.49551100000000003\n",
      "episode : 14190, reward mean : 0.17126639093967433, total_step : 404709, cur_epsilon : 0.49529100000000004\n",
      "synced target net\n",
      "episode : 14200, reward mean : 0.17128099207766354, total_step : 405026, cur_epsilon : 0.494974\n",
      "[EVAL] episode : 14200, reward mean : 0.29000000692903993, step mean : 32.0,\n",
      "episode : 14210, reward mean : 0.1716094361424163, total_step : 405398, cur_epsilon : 0.49460200000000004\n",
      "episode : 14220, reward mean : 0.17157314259116177, total_step : 405688, cur_epsilon : 0.49431200000000003\n",
      "episode : 14230, reward mean : 0.17157203708346658, total_step : 405928, cur_epsilon : 0.494072\n",
      "episode : 14240, reward mean : 0.17143750616200162, total_step : 406158, cur_epsilon : 0.493842\n",
      "episode : 14250, reward mean : 0.17172702370492512, total_step : 406384, cur_epsilon : 0.493616\n",
      "[EVAL] episode : 14250, reward mean : 0.7360000059008598, step mean : 27.4,\n",
      "episode : 14260, reward mean : 0.1717195012532227, total_step : 406732, cur_epsilon : 0.49326800000000004\n",
      "episode : 14270, reward mean : 0.1717246032195308, total_step : 407062, cur_epsilon : 0.49293800000000004\n",
      "episode : 14280, reward mean : 0.17192087451280394, total_step : 407519, cur_epsilon : 0.492481\n",
      "episode : 14290, reward mean : 0.1721679557821315, total_step : 407903, cur_epsilon : 0.492097\n",
      "episode : 14300, reward mean : 0.17246364252954408, total_step : 408118, cur_epsilon : 0.49188200000000004\n",
      "[EVAL] episode : 14300, reward mean : -0.45199999436736105, step mean : 26.2,\n",
      "episode : 14310, reward mean : 0.17249057220384884, total_step : 408416, cur_epsilon : 0.491584\n",
      "episode : 14320, reward mean : 0.17272137488152664, total_step : 408723, cur_epsilon : 0.491277\n",
      "episode : 14330, reward mean : 0.17294766841390002, total_step : 409036, cur_epsilon : 0.490964\n",
      "episode : 14340, reward mean : 0.17294352081054862, total_step : 409279, cur_epsilon : 0.490721\n",
      "episode : 14350, reward mean : 0.1733261385704115, total_step : 409567, cur_epsilon : 0.490433\n",
      "[EVAL] episode : 14350, reward mean : -0.15599999204277992, step mean : 36.6,\n",
      "episode : 14360, reward mean : 0.17345891981463474, total_step : 409813, cur_epsilon : 0.49018700000000004\n",
      "synced target net\n",
      "episode : 14370, reward mean : 0.17358873267862207, total_step : 410063, cur_epsilon : 0.489937\n",
      "episode : 14380, reward mean : 0.17390682118425407, total_step : 410242, cur_epsilon : 0.489758\n",
      "episode : 14390, reward mean : 0.17380959615629113, total_step : 410517, cur_epsilon : 0.489483\n",
      "episode : 14400, reward mean : 0.1741541728276449, total_step : 410657, cur_epsilon : 0.48934300000000003\n",
      "[EVAL] episode : 14400, reward mean : 0.5720000095665455, step mean : 43.8,\n",
      "episode : 14410, reward mean : 0.17450174106805896, total_step : 410992, cur_epsilon : 0.489008\n",
      "episode : 14420, reward mean : 0.17471845276426426, total_step : 411315, cur_epsilon : 0.48868500000000004\n",
      "episode : 14430, reward mean : 0.1746479618110728, total_step : 411652, cur_epsilon : 0.488348\n",
      "episode : 14440, reward mean : 0.1749432194583283, total_step : 411861, cur_epsilon : 0.48813900000000005\n",
      "episode : 14450, reward mean : 0.17509966013998204, total_step : 412070, cur_epsilon : 0.48793000000000003\n",
      "[EVAL] episode : 14450, reward mean : 0.2880000114440918, step mean : 52.0,\n",
      "episode : 14460, reward mean : 0.17539350546801136, total_step : 412280, cur_epsilon : 0.48772000000000004\n",
      "episode : 14470, reward mean : 0.17579682716802433, total_step : 412531, cur_epsilon : 0.48746900000000004\n",
      "episode : 14480, reward mean : 0.1761229343370159, total_step : 412893, cur_epsilon : 0.487107\n",
      "episode : 14490, reward mean : 0.17641684535847113, total_step : 413101, cur_epsilon : 0.486899\n",
      "episode : 14500, reward mean : 0.17642966133189097, total_step : 413415, cur_epsilon : 0.48658500000000005\n",
      "[EVAL] episode : 14500, reward mean : -0.10799998864531517, step mean : 51.6,\n",
      "episode : 14510, reward mean : 0.17676017156062065, total_step : 413569, cur_epsilon : 0.486431\n",
      "episode : 14520, reward mean : 0.17696074995997338, total_step : 413911, cur_epsilon : 0.48608900000000005\n",
      "episode : 14530, reward mean : 0.1773737157239269, total_step : 414144, cur_epsilon : 0.485856\n",
      "episode : 14540, reward mean : 0.17776066640486743, total_step : 414414, cur_epsilon : 0.485586\n",
      "episode : 14550, reward mean : 0.17813952505754954, total_step : 414695, cur_epsilon : 0.48530500000000004\n",
      "[EVAL] episode : 14550, reward mean : 0.026000003889203073, step mean : 18.4,\n",
      "episode : 14560, reward mean : 0.17809822044302046, total_step : 414987, cur_epsilon : 0.485013\n",
      "synced target net\n",
      "episode : 14570, reward mean : 0.17831640972672114, total_step : 415301, cur_epsilon : 0.48469900000000005\n",
      "episode : 14580, reward mean : 0.1786001433339864, total_step : 415719, cur_epsilon : 0.484281\n",
      "episode : 14590, reward mean : 0.17874435160086938, total_step : 415940, cur_epsilon : 0.48406000000000005\n",
      "episode : 14600, reward mean : 0.17911712944647934, total_step : 416227, cur_epsilon : 0.483773\n",
      "[EVAL] episode : 14600, reward mean : 0.6680000074207783, step mean : 34.2,\n",
      "episode : 14610, reward mean : 0.17908830184650126, total_step : 416500, cur_epsilon : 0.48350000000000004\n",
      "episode : 14620, reward mean : 0.1791176532159487, total_step : 416688, cur_epsilon : 0.483312\n",
      "episode : 14630, reward mean : 0.1794572857188816, total_step : 416921, cur_epsilon : 0.48307900000000004\n",
      "episode : 14640, reward mean : 0.17937842145628333, total_step : 417166, cur_epsilon : 0.48283400000000004\n",
      "episode : 14650, reward mean : 0.17951126895514363, total_step : 417501, cur_epsilon : 0.482499\n",
      "[EVAL] episode : 14650, reward mean : 0.7460000056773424, step mean : 26.4,\n",
      "episode : 14660, reward mean : 0.1797735395801913, total_step : 417747, cur_epsilon : 0.48225300000000004\n",
      "episode : 14670, reward mean : 0.18011929722516795, total_step : 417969, cur_epsilon : 0.48203100000000004\n",
      "episode : 14680, reward mean : 0.18036649116982353, total_step : 418335, cur_epsilon : 0.481665\n",
      "episode : 14690, reward mean : 0.1806725725262769, total_step : 418515, cur_epsilon : 0.481485\n",
      "episode : 14700, reward mean : 0.18091157078185455, total_step : 418892, cur_epsilon : 0.48110800000000004\n",
      "[EVAL] episode : 14700, reward mean : 0.6500000078231096, step mean : 36.0,\n",
      "episode : 14710, reward mean : 0.18110605646279354, total_step : 419233, cur_epsilon : 0.480767\n",
      "episode : 14720, reward mean : 0.18105910941793926, total_step : 419531, cur_epsilon : 0.48046900000000003\n",
      "episode : 14730, reward mean : 0.18135981606730517, total_step : 419717, cur_epsilon : 0.480283\n",
      "synced target net\n",
      "episode : 14740, reward mean : 0.181710996657984, total_step : 420028, cur_epsilon : 0.479972\n",
      "episode : 14750, reward mean : 0.18213966717154292, total_step : 420224, cur_epsilon : 0.47977600000000004\n",
      "[EVAL] episode : 14750, reward mean : 0.4020000044256449, step mean : 20.8,\n",
      "episode : 14760, reward mean : 0.18253388149275124, total_step : 420470, cur_epsilon : 0.47953\n",
      "episode : 14770, reward mean : 0.18269262632863317, total_step : 420663, cur_epsilon : 0.479337\n",
      "episode : 14780, reward mean : 0.1828315352450134, total_step : 420885, cur_epsilon : 0.479115\n",
      "episode : 14790, reward mean : 0.18294050649093102, total_step : 421250, cur_epsilon : 0.47875\n",
      "episode : 14800, reward mean : 0.18311351966465245, total_step : 421421, cur_epsilon : 0.47857900000000003\n",
      "[EVAL] episode : 14800, reward mean : 0.7660000052303075, step mean : 24.4,\n",
      "episode : 14810, reward mean : 0.18320392242555458, total_step : 421813, cur_epsilon : 0.47818700000000003\n",
      "episode : 14820, reward mean : 0.18327868361611738, total_step : 422129, cur_epsilon : 0.47787100000000005\n",
      "episode : 14830, reward mean : 0.18346258201400875, total_step : 422483, cur_epsilon : 0.477517\n",
      "episode : 14840, reward mean : 0.18348181208286865, total_step : 422681, cur_epsilon : 0.47731900000000005\n",
      "episode : 14850, reward mean : 0.18354142029515721, total_step : 423019, cur_epsilon : 0.47698100000000004\n",
      "[EVAL] episode : 14850, reward mean : 0.43600000366568564, step mean : 17.4,\n",
      "episode : 14860, reward mean : 0.1837819711617313, total_step : 423387, cur_epsilon : 0.476613\n",
      "episode : 14870, reward mean : 0.18401749102372447, total_step : 423663, cur_epsilon : 0.476337\n",
      "episode : 14880, reward mean : 0.1840510814220354, total_step : 423839, cur_epsilon : 0.476161\n",
      "episode : 14890, reward mean : 0.18437206793909228, total_step : 424086, cur_epsilon : 0.475914\n",
      "episode : 14900, reward mean : 0.18462886521242708, total_step : 424329, cur_epsilon : 0.475671\n",
      "[EVAL] episode : 14900, reward mean : 0.8220000039786101, step mean : 18.8,\n",
      "episode : 14910, reward mean : 0.18472032808367037, total_step : 424618, cur_epsilon : 0.475382\n",
      "episode : 14920, reward mean : 0.18499330373820486, total_step : 424836, cur_epsilon : 0.47516400000000003\n",
      "synced target net\n",
      "episode : 14930, reward mean : 0.18512592711470327, total_step : 425063, cur_epsilon : 0.474937\n",
      "episode : 14940, reward mean : 0.18526774376925015, total_step : 425474, cur_epsilon : 0.474526\n",
      "episode : 14950, reward mean : 0.1852675646796424, total_step : 425699, cur_epsilon : 0.47430100000000003\n",
      "[EVAL] episode : 14950, reward mean : 0.8620000030845404, step mean : 14.8,\n",
      "episode : 14960, reward mean : 0.18551337513475016, total_step : 425956, cur_epsilon : 0.474044\n",
      "episode : 14970, reward mean : 0.1856840408851972, total_step : 426125, cur_epsilon : 0.47387500000000005\n",
      "episode : 14980, reward mean : 0.18598131455869, total_step : 426304, cur_epsilon : 0.473696\n",
      "episode : 14990, reward mean : 0.18612408886966186, total_step : 426613, cur_epsilon : 0.473387\n",
      "episode : 15000, reward mean : 0.18642000614777207, total_step : 426892, cur_epsilon : 0.47310800000000003\n",
      "[EVAL] episode : 15000, reward mean : 0.20400000885128974, step mean : 40.6,\n",
      "episode : 15010, reward mean : 0.18632512273494004, total_step : 427058, cur_epsilon : 0.47294200000000003\n",
      "episode : 15020, reward mean : 0.18669241626564656, total_step : 427330, cur_epsilon : 0.47267000000000003\n",
      "episode : 15030, reward mean : 0.18701863555139747, total_step : 427463, cur_epsilon : 0.47253700000000004\n",
      "episode : 15040, reward mean : 0.1871163625263936, total_step : 427739, cur_epsilon : 0.47226100000000004\n",
      "episode : 15050, reward mean : 0.1870850559785665, total_step : 428108, cur_epsilon : 0.47189200000000003\n",
      "[EVAL] episode : 15050, reward mean : 0.35200000554323196, step mean : 25.8,\n",
      "episode : 15060, reward mean : 0.1873293554138239, total_step : 428363, cur_epsilon : 0.47163700000000003\n",
      "episode : 15070, reward mean : 0.18762840693884522, total_step : 428535, cur_epsilon : 0.471465\n",
      "episode : 15080, reward mean : 0.18776525813241338, total_step : 428850, cur_epsilon : 0.47115\n",
      "episode : 15090, reward mean : 0.1881868848691937, total_step : 429036, cur_epsilon : 0.47096400000000005\n",
      "episode : 15100, reward mean : 0.18803775448685153, total_step : 429382, cur_epsilon : 0.47061800000000004\n",
      "[EVAL] episode : 15100, reward mean : 0.8500000033527613, step mean : 16.0,\n",
      "episode : 15110, reward mean : 0.18818597569778156, total_step : 429580, cur_epsilon : 0.47042\n",
      "episode : 15120, reward mean : 0.18843519132579406, total_step : 429825, cur_epsilon : 0.470175\n",
      "synced target net\n",
      "episode : 15130, reward mean : 0.18847720376231264, total_step : 430181, cur_epsilon : 0.46981900000000004\n",
      "episode : 15140, reward mean : 0.1888553562072134, total_step : 430430, cur_epsilon : 0.46957000000000004\n",
      "episode : 15150, reward mean : 0.18911419755948258, total_step : 430659, cur_epsilon : 0.469341\n",
      "[EVAL] episode : 15150, reward mean : 0.7660000052303075, step mean : 24.4,\n",
      "episode : 15160, reward mean : 0.18941755231265914, total_step : 430820, cur_epsilon : 0.46918000000000004\n",
      "episode : 15170, reward mean : 0.18957548405393104, total_step : 431001, cur_epsilon : 0.468999\n",
      "episode : 15180, reward mean : 0.18973320771649244, total_step : 431182, cur_epsilon : 0.468818\n",
      "episode : 15190, reward mean : 0.1897972411605443, total_step : 431604, cur_epsilon : 0.46839600000000003\n",
      "episode : 15200, reward mean : 0.18988158508498026, total_step : 431896, cur_epsilon : 0.468104\n",
      "[EVAL] episode : 15200, reward mean : 0.542000001296401, step mean : 6.8,\n",
      "episode : 15210, reward mean : 0.18992111067227005, total_step : 432056, cur_epsilon : 0.467944\n",
      "episode : 15220, reward mean : 0.1901984292619743, total_step : 432254, cur_epsilon : 0.467746\n",
      "episode : 15230, reward mean : 0.19044124053978287, total_step : 432504, cur_epsilon : 0.467496\n",
      "episode : 15240, reward mean : 0.19064895626551073, total_step : 432807, cur_epsilon : 0.467193\n",
      "episode : 15250, reward mean : 0.19086033400331365, total_step : 433104, cur_epsilon : 0.46689600000000003\n",
      "[EVAL] episode : 15250, reward mean : 0.1140000019222498, step mean : 9.6,\n",
      "episode : 15260, reward mean : 0.19114876104772657, total_step : 433283, cur_epsilon : 0.46671700000000005\n",
      "episode : 15270, reward mean : 0.191444668868592, total_step : 433450, cur_epsilon : 0.46655\n",
      "episode : 15280, reward mean : 0.1917166291666583, total_step : 433653, cur_epsilon : 0.466347\n",
      "episode : 15290, reward mean : 0.191687383500529, total_step : 433916, cur_epsilon : 0.466084\n",
      "episode : 15300, reward mean : 0.191675169529162, total_step : 434252, cur_epsilon : 0.465748\n",
      "[EVAL] episode : 15300, reward mean : 0.3360000059008598, step mean : 27.4,\n",
      "episode : 15310, reward mean : 0.19193926151807814, total_step : 434466, cur_epsilon : 0.465534\n",
      "episode : 15320, reward mean : 0.19228982336177217, total_step : 434747, cur_epsilon : 0.465253\n",
      "synced target net\n",
      "episode : 15330, reward mean : 0.19246249797645243, total_step : 435100, cur_epsilon : 0.46490000000000004\n",
      "episode : 15340, reward mean : 0.19249153155396356, total_step : 435372, cur_epsilon : 0.46462800000000004\n",
      "episode : 15350, reward mean : 0.19285082046221638, total_step : 435638, cur_epsilon : 0.464362\n",
      "[EVAL] episode : 15350, reward mean : -0.1179999928921461, step mean : 32.8,\n",
      "episode : 15360, reward mean : 0.19288151654642813, total_step : 435907, cur_epsilon : 0.46409300000000003\n",
      "episode : 15370, reward mean : 0.19315420261516963, total_step : 436105, cur_epsilon : 0.463895\n",
      "episode : 15380, reward mean : 0.1934180815500705, total_step : 436316, cur_epsilon : 0.46368400000000004\n",
      "episode : 15390, reward mean : 0.19363613348282374, total_step : 436597, cur_epsilon : 0.463403\n",
      "episode : 15400, reward mean : 0.1938564996337881, total_step : 436874, cur_epsilon : 0.46312600000000004\n",
      "[EVAL] episode : 15400, reward mean : 0.5340000104159117, step mean : 47.6,\n",
      "episode : 15410, reward mean : 0.19395003857358198, total_step : 437146, cur_epsilon : 0.46285400000000004\n",
      "episode : 15420, reward mean : 0.1943216663081049, total_step : 437389, cur_epsilon : 0.46261100000000005\n",
      "episode : 15430, reward mean : 0.194405709302182, total_step : 437675, cur_epsilon : 0.46232500000000004\n",
      "episode : 15440, reward mean : 0.19458096467520694, total_step : 437919, cur_epsilon : 0.462081\n",
      "episode : 15450, reward mean : 0.1948368993289407, total_step : 438139, cur_epsilon : 0.461861\n",
      "[EVAL] episode : 15450, reward mean : 0.8460000034421682, step mean : 16.4,\n",
      "episode : 15460, reward mean : 0.19523998025050543, total_step : 438331, cur_epsilon : 0.461669\n",
      "episode : 15470, reward mean : 0.19558630218053913, total_step : 438610, cur_epsilon : 0.46139\n",
      "episode : 15480, reward mean : 0.19565956684452715, total_step : 438711, cur_epsilon : 0.461289\n",
      "episode : 15490, reward mean : 0.196076829876604, total_step : 438879, cur_epsilon : 0.461121\n",
      "episode : 15500, reward mean : 0.19638387708498106, total_step : 439017, cur_epsilon : 0.46098300000000003\n",
      "[EVAL] episode : 15500, reward mean : 0.640000008046627, step mean : 37.0,\n",
      "episode : 15510, reward mean : 0.19637976111453878, total_step : 439336, cur_epsilon : 0.460664\n",
      "episode : 15520, reward mean : 0.19640077931215968, total_step : 439517, cur_epsilon : 0.46048300000000003\n",
      "episode : 15530, reward mean : 0.19638313554402462, total_step : 439857, cur_epsilon : 0.460143\n",
      "synced target net\n",
      "episode : 15540, reward mean : 0.19661133172807968, total_step : 440116, cur_epsilon : 0.459884\n",
      "episode : 15550, reward mean : 0.19670418618085203, total_step : 440385, cur_epsilon : 0.459615\n",
      "[EVAL] episode : 15550, reward mean : 0.8300000037997961, step mean : 18.0,\n",
      "episode : 15560, reward mean : 0.19695309094862012, total_step : 440611, cur_epsilon : 0.45938900000000005\n",
      "episode : 15570, reward mean : 0.19726525980933746, total_step : 440938, cur_epsilon : 0.459062\n",
      "episode : 15580, reward mean : 0.19718742588456045, total_step : 441171, cur_epsilon : 0.45882900000000004\n",
      "episode : 15590, reward mean : 0.19731110297167354, total_step : 441391, cur_epsilon : 0.45860900000000004\n",
      "episode : 15600, reward mean : 0.19715833944901345, total_step : 441741, cur_epsilon : 0.458259\n",
      "[EVAL] episode : 15600, reward mean : 0.7340000059455634, step mean : 27.6,\n",
      "episode : 15610, reward mean : 0.19723895550714443, total_step : 442028, cur_epsilon : 0.45797200000000005\n",
      "episode : 15620, reward mean : 0.19757234926565662, total_step : 442320, cur_epsilon : 0.45768000000000003\n",
      "episode : 15630, reward mean : 0.19786436951916453, total_step : 442575, cur_epsilon : 0.457425\n",
      "episode : 15640, reward mean : 0.19825448181761232, total_step : 442777, cur_epsilon : 0.45722300000000005\n",
      "episode : 15650, reward mean : 0.19846326490025074, total_step : 443062, cur_epsilon : 0.456938\n",
      "[EVAL] episode : 15650, reward mean : 0.5840000092983246, step mean : 42.6,\n",
      "episode : 15660, reward mean : 0.1986992398301903, total_step : 443304, cur_epsilon : 0.45669600000000005\n",
      "episode : 15670, reward mean : 0.19884557088571955, total_step : 443486, cur_epsilon : 0.45651400000000003\n",
      "episode : 15680, reward mean : 0.19906314386860277, total_step : 443855, cur_epsilon : 0.456145\n",
      "episode : 15690, reward mean : 0.1991841998665936, total_step : 444076, cur_epsilon : 0.455924\n",
      "episode : 15700, reward mean : 0.19957962394567433, total_step : 444266, cur_epsilon : 0.45573400000000003\n",
      "[EVAL] episode : 15700, reward mean : 0.3680000096559525, step mean : 44.0,\n",
      "episode : 15710, reward mean : 0.1994723167399657, total_step : 444445, cur_epsilon : 0.45555500000000004\n",
      "episode : 15720, reward mean : 0.1997360111976104, total_step : 444641, cur_epsilon : 0.455359\n",
      "episode : 15730, reward mean : 0.20004132842326516, total_step : 444971, cur_epsilon : 0.455029\n",
      "synced target net\n",
      "episode : 15740, reward mean : 0.20028018399923944, total_step : 445205, cur_epsilon : 0.454795\n",
      "episode : 15750, reward mean : 0.20042349817201732, total_step : 445488, cur_epsilon : 0.454512\n",
      "[EVAL] episode : 15750, reward mean : 0.43600000366568564, step mean : 17.4,\n",
      "episode : 15760, reward mean : 0.20047145280975207, total_step : 445822, cur_epsilon : 0.454178\n",
      "episode : 15770, reward mean : 0.20034686723687392, total_step : 446028, cur_epsilon : 0.45397200000000004\n",
      "episode : 15780, reward mean : 0.20049620382664832, total_step : 446301, cur_epsilon : 0.453699\n",
      "episode : 15790, reward mean : 0.20062255202211615, total_step : 446511, cur_epsilon : 0.45348900000000003\n",
      "episode : 15800, reward mean : 0.20101013268803872, total_step : 446708, cur_epsilon : 0.45329200000000003\n",
      "[EVAL] episode : 15800, reward mean : 0.8860000025480985, step mean : 12.4,\n",
      "episode : 15810, reward mean : 0.20101708390483142, total_step : 447005, cur_epsilon : 0.45299500000000004\n",
      "episode : 15820, reward mean : 0.20120417804065174, total_step : 447318, cur_epsilon : 0.45268200000000003\n",
      "episode : 15830, reward mean : 0.20134492082390712, total_step : 447504, cur_epsilon : 0.452496\n",
      "episode : 15840, reward mean : 0.20144823842771314, total_step : 447749, cur_epsilon : 0.452251\n",
      "episode : 15850, reward mean : 0.20180883891140156, total_step : 447986, cur_epsilon : 0.452014\n",
      "[EVAL] episode : 15850, reward mean : 0.4100000042468309, step mean : 20.0,\n",
      "episode : 15860, reward mean : 0.2018650754612094, total_step : 448305, cur_epsilon : 0.451695\n",
      "episode : 15870, reward mean : 0.20200441694385932, total_step : 448692, cur_epsilon : 0.45130800000000004\n",
      "episode : 15880, reward mean : 0.2021574368346154, total_step : 448857, cur_epsilon : 0.451143\n",
      "episode : 15890, reward mean : 0.202163631024274, total_step : 449055, cur_epsilon : 0.45094500000000004\n",
      "episode : 15900, reward mean : 0.20225786773780793, total_step : 449313, cur_epsilon : 0.450687\n",
      "[EVAL] episode : 15900, reward mean : 0.3140000108629465, step mean : 49.4,\n",
      "episode : 15910, reward mean : 0.20249403501450453, total_step : 449545, cur_epsilon : 0.45045500000000005\n",
      "episode : 15920, reward mean : 0.20265955384282497, total_step : 449988, cur_epsilon : 0.450012\n",
      "synced target net\n",
      "episode : 15930, reward mean : 0.20284746373159962, total_step : 450296, cur_epsilon : 0.44970400000000005\n",
      "episode : 15940, reward mean : 0.20298306758374143, total_step : 450487, cur_epsilon : 0.449513\n",
      "episode : 15950, reward mean : 0.20300063306097887, total_step : 450666, cur_epsilon : 0.449334\n",
      "[EVAL] episode : 15950, reward mean : 0.7300000060349703, step mean : 28.0,\n",
      "episode : 15960, reward mean : 0.20311466775645495, total_step : 450990, cur_epsilon : 0.44901\n",
      "episode : 15970, reward mean : 0.20351096414713074, total_step : 451164, cur_epsilon : 0.448836\n",
      "episode : 15980, reward mean : 0.20364268444751266, total_step : 451360, cur_epsilon : 0.44864000000000004\n",
      "episode : 15990, reward mean : 0.20371482786329334, total_step : 451651, cur_epsilon : 0.448349\n",
      "episode : 16000, reward mean : 0.20382375610037706, total_step : 451982, cur_epsilon : 0.448018\n",
      "[EVAL] episode : 16000, reward mean : 0.872000002861023, step mean : 13.8,\n",
      "episode : 16010, reward mean : 0.20403123658120184, total_step : 452256, cur_epsilon : 0.44774400000000003\n",
      "episode : 16020, reward mean : 0.20401186627439327, total_step : 452493, cur_epsilon : 0.44750700000000004\n",
      "episode : 16030, reward mean : 0.20435184639874504, total_step : 452754, cur_epsilon : 0.44724600000000003\n",
      "episode : 16040, reward mean : 0.20452120310750016, total_step : 453088, cur_epsilon : 0.44691200000000003\n",
      "episode : 16050, reward mean : 0.2049252397431252, total_step : 453245, cur_epsilon : 0.446755\n",
      "[EVAL] episode : 16050, reward mean : 0.8040000043809414, step mean : 20.6,\n",
      "episode : 16060, reward mean : 0.20492902228638882, total_step : 453444, cur_epsilon : 0.446556\n",
      "episode : 16070, reward mean : 0.20519602352077537, total_step : 453719, cur_epsilon : 0.44628100000000004\n",
      "episode : 16080, reward mean : 0.20533893644376497, total_step : 453894, cur_epsilon : 0.446106\n",
      "episode : 16090, reward mean : 0.20548726526106956, total_step : 454060, cur_epsilon : 0.44594\n",
      "episode : 16100, reward mean : 0.2058279564040742, total_step : 454316, cur_epsilon : 0.445684\n",
      "[EVAL] episode : 16100, reward mean : 0.826000003889203, step mean : 18.4,\n",
      "episode : 16110, reward mean : 0.20610987574063255, total_step : 454666, cur_epsilon : 0.445334\n",
      "episode : 16120, reward mean : 0.20623387706248789, total_step : 454969, cur_epsilon : 0.445031\n",
      "synced target net\n",
      "episode : 16130, reward mean : 0.20591693108108053, total_step : 455383, cur_epsilon : 0.44461700000000004\n",
      "episode : 16140, reward mean : 0.20619455380238305, total_step : 455539, cur_epsilon : 0.444461\n",
      "episode : 16150, reward mean : 0.20652012993238475, total_step : 455716, cur_epsilon : 0.444284\n",
      "[EVAL] episode : 16150, reward mean : 0.7960000045597553, step mean : 21.4,\n",
      "episode : 16160, reward mean : 0.20664542688432583, total_step : 455917, cur_epsilon : 0.444083\n",
      "episode : 16170, reward mean : 0.2069511501866133, total_step : 456226, cur_epsilon : 0.443774\n",
      "episode : 16180, reward mean : 0.20695797889867545, total_step : 456517, cur_epsilon : 0.443483\n",
      "episode : 16190, reward mean : 0.2072118652638202, total_step : 456709, cur_epsilon : 0.44329100000000005\n",
      "episode : 16200, reward mean : 0.20743087028854607, total_step : 456957, cur_epsilon : 0.443043\n",
      "[EVAL] episode : 16200, reward mean : 0.36200000531971455, step mean : 24.8,\n",
      "episode : 16210, reward mean : 0.20753486111810698, total_step : 457191, cur_epsilon : 0.442809\n",
      "episode : 16220, reward mean : 0.20769359425396994, total_step : 457536, cur_epsilon : 0.442464\n",
      "episode : 16230, reward mean : 0.20785521249713185, total_step : 457676, cur_epsilon : 0.442324\n",
      "episode : 16240, reward mean : 0.2078756218526072, total_step : 457944, cur_epsilon : 0.442056\n",
      "episode : 16250, reward mean : 0.20800246762633323, total_step : 458140, cur_epsilon : 0.44186000000000003\n",
      "[EVAL] episode : 16250, reward mean : 0.7920000046491623, step mean : 21.8,\n",
      "episode : 16260, reward mean : 0.20819926808048658, total_step : 458422, cur_epsilon : 0.441578\n",
      "episode : 16270, reward mean : 0.20829195446004495, total_step : 458772, cur_epsilon : 0.441228\n",
      "episode : 16280, reward mean : 0.20859828618462645, total_step : 458875, cur_epsilon : 0.44112500000000004\n",
      "episode : 16290, reward mean : 0.20858195820483713, total_step : 459202, cur_epsilon : 0.440798\n",
      "episode : 16300, reward mean : 0.20865767479857053, total_step : 459480, cur_epsilon : 0.44052\n",
      "[EVAL] episode : 16300, reward mean : 0.3460000056773424, step mean : 26.4,\n",
      "episode : 16310, reward mean : 0.20885469646225635, total_step : 459859, cur_epsilon : 0.440141\n",
      "synced target net\n",
      "episode : 16320, reward mean : 0.20899755510629392, total_step : 460027, cur_epsilon : 0.439973\n",
      "episode : 16330, reward mean : 0.20928169622554216, total_step : 460164, cur_epsilon : 0.439836\n",
      "episode : 16340, reward mean : 0.2094816462314634, total_step : 460438, cur_epsilon : 0.439562\n",
      "episode : 16350, reward mean : 0.20971621403476332, total_step : 460655, cur_epsilon : 0.43934500000000004\n",
      "[EVAL] episode : 16350, reward mean : 0.15200001001358032, step mean : 45.8,\n",
      "episode : 16360, reward mean : 0.2097347249085662, total_step : 460825, cur_epsilon : 0.43917500000000004\n",
      "episode : 16370, reward mean : 0.21006414780453547, total_step : 461086, cur_epsilon : 0.438914\n",
      "episode : 16380, reward mean : 0.21012027470249414, total_step : 461394, cur_epsilon : 0.438606\n",
      "episode : 16390, reward mean : 0.21019280657167658, total_step : 461774, cur_epsilon : 0.438226\n",
      "episode : 16400, reward mean : 0.2101932987653673, total_step : 461973, cur_epsilon : 0.438027\n",
      "[EVAL] episode : 16400, reward mean : 0.4520000033080578, step mean : 15.8,\n",
      "episode : 16410, reward mean : 0.21020171235779175, total_step : 462159, cur_epsilon : 0.43784100000000004\n",
      "episode : 16420, reward mean : 0.21054446405859326, total_step : 462396, cur_epsilon : 0.43760400000000005\n",
      "episode : 16430, reward mean : 0.21079976262203892, total_step : 462576, cur_epsilon : 0.43742400000000004\n",
      "episode : 16440, reward mean : 0.21081387469082744, total_step : 462752, cur_epsilon : 0.437248\n",
      "episode : 16450, reward mean : 0.21084255926877168, total_step : 463003, cur_epsilon : 0.436997\n",
      "[EVAL] episode : 16450, reward mean : 0.8100000042468309, step mean : 20.0,\n",
      "episode : 16460, reward mean : 0.21119563183583148, total_step : 463221, cur_epsilon : 0.43677900000000003\n",
      "episode : 16470, reward mean : 0.21147966606493837, total_step : 463552, cur_epsilon : 0.436448\n",
      "episode : 16480, reward mean : 0.21164138957019443, total_step : 463684, cur_epsilon : 0.43631600000000004\n",
      "episode : 16490, reward mean : 0.2117592541031029, total_step : 463888, cur_epsilon : 0.436112\n",
      "episode : 16500, reward mean : 0.21164849092382373, total_step : 464269, cur_epsilon : 0.43573100000000003\n",
      "[EVAL] episode : 16500, reward mean : -0.0559999942779541, step mean : 26.6,\n",
      "episode : 16510, reward mean : 0.21170987887990558, total_step : 464566, cur_epsilon : 0.43543400000000004\n",
      "episode : 16520, reward mean : 0.21189346854545663, total_step : 464861, cur_epsilon : 0.435139\n",
      "synced target net\n",
      "episode : 16530, reward mean : 0.2120865154525753, total_step : 465140, cur_epsilon : 0.43486\n",
      "episode : 16540, reward mean : 0.2121934764524145, total_step : 465559, cur_epsilon : 0.434441\n",
      "episode : 16550, reward mean : 0.21231722662287267, total_step : 465952, cur_epsilon : 0.43404800000000004\n",
      "[EVAL] episode : 16550, reward mean : 0.7760000050067901, step mean : 23.4,\n",
      "episode : 16560, reward mean : 0.2125434843391396, total_step : 466175, cur_epsilon : 0.433825\n",
      "episode : 16570, reward mean : 0.21282076648940648, total_step : 466513, cur_epsilon : 0.433487\n",
      "episode : 16580, reward mean : 0.21300000607926933, total_step : 466813, cur_epsilon : 0.43318700000000004\n",
      "episode : 16590, reward mean : 0.21324533458918135, total_step : 467003, cur_epsilon : 0.432997\n",
      "episode : 16600, reward mean : 0.21347831933007363, total_step : 467213, cur_epsilon : 0.43278700000000003\n",
      "[EVAL] episode : 16600, reward mean : 0.7500000055879354, step mean : 26.0,\n",
      "episode : 16610, reward mean : 0.21359783870687798, total_step : 467411, cur_epsilon : 0.432589\n",
      "episode : 16620, reward mean : 0.21358845372842322, total_step : 467623, cur_epsilon : 0.432377\n",
      "episode : 16630, reward mean : 0.2138562898989935, total_step : 467873, cur_epsilon : 0.43212700000000004\n",
      "episode : 16640, reward mean : 0.2139921935730784, total_step : 468043, cur_epsilon : 0.43195700000000004\n",
      "episode : 16650, reward mean : 0.21422763369966288, total_step : 468247, cur_epsilon : 0.431753\n",
      "[EVAL] episode : 16650, reward mean : 0.6840000070631505, step mean : 32.6,\n",
      "episode : 16660, reward mean : 0.21424070235312717, total_step : 468720, cur_epsilon : 0.43128\n",
      "episode : 16670, reward mean : 0.21444931621213534, total_step : 468968, cur_epsilon : 0.431032\n",
      "episode : 16680, reward mean : 0.2144166727404041, total_step : 469218, cur_epsilon : 0.430782\n",
      "episode : 16690, reward mean : 0.21436609355258487, total_step : 469597, cur_epsilon : 0.43040300000000004\n",
      "episode : 16700, reward mean : 0.21447006595449267, total_step : 469819, cur_epsilon : 0.43018100000000004\n",
      "[EVAL] episode : 16700, reward mean : 0.04400000348687172, step mean : 16.6,\n",
      "synced target net\n",
      "episode : 16710, reward mean : 0.214782172441594, total_step : 470093, cur_epsilon : 0.42990700000000004\n",
      "episode : 16720, reward mean : 0.21473445583482464, total_step : 470368, cur_epsilon : 0.429632\n",
      "episode : 16730, reward mean : 0.21495517642864623, total_step : 470794, cur_epsilon : 0.42920600000000003\n",
      "episode : 16740, reward mean : 0.21504002997046393, total_step : 471047, cur_epsilon : 0.42895300000000003\n",
      "episode : 16750, reward mean : 0.2153044836858093, total_step : 471199, cur_epsilon : 0.42880100000000004\n",
      "[EVAL] episode : 16750, reward mean : 0.7720000050961972, step mean : 23.8,\n",
      "episode : 16760, reward mean : 0.21538306096622048, total_step : 471462, cur_epsilon : 0.42853800000000003\n",
      "episode : 16770, reward mean : 0.2156231426257922, total_step : 471654, cur_epsilon : 0.428346\n",
      "episode : 16780, reward mean : 0.21541359367490667, total_step : 471800, cur_epsilon : 0.4282\n",
      "episode : 16790, reward mean : 0.21563431220445306, total_step : 472024, cur_epsilon : 0.427976\n",
      "episode : 16800, reward mean : 0.2158089346428668, total_step : 472424, cur_epsilon : 0.427576\n",
      "[EVAL] episode : 16800, reward mean : 0.9100000020116568, step mean : 10.0,\n",
      "episode : 16810, reward mean : 0.21571505663751367, total_step : 472776, cur_epsilon : 0.42722400000000005\n",
      "episode : 16820, reward mean : 0.21593579679692118, total_step : 472999, cur_epsilon : 0.427001\n",
      "episode : 16830, reward mean : 0.2161598397011576, total_step : 473216, cur_epsilon : 0.426784\n",
      "episode : 16840, reward mean : 0.21642577804043234, total_step : 473362, cur_epsilon : 0.426638\n",
      "episode : 16850, reward mean : 0.21654600013301706, total_step : 473553, cur_epsilon : 0.426447\n",
      "[EVAL] episode : 16850, reward mean : 0.8900000024586916, step mean : 12.0,\n",
      "episode : 16860, reward mean : 0.21678529669522956, total_step : 473743, cur_epsilon : 0.426257\n",
      "episode : 16870, reward mean : 0.21667339077443504, total_step : 474125, cur_epsilon : 0.425875\n",
      "episode : 16880, reward mean : 0.216924176682493, total_step : 474295, cur_epsilon : 0.425705\n",
      "episode : 16890, reward mean : 0.21715808777042647, total_step : 474493, cur_epsilon : 0.425507\n",
      "episode : 16900, reward mean : 0.21751953269118035, total_step : 474675, cur_epsilon : 0.425325\n",
      "[EVAL] episode : 16900, reward mean : 0.8360000036656856, step mean : 17.4,\n",
      "episode : 16910, reward mean : 0.2177610941762176, total_step : 474859, cur_epsilon : 0.42514100000000005\n",
      "synced target net\n",
      "episode : 16920, reward mean : 0.21805319755225847, total_step : 475157, cur_epsilon : 0.424843\n",
      "episode : 16930, reward mean : 0.21811105154445906, total_step : 475451, cur_epsilon : 0.424549\n",
      "episode : 16940, reward mean : 0.218274504292554, total_step : 475766, cur_epsilon : 0.424234\n",
      "episode : 16950, reward mean : 0.2185657877746453, total_step : 476064, cur_epsilon : 0.42393600000000004\n",
      "[EVAL] episode : 16950, reward mean : 0.3860000092536211, step mean : 42.2,\n",
      "episode : 16960, reward mean : 0.21886675134674594, total_step : 476345, cur_epsilon : 0.423655\n",
      "episode : 16970, reward mean : 0.21869122586479223, total_step : 476733, cur_epsilon : 0.423267\n",
      "episode : 16980, reward mean : 0.2189740932269024, total_step : 477044, cur_epsilon : 0.422956\n",
      "episode : 16990, reward mean : 0.21887522678331128, total_step : 477302, cur_epsilon : 0.422698\n",
      "episode : 17000, reward mean : 0.21898177077150083, total_step : 477611, cur_epsilon : 0.422389\n",
      "[EVAL] episode : 17000, reward mean : 0.0840000070631504, step mean : 32.4,\n",
      "episode : 17010, reward mean : 0.2190376309923245, total_step : 477907, cur_epsilon : 0.422093\n",
      "episode : 17020, reward mean : 0.21911516470241454, total_step : 478166, cur_epsilon : 0.42183400000000004\n",
      "episode : 17030, reward mean : 0.21926130964822688, total_step : 478506, cur_epsilon : 0.42149400000000004\n",
      "episode : 17040, reward mean : 0.21935622672421395, total_step : 478834, cur_epsilon : 0.42116600000000004\n",
      "episode : 17050, reward mean : 0.2195882758605253, total_step : 479029, cur_epsilon : 0.42097100000000004\n",
      "[EVAL] episode : 17050, reward mean : 0.11000000648200511, step mean : 29.8,\n",
      "episode : 17060, reward mean : 0.21982708695548797, total_step : 479212, cur_epsilon : 0.420788\n",
      "episode : 17070, reward mean : 0.22014177524985, total_step : 479465, cur_epsilon : 0.42053500000000005\n",
      "episode : 17080, reward mean : 0.22048302714045506, total_step : 479672, cur_epsilon : 0.42032800000000003\n",
      "episode : 17090, reward mean : 0.2206103044818714, total_step : 479844, cur_epsilon : 0.42015600000000003\n",
      "synced target net\n",
      "episode : 17100, reward mean : 0.22081345635362798, total_step : 480086, cur_epsilon : 0.419914\n",
      "[EVAL] episode : 17100, reward mean : 0.4520000033080578, step mean : 15.8,\n",
      "episode : 17110, reward mean : 0.22084804814121636, total_step : 480315, cur_epsilon : 0.41968500000000003\n",
      "episode : 17120, reward mean : 0.22095911820902203, total_step : 480514, cur_epsilon : 0.419486\n",
      "episode : 17130, reward mean : 0.22108990681687696, total_step : 480679, cur_epsilon : 0.419321\n",
      "episode : 17140, reward mean : 0.22125438178760135, total_step : 480986, cur_epsilon : 0.419014\n",
      "episode : 17150, reward mean : 0.22132362121851903, total_step : 481256, cur_epsilon : 0.418744\n",
      "[EVAL] episode : 17150, reward mean : 0.3460000056773424, step mean : 26.4,\n",
      "episode : 17160, reward mean : 0.22120513426474273, total_step : 481646, cur_epsilon : 0.418354\n",
      "episode : 17170, reward mean : 0.22117239976910377, total_step : 481891, cur_epsilon : 0.418109\n",
      "episode : 17180, reward mean : 0.22137602468620773, total_step : 482229, cur_epsilon : 0.417771\n",
      "episode : 17190, reward mean : 0.22151134986280874, total_step : 482385, cur_epsilon : 0.417615\n",
      "episode : 17200, reward mean : 0.22173488977820027, total_step : 482589, cur_epsilon : 0.41741100000000003\n",
      "[EVAL] episode : 17200, reward mean : 0.33400000594556334, step mean : 27.6,\n",
      "episode : 17210, reward mean : 0.2220447474863339, total_step : 482844, cur_epsilon : 0.417156\n",
      "episode : 17220, reward mean : 0.22214867039978434, total_step : 483053, cur_epsilon : 0.416947\n",
      "episode : 17230, reward mean : 0.22207139317201396, total_step : 483374, cur_epsilon : 0.416626\n",
      "episode : 17240, reward mean : 0.22231613134764341, total_step : 483740, cur_epsilon : 0.41626\n",
      "episode : 17250, reward mean : 0.22245855678045665, total_step : 483882, cur_epsilon : 0.41611800000000004\n",
      "[EVAL] episode : 17250, reward mean : 0.636000008136034, step mean : 37.4,\n",
      "episode : 17260, reward mean : 0.22242179052949573, total_step : 484232, cur_epsilon : 0.415768\n",
      "episode : 17270, reward mean : 0.2225194038555768, total_step : 484451, cur_epsilon : 0.415549\n",
      "episode : 17280, reward mean : 0.2227112329074535, total_step : 484707, cur_epsilon : 0.415293\n",
      "episode : 17290, reward mean : 0.22271082155468958, total_step : 484895, cur_epsilon : 0.415105\n",
      "synced target net\n",
      "episode : 17300, reward mean : 0.2229630118356533, total_step : 485246, cur_epsilon : 0.414754\n",
      "[EVAL] episode : 17300, reward mean : 0.356000005453825, step mean : 25.4,\n",
      "episode : 17310, reward mean : 0.22315194135255237, total_step : 485506, cur_epsilon : 0.41449400000000003\n",
      "episode : 17320, reward mean : 0.22307621852664694, total_step : 485824, cur_epsilon : 0.41417600000000004\n",
      "episode : 17330, reward mean : 0.22299192758084668, total_step : 486256, cur_epsilon : 0.413744\n",
      "episode : 17340, reward mean : 0.22299769925363921, total_step : 486633, cur_epsilon : 0.41336700000000004\n",
      "episode : 17350, reward mean : 0.22303228271507822, total_step : 486859, cur_epsilon : 0.41314100000000004\n",
      "[EVAL] episode : 17350, reward mean : 0.09400000236928463, step mean : 11.6,\n",
      "episode : 17360, reward mean : 0.22322235628812304, total_step : 487116, cur_epsilon : 0.41288400000000003\n",
      "episode : 17370, reward mean : 0.22343754203841049, total_step : 487329, cur_epsilon : 0.412671\n",
      "episode : 17380, reward mean : 0.2237128944346259, total_step : 487637, cur_epsilon : 0.41236300000000004\n",
      "episode : 17390, reward mean : 0.22404543446337288, total_step : 487845, cur_epsilon : 0.41215500000000005\n",
      "episode : 17400, reward mean : 0.22438218996324844, total_step : 488045, cur_epsilon : 0.411955\n",
      "[EVAL] episode : 17400, reward mean : 0.43400000371038916, step mean : 17.6,\n",
      "episode : 17410, reward mean : 0.22457955803632274, total_step : 488287, cur_epsilon : 0.411713\n",
      "episode : 17420, reward mean : 0.22471527585783282, total_step : 488436, cur_epsilon : 0.41156400000000004\n",
      "episode : 17430, reward mean : 0.224917389873396, total_step : 488669, cur_epsilon : 0.411331\n",
      "episode : 17440, reward mean : 0.22497649687810142, total_step : 488951, cur_epsilon : 0.411049\n",
      "episode : 17450, reward mean : 0.2250481435879638, total_step : 489211, cur_epsilon : 0.410789\n",
      "[EVAL] episode : 17450, reward mean : 0.8460000034421682, step mean : 16.4,\n",
      "episode : 17460, reward mean : 0.22495189608743613, total_step : 489562, cur_epsilon : 0.410438\n",
      "episode : 17470, reward mean : 0.22508586752822843, total_step : 489713, cur_epsilon : 0.410287\n",
      "synced target net\n",
      "episode : 17480, reward mean : 0.22513158499908972, total_step : 490018, cur_epsilon : 0.409982\n",
      "episode : 17490, reward mean : 0.2250011495639889, total_step : 490429, cur_epsilon : 0.409571\n",
      "episode : 17500, reward mean : 0.22503829176947474, total_step : 490848, cur_epsilon : 0.409152\n",
      "[EVAL] episode : 17500, reward mean : 0.32400000616908076, step mean : 28.6,\n",
      "episode : 17510, reward mean : 0.2252244492302378, total_step : 491107, cur_epsilon : 0.408893\n",
      "episode : 17520, reward mean : 0.22510788276606583, total_step : 491296, cur_epsilon : 0.408704\n",
      "episode : 17530, reward mean : 0.2253947579077165, total_step : 491578, cur_epsilon : 0.408422\n",
      "episode : 17540, reward mean : 0.22553421357761275, total_step : 491718, cur_epsilon : 0.40828200000000003\n",
      "episode : 17550, reward mean : 0.22576353881423206, total_step : 491900, cur_epsilon : 0.4081\n",
      "[EVAL] episode : 17550, reward mean : 0.03200000822544098, step mean : 37.6,\n",
      "episode : 17560, reward mean : 0.22594305844234985, total_step : 492169, cur_epsilon : 0.407831\n",
      "episode : 17570, reward mean : 0.22603984668842417, total_step : 492482, cur_epsilon : 0.40751800000000005\n",
      "episode : 17580, reward mean : 0.22632423813292699, total_step : 492766, cur_epsilon : 0.40723400000000004\n",
      "episode : 17590, reward mean : 0.22630586165123667, total_step : 493081, cur_epsilon : 0.40691900000000003\n",
      "episode : 17600, reward mean : 0.22663580150502227, total_step : 493284, cur_epsilon : 0.406716\n",
      "[EVAL] episode : 17600, reward mean : 0.04800000786781311, step mean : 36.0,\n",
      "episode : 17610, reward mean : 0.22672913722516222, total_step : 493503, cur_epsilon : 0.406497\n",
      "episode : 17620, reward mean : 0.22688763374579007, total_step : 493807, cur_epsilon : 0.406193\n",
      "episode : 17630, reward mean : 0.22714407865414465, total_step : 494138, cur_epsilon : 0.405862\n",
      "episode : 17640, reward mean : 0.2272624777046047, total_step : 494312, cur_epsilon : 0.40568800000000005\n",
      "episode : 17650, reward mean : 0.22739887290557703, total_step : 494654, cur_epsilon : 0.40534600000000004\n",
      "[EVAL] episode : 17650, reward mean : 0.24400000795722007, step mean : 36.6,\n",
      "synced target net\n",
      "episode : 17660, reward mean : 0.22756059495299752, total_step : 495050, cur_epsilon : 0.40495000000000003\n",
      "episode : 17670, reward mean : 0.22765365630540055, total_step : 495268, cur_epsilon : 0.40473200000000004\n",
      "episode : 17680, reward mean : 0.2277641463223044, total_step : 495554, cur_epsilon : 0.404446\n",
      "episode : 17690, reward mean : 0.2279248223326776, total_step : 495951, cur_epsilon : 0.40404900000000005\n",
      "episode : 17700, reward mean : 0.22809379136305577, total_step : 496234, cur_epsilon : 0.403766\n",
      "[EVAL] episode : 17700, reward mean : -0.09799999333918094, step mean : 30.8,\n",
      "episode : 17710, reward mean : 0.22814060458445418, total_step : 496533, cur_epsilon : 0.403467\n",
      "episode : 17720, reward mean : 0.22826355006913687, total_step : 496697, cur_epsilon : 0.403303\n",
      "episode : 17730, reward mean : 0.22850762026261556, total_step : 496846, cur_epsilon : 0.403154\n",
      "episode : 17740, reward mean : 0.22866122363714803, total_step : 497155, cur_epsilon : 0.402845\n",
      "episode : 17750, reward mean : 0.22901859759738746, total_step : 497302, cur_epsilon : 0.402698\n",
      "[EVAL] episode : 17750, reward mean : 0.432000008225441, step mean : 37.6,\n",
      "episode : 17760, reward mean : 0.22936149253321156, total_step : 497474, cur_epsilon : 0.402526\n",
      "episode : 17770, reward mean : 0.22956275224872247, total_step : 497796, cur_epsilon : 0.402204\n",
      "episode : 17780, reward mean : 0.22966142337165849, total_step : 498199, cur_epsilon : 0.401801\n",
      "episode : 17790, reward mean : 0.22958741470576674, total_step : 498511, cur_epsilon : 0.40148900000000004\n",
      "episode : 17800, reward mean : 0.2297376464990627, total_step : 498824, cur_epsilon : 0.40117600000000003\n",
      "[EVAL] episode : 17800, reward mean : 0.7620000053197146, step mean : 24.8,\n",
      "episode : 17810, reward mean : 0.22982987690788098, total_step : 499040, cur_epsilon : 0.40096000000000004\n",
      "episode : 17820, reward mean : 0.22999214970775358, total_step : 499331, cur_epsilon : 0.400669\n",
      "episode : 17830, reward mean : 0.2303482954471309, total_step : 499476, cur_epsilon : 0.40052400000000005\n",
      "episode : 17840, reward mean : 0.23019283116030853, total_step : 499832, cur_epsilon : 0.400168\n",
      "synced target net\n",
      "episode : 17850, reward mean : 0.23016191080953477, total_step : 500067, cur_epsilon : 0.399933\n",
      "[EVAL] episode : 17850, reward mean : 0.4200000040233135, step mean : 19.0,\n",
      "episode : 17860, reward mean : 0.2300308011220883, total_step : 500481, cur_epsilon : 0.39951900000000007\n",
      "episode : 17870, reward mean : 0.2300285455009368, total_step : 500764, cur_epsilon : 0.39923600000000004\n",
      "episode : 17880, reward mean : 0.23027685168761786, total_step : 501100, cur_epsilon : 0.39890000000000003\n",
      "episode : 17890, reward mean : 0.2302867584241801, total_step : 501262, cur_epsilon : 0.39873800000000004\n",
      "episode : 17900, reward mean : 0.2303899501840579, total_step : 501655, cur_epsilon : 0.39834500000000006\n",
      "[EVAL] episode : 17900, reward mean : 0.1440000057220459, step mean : 26.4,\n",
      "episode : 17910, reward mean : 0.23045450074796833, total_step : 502117, cur_epsilon : 0.397883\n",
      "episode : 17920, reward mean : 0.2306423051601866, total_step : 502459, cur_epsilon : 0.39754100000000003\n",
      "episode : 17930, reward mean : 0.2307445682382665, total_step : 502655, cur_epsilon : 0.39734500000000006\n",
      "episode : 17940, reward mean : 0.2306137184257367, total_step : 502869, cur_epsilon : 0.397131\n",
      "episode : 17950, reward mean : 0.23069025674705138, total_step : 503111, cur_epsilon : 0.39688900000000005\n",
      "[EVAL] episode : 17950, reward mean : 0.3540000054985285, step mean : 25.6,\n",
      "episode : 17960, reward mean : 0.23098330226413624, total_step : 503364, cur_epsilon : 0.396636\n",
      "episode : 17970, reward mean : 0.23116750744112877, total_step : 503612, cur_epsilon : 0.3963880000000001\n",
      "episode : 17980, reward mean : 0.2313020082728293, total_step : 503749, cur_epsilon : 0.396251\n",
      "episode : 17990, reward mean : 0.23137465863374204, total_step : 504096, cur_epsilon : 0.39590400000000003\n",
      "episode : 18000, reward mean : 0.23145722827304982, total_step : 504526, cur_epsilon : 0.395474\n",
      "[EVAL] episode : 18000, reward mean : 0.7680000051856041, step mean : 24.2,\n",
      "episode : 18010, reward mean : 0.23152804602949006, total_step : 504876, cur_epsilon : 0.39512400000000003\n",
      "synced target net\n",
      "episode : 18020, reward mean : 0.23159157097935015, total_step : 505140, cur_epsilon : 0.39486\n",
      "episode : 18030, reward mean : 0.23161287349289778, total_step : 505280, cur_epsilon : 0.39472000000000007\n",
      "episode : 18040, reward mean : 0.23171009474182028, total_step : 505582, cur_epsilon : 0.39441800000000005\n",
      "episode : 18050, reward mean : 0.23195125258819219, total_step : 505925, cur_epsilon : 0.39407500000000006\n",
      "[EVAL] episode : 18050, reward mean : 0.818000004068017, step mean : 19.2,\n",
      "episode : 18060, reward mean : 0.23200000605091106, total_step : 506215, cur_epsilon : 0.39378500000000005\n",
      "episode : 18070, reward mean : 0.23189264578764354, total_step : 506686, cur_epsilon : 0.39331400000000005\n",
      "episode : 18080, reward mean : 0.23209624499045478, total_step : 506896, cur_epsilon : 0.393104\n",
      "episode : 18090, reward mean : 0.2323178612209564, total_step : 507172, cur_epsilon : 0.39282800000000007\n",
      "episode : 18100, reward mean : 0.23242100052761933, total_step : 507462, cur_epsilon : 0.39253800000000005\n",
      "[EVAL] episode : 18100, reward mean : 0.8700000029057264, step mean : 14.0,\n",
      "episode : 18110, reward mean : 0.23250249086810798, total_step : 507791, cur_epsilon : 0.39220900000000003\n",
      "episode : 18120, reward mean : 0.23278035925388763, total_step : 508065, cur_epsilon : 0.39193500000000003\n",
      "episode : 18130, reward mean : 0.2327893055551128, total_step : 508226, cur_epsilon : 0.39177400000000007\n",
      "episode : 18140, reward mean : 0.23297630153060403, total_step : 508464, cur_epsilon : 0.391536\n",
      "episode : 18150, reward mean : 0.23327163139486445, total_step : 508705, cur_epsilon : 0.39129500000000006\n",
      "[EVAL] episode : 18150, reward mean : 0.872000002861023, step mean : 13.8,\n",
      "episode : 18160, reward mean : 0.23325826596388682, total_step : 509106, cur_epsilon : 0.3908940000000001\n",
      "episode : 18170, reward mean : 0.2334826697832733, total_step : 509374, cur_epsilon : 0.39062600000000003\n",
      "episode : 18180, reward mean : 0.23340209626138816, total_step : 509697, cur_epsilon : 0.39030300000000007\n",
      "synced target net\n",
      "episode : 18190, reward mean : 0.2333892309026503, total_step : 510095, cur_epsilon : 0.38990500000000006\n",
      "episode : 18200, reward mean : 0.2334653906686907, total_step : 510333, cur_epsilon : 0.389667\n",
      "[EVAL] episode : 18200, reward mean : 0.5660000097006559, step mean : 44.4,\n",
      "episode : 18210, reward mean : 0.23365733718938353, total_step : 510560, cur_epsilon : 0.38944\n",
      "episode : 18220, reward mean : 0.2338463287755569, total_step : 510891, cur_epsilon : 0.38910900000000004\n",
      "episode : 18230, reward mean : 0.23388371422449541, total_step : 510999, cur_epsilon : 0.38900100000000004\n",
      "episode : 18240, reward mean : 0.23382127798109298, total_step : 511289, cur_epsilon : 0.38871100000000003\n",
      "episode : 18250, reward mean : 0.23399069098260714, total_step : 511556, cur_epsilon : 0.388444\n",
      "[EVAL] episode : 18250, reward mean : 0.18800000920891763, step mean : 42.2,\n",
      "episode : 18260, reward mean : 0.23422946935960245, total_step : 511896, cur_epsilon : 0.388104\n",
      "episode : 18270, reward mean : 0.2344236513725153, total_step : 512216, cur_epsilon : 0.387784\n",
      "episode : 18280, reward mean : 0.2346230913906336, total_step : 512427, cur_epsilon : 0.38757300000000006\n",
      "episode : 18290, reward mean : 0.23489175017397626, total_step : 512711, cur_epsilon : 0.387289\n",
      "episode : 18300, reward mean : 0.23488743774552162, total_step : 512993, cur_epsilon : 0.387007\n",
      "[EVAL] episode : 18300, reward mean : 0.8060000043362379, step mean : 20.4,\n",
      "episode : 18310, reward mean : 0.23513654346325027, total_step : 513312, cur_epsilon : 0.38668800000000003\n",
      "episode : 18320, reward mean : 0.23505841216721113, total_step : 513729, cur_epsilon : 0.38627100000000003\n",
      "episode : 18330, reward mean : 0.23513748559722777, total_step : 514159, cur_epsilon : 0.385841\n",
      "episode : 18340, reward mean : 0.23520611292533292, total_step : 514408, cur_epsilon : 0.38559200000000005\n",
      "episode : 18350, reward mean : 0.23530136845193864, total_step : 514608, cur_epsilon : 0.38539200000000007\n",
      "[EVAL] episode : 18350, reward mean : 0.8400000035762787, step mean : 17.0,\n",
      "episode : 18360, reward mean : 0.2354613350301551, total_step : 514889, cur_epsilon : 0.385111\n",
      "synced target net\n",
      "episode : 18370, reward mean : 0.23567338656709125, total_step : 515274, cur_epsilon : 0.384726\n",
      "episode : 18380, reward mean : 0.23570566438006743, total_step : 515589, cur_epsilon : 0.38441100000000006\n",
      "episode : 18390, reward mean : 0.23562697723746914, total_step : 516007, cur_epsilon : 0.38399300000000003\n",
      "episode : 18400, reward mean : 0.23575978866677083, total_step : 516337, cur_epsilon : 0.383663\n",
      "[EVAL] episode : 18400, reward mean : 0.7700000051409006, step mean : 24.0,\n",
      "episode : 18410, reward mean : 0.23588648079836308, total_step : 516478, cur_epsilon : 0.38352200000000003\n",
      "episode : 18420, reward mean : 0.23580293765303706, total_step : 516804, cur_epsilon : 0.383196\n",
      "episode : 18430, reward mean : 0.23599566530741498, total_step : 517023, cur_epsilon : 0.382977\n",
      "episode : 18440, reward mean : 0.236287967010084, total_step : 517258, cur_epsilon : 0.382742\n",
      "episode : 18450, reward mean : 0.23634472150264715, total_step : 517527, cur_epsilon : 0.38247300000000006\n",
      "[EVAL] episode : 18450, reward mean : 0.6720000073313713, step mean : 33.8,\n",
      "episode : 18460, reward mean : 0.23657584570838644, total_step : 517773, cur_epsilon : 0.382227\n",
      "episode : 18470, reward mean : 0.2367163027540398, total_step : 518186, cur_epsilon : 0.381814\n",
      "episode : 18480, reward mean : 0.23666721384869527, total_step : 518450, cur_epsilon : 0.38155000000000006\n",
      "episode : 18490, reward mean : 0.23688967614699097, total_step : 518612, cur_epsilon : 0.38138800000000006\n",
      "episode : 18500, reward mean : 0.2370864925406672, total_step : 518821, cur_epsilon : 0.38117900000000005\n",
      "[EVAL] episode : 18500, reward mean : 0.1500000100582838, step mean : 46.0,\n",
      "episode : 18510, reward mean : 0.2371534366318619, total_step : 519070, cur_epsilon : 0.38093\n",
      "episode : 18520, reward mean : 0.2374200924468623, total_step : 519349, cur_epsilon : 0.3806510000000001\n",
      "episode : 18530, reward mean : 0.23743335737797033, total_step : 519796, cur_epsilon : 0.380204\n",
      "synced target net\n",
      "episode : 18540, reward mean : 0.23761489278664, total_step : 520032, cur_epsilon : 0.379968\n",
      "episode : 18550, reward mean : 0.237695423845513, total_step : 520354, cur_epsilon : 0.37964600000000004\n",
      "[EVAL] episode : 18550, reward mean : 0.7100000064820051, step mean : 30.0,\n",
      "episode : 18560, reward mean : 0.23776347588292115, total_step : 520600, cur_epsilon : 0.37940000000000007\n",
      "episode : 18570, reward mean : 0.23808455101906947, total_step : 520776, cur_epsilon : 0.379224\n",
      "episode : 18580, reward mean : 0.23828095330829674, total_step : 520983, cur_epsilon : 0.37901700000000005\n",
      "episode : 18590, reward mean : 0.23829855365771851, total_step : 521122, cur_epsilon : 0.37887800000000005\n",
      "episode : 18600, reward mean : 0.2384672103522806, total_step : 521380, cur_epsilon : 0.37862000000000007\n",
      "[EVAL] episode : 18600, reward mean : 0.4320000037550926, step mean : 17.8,\n",
      "episode : 18610, reward mean : 0.2386727626325862, total_step : 521569, cur_epsilon : 0.3784310000000001\n",
      "episode : 18620, reward mean : 0.23884103719913324, total_step : 521827, cur_epsilon : 0.378173\n",
      "episode : 18630, reward mean : 0.23892539520630482, total_step : 522041, cur_epsilon : 0.37795900000000004\n",
      "episode : 18640, reward mean : 0.2391105210710349, total_step : 522366, cur_epsilon : 0.377634\n",
      "episode : 18650, reward mean : 0.2391458505538889, total_step : 522671, cur_epsilon : 0.377329\n",
      "[EVAL] episode : 18650, reward mean : 0.4080000087618828, step mean : 40.0,\n",
      "episode : 18660, reward mean : 0.23921811966133671, total_step : 522907, cur_epsilon : 0.377093\n",
      "episode : 18670, reward mean : 0.2392404988190597, total_step : 523236, cur_epsilon : 0.376764\n",
      "episode : 18680, reward mean : 0.23951713667057725, total_step : 523490, cur_epsilon : 0.37651\n",
      "episode : 18690, reward mean : 0.23927448437957283, total_step : 523714, cur_epsilon : 0.376286\n",
      "episode : 18700, reward mean : 0.23933102209150872, total_step : 523979, cur_epsilon : 0.37602100000000005\n",
      "[EVAL] episode : 18700, reward mean : 0.7380000058561563, step mean : 27.2,\n",
      "episode : 18710, reward mean : 0.23933031069735022, total_step : 524151, cur_epsilon : 0.375849\n",
      "episode : 18720, reward mean : 0.2394460530544663, total_step : 524305, cur_epsilon : 0.375695\n",
      "episode : 18730, reward mean : 0.23963268089838607, total_step : 524526, cur_epsilon : 0.375474\n",
      "episode : 18740, reward mean : 0.23980256741212455, total_step : 524877, cur_epsilon : 0.375123\n",
      "synced target net\n",
      "episode : 18750, reward mean : 0.2400752060458064, total_step : 525136, cur_epsilon : 0.374864\n",
      "[EVAL] episode : 18750, reward mean : -0.4999999888241291, step mean : 50.8,\n",
      "episode : 18760, reward mean : 0.24027665849677773, total_step : 525328, cur_epsilon : 0.374672\n",
      "episode : 18770, reward mean : 0.24011188670536368, total_step : 525607, cur_epsilon : 0.374393\n",
      "episode : 18780, reward mean : 0.24022364821582556, total_step : 525767, cur_epsilon : 0.37423300000000004\n",
      "episode : 18790, reward mean : 0.24035285330303668, total_step : 526094, cur_epsilon : 0.37390600000000007\n",
      "episode : 18800, reward mean : 0.24063192093705243, total_step : 526339, cur_epsilon : 0.373661\n",
      "[EVAL] episode : 18800, reward mean : 0.23200001269578935, step mean : 57.6,\n",
      "episode : 18810, reward mean : 0.2409542856805308, total_step : 526502, cur_epsilon : 0.373498\n",
      "episode : 18820, reward mean : 0.24077630784891063, total_step : 526806, cur_epsilon : 0.373194\n",
      "episode : 18830, reward mean : 0.24084599648379246, total_step : 527143, cur_epsilon : 0.372857\n",
      "episode : 18840, reward mean : 0.24097983619206112, total_step : 527460, cur_epsilon : 0.37254\n",
      "episode : 18850, reward mean : 0.2410901917219755, total_step : 527920, cur_epsilon : 0.3720800000000001\n",
      "[EVAL] episode : 18850, reward mean : 0.4560000032186508, step mean : 15.4,\n",
      "episode : 18860, reward mean : 0.2413754037120167, total_step : 528151, cur_epsilon : 0.371849\n",
      "episode : 18870, reward mean : 0.24142024981822358, total_step : 528433, cur_epsilon : 0.371567\n",
      "episode : 18880, reward mean : 0.24174841706043157, total_step : 528582, cur_epsilon : 0.371418\n",
      "episode : 18890, reward mean : 0.24199841790179322, total_step : 528878, cur_epsilon : 0.37112200000000006\n",
      "episode : 18900, reward mean : 0.2419280483719533, total_step : 529179, cur_epsilon : 0.37082100000000007\n",
      "[EVAL] episode : 18900, reward mean : -0.01799999512732029, step mean : 22.8,\n",
      "episode : 18910, reward mean : 0.24180381355343247, total_step : 529481, cur_epsilon : 0.37051900000000004\n",
      "episode : 18920, reward mean : 0.24196195107575577, total_step : 529750, cur_epsilon : 0.37024999999999997\n",
      "episode : 18930, reward mean : 0.2421442215742688, total_step : 529973, cur_epsilon : 0.370027\n",
      "synced target net\n",
      "episode : 18940, reward mean : 0.24242133656034784, total_step : 530216, cur_epsilon : 0.369784\n",
      "episode : 18950, reward mean : 0.2426195311090248, total_step : 530507, cur_epsilon : 0.36949300000000007\n",
      "[EVAL] episode : 18950, reward mean : 0.9320000015199185, step mean : 7.8,\n",
      "episode : 18960, reward mean : 0.2428813351568754, total_step : 530778, cur_epsilon : 0.36922200000000005\n",
      "episode : 18970, reward mean : 0.2431381188526467, total_step : 531058, cur_epsilon : 0.368942\n",
      "episode : 18980, reward mean : 0.24304426315763633, total_step : 531500, cur_epsilon : 0.36850000000000005\n",
      "episode : 18990, reward mean : 0.2431337606527692, total_step : 531796, cur_epsilon : 0.368204\n",
      "episode : 19000, reward mean : 0.243365795519301, total_step : 532122, cur_epsilon : 0.36787800000000004\n",
      "[EVAL] episode : 19000, reward mean : 0.8740000028163195, step mean : 13.6,\n",
      "episode : 19010, reward mean : 0.24354919068454553, total_step : 532340, cur_epsilon : 0.36766\n",
      "episode : 19020, reward mean : 0.2437444855391462, total_step : 532535, cur_epsilon : 0.36746500000000004\n",
      "episode : 19030, reward mean : 0.24377614897756564, total_step : 532940, cur_epsilon : 0.36706000000000005\n",
      "episode : 19040, reward mean : 0.2439285774750463, total_step : 533315, cur_epsilon : 0.36668500000000004\n",
      "episode : 19050, reward mean : 0.24419423176801894, total_step : 533575, cur_epsilon : 0.366425\n",
      "[EVAL] episode : 19050, reward mean : 0.7240000061690808, step mean : 28.6,\n",
      "episode : 19060, reward mean : 0.24424449712690846, total_step : 533845, cur_epsilon : 0.366155\n",
      "episode : 19070, reward mean : 0.244400110922746, total_step : 534114, cur_epsilon : 0.36588600000000004\n",
      "episode : 19080, reward mean : 0.24456604378145633, total_step : 534363, cur_epsilon : 0.365637\n",
      "episode : 19090, reward mean : 0.2447265644540699, total_step : 534721, cur_epsilon : 0.365279\n",
      "episode : 19100, reward mean : 0.24501204793063525, total_step : 534941, cur_epsilon : 0.365059\n",
      "[EVAL] episode : 19100, reward mean : 0.024000003933906555, step mean : 18.6,\n",
      "synced target net\n",
      "episode : 19110, reward mean : 0.24529095319334138, total_step : 535173, cur_epsilon : 0.364827\n",
      "episode : 19120, reward mean : 0.24536820688249864, total_step : 535489, cur_epsilon : 0.36451100000000003\n",
      "episode : 19130, reward mean : 0.2452498753621816, total_step : 535880, cur_epsilon : 0.36412\n",
      "episode : 19140, reward mean : 0.24543835505339062, total_step : 536084, cur_epsilon : 0.363916\n",
      "episode : 19150, reward mean : 0.24512585460984054, total_step : 536346, cur_epsilon : 0.36365400000000003\n",
      "[EVAL] episode : 19150, reward mean : 0.3060000065714121, step mean : 30.4,\n",
      "episode : 19160, reward mean : 0.24513205197573656, total_step : 536699, cur_epsilon : 0.363301\n",
      "episode : 19170, reward mean : 0.24526604673561403, total_step : 537007, cur_epsilon : 0.362993\n",
      "episode : 19180, reward mean : 0.2454296202281329, total_step : 537258, cur_epsilon : 0.362742\n",
      "episode : 19190, reward mean : 0.24559198103334567, total_step : 537511, cur_epsilon : 0.36248900000000006\n",
      "episode : 19200, reward mean : 0.24560625604819505, total_step : 537947, cur_epsilon : 0.36205300000000007\n",
      "[EVAL] episode : 19200, reward mean : 0.4840000070631504, step mean : 32.4,\n",
      "episode : 19210, reward mean : 0.24571890245710015, total_step : 538295, cur_epsilon : 0.36170500000000005\n",
      "episode : 19220, reward mean : 0.24585068242926766, total_step : 538705, cur_epsilon : 0.36129500000000003\n",
      "episode : 19230, reward mean : 0.24605096809011973, total_step : 539084, cur_epsilon : 0.360916\n",
      "episode : 19240, reward mean : 0.24615437195698292, total_step : 539449, cur_epsilon : 0.36055100000000007\n",
      "episode : 19250, reward mean : 0.24632987618185095, total_step : 539675, cur_epsilon : 0.360325\n",
      "[EVAL] episode : 19250, reward mean : 0.8520000033080578, step mean : 15.8,\n",
      "episode : 19260, reward mean : 0.2463535886057696, total_step : 539892, cur_epsilon : 0.360108\n",
      "synced target net\n",
      "episode : 19270, reward mean : 0.24646964798008197, total_step : 540032, cur_epsilon : 0.35996800000000007\n",
      "episode : 19280, reward mean : 0.246442433435922, total_step : 540347, cur_epsilon : 0.359653\n",
      "episode : 19290, reward mean : 0.24653914550163852, total_step : 540722, cur_epsilon : 0.359278\n",
      "episode : 19300, reward mean : 0.2466886070873492, total_step : 540997, cur_epsilon : 0.35900300000000007\n",
      "[EVAL] episode : 19300, reward mean : -0.003999995440244675, step mean : 21.4,\n",
      "episode : 19310, reward mean : 0.24667582169188654, total_step : 541385, cur_epsilon : 0.358615\n",
      "episode : 19320, reward mean : 0.2467334429049907, total_step : 541637, cur_epsilon : 0.358363\n",
      "episode : 19330, reward mean : 0.24673409813545755, total_step : 541799, cur_epsilon : 0.358201\n",
      "episode : 19340, reward mean : 0.2468360970523921, total_step : 541965, cur_epsilon : 0.358035\n",
      "episode : 19350, reward mean : 0.24690749959100894, total_step : 542388, cur_epsilon : 0.35761200000000004\n",
      "[EVAL] episode : 19350, reward mean : 0.7820000048726797, step mean : 22.8,\n",
      "episode : 19360, reward mean : 0.24687603911046277, total_step : 542810, cur_epsilon : 0.35719\n",
      "episode : 19370, reward mean : 0.24700000605309863, total_step : 543133, cur_epsilon : 0.35686700000000005\n",
      "episode : 19380, reward mean : 0.24711507313324463, total_step : 543473, cur_epsilon : 0.35652700000000004\n",
      "episode : 19390, reward mean : 0.24729861358373728, total_step : 543779, cur_epsilon : 0.356221\n",
      "episode : 19400, reward mean : 0.24729845966327665, total_step : 544142, cur_epsilon : 0.355858\n",
      "[EVAL] episode : 19400, reward mean : 0.5160000063478947, step mean : 29.2,\n",
      "episode : 19410, reward mean : 0.24755848106764552, total_step : 544400, cur_epsilon : 0.3556\n",
      "episode : 19420, reward mean : 0.24763749318061173, total_step : 544609, cur_epsilon : 0.355391\n",
      "episode : 19430, reward mean : 0.24787648572474794, total_step : 544907, cur_epsilon : 0.355093\n",
      "synced target net\n",
      "episode : 19440, reward mean : 0.24784517066252376, total_step : 545130, cur_epsilon : 0.35487\n",
      "episode : 19450, reward mean : 0.2479403659505985, total_step : 545406, cur_epsilon : 0.3545940000000001\n",
      "[EVAL] episode : 19450, reward mean : 0.5400000102818012, step mean : 47.0,\n",
      "episode : 19460, reward mean : 0.2482081252718794, total_step : 545647, cur_epsilon : 0.35435300000000003\n",
      "episode : 19470, reward mean : 0.2484062721020995, total_step : 545823, cur_epsilon : 0.3541770000000001\n",
      "episode : 19480, reward mean : 0.24833727504578046, total_step : 546119, cur_epsilon : 0.353881\n",
      "episode : 19490, reward mean : 0.24835505992739917, total_step : 546543, cur_epsilon : 0.353457\n",
      "episode : 19500, reward mean : 0.24846718554150982, total_step : 546886, cur_epsilon : 0.35311400000000004\n",
      "[EVAL] episode : 19500, reward mean : 0.7100000064820051, step mean : 30.0,\n",
      "episode : 19510, reward mean : 0.2484551572588882, total_step : 547170, cur_epsilon : 0.35283\n",
      "episode : 19520, reward mean : 0.2486116863819618, total_step : 547426, cur_epsilon : 0.35257400000000005\n",
      "episode : 19530, reward mean : 0.24867435321183204, total_step : 547665, cur_epsilon : 0.35233500000000006\n",
      "episode : 19540, reward mean : 0.24855118312538357, total_step : 547867, cur_epsilon : 0.35213300000000003\n",
      "episode : 19550, reward mean : 0.2486056326508827, total_step : 548122, cur_epsilon : 0.351878\n",
      "[EVAL] episode : 19550, reward mean : 0.7120000064373017, step mean : 29.8,\n",
      "episode : 19560, reward mean : 0.24881800196089748, total_step : 548268, cur_epsilon : 0.35173200000000004\n",
      "episode : 19570, reward mean : 0.2491073131526063, total_step : 548463, cur_epsilon : 0.351537\n",
      "episode : 19580, reward mean : 0.24928192637605825, total_step : 548682, cur_epsilon : 0.351318\n",
      "episode : 19590, reward mean : 0.249381833514289, total_step : 549146, cur_epsilon : 0.350854\n",
      "episode : 19600, reward mean : 0.24942857747973532, total_step : 549415, cur_epsilon : 0.35058500000000004\n",
      "[EVAL] episode : 19600, reward mean : 0.7260000061243772, step mean : 28.4,\n",
      "episode : 19610, reward mean : 0.24958695148795554, total_step : 549764, cur_epsilon : 0.350236\n",
      "synced target net\n",
      "episode : 19620, reward mean : 0.24972324764230702, total_step : 550057, cur_epsilon : 0.349943\n",
      "episode : 19630, reward mean : 0.2498721405399761, total_step : 550325, cur_epsilon : 0.34967500000000007\n",
      "episode : 19640, reward mean : 0.24990988385241455, total_step : 550611, cur_epsilon : 0.34938900000000006\n",
      "episode : 19650, reward mean : 0.25019440808672333, total_step : 550812, cur_epsilon : 0.34918800000000005\n",
      "[EVAL] episode : 19650, reward mean : 0.6380000080913305, step mean : 37.2,\n",
      "episode : 19660, reward mean : 0.250182101676773, total_step : 551095, cur_epsilon : 0.348905\n",
      "episode : 19670, reward mean : 0.25045145495752574, total_step : 551325, cur_epsilon : 0.34867500000000007\n",
      "episode : 19680, reward mean : 0.25056403044145553, total_step : 551663, cur_epsilon : 0.348337\n",
      "episode : 19690, reward mean : 0.25063890904801156, total_step : 551974, cur_epsilon : 0.34802600000000006\n",
      "episode : 19700, reward mean : 0.25063655427699616, total_step : 552437, cur_epsilon : 0.34756300000000007\n",
      "[EVAL] episode : 19700, reward mean : 0.6500000078231096, step mean : 36.0,\n",
      "episode : 19710, reward mean : 0.2506752977837567, total_step : 552720, cur_epsilon : 0.34728000000000003\n",
      "episode : 19720, reward mean : 0.25098428597105155, total_step : 552870, cur_epsilon : 0.34713000000000005\n",
      "episode : 19730, reward mean : 0.2512681256660609, total_step : 553069, cur_epsilon : 0.346931\n",
      "episode : 19740, reward mean : 0.25128470716626206, total_step : 553393, cur_epsilon : 0.346607\n",
      "episode : 19750, reward mean : 0.25124608200175474, total_step : 553727, cur_epsilon : 0.34627300000000005\n",
      "[EVAL] episode : 19750, reward mean : 0.6780000071972608, step mean : 33.2,\n",
      "episode : 19760, reward mean : 0.25122672670260676, total_step : 554223, cur_epsilon : 0.345777\n",
      "episode : 19770, reward mean : 0.25128933331897585, total_step : 554557, cur_epsilon : 0.34544300000000006\n",
      "episode : 19780, reward mean : 0.25148787258825706, total_step : 554923, cur_epsilon : 0.34507699999999997\n",
      "synced target net\n",
      "episode : 19790, reward mean : 0.2516083941316329, total_step : 555243, cur_epsilon : 0.344757\n",
      "episode : 19800, reward mean : 0.25172071312817584, total_step : 555579, cur_epsilon : 0.344421\n",
      "[EVAL] episode : 19800, reward mean : 0.35200000554323196, step mean : 25.8,\n",
      "episode : 19810, reward mean : 0.25180969813444637, total_step : 556060, cur_epsilon : 0.34394\n",
      "episode : 19820, reward mean : 0.252059541881934, total_step : 556323, cur_epsilon : 0.343677\n",
      "episode : 19830, reward mean : 0.2522808936040847, total_step : 556642, cur_epsilon : 0.34335800000000005\n",
      "episode : 19840, reward mean : 0.252322586705052, total_step : 556917, cur_epsilon : 0.343083\n",
      "episode : 19850, reward mean : 0.2523979909468433, total_step : 557224, cur_epsilon : 0.34277599999999997\n",
      "[EVAL] episode : 19850, reward mean : 0.36600000523030757, step mean : 24.4,\n",
      "episode : 19860, reward mean : 0.25255791139709205, total_step : 557464, cur_epsilon : 0.34253600000000006\n",
      "episode : 19870, reward mean : 0.2527664881942969, total_step : 557807, cur_epsilon : 0.34219299999999997\n",
      "episode : 19880, reward mean : 0.2528118772887563, total_step : 558173, cur_epsilon : 0.341827\n",
      "episode : 19890, reward mean : 0.2528632539254695, total_step : 558527, cur_epsilon : 0.341473\n",
      "episode : 19900, reward mean : 0.25302161410310536, total_step : 558868, cur_epsilon : 0.341132\n",
      "[EVAL] episode : 19900, reward mean : -0.305999993160367, step mean : 31.4,\n",
      "episode : 19910, reward mean : 0.2533169322293991, total_step : 559037, cur_epsilon : 0.340963\n",
      "episode : 19920, reward mean : 0.2534764116836607, total_step : 559276, cur_epsilon : 0.340724\n",
      "episode : 19930, reward mean : 0.25357802914052724, total_step : 559430, cur_epsilon : 0.34057000000000004\n",
      "episode : 19940, reward mean : 0.25385105921844825, total_step : 559642, cur_epsilon : 0.34035800000000005\n",
      "episode : 19950, reward mean : 0.2539619108203584, total_step : 559876, cur_epsilon : 0.340124\n",
      "[EVAL] episode : 19950, reward mean : 0.7640000052750111, step mean : 24.6,\n",
      "synced target net\n",
      "episode : 19960, reward mean : 0.25408868341452867, total_step : 560278, cur_epsilon : 0.33972199999999997\n",
      "episode : 19970, reward mean : 0.25416475317963716, total_step : 560482, cur_epsilon : 0.339518\n",
      "episode : 19980, reward mean : 0.25422072677926216, total_step : 560726, cur_epsilon : 0.3392740000000001\n",
      "episode : 19990, reward mean : 0.25427214212626176, total_step : 560979, cur_epsilon : 0.339021\n",
      "episode : 20000, reward mean : 0.2542540060583502, total_step : 561270, cur_epsilon : 0.33873\n",
      "[EVAL] episode : 20000, reward mean : 0.10600000210106372, step mean : 10.4,\n",
      "episode : 20010, reward mean : 0.2543013553834636, total_step : 561531, cur_epsilon : 0.338469\n",
      "episode : 20020, reward mean : 0.2545324735907274, total_step : 561824, cur_epsilon : 0.33817600000000003\n",
      "episode : 20030, reward mean : 0.2547928168415592, total_step : 562058, cur_epsilon : 0.3379420000000001\n",
      "episode : 20040, reward mean : 0.2549491078548223, total_step : 562399, cur_epsilon : 0.33760100000000004\n",
      "episode : 20050, reward mean : 0.25502843498626254, total_step : 562694, cur_epsilon : 0.337306\n",
      "[EVAL] episode : 20050, reward mean : -0.12199999280273914, step mean : 33.2,\n",
      "episode : 20060, reward mean : 0.25509721443385763, total_step : 563010, cur_epsilon : 0.33699\n",
      "episode : 20070, reward mean : 0.2550588002786649, total_step : 563242, cur_epsilon : 0.336758\n",
      "episode : 20080, reward mean : 0.2551389502816874, total_step : 563535, cur_epsilon : 0.336465\n",
      "episode : 20090, reward mean : 0.25516128032470575, total_step : 563845, cur_epsilon : 0.336155\n",
      "episode : 20100, reward mean : 0.2551039861582813, total_step : 564115, cur_epsilon : 0.335885\n",
      "[EVAL] episode : 20100, reward mean : 0.37200000509619713, step mean : 23.8,\n",
      "episode : 20110, reward mean : 0.25514421292095785, total_step : 564389, cur_epsilon : 0.335611\n",
      "episode : 20120, reward mean : 0.25519384303765286, total_step : 564743, cur_epsilon : 0.335257\n",
      "synced target net\n",
      "episode : 20130, reward mean : 0.2552737268754797, total_step : 565137, cur_epsilon : 0.334863\n",
      "episode : 20140, reward mean : 0.2552467786529413, total_step : 565445, cur_epsilon : 0.33455500000000005\n",
      "episode : 20150, reward mean : 0.2553151425379086, total_step : 565761, cur_epsilon : 0.33423900000000006\n",
      "[EVAL] episode : 20150, reward mean : 0.5120000019669533, step mean : 9.8,\n",
      "episode : 20160, reward mean : 0.25547272431496015, total_step : 565998, cur_epsilon : 0.334002\n",
      "episode : 20170, reward mean : 0.2556569222732086, total_step : 566280, cur_epsilon : 0.33372\n",
      "episode : 20180, reward mean : 0.2559113043757309, total_step : 566521, cur_epsilon : 0.33347899999999997\n",
      "episode : 20190, reward mean : 0.25597375544307477, total_step : 566949, cur_epsilon : 0.333051\n",
      "episode : 20200, reward mean : 0.25596881794429727, total_step : 567311, cur_epsilon : 0.332689\n",
      "[EVAL] episode : 20200, reward mean : 0.8160000041127204, step mean : 19.4,\n",
      "episode : 20210, reward mean : 0.2558159387694881, total_step : 567574, cur_epsilon : 0.332426\n",
      "episode : 20220, reward mean : 0.25596439775400065, total_step : 567828, cur_epsilon : 0.332172\n",
      "episode : 20230, reward mean : 0.25611172133667215, total_step : 568084, cur_epsilon : 0.331916\n",
      "episode : 20240, reward mean : 0.25634783214922596, total_step : 568360, cur_epsilon : 0.33164000000000005\n",
      "episode : 20250, reward mean : 0.2565224751971754, total_step : 568560, cur_epsilon : 0.33144000000000007\n",
      "[EVAL] episode : 20250, reward mean : 0.21800000853836538, step mean : 39.2,\n",
      "episode : 20260, reward mean : 0.2565696013227164, total_step : 568818, cur_epsilon : 0.331182\n",
      "episode : 20270, reward mean : 0.2567375492276077, total_step : 569031, cur_epsilon : 0.33096900000000007\n",
      "episode : 20280, reward mean : 0.2566577969877604, total_step : 569346, cur_epsilon : 0.330654\n",
      "episode : 20290, reward mean : 0.2565840376047128, total_step : 569748, cur_epsilon : 0.330252\n",
      "synced target net\n",
      "episode : 20300, reward mean : 0.25663301098788976, total_step : 570002, cur_epsilon : 0.329998\n",
      "[EVAL] episode : 20300, reward mean : 0.46200000308454037, step mean : 14.8,\n",
      "episode : 20310, reward mean : 0.2568611582036803, total_step : 570292, cur_epsilon : 0.329708\n",
      "episode : 20320, reward mean : 0.2570910493690176, total_step : 570578, cur_epsilon : 0.329422\n",
      "episode : 20330, reward mean : 0.257101334149061, total_step : 570910, cur_epsilon : 0.32909\n",
      "episode : 20340, reward mean : 0.25711996673149373, total_step : 571225, cur_epsilon : 0.32877500000000004\n",
      "episode : 20350, reward mean : 0.2573744532367997, total_step : 571460, cur_epsilon : 0.32854000000000005\n",
      "[EVAL] episode : 20350, reward mean : 0.42000000849366187, step mean : 38.8,\n",
      "episode : 20360, reward mean : 0.2574518724672189, total_step : 571754, cur_epsilon : 0.32824600000000004\n",
      "episode : 20370, reward mean : 0.25746441450569857, total_step : 571980, cur_epsilon : 0.32802\n",
      "episode : 20380, reward mean : 0.257476944237092, total_step : 572307, cur_epsilon : 0.327693\n",
      "episode : 20390, reward mean : 0.257659643139396, total_step : 572586, cur_epsilon : 0.327414\n",
      "episode : 20400, reward mean : 0.2576553982201343, total_step : 572947, cur_epsilon : 0.32705300000000004\n",
      "[EVAL] episode : 20400, reward mean : 0.004000008851289749, step mean : 40.4,\n",
      "episode : 20410, reward mean : 0.25783489092478107, total_step : 573333, cur_epsilon : 0.32666700000000004\n",
      "episode : 20420, reward mean : 0.25787267991461144, total_step : 573707, cur_epsilon : 0.32629300000000006\n",
      "episode : 20430, reward mean : 0.25802741673553015, total_step : 573943, cur_epsilon : 0.32605700000000004\n",
      "episode : 20440, reward mean : 0.2581927653597103, total_step : 574157, cur_epsilon : 0.325843\n",
      "episode : 20450, reward mean : 0.2582234779469359, total_step : 574446, cur_epsilon : 0.325554\n",
      "[EVAL] episode : 20450, reward mean : -0.013999995216727257, step mean : 22.4,\n",
      "episode : 20460, reward mean : 0.25836804125463464, total_step : 574702, cur_epsilon : 0.325298\n",
      "synced target net\n",
      "episode : 20470, reward mean : 0.2584142708425689, total_step : 575058, cur_epsilon : 0.32494200000000006\n",
      "episode : 20480, reward mean : 0.25832568966025066, total_step : 575490, cur_epsilon : 0.3245100000000001\n",
      "episode : 20490, reward mean : 0.2583474926466896, total_step : 575896, cur_epsilon : 0.32410400000000006\n",
      "episode : 20500, reward mean : 0.25845171338556017, total_step : 576234, cur_epsilon : 0.323766\n",
      "[EVAL] episode : 20500, reward mean : 0.21200000867247581, step mean : 39.8,\n",
      "episode : 20510, reward mean : 0.2586104400025482, total_step : 576460, cur_epsilon : 0.32354000000000005\n",
      "episode : 20520, reward mean : 0.2588689144492897, total_step : 576681, cur_epsilon : 0.323319\n",
      "episode : 20530, reward mean : 0.25892207133713013, total_step : 576923, cur_epsilon : 0.32307700000000006\n",
      "episode : 20540, reward mean : 0.2590793634177219, total_step : 577151, cur_epsilon : 0.32284900000000005\n",
      "episode : 20550, reward mean : 0.25938054134439303, total_step : 577283, cur_epsilon : 0.32271700000000003\n",
      "[EVAL] episode : 20550, reward mean : 0.27600000724196433, step mean : 33.4,\n",
      "episode : 20560, reward mean : 0.25944018116128426, total_step : 577511, cur_epsilon : 0.322489\n",
      "episode : 20570, reward mean : 0.259708805285271, total_step : 577709, cur_epsilon : 0.322291\n",
      "episode : 20580, reward mean : 0.2597677417293162, total_step : 578037, cur_epsilon : 0.321963\n",
      "episode : 20590, reward mean : 0.2598907297160504, total_step : 578334, cur_epsilon : 0.321666\n",
      "episode : 20600, reward mean : 0.26000922936611603, total_step : 578739, cur_epsilon : 0.321261\n",
      "[EVAL] episode : 20600, reward mean : 0.7220000062137842, step mean : 28.8,\n",
      "episode : 20610, reward mean : 0.2601280992240625, total_step : 579044, cur_epsilon : 0.320956\n",
      "episode : 20620, reward mean : 0.2601120332246023, total_step : 579425, cur_epsilon : 0.32057500000000005\n",
      "episode : 20630, reward mean : 0.26028115003073155, total_step : 579626, cur_epsilon : 0.32037400000000005\n",
      "episode : 20640, reward mean : 0.26029506420559617, total_step : 579947, cur_epsilon : 0.32005300000000003\n",
      "synced target net\n",
      "episode : 20650, reward mean : 0.26031574456512785, total_step : 580254, cur_epsilon : 0.319746\n",
      "[EVAL] episode : 20650, reward mean : 0.12800001055002214, step mean : 48.2,\n",
      "episode : 20660, reward mean : 0.26033979309569516, total_step : 580653, cur_epsilon : 0.31934700000000005\n",
      "episode : 20670, reward mean : 0.26052540519554573, total_step : 580819, cur_epsilon : 0.31918100000000005\n",
      "episode : 20680, reward mean : 0.26078143740033216, total_step : 581039, cur_epsilon : 0.31896100000000005\n",
      "episode : 20690, reward mean : 0.2607815430408236, total_step : 581388, cur_epsilon : 0.318612\n",
      "episode : 20700, reward mean : 0.26079469205829614, total_step : 581809, cur_epsilon : 0.318191\n",
      "[EVAL] episode : 20700, reward mean : 0.7320000059902668, step mean : 27.8,\n",
      "episode : 20710, reward mean : 0.2609531687927859, total_step : 582230, cur_epsilon : 0.31777\n",
      "episode : 20720, reward mean : 0.2609845620551251, total_step : 582613, cur_epsilon : 0.317387\n",
      "episode : 20730, reward mean : 0.2610511396936551, total_step : 582923, cur_epsilon : 0.31707700000000005\n",
      "episode : 20740, reward mean : 0.26119190578132495, total_step : 583180, cur_epsilon : 0.31682\n",
      "episode : 20750, reward mean : 0.2610135000469275, total_step : 583497, cur_epsilon : 0.316503\n",
      "[EVAL] episode : 20750, reward mean : 0.15600000992417334, step mean : 45.4,\n",
      "episode : 20760, reward mean : 0.26121002533948606, total_step : 583838, cur_epsilon : 0.31616200000000005\n",
      "episode : 20770, reward mean : 0.2614352492107994, total_step : 584119, cur_epsilon : 0.31588099999999997\n",
      "episode : 20780, reward mean : 0.26145717642907346, total_step : 584521, cur_epsilon : 0.31547900000000006\n",
      "episode : 20790, reward mean : 0.2615108285833037, total_step : 584758, cur_epsilon : 0.315242\n",
      "synced target net\n",
      "episode : 20800, reward mean : 0.26174808299527147, total_step : 585013, cur_epsilon : 0.314987\n",
      "[EVAL] episode : 20800, reward mean : 0.3920000046491623, step mean : 21.8,\n",
      "episode : 20810, reward mean : 0.26180971294322847, total_step : 585233, cur_epsilon : 0.314767\n",
      "episode : 20820, reward mean : 0.26191739320082236, total_step : 585656, cur_epsilon : 0.31434400000000007\n",
      "episode : 20830, reward mean : 0.2620710574428493, total_step : 586084, cur_epsilon : 0.313916\n",
      "episode : 20840, reward mean : 0.26200528438667803, total_step : 586468, cur_epsilon : 0.31353200000000003\n",
      "episode : 20850, reward mean : 0.26217458641161023, total_step : 586762, cur_epsilon : 0.313238\n",
      "[EVAL] episode : 20850, reward mean : 0.44000000804662703, step mean : 36.8,\n",
      "episode : 20860, reward mean : 0.2622243589049246, total_step : 587105, cur_epsilon : 0.31289500000000003\n",
      "episode : 20870, reward mean : 0.26231385370449595, total_step : 587365, cur_epsilon : 0.312635\n",
      "episode : 20880, reward mean : 0.2626048911322743, total_step : 587505, cur_epsilon : 0.31249499999999997\n",
      "episode : 20890, reward mean : 0.26262135600440234, total_step : 587917, cur_epsilon : 0.312083\n",
      "episode : 20900, reward mean : 0.262786608947236, total_step : 588218, cur_epsilon : 0.311782\n",
      "[EVAL] episode : 20900, reward mean : 0.020000012964010237, step mean : 58.6,\n",
      "episode : 20910, reward mean : 0.26296652927201924, total_step : 588589, cur_epsilon : 0.311411\n",
      "episode : 20920, reward mean : 0.263023428641078, total_step : 589015, cur_epsilon : 0.31098500000000007\n",
      "episode : 20930, reward mean : 0.2631567189311524, total_step : 589283, cur_epsilon : 0.310717\n",
      "episode : 20940, reward mean : 0.2632139506818301, total_step : 589510, cur_epsilon : 0.31049000000000004\n",
      "episode : 20950, reward mean : 0.26337470774807764, total_step : 589720, cur_epsilon : 0.31028\n",
      "[EVAL] episode : 20950, reward mean : -0.13799998797476293, step mean : 54.6,\n",
      "episode : 20960, reward mean : 0.2634508648550472, total_step : 589907, cur_epsilon : 0.31009300000000006\n",
      "synced target net\n",
      "episode : 20970, reward mean : 0.26354507045481934, total_step : 590254, cur_epsilon : 0.3097460000000001\n",
      "episode : 20980, reward mean : 0.2635076323894338, total_step : 590677, cur_epsilon : 0.309323\n",
      "episode : 20990, reward mean : 0.2635416925959768, total_step : 590952, cur_epsilon : 0.309048\n",
      "episode : 21000, reward mean : 0.2636485775075853, total_step : 591274, cur_epsilon : 0.30872600000000006\n",
      "[EVAL] episode : 21000, reward mean : 0.8980000022798776, step mean : 11.2,\n",
      "episode : 21010, reward mean : 0.2636592159795086, total_step : 591598, cur_epsilon : 0.30840200000000006\n",
      "episode : 21020, reward mean : 0.26374548657511054, total_step : 591963, cur_epsilon : 0.308037\n",
      "episode : 21030, reward mean : 0.2637570198705926, total_step : 592285, cur_epsilon : 0.30771500000000007\n",
      "episode : 21040, reward mean : 0.2638065650167666, total_step : 592626, cur_epsilon : 0.30737400000000004\n",
      "episode : 21050, reward mean : 0.2638014312592864, total_step : 592882, cur_epsilon : 0.307118\n",
      "[EVAL] episode : 21050, reward mean : 0.5860000092536211, step mean : 42.4,\n",
      "episode : 21060, reward mean : 0.2638675274489251, total_step : 593188, cur_epsilon : 0.306812\n",
      "episode : 21070, reward mean : 0.2637275808339202, total_step : 593627, cur_epsilon : 0.306373\n",
      "episode : 21080, reward mean : 0.2636760022887957, total_step : 593981, cur_epsilon : 0.30601900000000004\n",
      "episode : 21090, reward mean : 0.26385728441463185, total_step : 594244, cur_epsilon : 0.30575600000000003\n",
      "episode : 21100, reward mean : 0.26402465063248765, total_step : 594437, cur_epsilon : 0.30556300000000003\n",
      "[EVAL] episode : 21100, reward mean : 0.09400000683963299, step mean : 31.4,\n",
      "episode : 21110, reward mean : 0.2641184333701274, total_step : 594884, cur_epsilon : 0.30511600000000005\n",
      "synced target net\n",
      "episode : 21120, reward mean : 0.26426657805471526, total_step : 595216, cur_epsilon : 0.30478400000000005\n",
      "episode : 21130, reward mean : 0.264303839498125, total_step : 595582, cur_epsilon : 0.3044180000000001\n",
      "episode : 21140, reward mean : 0.26444418773278877, total_step : 595930, cur_epsilon : 0.30407000000000006\n",
      "episode : 21150, reward mean : 0.26450024249476883, total_step : 596355, cur_epsilon : 0.30364500000000005\n",
      "[EVAL] episode : 21150, reward mean : 0.7900000046938658, step mean : 22.0,\n",
      "episode : 21160, reward mean : 0.26463516676844234, total_step : 596615, cur_epsilon : 0.303385\n",
      "episode : 21170, reward mean : 0.26479594373576654, total_step : 596919, cur_epsilon : 0.30308100000000004\n",
      "episode : 21180, reward mean : 0.26493248956285004, total_step : 597175, cur_epsilon : 0.302825\n",
      "episode : 21190, reward mean : 0.26519915662950266, total_step : 597355, cur_epsilon : 0.30264500000000005\n",
      "episode : 21200, reward mean : 0.26535896835119444, total_step : 597660, cur_epsilon : 0.30234000000000005\n",
      "[EVAL] episode : 21200, reward mean : 0.7400000058114529, step mean : 27.0,\n",
      "episode : 21210, reward mean : 0.2654356496513965, total_step : 597941, cur_epsilon : 0.3020590000000001\n",
      "episode : 21220, reward mean : 0.26556362531513933, total_step : 598313, cur_epsilon : 0.30168700000000004\n",
      "episode : 21230, reward mean : 0.2655016547004906, total_step : 598787, cur_epsilon : 0.30121300000000006\n",
      "episode : 21240, reward mean : 0.265562152983042, total_step : 599102, cur_epsilon : 0.300898\n",
      "episode : 21250, reward mean : 0.2653901237398386, total_step : 599610, cur_epsilon : 0.30039000000000005\n",
      "[EVAL] episode : 21250, reward mean : 0.8040000043809414, step mean : 20.6,\n",
      "synced target net\n",
      "episode : 21260, reward mean : 0.2654797803184766, total_step : 600063, cur_epsilon : 0.299937\n",
      "episode : 21270, reward mean : 0.2655402035582469, total_step : 600578, cur_epsilon : 0.2994220000000001\n",
      "episode : 21280, reward mean : 0.2656315850451084, total_step : 600926, cur_epsilon : 0.29907400000000006\n",
      "episode : 21290, reward mean : 0.2659013682384063, total_step : 601096, cur_epsilon : 0.29890400000000006\n",
      "episode : 21300, reward mean : 0.26610657886571376, total_step : 601302, cur_epsilon : 0.298698\n",
      "[EVAL] episode : 21300, reward mean : 0.4280000038444996, step mean : 18.2,\n",
      "episode : 21310, reward mean : 0.2662815640486011, total_step : 601473, cur_epsilon : 0.298527\n",
      "episode : 21320, reward mean : 0.26638462147963765, total_step : 601797, cur_epsilon : 0.298203\n",
      "episode : 21330, reward mean : 0.26646226582410354, total_step : 602173, cur_epsilon : 0.29782700000000006\n",
      "episode : 21340, reward mean : 0.26658201172141377, total_step : 602661, cur_epsilon : 0.297339\n",
      "episode : 21350, reward mean : 0.2665564463793603, total_step : 602958, cur_epsilon : 0.29704200000000003\n",
      "[EVAL] episode : 21350, reward mean : 0.2960000067949295, step mean : 31.4,\n",
      "episode : 21360, reward mean : 0.26646348924606283, total_step : 603399, cur_epsilon : 0.296601\n",
      "episode : 21370, reward mean : 0.2664819901911076, total_step : 603802, cur_epsilon : 0.2961980000000001\n",
      "episode : 21380, reward mean : 0.2664789583917334, total_step : 603952, cur_epsilon : 0.296048\n",
      "episode : 21390, reward mean : 0.2665867288688798, total_step : 604364, cur_epsilon : 0.295636\n",
      "episode : 21400, reward mean : 0.26645935189701336, total_step : 604778, cur_epsilon : 0.295222\n",
      "[EVAL] episode : 21400, reward mean : 0.7920000046491623, step mean : 21.8,\n",
      "synced target net\n",
      "episode : 21410, reward mean : 0.2663918790613231, total_step : 605266, cur_epsilon : 0.29473400000000005\n",
      "episode : 21420, reward mean : 0.26656629928752057, total_step : 605436, cur_epsilon : 0.29456400000000005\n",
      "episode : 21430, reward mean : 0.2667027592530358, total_step : 605687, cur_epsilon : 0.29431300000000005\n",
      "episode : 21440, reward mean : 0.26673414789421707, total_step : 605963, cur_epsilon : 0.294037\n",
      "episode : 21450, reward mean : 0.26693100843457007, total_step : 606284, cur_epsilon : 0.293716\n",
      "[EVAL] episode : 21450, reward mean : 0.32600000612437724, step mean : 28.4,\n",
      "episode : 21460, reward mean : 0.26708155316825805, total_step : 606603, cur_epsilon : 0.293397\n",
      "episode : 21470, reward mean : 0.26721006665338903, total_step : 606870, cur_epsilon : 0.29313\n",
      "episode : 21480, reward mean : 0.26740736178386076, total_step : 607189, cur_epsilon : 0.29281100000000004\n",
      "episode : 21490, reward mean : 0.26749372411330335, total_step : 607546, cur_epsilon : 0.292454\n",
      "episode : 21500, reward mean : 0.2675976805249446, total_step : 607964, cur_epsilon : 0.2920360000000001\n",
      "[EVAL] episode : 21500, reward mean : 0.3960000090301037, step mean : 41.2,\n",
      "episode : 21510, reward mean : 0.2677085137774069, total_step : 608268, cur_epsilon : 0.291732\n",
      "episode : 21520, reward mean : 0.267780675251577, total_step : 608554, cur_epsilon : 0.291446\n",
      "episode : 21530, reward mean : 0.26784208692433775, total_step : 608863, cur_epsilon : 0.291137\n",
      "episode : 21540, reward mean : 0.26789276376859517, total_step : 609294, cur_epsilon : 0.290706\n",
      "episode : 21550, reward mean : 0.268061723044691, total_step : 609472, cur_epsilon : 0.290528\n",
      "[EVAL] episode : 21550, reward mean : -0.20799999088048934, step mean : 41.8,\n",
      "episode : 21560, reward mean : 0.2681442547167076, total_step : 609836, cur_epsilon : 0.290164\n",
      "synced target net\n",
      "episode : 21570, reward mean : 0.26828976039596397, total_step : 610064, cur_epsilon : 0.28993599999999997\n",
      "episode : 21580, reward mean : 0.26844949637754606, total_step : 610461, cur_epsilon : 0.289539\n",
      "episode : 21590, reward mean : 0.2685789778557722, total_step : 610822, cur_epsilon : 0.28917800000000005\n",
      "episode : 21600, reward mean : 0.2687458394439374, total_step : 611203, cur_epsilon : 0.28879699999999997\n",
      "[EVAL] episode : 21600, reward mean : 0.5000000022351742, step mean : 11.0,\n",
      "episode : 21610, reward mean : 0.2687121764024524, total_step : 611516, cur_epsilon : 0.2884840000000001\n",
      "episode : 21620, reward mean : 0.268953290106565, total_step : 611736, cur_epsilon : 0.2882640000000001\n",
      "episode : 21630, reward mean : 0.2691706025040348, total_step : 612007, cur_epsilon : 0.28799300000000005\n",
      "episode : 21640, reward mean : 0.2693701539847091, total_step : 612316, cur_epsilon : 0.28768400000000005\n",
      "episode : 21650, reward mean : 0.26940693451767017, total_step : 612676, cur_epsilon : 0.287324\n",
      "[EVAL] episode : 21650, reward mean : 0.02600000835955143, step mean : 38.2,\n",
      "episode : 21660, reward mean : 0.2696302000167483, total_step : 612933, cur_epsilon : 0.28706700000000007\n",
      "episode : 21670, reward mean : 0.26978865401139446, total_step : 613229, cur_epsilon : 0.286771\n",
      "episode : 21680, reward mean : 0.2697818326811062, total_step : 613683, cur_epsilon : 0.28631700000000004\n",
      "episode : 21690, reward mean : 0.2698658429047564, total_step : 614041, cur_epsilon : 0.2859590000000001\n",
      "episode : 21700, reward mean : 0.26996544390307387, total_step : 614464, cur_epsilon : 0.285536\n",
      "[EVAL] episode : 21700, reward mean : 0.4780000027269125, step mean : 13.2,\n",
      "episode : 21710, reward mean : 0.27014556115948757, total_step : 614813, cur_epsilon : 0.28518699999999997\n",
      "synced target net\n",
      "episode : 21720, reward mean : 0.27022330261651994, total_step : 615083, cur_epsilon : 0.284917\n",
      "episode : 21730, reward mean : 0.27042246354796123, total_step : 615390, cur_epsilon : 0.28461000000000003\n",
      "episode : 21740, reward mean : 0.27062328118503626, total_step : 615693, cur_epsilon : 0.284307\n",
      "episode : 21750, reward mean : 0.2708593164602744, total_step : 615919, cur_epsilon : 0.28408100000000003\n",
      "[EVAL] episode : 21750, reward mean : 0.29200000688433647, step mean : 31.8,\n",
      "episode : 21760, reward mean : 0.2711148958202958, total_step : 616102, cur_epsilon : 0.283898\n",
      "episode : 21770, reward mean : 0.2712462164963851, total_step : 616454, cur_epsilon : 0.2835460000000001\n",
      "episode : 21780, reward mean : 0.271316351386042, total_step : 616739, cur_epsilon : 0.283261\n",
      "episode : 21790, reward mean : 0.2714584733024, total_step : 617067, cur_epsilon : 0.282933\n",
      "episode : 21800, reward mean : 0.27125505198818556, total_step : 617447, cur_epsilon : 0.28255300000000005\n",
      "[EVAL] episode : 21800, reward mean : -0.023999990522861482, step mean : 43.2,\n",
      "episode : 21810, reward mean : 0.2712847378909424, total_step : 617721, cur_epsilon : 0.28227900000000006\n",
      "episode : 21820, reward mean : 0.2714161381052037, total_step : 617973, cur_epsilon : 0.282027\n",
      "episode : 21830, reward mean : 0.27146908536534614, total_step : 618295, cur_epsilon : 0.281705\n",
      "episode : 21840, reward mean : 0.27148077534869886, total_step : 618707, cur_epsilon : 0.281293\n",
      "episode : 21850, reward mean : 0.27161831275361764, total_step : 618945, cur_epsilon : 0.28105500000000005\n",
      "[EVAL] episode : 21850, reward mean : 0.2760000117123127, step mean : 53.2,\n",
      "episode : 21860, reward mean : 0.27184996037127934, total_step : 619177, cur_epsilon : 0.28082300000000004\n",
      "episode : 21870, reward mean : 0.2719213595701561, total_step : 619557, cur_epsilon : 0.280443\n",
      "episode : 21880, reward mean : 0.27212477759883774, total_step : 619850, cur_epsilon : 0.28015\n",
      "synced target net\n",
      "episode : 21890, reward mean : 0.2723115639074405, total_step : 620179, cur_epsilon : 0.279821\n",
      "episode : 21900, reward mean : 0.2724082252970093, total_step : 620505, cur_epsilon : 0.27949500000000005\n",
      "[EVAL] episode : 21900, reward mean : -0.2659999940544367, step mean : 27.4,\n",
      "episode : 21910, reward mean : 0.27240666974444727, total_step : 620945, cur_epsilon : 0.27905500000000005\n",
      "episode : 21920, reward mean : 0.2725091302094396, total_step : 621357, cur_epsilon : 0.278643\n",
      "episode : 21930, reward mean : 0.27258778542044076, total_step : 621722, cur_epsilon : 0.278278\n",
      "episode : 21940, reward mean : 0.27251778187581704, total_step : 622112, cur_epsilon : 0.277888\n",
      "episode : 21950, reward mean : 0.2726095733234918, total_step : 622547, cur_epsilon : 0.27745300000000006\n",
      "[EVAL] episode : 21950, reward mean : 0.7680000051856041, step mean : 24.2,\n",
      "episode : 21960, reward mean : 0.27267805712818355, total_step : 622934, cur_epsilon : 0.27706600000000003\n",
      "episode : 21970, reward mean : 0.2727341891044702, total_step : 623346, cur_epsilon : 0.27665400000000007\n",
      "episode : 21980, reward mean : 0.27270928729245875, total_step : 623637, cur_epsilon : 0.276363\n",
      "episode : 21990, reward mean : 0.2728876823448441, total_step : 623982, cur_epsilon : 0.276018\n",
      "episode : 22000, reward mean : 0.27288455158455127, total_step : 624425, cur_epsilon : 0.275575\n",
      "[EVAL] episode : 22000, reward mean : 0.8040000043809414, step mean : 20.6,\n",
      "episode : 22010, reward mean : 0.2729677480672135, total_step : 624878, cur_epsilon : 0.275122\n",
      "synced target net\n",
      "episode : 22020, reward mean : 0.2731571360130022, total_step : 625097, cur_epsilon : 0.274903\n",
      "episode : 22030, reward mean : 0.2731974641435251, total_step : 625444, cur_epsilon : 0.274556\n",
      "episode : 22040, reward mean : 0.2733430188352043, total_step : 625660, cur_epsilon : 0.27434000000000003\n",
      "episode : 22050, reward mean : 0.2733959244977252, total_step : 625880, cur_epsilon : 0.27412000000000003\n",
      "[EVAL] episode : 22050, reward mean : -0.015999995172023773, step mean : 22.6,\n",
      "episode : 22060, reward mean : 0.2734660079446051, total_step : 626262, cur_epsilon : 0.27373800000000004\n",
      "episode : 22070, reward mean : 0.2733923939883365, total_step : 626561, cur_epsilon : 0.273439\n",
      "episode : 22080, reward mean : 0.2733410387408778, total_step : 626910, cur_epsilon : 0.27309000000000005\n",
      "episode : 22090, reward mean : 0.27331598621374437, total_step : 627201, cur_epsilon : 0.272799\n",
      "episode : 22100, reward mean : 0.2732886939162043, total_step : 627598, cur_epsilon : 0.27240200000000003\n",
      "[EVAL] episode : 22100, reward mean : 0.5300000015646219, step mean : 8.0,\n",
      "episode : 22110, reward mean : 0.2732980613133814, total_step : 628013, cur_epsilon : 0.271987\n",
      "episode : 22120, reward mean : 0.2733422303657346, total_step : 628252, cur_epsilon : 0.271748\n",
      "episode : 22130, reward mean : 0.2732404037853017, total_step : 628612, cur_epsilon : 0.2713880000000001\n",
      "episode : 22140, reward mean : 0.27333695283925197, total_step : 629034, cur_epsilon : 0.27096600000000004\n",
      "episode : 22150, reward mean : 0.2734176133616783, total_step : 629491, cur_epsilon : 0.270509\n",
      "[EVAL] episode : 22150, reward mean : 0.7800000049173832, step mean : 23.0,\n",
      "episode : 22160, reward mean : 0.27348827328616987, total_step : 629770, cur_epsilon : 0.27022999999999997\n",
      "synced target net\n",
      "episode : 22170, reward mean : 0.27364411980533754, total_step : 630060, cur_epsilon : 0.26994000000000007\n",
      "episode : 22180, reward mean : 0.273647887113515, total_step : 630487, cur_epsilon : 0.269513\n",
      "episode : 22190, reward mean : 0.27375845589287623, total_step : 630877, cur_epsilon : 0.269123\n",
      "episode : 22200, reward mean : 0.27385631244738695, total_step : 631196, cur_epsilon : 0.26880400000000004\n",
      "[EVAL] episode : 22200, reward mean : 0.7760000050067901, step mean : 23.4,\n",
      "episode : 22210, reward mean : 0.27401036183781274, total_step : 631590, cur_epsilon : 0.26841000000000004\n",
      "episode : 22220, reward mean : 0.27415842198499357, total_step : 631997, cur_epsilon : 0.268003\n",
      "episode : 22230, reward mean : 0.2742496687629827, total_step : 632429, cur_epsilon : 0.267571\n",
      "episode : 22240, reward mean : 0.2742985672964011, total_step : 632755, cur_epsilon : 0.26724500000000007\n",
      "episode : 22250, reward mean : 0.27436135446003984, total_step : 633050, cur_epsilon : 0.26695\n",
      "[EVAL] episode : 22250, reward mean : 0.40800000429153443, step mean : 20.2,\n",
      "episode : 22260, reward mean : 0.2742686494533893, total_step : 633491, cur_epsilon : 0.266509\n",
      "episode : 22270, reward mean : 0.2743385782180129, total_step : 633869, cur_epsilon : 0.266131\n",
      "episode : 22280, reward mean : 0.27446724133611167, total_step : 634118, cur_epsilon : 0.26588200000000006\n",
      "episode : 22290, reward mean : 0.27469897429409074, total_step : 634337, cur_epsilon : 0.265663\n",
      "episode : 22300, reward mean : 0.27476099269414495, total_step : 634633, cur_epsilon : 0.265367\n",
      "[EVAL] episode : 22300, reward mean : 0.48600000254809855, step mean : 12.4,\n",
      "episode : 22310, reward mean : 0.2750049366705464, total_step : 634824, cur_epsilon : 0.26517599999999997\n",
      "synced target net\n",
      "episode : 22320, reward mean : 0.27508378750786494, total_step : 635082, cur_epsilon : 0.264918\n",
      "episode : 22330, reward mean : 0.2751007674546371, total_step : 635478, cur_epsilon : 0.26452200000000003\n",
      "episode : 22340, reward mean : 0.27534423175033584, total_step : 635669, cur_epsilon : 0.264331\n",
      "episode : 22350, reward mean : 0.27544072203104797, total_step : 636087, cur_epsilon : 0.26391300000000006\n",
      "[EVAL] episode : 22350, reward mean : 0.39000000469386575, step mean : 22.0,\n",
      "episode : 22360, reward mean : 0.27556485408985376, total_step : 636344, cur_epsilon : 0.263656\n",
      "episode : 22370, reward mean : 0.2756902162496052, total_step : 636598, cur_epsilon : 0.263402\n",
      "episode : 22380, reward mean : 0.2755549659338298, total_step : 637033, cur_epsilon : 0.26296700000000006\n",
      "episode : 22390, reward mean : 0.2755944679612265, total_step : 637279, cur_epsilon : 0.262721\n",
      "episode : 22400, reward mean : 0.2757406311471381, total_step : 637486, cur_epsilon : 0.262514\n",
      "[EVAL] episode : 22400, reward mean : 0.45400000773370264, step mean : 35.4,\n",
      "episode : 22410, reward mean : 0.275894249787422, total_step : 637676, cur_epsilon : 0.262324\n",
      "episode : 22420, reward mean : 0.27602766002615603, total_step : 637911, cur_epsilon : 0.262089\n",
      "episode : 22430, reward mean : 0.2759527480104341, total_step : 638310, cur_epsilon : 0.26169\n",
      "episode : 22440, reward mean : 0.2760650685364165, total_step : 638691, cur_epsilon : 0.261309\n",
      "episode : 22450, reward mean : 0.27624989478958806, total_step : 639010, cur_epsilon : 0.26099000000000006\n",
      "[EVAL] episode : 22450, reward mean : 0.7320000059902668, step mean : 27.8,\n",
      "episode : 22460, reward mean : 0.276360202052001, total_step : 639296, cur_epsilon : 0.26070400000000005\n",
      "episode : 22470, reward mean : 0.276384073795861, total_step : 639774, cur_epsilon : 0.26022600000000007\n",
      "synced target net\n",
      "episode : 22480, reward mean : 0.2765124616667204, total_step : 640118, cur_epsilon : 0.25988200000000006\n",
      "episode : 22490, reward mean : 0.27651845879667253, total_step : 640438, cur_epsilon : 0.25956200000000007\n",
      "episode : 22500, reward mean : 0.27640756170857284, total_step : 640920, cur_epsilon : 0.25908\n",
      "[EVAL] episode : 22500, reward mean : 0.20200000889599323, step mean : 40.8,\n",
      "episode : 22510, reward mean : 0.2764495841198924, total_step : 641357, cur_epsilon : 0.25864300000000007\n",
      "episode : 22520, reward mean : 0.2765790470066889, total_step : 641599, cur_epsilon : 0.258401\n",
      "episode : 22530, reward mean : 0.27670129332650095, total_step : 641857, cur_epsilon : 0.258143\n",
      "episode : 22540, reward mean : 0.27676575593104935, total_step : 642045, cur_epsilon : 0.25795500000000005\n",
      "episode : 22550, reward mean : 0.2768625338703493, total_step : 642459, cur_epsilon : 0.257541\n",
      "[EVAL] episode : 22550, reward mean : 0.10000000670552253, step mean : 30.8,\n",
      "episode : 22560, reward mean : 0.277014633812793, total_step : 642649, cur_epsilon : 0.257351\n",
      "episode : 22570, reward mean : 0.27706070619894835, total_step : 643076, cur_epsilon : 0.25692400000000004\n",
      "episode : 22580, reward mean : 0.276972548228724, total_step : 643507, cur_epsilon : 0.25649299999999997\n",
      "episode : 22590, reward mean : 0.27707526069365346, total_step : 643808, cur_epsilon : 0.256192\n",
      "episode : 22600, reward mean : 0.2770743424419017, total_step : 644341, cur_epsilon : 0.25565899999999997\n",
      "[EVAL] episode : 22600, reward mean : 0.6780000071972608, step mean : 33.2,\n",
      "episode : 22610, reward mean : 0.2771928411882498, total_step : 644705, cur_epsilon : 0.25529500000000005\n",
      "synced target net\n",
      "episode : 22620, reward mean : 0.277219281138261, total_step : 645077, cur_epsilon : 0.254923\n",
      "episode : 22630, reward mean : 0.2773601475648724, total_step : 645291, cur_epsilon : 0.2547090000000001\n",
      "episode : 22640, reward mean : 0.27740327471039544, total_step : 645526, cur_epsilon : 0.254474\n",
      "episode : 22650, reward mean : 0.2773527655416635, total_step : 645872, cur_epsilon : 0.254128\n",
      "[EVAL] episode : 22650, reward mean : 0.4000000089406967, step mean : 40.8,\n",
      "episode : 22660, reward mean : 0.277458523371769, total_step : 646264, cur_epsilon : 0.2537360000000001\n",
      "episode : 22670, reward mean : 0.2775461023236306, total_step : 646598, cur_epsilon : 0.253402\n",
      "episode : 22680, reward mean : 0.2777680837621541, total_step : 646827, cur_epsilon : 0.253173\n",
      "episode : 22690, reward mean : 0.278004413387714, total_step : 647023, cur_epsilon : 0.252977\n",
      "episode : 22700, reward mean : 0.2779189488925204, total_step : 647448, cur_epsilon : 0.252552\n",
      "[EVAL] episode : 22700, reward mean : 0.3440000057220459, step mean : 26.6,\n",
      "episode : 22710, reward mean : 0.27783576133552296, total_step : 647769, cur_epsilon : 0.252231\n",
      "episode : 22720, reward mean : 0.27788600968401944, total_step : 648185, cur_epsilon : 0.251815\n",
      "episode : 22730, reward mean : 0.2778543836387588, total_step : 648587, cur_epsilon : 0.251413\n",
      "episode : 22740, reward mean : 0.27796438611124824, total_step : 648869, cur_epsilon : 0.251131\n",
      "episode : 22750, reward mean : 0.278000006165642, total_step : 649318, cur_epsilon : 0.25068200000000007\n",
      "[EVAL] episode : 22750, reward mean : 0.6080000087618828, step mean : 40.2,\n",
      "episode : 22760, reward mean : 0.27800791477653575, total_step : 649531, cur_epsilon : 0.25046900000000005\n",
      "episode : 22770, reward mean : 0.2780887193856765, total_step : 649978, cur_epsilon : 0.2500220000000001\n",
      "synced target net\n",
      "episode : 22780, reward mean : 0.277985958758287, total_step : 650443, cur_epsilon : 0.24955700000000003\n",
      "episode : 22790, reward mean : 0.2779438411840657, total_step : 650770, cur_epsilon : 0.24923000000000006\n",
      "episode : 22800, reward mean : 0.2780456202039332, total_step : 651068, cur_epsilon : 0.24893200000000004\n",
      "[EVAL] episode : 22800, reward mean : 0.3180000063031912, step mean : 29.2,\n",
      "episode : 22810, reward mean : 0.2780399009524136, total_step : 651413, cur_epsilon : 0.248587\n",
      "episode : 22820, reward mean : 0.2780162200188847, total_step : 651898, cur_epsilon : 0.24810200000000004\n",
      "episode : 22830, reward mean : 0.2779824853666287, total_step : 652305, cur_epsilon : 0.247695\n",
      "episode : 22840, reward mean : 0.2779518450538574, total_step : 652804, cur_epsilon : 0.24719599999999997\n",
      "episode : 22850, reward mean : 0.27784114403222204, total_step : 653286, cur_epsilon : 0.246714\n",
      "[EVAL] episode : 22850, reward mean : 0.7960000045597553, step mean : 21.4,\n",
      "episode : 22860, reward mean : 0.2779759466834692, total_step : 653510, cur_epsilon : 0.24649\n",
      "episode : 22870, reward mean : 0.2781464862815179, total_step : 653852, cur_epsilon : 0.24614800000000003\n",
      "episode : 22880, reward mean : 0.2781621565263217, total_step : 654148, cur_epsilon : 0.24585200000000007\n",
      "episode : 22890, reward mean : 0.27816863876762166, total_step : 654465, cur_epsilon : 0.24553500000000006\n",
      "episode : 22900, reward mean : 0.2782637616357559, total_step : 654779, cur_epsilon : 0.24522100000000002\n",
      "[EVAL] episode : 22900, reward mean : 0.14000001028180123, step mean : 46.6,\n",
      "synced target net\n",
      "episode : 22910, reward mean : 0.27830729556998823, total_step : 655011, cur_epsilon : 0.244989\n",
      "episode : 22920, reward mean : 0.27834599221565715, total_step : 655353, cur_epsilon : 0.24464700000000006\n",
      "episode : 22930, reward mean : 0.27832359972475296, total_step : 655835, cur_epsilon : 0.24416500000000008\n",
      "episode : 22940, reward mean : 0.2783317411411797, total_step : 656247, cur_epsilon : 0.243753\n",
      "episode : 22950, reward mean : 0.2783878057448433, total_step : 656549, cur_epsilon : 0.24345099999999997\n",
      "[EVAL] episode : 22950, reward mean : 0.6220000084489584, step mean : 38.8,\n",
      "episode : 22960, reward mean : 0.27830531977170603, total_step : 657068, cur_epsilon : 0.24293200000000004\n",
      "episode : 22970, reward mean : 0.27850762481566455, total_step : 657335, cur_epsilon : 0.24266500000000002\n",
      "episode : 22980, reward mean : 0.27838599399936664, total_step : 657746, cur_epsilon : 0.24225399999999997\n",
      "episode : 22990, reward mean : 0.27839235068274976, total_step : 658162, cur_epsilon : 0.241838\n",
      "episode : 23000, reward mean : 0.2785260931410543, total_step : 658386, cur_epsilon : 0.241614\n",
      "[EVAL] episode : 23000, reward mean : 0.42600000835955143, step mean : 38.2,\n",
      "episode : 23010, reward mean : 0.27854498662783084, total_step : 658674, cur_epsilon : 0.24132600000000004\n",
      "episode : 23020, reward mean : 0.27877237803434596, total_step : 658882, cur_epsilon : 0.24111800000000005\n",
      "episode : 23030, reward mean : 0.27883326714875395, total_step : 659271, cur_epsilon : 0.24072899999999997\n",
      "episode : 23040, reward mean : 0.2788324714649611, total_step : 659802, cur_epsilon : 0.24019800000000002\n",
      "synced target net\n",
      "episode : 23050, reward mean : 0.2788997892692782, total_step : 660277, cur_epsilon : 0.23972300000000002\n",
      "[EVAL] episode : 23050, reward mean : 0.7700000051409006, step mean : 24.0,\n",
      "episode : 23060, reward mean : 0.2788647069717723, total_step : 660788, cur_epsilon : 0.23921199999999998\n",
      "episode : 23070, reward mean : 0.27893195244260405, total_step : 661164, cur_epsilon : 0.23883600000000005\n",
      "episode : 23080, reward mean : 0.27903466823815887, total_step : 661557, cur_epsilon : 0.23844300000000007\n",
      "episode : 23090, reward mean : 0.2790199282370595, total_step : 661821, cur_epsilon : 0.23817900000000003\n",
      "episode : 23100, reward mean : 0.2791008719943012, total_step : 662163, cur_epsilon : 0.23783700000000008\n",
      "[EVAL] episode : 23100, reward mean : 0.694000006839633, step mean : 31.6,\n",
      "episode : 23110, reward mean : 0.2791704951590858, total_step : 662432, cur_epsilon : 0.237568\n",
      "episode : 23120, reward mean : 0.27919334529449513, total_step : 662809, cur_epsilon : 0.23719100000000004\n",
      "episode : 23130, reward mean : 0.27919888211413413, total_step : 663226, cur_epsilon : 0.23677400000000004\n",
      "episode : 23140, reward mean : 0.27931893445879813, total_step : 663578, cur_epsilon : 0.23642200000000002\n",
      "episode : 23150, reward mean : 0.27947300835671485, total_step : 663952, cur_epsilon : 0.23604800000000004\n",
      "[EVAL] episode : 23150, reward mean : 0.5920000091195107, step mean : 41.8,\n",
      "episode : 23160, reward mean : 0.2796696953159405, total_step : 664227, cur_epsilon : 0.235773\n",
      "episode : 23170, reward mean : 0.2797811887612471, total_step : 664598, cur_epsilon : 0.235402\n",
      "episode : 23180, reward mean : 0.27983046348863205, total_step : 664913, cur_epsilon : 0.23508700000000005\n",
      "synced target net\n",
      "episode : 23190, reward mean : 0.27983226148165047, total_step : 665338, cur_epsilon : 0.23466200000000004\n",
      "episode : 23200, reward mean : 0.2799461268886506, total_step : 665604, cur_epsilon : 0.23439600000000005\n",
      "[EVAL] episode : 23200, reward mean : 0.19200000911951065, step mean : 41.8,\n",
      "episode : 23210, reward mean : 0.2798750600570918, total_step : 666097, cur_epsilon : 0.23390299999999997\n",
      "episode : 23220, reward mean : 0.2799956995703466, total_step : 666547, cur_epsilon : 0.23345300000000002\n",
      "episode : 23230, reward mean : 0.2801196790399335, total_step : 666888, cur_epsilon : 0.23311199999999999\n",
      "episode : 23240, reward mean : 0.2801704020734729, total_step : 667298, cur_epsilon : 0.23270200000000008\n",
      "episode : 23250, reward mean : 0.28033549007581127, total_step : 667644, cur_epsilon : 0.232356\n",
      "[EVAL] episode : 23250, reward mean : 0.2680000074207783, step mean : 34.2,\n",
      "episode : 23260, reward mean : 0.28032287808955164, total_step : 668102, cur_epsilon : 0.23189800000000005\n",
      "episode : 23270, reward mean : 0.28012849782750837, total_step : 668484, cur_epsilon : 0.23151600000000006\n",
      "episode : 23280, reward mean : 0.2802323945235506, total_step : 668772, cur_epsilon : 0.231228\n",
      "episode : 23290, reward mean : 0.28019193407474424, total_step : 669194, cur_epsilon : 0.23080600000000007\n",
      "episode : 23300, reward mean : 0.280141207925064, total_step : 669442, cur_epsilon : 0.23055800000000004\n",
      "[EVAL] episode : 23300, reward mean : 0.11400000639259815, step mean : 29.4,\n",
      "episode : 23310, reward mean : 0.28013471234453974, total_step : 669886, cur_epsilon : 0.23011400000000004\n",
      "synced target net\n",
      "episode : 23320, reward mean : 0.2801766785944766, total_step : 670217, cur_epsilon : 0.22978300000000007\n",
      "episode : 23330, reward mean : 0.2801954627068984, total_step : 670802, cur_epsilon : 0.229198\n",
      "episode : 23340, reward mean : 0.28019280827108645, total_step : 671237, cur_epsilon : 0.22876300000000005\n",
      "episode : 23350, reward mean : 0.28026767216806303, total_step : 671590, cur_epsilon : 0.22841\n",
      "[EVAL] episode : 23350, reward mean : 0.7840000048279763, step mean : 22.6,\n",
      "episode : 23360, reward mean : 0.280140845257588, total_step : 672014, cur_epsilon : 0.22798600000000002\n",
      "episode : 23370, reward mean : 0.28003167074459023, total_step : 672397, cur_epsilon : 0.227603\n",
      "episode : 23380, reward mean : 0.2800615973216127, total_step : 672756, cur_epsilon : 0.227244\n",
      "episode : 23390, reward mean : 0.2801817078005591, total_step : 673104, cur_epsilon : 0.226896\n",
      "episode : 23400, reward mean : 0.28025898057824145, total_step : 673453, cur_epsilon : 0.22654700000000005\n",
      "[EVAL] episode : 23400, reward mean : -0.1599999874830246, step mean : 56.8,\n",
      "episode : 23410, reward mean : 0.2799773663237042, total_step : 673939, cur_epsilon : 0.22606100000000007\n",
      "episode : 23420, reward mean : 0.2798919788945107, total_step : 674269, cur_epsilon : 0.22573100000000001\n",
      "episode : 23430, reward mean : 0.2798476374640145, total_step : 674602, cur_epsilon : 0.225398\n",
      "episode : 23440, reward mean : 0.28000683216123806, total_step : 674959, cur_epsilon : 0.22504100000000005\n",
      "synced target net\n",
      "episode : 23450, reward mean : 0.2801304966273275, total_step : 675199, cur_epsilon : 0.22480100000000003\n",
      "[EVAL] episode : 23450, reward mean : 0.8500000033527613, step mean : 16.0,\n",
      "episode : 23460, reward mean : 0.2802280539631659, total_step : 675500, cur_epsilon : 0.22450000000000003\n",
      "episode : 23470, reward mean : 0.28024627806060887, total_step : 675987, cur_epsilon : 0.22401300000000002\n",
      "episode : 23480, reward mean : 0.2803402958325947, total_step : 676296, cur_epsilon : 0.22370400000000001\n",
      "episode : 23490, reward mean : 0.2803984736574107, total_step : 676588, cur_epsilon : 0.22341200000000005\n",
      "episode : 23500, reward mean : 0.2803285168637621, total_step : 676981, cur_epsilon : 0.22301899999999997\n",
      "[EVAL] episode : 23500, reward mean : 0.26600000746548175, step mean : 34.4,\n",
      "episode : 23510, reward mean : 0.2804917118831778, total_step : 677327, cur_epsilon : 0.222673\n",
      "episode : 23520, reward mean : 0.280371604864739, total_step : 677539, cur_epsilon : 0.22246100000000002\n",
      "episode : 23530, reward mean : 0.28041564583464906, total_step : 677864, cur_epsilon : 0.222136\n",
      "episode : 23540, reward mean : 0.2805271939909404, total_step : 678131, cur_epsilon : 0.22186899999999998\n",
      "episode : 23550, reward mean : 0.2806165667352783, total_step : 678450, cur_epsilon : 0.22155000000000002\n",
      "[EVAL] episode : 23550, reward mean : 0.20600001327693462, step mean : 60.2,\n",
      "episode : 23560, reward mean : 0.2806035715914276, total_step : 678810, cur_epsilon : 0.22119\n",
      "episode : 23570, reward mean : 0.2805901632062999, total_step : 679171, cur_epsilon : 0.22082900000000005\n",
      "episode : 23580, reward mean : 0.2805432632259797, total_step : 679609, cur_epsilon : 0.220391\n",
      "episode : 23590, reward mean : 0.2807257374706739, total_step : 679908, cur_epsilon : 0.22009200000000007\n",
      "synced target net\n",
      "episode : 23600, reward mean : 0.28057712487450215, total_step : 680386, cur_epsilon : 0.21961399999999998\n",
      "[EVAL] episode : 23600, reward mean : 0.8460000034421682, step mean : 16.4,\n",
      "episode : 23610, reward mean : 0.2805756097877211, total_step : 680618, cur_epsilon : 0.21938200000000008\n",
      "episode : 23620, reward mean : 0.2806786683806006, total_step : 680904, cur_epsilon : 0.21909600000000007\n",
      "episode : 23630, reward mean : 0.28064579548289775, total_step : 681410, cur_epsilon : 0.21859000000000006\n",
      "episode : 23640, reward mean : 0.2807842701914611, total_step : 681711, cur_epsilon : 0.21828900000000007\n",
      "episode : 23650, reward mean : 0.2808186108837924, total_step : 682058, cur_epsilon : 0.21794199999999997\n",
      "[EVAL] episode : 23650, reward mean : 0.28200001157820226, step mean : 52.6,\n",
      "episode : 23660, reward mean : 0.28075951595349036, total_step : 682327, cur_epsilon : 0.217673\n",
      "episode : 23670, reward mean : 0.2806400569297458, total_step : 682638, cur_epsilon : 0.21736200000000006\n",
      "episode : 23680, reward mean : 0.28046284407389827, total_step : 683284, cur_epsilon : 0.21671600000000002\n",
      "episode : 23690, reward mean : 0.2805618466751209, total_step : 683579, cur_epsilon : 0.21642099999999997\n",
      "episode : 23700, reward mean : 0.28062658851636174, total_step : 684054, cur_epsilon : 0.21594599999999997\n",
      "[EVAL] episode : 23700, reward mean : 0.7620000053197146, step mean : 24.8,\n",
      "episode : 23710, reward mean : 0.28068199696106494, total_step : 684551, cur_epsilon : 0.215449\n",
      "episode : 23720, reward mean : 0.28073651551576817, total_step : 684951, cur_epsilon : 0.21504900000000005\n",
      "synced target net\n",
      "episode : 23730, reward mean : 0.2807109206961213, total_step : 685341, cur_epsilon : 0.21465900000000004\n",
      "episode : 23740, reward mean : 0.280786021406705, total_step : 685692, cur_epsilon : 0.21430800000000005\n",
      "episode : 23750, reward mean : 0.28094105887467924, total_step : 686053, cur_epsilon : 0.213947\n",
      "[EVAL] episode : 23750, reward mean : 0.8560000032186508, step mean : 15.4,\n",
      "episode : 23760, reward mean : 0.28105051129341424, total_step : 686322, cur_epsilon : 0.21367800000000003\n",
      "episode : 23770, reward mean : 0.28090534911380877, total_step : 686794, cur_epsilon : 0.213206\n",
      "episode : 23780, reward mean : 0.28103448900263334, total_step : 687016, cur_epsilon : 0.21298400000000006\n",
      "episode : 23790, reward mean : 0.2812501113294941, total_step : 687232, cur_epsilon : 0.21276800000000007\n",
      "episode : 23800, reward mean : 0.28137353565551715, total_step : 687566, cur_epsilon : 0.212434\n",
      "[EVAL] episode : 23800, reward mean : 0.6720000073313713, step mean : 33.8,\n",
      "episode : 23810, reward mean : 0.2814464573153474, total_step : 687921, cur_epsilon : 0.21207900000000002\n",
      "episode : 23820, reward mean : 0.28151637904102017, total_step : 688283, cur_epsilon : 0.21171700000000004\n",
      "episode : 23830, reward mean : 0.2816282059939479, total_step : 688644, cur_epsilon : 0.211356\n",
      "episode : 23840, reward mean : 0.28163675121422355, total_step : 689150, cur_epsilon : 0.21084999999999998\n",
      "episode : 23850, reward mean : 0.28174591819726447, total_step : 689418, cur_epsilon : 0.21058200000000005\n",
      "[EVAL] episode : 23850, reward mean : -0.07399998940527439, step mean : 48.2,\n",
      "episode : 23860, reward mean : 0.28173345134555294, total_step : 689875, cur_epsilon : 0.210125\n",
      "synced target net\n",
      "episode : 23870, reward mean : 0.28170884579855515, total_step : 690361, cur_epsilon : 0.20963900000000002\n",
      "episode : 23880, reward mean : 0.2818023513107119, total_step : 690765, cur_epsilon : 0.20923500000000006\n",
      "episode : 23890, reward mean : 0.28200586644418824, total_step : 691007, cur_epsilon : 0.20899299999999998\n",
      "episode : 23900, reward mean : 0.2820769936999766, total_step : 691365, cur_epsilon : 0.20863500000000001\n",
      "[EVAL] episode : 23900, reward mean : 0.3420000147074461, step mean : 66.8,\n",
      "episode : 23910, reward mean : 0.28220619613126335, total_step : 691683, cur_epsilon : 0.20831699999999997\n",
      "episode : 23920, reward mean : 0.28224373534999253, total_step : 692020, cur_epsilon : 0.20798000000000005\n",
      "episode : 23930, reward mean : 0.2822327684776875, total_step : 692473, cur_epsilon : 0.20752700000000002\n",
      "episode : 23940, reward mean : 0.2821219778511558, total_step : 692864, cur_epsilon : 0.207136\n",
      "episode : 23950, reward mean : 0.282072240078359, total_step : 693410, cur_epsilon : 0.20659000000000005\n",
      "[EVAL] episode : 23950, reward mean : 0.0520000122487545, step mean : 55.4,\n",
      "episode : 23960, reward mean : 0.28214232679277673, total_step : 693770, cur_epsilon : 0.20623000000000002\n",
      "episode : 23970, reward mean : 0.28221777847528606, total_step : 694216, cur_epsilon : 0.20578399999999997\n",
      "episode : 23980, reward mean : 0.2822364533000825, total_step : 694598, cur_epsilon : 0.20540199999999997\n",
      "episode : 23990, reward mean : 0.28239767195539656, total_step : 694939, cur_epsilon : 0.20506100000000005\n",
      "synced target net\n",
      "episode : 24000, reward mean : 0.2824704229286096, total_step : 695292, cur_epsilon : 0.204708\n",
      "[EVAL] episode : 24000, reward mean : 0.2900000113993883, step mean : 51.8,\n",
      "episode : 24010, reward mean : 0.2824544002660789, total_step : 695757, cur_epsilon : 0.20424300000000006\n",
      "episode : 24020, reward mean : 0.2824075832841243, total_step : 696197, cur_epsilon : 0.20380300000000007\n",
      "episode : 24030, reward mean : 0.28227757597090214, total_step : 696637, cur_epsilon : 0.20336300000000007\n",
      "episode : 24040, reward mean : 0.2823436002752212, total_step : 696806, cur_epsilon : 0.20319399999999999\n",
      "episode : 24050, reward mean : 0.2822681975345323, total_step : 697214, cur_epsilon : 0.20278600000000002\n",
      "[EVAL] episode : 24050, reward mean : 0.7400000058114529, step mean : 27.0,\n",
      "episode : 24060, reward mean : 0.2823279364417746, total_step : 697598, cur_epsilon : 0.20240199999999997\n",
      "episode : 24070, reward mean : 0.28222019737923004, total_step : 698183, cur_epsilon : 0.20181700000000002\n",
      "episode : 24080, reward mean : 0.2822570660720081, total_step : 698620, cur_epsilon : 0.20138\n",
      "episode : 24090, reward mean : 0.2822220901246911, total_step : 699030, cur_epsilon : 0.20096999999999998\n",
      "episode : 24100, reward mean : 0.2823332012938606, total_step : 699389, cur_epsilon : 0.20061099999999998\n",
      "[EVAL] episode : 24100, reward mean : 0.42000000849366187, step mean : 38.8,\n",
      "episode : 24110, reward mean : 0.2824807196707986, total_step : 699761, cur_epsilon : 0.20023900000000006\n",
      "synced target net\n",
      "episode : 24120, reward mean : 0.2825480991435449, total_step : 700126, cur_epsilon : 0.199874\n",
      "episode : 24130, reward mean : 0.2824148425803058, total_step : 700674, cur_epsilon : 0.199326\n",
      "episode : 24140, reward mean : 0.28246148100698426, total_step : 700889, cur_epsilon : 0.19911100000000004\n",
      "episode : 24150, reward mean : 0.28225797729285174, total_step : 701405, cur_epsilon : 0.19859500000000008\n",
      "[EVAL] episode : 24150, reward mean : -0.21799999065697193, step mean : 42.8,\n",
      "episode : 24160, reward mean : 0.2823452049550224, total_step : 701821, cur_epsilon : 0.198179\n",
      "episode : 24170, reward mean : 0.28221432154872567, total_step : 702362, cur_epsilon : 0.19763799999999998\n",
      "episode : 24180, reward mean : 0.28240902199701856, total_step : 702619, cur_epsilon : 0.19738100000000003\n",
      "episode : 24190, reward mean : 0.2822645784205164, total_step : 703094, cur_epsilon : 0.19690600000000003\n",
      "episode : 24200, reward mean : 0.28215661785442164, total_step : 703481, cur_epsilon : 0.196519\n",
      "[EVAL] episode : 24200, reward mean : 0.18800000920891763, step mean : 42.2,\n",
      "episode : 24210, reward mean : 0.2821429224342465, total_step : 703741, cur_epsilon : 0.19625900000000007\n",
      "episode : 24220, reward mean : 0.2821635075224199, total_step : 704019, cur_epsilon : 0.19598100000000007\n",
      "episode : 24230, reward mean : 0.2822018222148433, total_step : 704353, cur_epsilon : 0.19564700000000002\n",
      "episode : 24240, reward mean : 0.28225660694520244, total_step : 704746, cur_epsilon : 0.19525400000000004\n",
      "synced target net\n",
      "episode : 24250, reward mean : 0.2822833052557922, total_step : 705207, cur_epsilon : 0.194793\n",
      "[EVAL] episode : 24250, reward mean : 0.7760000050067901, step mean : 23.4,\n",
      "episode : 24260, reward mean : 0.28227247125019994, total_step : 705561, cur_epsilon : 0.19443900000000003\n",
      "episode : 24270, reward mean : 0.2820865328659675, total_step : 706136, cur_epsilon : 0.19386400000000004\n",
      "episode : 24280, reward mean : 0.2822537130447599, total_step : 706458, cur_epsilon : 0.193542\n",
      "episode : 24290, reward mean : 0.2821580960424889, total_step : 707115, cur_epsilon : 0.19288499999999997\n",
      "episode : 24300, reward mean : 0.2821967141138612, total_step : 707547, cur_epsilon : 0.19245299999999999\n",
      "[EVAL] episode : 24300, reward mean : -0.46199998520314695, step mean : 66.8,\n",
      "episode : 24310, reward mean : 0.2824014871664336, total_step : 707777, cur_epsilon : 0.19222300000000003\n",
      "episode : 24320, reward mean : 0.28244614116211825, total_step : 707996, cur_epsilon : 0.19200400000000006\n",
      "episode : 24330, reward mean : 0.2823855385600761, total_step : 708469, cur_epsilon : 0.191531\n",
      "episode : 24340, reward mean : 0.2825147967648615, total_step : 708882, cur_epsilon : 0.191118\n",
      "episode : 24350, reward mean : 0.28251787077497836, total_step : 709400, cur_epsilon : 0.1906\n",
      "[EVAL] episode : 24350, reward mean : 0.4100000042468309, step mean : 20.0,\n",
      "episode : 24360, reward mean : 0.2825923708311094, total_step : 709746, cur_epsilon : 0.19025400000000003\n",
      "synced target net\n",
      "episode : 24370, reward mean : 0.28259664150837704, total_step : 710261, cur_epsilon : 0.189739\n",
      "episode : 24380, reward mean : 0.28237859531007004, total_step : 710718, cur_epsilon : 0.18928200000000006\n",
      "episode : 24390, reward mean : 0.2824944712478223, total_step : 711062, cur_epsilon : 0.18893800000000005\n",
      "episode : 24400, reward mean : 0.282556973517964, total_step : 711536, cur_epsilon : 0.18846400000000008\n",
      "[EVAL] episode : 24400, reward mean : 0.8280000038444996, step mean : 18.2,\n",
      "episode : 24410, reward mean : 0.28247604071910826, total_step : 712057, cur_epsilon : 0.18794299999999997\n",
      "episode : 24420, reward mean : 0.28242998173831063, total_step : 712495, cur_epsilon : 0.18750500000000003\n",
      "episode : 24430, reward mean : 0.2822738499461086, total_step : 713101, cur_epsilon : 0.18689900000000004\n",
      "episode : 24440, reward mean : 0.28230769861940713, total_step : 713445, cur_epsilon : 0.18655500000000003\n",
      "episode : 24450, reward mean : 0.2823137077452245, total_step : 713958, cur_epsilon : 0.18604200000000004\n",
      "[EVAL] episode : 24450, reward mean : -0.053999985381960866, step mean : 66.0,\n",
      "episode : 24460, reward mean : 0.28226411097491877, total_step : 714306, cur_epsilon : 0.18569400000000003\n",
      "episode : 24470, reward mean : 0.2823036434212612, total_step : 714636, cur_epsilon : 0.18536399999999997\n",
      "synced target net\n",
      "episode : 24480, reward mean : 0.28226634618594904, total_step : 715154, cur_epsilon : 0.18484600000000007\n",
      "episode : 24490, reward mean : 0.28224418761688574, total_step : 715633, cur_epsilon : 0.18436700000000006\n",
      "episode : 24500, reward mean : 0.2821861287680816, total_step : 716002, cur_epsilon : 0.183998\n",
      "[EVAL] episode : 24500, reward mean : -0.7959999866783619, step mean : 60.6,\n",
      "episode : 24510, reward mean : 0.28210894144875437, total_step : 716418, cur_epsilon : 0.18358200000000002\n",
      "episode : 24520, reward mean : 0.2820852428635956, total_step : 716901, cur_epsilon : 0.183099\n",
      "episode : 24530, reward mean : 0.28221321463853705, total_step : 717214, cur_epsilon : 0.182786\n",
      "episode : 24540, reward mean : 0.2823068522889176, total_step : 717611, cur_epsilon : 0.18238900000000002\n",
      "episode : 24550, reward mean : 0.2822040796446066, total_step : 718088, cur_epsilon : 0.18191200000000007\n",
      "[EVAL] episode : 24550, reward mean : 0.24800000786781312, step mean : 36.2,\n",
      "episode : 24560, reward mean : 0.2821767164253196, total_step : 718681, cur_epsilon : 0.181319\n",
      "episode : 24570, reward mean : 0.28209036042019997, total_step : 719219, cur_epsilon : 0.18078099999999997\n",
      "episode : 24580, reward mean : 0.2821155474208253, total_step : 719584, cur_epsilon : 0.18041600000000002\n",
      "synced target net\n",
      "episode : 24590, reward mean : 0.28206507343239856, total_step : 720034, cur_epsilon : 0.17996600000000007\n",
      "episode : 24600, reward mean : 0.28201382747253145, total_step : 720585, cur_epsilon : 0.179415\n",
      "[EVAL] episode : 24600, reward mean : 0.8220000039786101, step mean : 18.8,\n",
      "episode : 24610, reward mean : 0.2820121965021779, total_step : 721016, cur_epsilon : 0.17898400000000003\n",
      "episode : 24620, reward mean : 0.28197644825292423, total_step : 721331, cur_epsilon : 0.17866899999999997\n",
      "episode : 24630, reward mean : 0.2819667136038748, total_step : 721681, cur_epsilon : 0.178319\n",
      "episode : 24640, reward mean : 0.28186648361068883, total_step : 722155, cur_epsilon : 0.17784500000000003\n",
      "episode : 24650, reward mean : 0.2819054830117382, total_step : 722486, cur_epsilon : 0.17751400000000006\n",
      "[EVAL] episode : 24650, reward mean : -0.28199998922646047, step mean : 48.8,\n",
      "episode : 24660, reward mean : 0.2819079544335537, total_step : 722907, cur_epsilon : 0.17709300000000006\n",
      "episode : 24670, reward mean : 0.28185934967376114, total_step : 723452, cur_epsilon : 0.17654800000000004\n",
      "episode : 24680, reward mean : 0.2818095687428124, total_step : 724000, cur_epsilon : 0.17600000000000005\n",
      "episode : 24690, reward mean : 0.2818521732135509, total_step : 724322, cur_epsilon : 0.175678\n",
      "episode : 24700, reward mean : 0.28177207112380187, total_step : 724846, cur_epsilon : 0.17515400000000003\n",
      "[EVAL] episode : 24700, reward mean : 0.13600001037120818, step mean : 47.4,\n",
      "synced target net\n",
      "episode : 24710, reward mean : 0.2815985494497176, total_step : 725500, cur_epsilon : 0.1745\n",
      "episode : 24720, reward mean : 0.28167314550968964, total_step : 725943, cur_epsilon : 0.17405700000000002\n",
      "episode : 24730, reward mean : 0.2815345797453526, total_step : 726412, cur_epsilon : 0.17358800000000008\n",
      "episode : 24740, reward mean : 0.28162288428333165, total_step : 726722, cur_epsilon : 0.17327800000000004\n",
      "episode : 24750, reward mean : 0.2816266730215513, total_step : 727239, cur_epsilon : 0.17276100000000005\n",
      "[EVAL] episode : 24750, reward mean : -0.17999998703598977, step mean : 58.8,\n",
      "episode : 24760, reward mean : 0.28144588680977006, total_step : 727813, cur_epsilon : 0.17218699999999998\n",
      "episode : 24770, reward mean : 0.281337915119844, total_step : 728306, cur_epsilon : 0.171694\n",
      "episode : 24780, reward mean : 0.2815056560769274, total_step : 728619, cur_epsilon : 0.171381\n",
      "episode : 24790, reward mean : 0.28138161184767374, total_step : 729152, cur_epsilon : 0.170848\n",
      "episode : 24800, reward mean : 0.28121371604126666, total_step : 729693, cur_epsilon : 0.170307\n",
      "[EVAL] episode : 24800, reward mean : -0.44799998551607134, step mean : 65.4,\n",
      "synced target net\n",
      "episode : 24810, reward mean : 0.2812172574734001, total_step : 730110, cur_epsilon : 0.16988999999999999\n",
      "episode : 24820, reward mean : 0.28112812884914407, total_step : 730658, cur_epsilon : 0.169342\n",
      "episode : 24830, reward mean : 0.281135326545525, total_step : 731068, cur_epsilon : 0.16893199999999997\n",
      "episode : 24840, reward mean : 0.2810483155484145, total_step : 731512, cur_epsilon : 0.16848799999999997\n",
      "episode : 24850, reward mean : 0.28089256170455, total_step : 732125, cur_epsilon : 0.167875\n",
      "[EVAL] episode : 24850, reward mean : 0.2620000120252371, step mean : 54.6,\n",
      "episode : 24860, reward mean : 0.2808624359793296, total_step : 732527, cur_epsilon : 0.16747299999999998\n",
      "episode : 24870, reward mean : 0.2807832793962131, total_step : 733150, cur_epsilon : 0.16685000000000005\n",
      "episode : 24880, reward mean : 0.28071262695717286, total_step : 733653, cur_epsilon : 0.16634700000000002\n",
      "episode : 24890, reward mean : 0.28065488785849535, total_step : 734126, cur_epsilon : 0.16587400000000008\n",
      "episode : 24900, reward mean : 0.28077671320667585, total_step : 734352, cur_epsilon : 0.16564800000000002\n",
      "[EVAL] episode : 24900, reward mean : 0.048000012338161466, step mean : 55.8,\n",
      "episode : 24910, reward mean : 0.28082899072303596, total_step : 734650, cur_epsilon : 0.16535\n",
      "synced target net\n",
      "episode : 24920, reward mean : 0.28093620220753834, total_step : 735112, cur_epsilon : 0.16488800000000003\n",
      "episode : 24930, reward mean : 0.2808728503457069, total_step : 735597, cur_epsilon : 0.16440300000000008\n",
      "episode : 24940, reward mean : 0.2808644811224727, total_step : 735945, cur_epsilon : 0.16405500000000006\n",
      "episode : 24950, reward mean : 0.2808048160030751, total_step : 736322, cur_epsilon : 0.163678\n",
      "[EVAL] episode : 24950, reward mean : -0.013999990746378898, step mean : 42.2,\n",
      "episode : 24960, reward mean : 0.2808697980502471, total_step : 736588, cur_epsilon : 0.163412\n",
      "episode : 24970, reward mean : 0.28073729112763723, total_step : 737145, cur_epsilon : 0.16285499999999997\n",
      "episode : 24980, reward mean : 0.280804249782151, total_step : 737606, cur_epsilon : 0.16239400000000004\n",
      "episode : 24990, reward mean : 0.2809119711739241, total_step : 737965, cur_epsilon : 0.16203500000000004\n",
      "episode : 25000, reward mean : 0.280920406389907, total_step : 738471, cur_epsilon : 0.16152900000000003\n",
      "[EVAL] episode : 25000, reward mean : -0.08399999365210534, step mean : 29.4,\n",
      "episode : 25010, reward mean : 0.280653345058026, total_step : 739163, cur_epsilon : 0.160837\n",
      "episode : 25020, reward mean : 0.2807649944035662, total_step : 739512, cur_epsilon : 0.16048800000000008\n",
      "episode : 25030, reward mean : 0.28074671035082555, total_step : 739984, cur_epsilon : 0.16001600000000005\n",
      "synced target net\n",
      "episode : 25040, reward mean : 0.28065216294571627, total_step : 740348, cur_epsilon : 0.15965200000000002\n",
      "episode : 25050, reward mean : 0.28067345949072026, total_step : 740723, cur_epsilon : 0.159277\n",
      "[EVAL] episode : 25050, reward mean : 0.34200001023709775, step mean : 46.6,\n",
      "episode : 25060, reward mean : 0.28083280767414476, total_step : 741053, cur_epsilon : 0.15894700000000006\n",
      "episode : 25070, reward mean : 0.2809278085520235, total_step : 741443, cur_epsilon : 0.15855700000000006\n",
      "episode : 25080, reward mean : 0.2808086188412026, total_step : 742067, cur_epsilon : 0.157933\n",
      "episode : 25090, reward mean : 0.28084536311808705, total_step : 742502, cur_epsilon : 0.15749800000000003\n",
      "episode : 25100, reward mean : 0.280653392858358, total_step : 743012, cur_epsilon : 0.15698800000000002\n",
      "[EVAL] episode : 25100, reward mean : 0.3540000099688768, step mean : 45.4,\n",
      "episode : 25110, reward mean : 0.2806810099898594, total_step : 743472, cur_epsilon : 0.156528\n",
      "episode : 25120, reward mean : 0.28047214016499245, total_step : 743924, cur_epsilon : 0.156076\n",
      "episode : 25130, reward mean : 0.28043494472888675, total_step : 744345, cur_epsilon : 0.155655\n",
      "synced target net\n",
      "episode : 25140, reward mean : 0.28025656965736806, total_step : 745020, cur_epsilon : 0.15498\n",
      "episode : 25150, reward mean : 0.28032286923608296, total_step : 745482, cur_epsilon : 0.15451800000000004\n",
      "[EVAL] episode : 25150, reward mean : 0.5540000099688769, step mean : 45.6,\n",
      "episode : 25160, reward mean : 0.2803648712790152, total_step : 745904, cur_epsilon : 0.154096\n",
      "episode : 25170, reward mean : 0.2804243210751976, total_step : 746284, cur_epsilon : 0.15371600000000007\n",
      "episode : 25180, reward mean : 0.2805571946603963, total_step : 746679, cur_epsilon : 0.15332100000000004\n",
      "episode : 25190, reward mean : 0.28056967692082185, total_step : 747076, cur_epsilon : 0.15292400000000006\n",
      "episode : 25200, reward mean : 0.28069206990892925, total_step : 747396, cur_epsilon : 0.15260400000000007\n",
      "[EVAL] episode : 25200, reward mean : -0.28999998457729814, step mean : 69.8,\n",
      "episode : 25210, reward mean : 0.28060889178196186, total_step : 747933, cur_epsilon : 0.15206700000000006\n",
      "episode : 25220, reward mean : 0.2804587693065907, total_step : 748340, cur_epsilon : 0.15166000000000002\n",
      "episode : 25230, reward mean : 0.2803654443930349, total_step : 748903, cur_epsilon : 0.15109700000000004\n",
      "episode : 25240, reward mean : 0.28034905555227063, total_step : 749371, cur_epsilon : 0.150629\n",
      "episode : 25250, reward mean : 0.2804170361274097, total_step : 749729, cur_epsilon : 0.15027100000000004\n",
      "[EVAL] episode : 25250, reward mean : 0.6760000072419643, step mean : 33.4,\n",
      "synced target net\n",
      "episode : 25260, reward mean : 0.2804366651741804, total_step : 750108, cur_epsilon : 0.14989200000000003\n",
      "episode : 25270, reward mean : 0.2804590487703062, total_step : 750680, cur_epsilon : 0.14932\n",
      "episode : 25280, reward mean : 0.2803330760509589, total_step : 751326, cur_epsilon : 0.14867399999999997\n",
      "episode : 25290, reward mean : 0.2802673057611397, total_step : 751919, cur_epsilon : 0.14808100000000002\n",
      "episode : 25300, reward mean : 0.28028893924123155, total_step : 752392, cur_epsilon : 0.14760800000000007\n",
      "[EVAL] episode : 25300, reward mean : 0.27400001175701616, step mean : 53.4,\n",
      "episode : 25310, reward mean : 0.2804085406104963, total_step : 752619, cur_epsilon : 0.14738099999999998\n",
      "episode : 25320, reward mean : 0.2803882370844901, total_step : 753198, cur_epsilon : 0.146802\n",
      "episode : 25330, reward mean : 0.280388873394989, total_step : 753724, cur_epsilon : 0.14627600000000007\n",
      "episode : 25340, reward mean : 0.2804865889178461, total_step : 754105, cur_epsilon : 0.145895\n",
      "episode : 25350, reward mean : 0.2802485271534447, total_step : 754834, cur_epsilon : 0.14516600000000002\n",
      "[EVAL] episode : 25350, reward mean : 0.7000000067055225, step mean : 31.0,\n",
      "synced target net\n",
      "episode : 25360, reward mean : 0.2803615995037167, total_step : 755176, cur_epsilon : 0.14482400000000006\n",
      "episode : 25370, reward mean : 0.28023729458330865, total_step : 755918, cur_epsilon : 0.14408200000000004\n",
      "episode : 25380, reward mean : 0.28018755569872045, total_step : 756174, cur_epsilon : 0.143826\n",
      "episode : 25390, reward mean : 0.2802847642258584, total_step : 756457, cur_epsilon : 0.14354299999999998\n",
      "episode : 25400, reward mean : 0.2801716599933158, total_step : 757072, cur_epsilon : 0.14292800000000006\n",
      "[EVAL] episode : 25400, reward mean : -0.015999986231327055, step mean : 62.2,\n",
      "episode : 25410, reward mean : 0.2802530564305463, total_step : 757395, cur_epsilon : 0.14260499999999998\n",
      "episode : 25420, reward mean : 0.28015854303797605, total_step : 757961, cur_epsilon : 0.14203900000000003\n",
      "episode : 25430, reward mean : 0.28004286921598864, total_step : 758581, cur_epsilon : 0.14141900000000007\n",
      "episode : 25440, reward mean : 0.27999371714954435, total_step : 759133, cur_epsilon : 0.14086700000000008\n",
      "episode : 25450, reward mean : 0.2799693581293573, total_step : 759624, cur_epsilon : 0.14037600000000006\n",
      "[EVAL] episode : 25450, reward mean : -0.1059999842196703, step mean : 71.2,\n",
      "synced target net\n",
      "episode : 25460, reward mean : 0.27993951942274187, total_step : 760129, cur_epsilon : 0.13987099999999997\n",
      "episode : 25470, reward mean : 0.2798920363015932, total_step : 760578, cur_epsilon : 0.13942200000000005\n",
      "episode : 25480, reward mean : 0.27989835811280345, total_step : 761090, cur_epsilon : 0.13890999999999998\n",
      "episode : 25490, reward mean : 0.27993213671287714, total_step : 761532, cur_epsilon : 0.13846800000000004\n",
      "episode : 25500, reward mean : 0.2799102025457895, total_step : 762015, cur_epsilon : 0.13798500000000002\n",
      "[EVAL] episode : 25500, reward mean : 0.4460000079125166, step mean : 36.2,\n",
      "episode : 25510, reward mean : 0.2799325819294834, total_step : 762486, cur_epsilon : 0.13751400000000003\n",
      "episode : 25520, reward mean : 0.2799192854671865, total_step : 762949, cur_epsilon : 0.13705100000000003\n",
      "episode : 25530, reward mean : 0.2799345932326718, total_step : 763440, cur_epsilon : 0.13656000000000001\n",
      "episode : 25540, reward mean : 0.2799271795351697, total_step : 763886, cur_epsilon : 0.13611400000000007\n",
      "episode : 25550, reward mean : 0.2799698694880665, total_step : 764307, cur_epsilon : 0.13569300000000006\n",
      "[EVAL] episode : 25550, reward mean : -0.353999987617135, step mean : 56.4,\n",
      "episode : 25560, reward mean : 0.2798654211857043, total_step : 764702, cur_epsilon : 0.13529800000000003\n",
      "synced target net\n",
      "episode : 25570, reward mean : 0.27974150041558976, total_step : 765246, cur_epsilon : 0.13475400000000004\n",
      "episode : 25580, reward mean : 0.27970094471242196, total_step : 765777, cur_epsilon : 0.13422299999999998\n",
      "episode : 25590, reward mean : 0.27971630190899827, total_step : 766266, cur_epsilon : 0.13373400000000002\n",
      "episode : 25600, reward mean : 0.2797910221055645, total_step : 766504, cur_epsilon : 0.13349600000000006\n",
      "[EVAL] episode : 25600, reward mean : 0.07400001175701618, step mean : 53.2,\n",
      "episode : 25610, reward mean : 0.27954276321853677, total_step : 767066, cur_epsilon : 0.132934\n",
      "episode : 25620, reward mean : 0.27941374575223404, total_step : 767723, cur_epsilon : 0.13227699999999998\n",
      "episode : 25630, reward mean : 0.27946157496142005, total_step : 768129, cur_epsilon : 0.13187100000000007\n",
      "episode : 25640, reward mean : 0.2794567147565514, total_step : 768569, cur_epsilon : 0.13143100000000008\n",
      "episode : 25650, reward mean : 0.27926394411288274, total_step : 769190, cur_epsilon : 0.13080999999999998\n",
      "[EVAL] episode : 25650, reward mean : -0.4939999844878912, step mean : 70.0,\n",
      "episode : 25660, reward mean : 0.27919330345314297, total_step : 769700, cur_epsilon : 0.13029999999999997\n",
      "synced target net\n",
      "episode : 25670, reward mean : 0.27900779769238176, total_step : 770402, cur_epsilon : 0.129598\n",
      "episode : 25680, reward mean : 0.2791137136620501, total_step : 770760, cur_epsilon : 0.12924000000000002\n",
      "episode : 25690, reward mean : 0.2790669586196971, total_step : 771209, cur_epsilon : 0.128791\n",
      "episode : 25700, reward mean : 0.27888405319433734, total_step : 771905, cur_epsilon : 0.12809500000000007\n",
      "[EVAL] episode : 25700, reward mean : 0.374000009521842, step mean : 43.4,\n",
      "episode : 25710, reward mean : 0.2789412744924316, total_step : 772388, cur_epsilon : 0.12761200000000006\n",
      "episode : 25720, reward mean : 0.2788682024616713, total_step : 772905, cur_epsilon : 0.12709500000000007\n",
      "episode : 25730, reward mean : 0.27872678458783307, total_step : 773497, cur_epsilon : 0.12650300000000003\n",
      "episode : 25740, reward mean : 0.27854779205808206, total_step : 774087, cur_epsilon : 0.12591300000000005\n",
      "episode : 25750, reward mean : 0.27834447253191646, total_step : 774639, cur_epsilon : 0.12536100000000006\n",
      "[EVAL] episode : 25750, reward mean : 0.016000008583068846, step mean : 39.2,\n",
      "synced target net\n",
      "episode : 25760, reward mean : 0.2782111866399646, total_step : 775310, cur_epsilon : 0.12468999999999997\n",
      "episode : 25770, reward mean : 0.27810089902914004, total_step : 775924, cur_epsilon : 0.12407600000000008\n",
      "episode : 25780, reward mean : 0.2780616822380853, total_step : 776454, cur_epsilon : 0.12354600000000004\n",
      "episode : 25790, reward mean : 0.27807871920142224, total_step : 776940, cur_epsilon : 0.12306000000000006\n",
      "episode : 25800, reward mean : 0.2778178359829467, total_step : 777642, cur_epsilon : 0.12235799999999997\n",
      "[EVAL] episode : 25800, reward mean : 0.22200001291930677, step mean : 58.6,\n",
      "episode : 25810, reward mean : 0.27769314871984757, total_step : 778094, cur_epsilon : 0.12190600000000007\n",
      "episode : 25820, reward mean : 0.277566234259574, total_step : 778653, cur_epsilon : 0.12134699999999998\n",
      "episode : 25830, reward mean : 0.27757530656963747, total_step : 779160, cur_epsilon : 0.12084000000000006\n",
      "episode : 25840, reward mean : 0.2774102232514105, total_step : 779717, cur_epsilon : 0.12028300000000003\n",
      "synced target net\n",
      "episode : 25850, reward mean : 0.27732921349933115, total_step : 780356, cur_epsilon : 0.11964399999999997\n",
      "[EVAL] episode : 25850, reward mean : -0.4499999854713678, step mean : 65.6,\n",
      "episode : 25860, reward mean : 0.27729621690144074, total_step : 780871, cur_epsilon : 0.11912900000000004\n",
      "episode : 25870, reward mean : 0.2771039879864646, total_step : 781497, cur_epsilon : 0.11850300000000002\n",
      "episode : 25880, reward mean : 0.2769254315828032, total_step : 782189, cur_epsilon : 0.117811\n",
      "episode : 25890, reward mean : 0.2768914704308541, total_step : 782608, cur_epsilon : 0.11739200000000005\n",
      "episode : 25900, reward mean : 0.27674479419178544, total_step : 783119, cur_epsilon : 0.11688100000000001\n",
      "[EVAL] episode : 25900, reward mean : -0.2819999847561121, step mean : 69.0,\n",
      "episode : 25910, reward mean : 0.2766700181266094, total_step : 783545, cur_epsilon : 0.11645499999999998\n",
      "episode : 25920, reward mean : 0.2767029386480845, total_step : 783991, cur_epsilon : 0.11600900000000003\n",
      "episode : 25930, reward mean : 0.2766818422633067, total_step : 784577, cur_epsilon : 0.11542300000000005\n",
      "synced target net\n",
      "episode : 25940, reward mean : 0.27639360717097805, total_step : 785155, cur_epsilon : 0.11484499999999997\n",
      "episode : 25950, reward mean : 0.27645164432065106, total_step : 785637, cur_epsilon : 0.11436299999999999\n",
      "[EVAL] episode : 25950, reward mean : -0.2559999853372574, step mean : 66.4,\n",
      "episode : 25960, reward mean : 0.2763621020895674, total_step : 786201, cur_epsilon : 0.11379899999999998\n",
      "episode : 25970, reward mean : 0.2762818702497664, total_step : 786840, cur_epsilon : 0.11316000000000004\n",
      "episode : 25980, reward mean : 0.27629523366120423, total_step : 787438, cur_epsilon : 0.11256200000000005\n",
      "episode : 25990, reward mean : 0.2760711877905305, total_step : 788150, cur_epsilon : 0.11185\n",
      "episode : 26000, reward mean : 0.2758807757988286, total_step : 788577, cur_epsilon : 0.11142300000000005\n",
      "[EVAL] episode : 26000, reward mean : -0.709999979659915, step mean : 91.2,\n",
      "episode : 26010, reward mean : 0.2758581380575741, total_step : 789069, cur_epsilon : 0.110931\n",
      "episode : 26020, reward mean : 0.27588471064485787, total_step : 789532, cur_epsilon : 0.11046800000000001\n",
      "synced target net\n",
      "episode : 26030, reward mean : 0.2758885966613173, total_step : 790054, cur_epsilon : 0.10994599999999999\n",
      "episode : 26040, reward mean : 0.2758487008905436, total_step : 790490, cur_epsilon : 0.10951\n",
      "episode : 26050, reward mean : 0.2758092196270709, total_step : 790925, cur_epsilon : 0.10907500000000003\n",
      "[EVAL] episode : 26050, reward mean : -0.3679999873042107, step mean : 57.4,\n",
      "episode : 26060, reward mean : 0.27561896283283877, total_step : 791551, cur_epsilon : 0.10844900000000002\n",
      "episode : 26070, reward mean : 0.27540967286348994, total_step : 792128, cur_epsilon : 0.10787199999999997\n",
      "episode : 26080, reward mean : 0.275165650755327, total_step : 792794, cur_epsilon : 0.10720600000000002\n",
      "episode : 26090, reward mean : 0.27508088048460405, total_step : 793447, cur_epsilon : 0.10655300000000001\n",
      "episode : 26100, reward mean : 0.27494100275852673, total_step : 794143, cur_epsilon : 0.10585699999999998\n",
      "[EVAL] episode : 26100, reward mean : -0.011999986320734023, step mean : 61.8,\n",
      "episode : 26110, reward mean : 0.2746798227567187, total_step : 794954, cur_epsilon : 0.10504599999999997\n",
      "synced target net\n",
      "episode : 26120, reward mean : 0.2746585058302235, total_step : 795442, cur_epsilon : 0.10455800000000004\n",
      "episode : 26130, reward mean : 0.27450555577507785, total_step : 795973, cur_epsilon : 0.10402699999999998\n",
      "episode : 26140, reward mean : 0.2744280861714155, total_step : 796509, cur_epsilon : 0.103491\n",
      "episode : 26150, reward mean : 0.2743560295471526, total_step : 797130, cur_epsilon : 0.10287000000000002\n",
      "[EVAL] episode : 26150, reward mean : 0.22200001291930677, step mean : 58.6,\n",
      "episode : 26160, reward mean : 0.2742366274006363, total_step : 797774, cur_epsilon : 0.10222600000000004\n",
      "episode : 26170, reward mean : 0.2741276336621022, total_step : 798391, cur_epsilon : 0.10160900000000006\n",
      "episode : 26180, reward mean : 0.2740164313618324, total_step : 798917, cur_epsilon : 0.10108300000000003\n",
      "episode : 26190, reward mean : 0.27380947587636817, total_step : 799591, cur_epsilon : 0.10040899999999997\n",
      "synced target net\n",
      "episode : 26200, reward mean : 0.27387099898114575, total_step : 800065, cur_epsilon : 0.099935\n",
      "[EVAL] episode : 26200, reward mean : 0.5660000097006559, step mean : 44.4,\n",
      "episode : 26210, reward mean : 0.2738431962387468, total_step : 800573, cur_epsilon : 0.09942700000000004\n",
      "episode : 26220, reward mean : 0.27354882431630695, total_step : 801277, cur_epsilon : 0.098723\n",
      "episode : 26230, reward mean : 0.2736805251108932, total_step : 801668, cur_epsilon : 0.09833199999999997\n",
      "episode : 26240, reward mean : 0.273445128574797, total_step : 802317, cur_epsilon : 0.09768300000000008\n",
      "episode : 26250, reward mean : 0.2733459113879573, total_step : 802910, cur_epsilon : 0.09709000000000001\n",
      "[EVAL] episode : 26250, reward mean : -0.08799998462200165, step mean : 69.4,\n",
      "episode : 26260, reward mean : 0.2733716745634605, total_step : 803377, cur_epsilon : 0.09662300000000001\n",
      "episode : 26270, reward mean : 0.2733570679163594, total_step : 803950, cur_epsilon : 0.09604999999999997\n",
      "episode : 26280, reward mean : 0.27329490769766496, total_step : 804547, cur_epsilon : 0.09545300000000001\n",
      "episode : 26290, reward mean : 0.2733442439856741, total_step : 804954, cur_epsilon : 0.09504600000000007\n",
      "synced target net\n",
      "episode : 26300, reward mean : 0.27343764921848596, total_step : 805245, cur_epsilon : 0.09475500000000003\n",
      "[EVAL] episode : 26300, reward mean : 0.4280000127851963, step mean : 58.2,\n",
      "episode : 26310, reward mean : 0.27320183103727846, total_step : 805897, cur_epsilon : 0.09410300000000005\n",
      "episode : 26320, reward mean : 0.2729251586154845, total_step : 806657, cur_epsilon : 0.09334300000000006\n",
      "episode : 26330, reward mean : 0.2728780924756061, total_step : 807217, cur_epsilon : 0.09278300000000006\n",
      "episode : 26340, reward mean : 0.2727555115795613, total_step : 807774, cur_epsilon : 0.09222600000000003\n",
      "episode : 26350, reward mean : 0.27261594592641325, total_step : 808477, cur_epsilon : 0.09152300000000002\n",
      "[EVAL] episode : 26350, reward mean : 0.14800001457333564, step mean : 66.0,\n",
      "episode : 26360, reward mean : 0.2726460612761295, total_step : 808834, cur_epsilon : 0.09116599999999997\n",
      "episode : 26370, reward mean : 0.2723796046787861, total_step : 809470, cur_epsilon : 0.09053\n",
      "synced target net\n",
      "episode : 26380, reward mean : 0.2721865115807285, total_step : 810014, cur_epsilon : 0.08998600000000001\n",
      "episode : 26390, reward mean : 0.2722057664109088, total_step : 810400, cur_epsilon : 0.08960000000000001\n",
      "episode : 26400, reward mean : 0.2719590975665911, total_step : 811183, cur_epsilon : 0.08881700000000003\n",
      "[EVAL] episode : 26400, reward mean : 1.3411045074462891e-08, step mean : 60.6,\n",
      "episode : 26410, reward mean : 0.27183378174490536, total_step : 811749, cur_epsilon : 0.08825099999999997\n",
      "episode : 26420, reward mean : 0.2718160551093536, total_step : 812231, cur_epsilon : 0.08776899999999999\n",
      "episode : 26430, reward mean : 0.27161483829499217, total_step : 812897, cur_epsilon : 0.08710300000000004\n",
      "episode : 26440, reward mean : 0.2716164968317479, total_step : 813330, cur_epsilon : 0.08667000000000002\n",
      "episode : 26450, reward mean : 0.2714518025087893, total_step : 813900, cur_epsilon : 0.08610000000000007\n",
      "[EVAL] episode : 26450, reward mean : -0.4939999844878912, step mean : 70.0,\n",
      "episode : 26460, reward mean : 0.2712040883034907, total_step : 814589, cur_epsilon : 0.08541100000000001\n",
      "synced target net\n",
      "episode : 26470, reward mean : 0.2709618502713439, total_step : 815363, cur_epsilon : 0.08463700000000007\n",
      "episode : 26480, reward mean : 0.27074736317365045, total_step : 816064, cur_epsilon : 0.08393600000000001\n",
      "episode : 26490, reward mean : 0.2705534230654714, total_step : 816812, cur_epsilon : 0.08318800000000004\n",
      "episode : 26500, reward mean : 0.27049736517400674, total_step : 817298, cur_epsilon : 0.08270200000000005\n",
      "[EVAL] episode : 26500, reward mean : -0.03199998587369919, step mean : 63.8,\n",
      "episode : 26510, reward mean : 0.27019314135328887, total_step : 818038, cur_epsilon : 0.08196199999999998\n",
      "episode : 26520, reward mean : 0.26995890563512626, total_step : 818793, cur_epsilon : 0.08120700000000003\n",
      "episode : 26530, reward mean : 0.269914443180024, total_step : 819348, cur_epsilon : 0.08065200000000006\n",
      "synced target net\n",
      "episode : 26540, reward mean : 0.2695286427179566, total_step : 820106, cur_epsilon : 0.07989400000000002\n",
      "episode : 26550, reward mean : 0.2694331517086668, total_step : 820696, cur_epsilon : 0.07930400000000004\n",
      "[EVAL] episode : 26550, reward mean : 0.374000009521842, step mean : 43.4,\n",
      "episode : 26560, reward mean : 0.2692500067027622, total_step : 821418, cur_epsilon : 0.07858200000000004\n",
      "episode : 26570, reward mean : 0.26908243049299263, total_step : 822200, cur_epsilon : 0.07779999999999998\n",
      "episode : 26580, reward mean : 0.2689462068597201, total_step : 822800, cur_epsilon : 0.07720000000000005\n",
      "episode : 26590, reward mean : 0.26862467764307624, total_step : 823689, cur_epsilon : 0.07631100000000002\n",
      "episode : 26600, reward mean : 0.26862782626489834, total_step : 824220, cur_epsilon : 0.07578000000000007\n",
      "[EVAL] episode : 26600, reward mean : 0.6380000080913305, step mean : 37.2,\n",
      "episode : 26610, reward mean : 0.2684250349031915, total_step : 824800, cur_epsilon : 0.07520000000000004\n",
      "synced target net\n",
      "episode : 26620, reward mean : 0.2682381735137817, total_step : 825534, cur_epsilon : 0.07446600000000003\n",
      "episode : 26630, reward mean : 0.2680841223846308, total_step : 826282, cur_epsilon : 0.07371800000000006\n",
      "episode : 26640, reward mean : 0.26787951123325976, total_step : 826965, cur_epsilon : 0.07303500000000007\n",
      "episode : 26650, reward mean : 0.2676011324362868, total_step : 827742, cur_epsilon : 0.07225800000000004\n",
      "[EVAL] episode : 26650, reward mean : 0.130000014975667, step mean : 67.8,\n",
      "episode : 26660, reward mean : 0.26749250485994264, total_step : 828370, cur_epsilon : 0.07162999999999997\n",
      "episode : 26670, reward mean : 0.2671912328366303, total_step : 829209, cur_epsilon : 0.07079100000000005\n",
      "synced target net\n",
      "episode : 26680, reward mean : 0.26686582383553104, total_step : 830014, cur_epsilon : 0.06998599999999999\n",
      "episode : 26690, reward mean : 0.2665489014651963, total_step : 830797, cur_epsilon : 0.06920300000000001\n",
      "episode : 26700, reward mean : 0.26638277828595724, total_step : 831380, cur_epsilon : 0.06862000000000001\n",
      "[EVAL] episode : 26700, reward mean : 0.3620000097900629, step mean : 44.6,\n",
      "episode : 26710, reward mean : 0.26607862899295404, total_step : 832130, cur_epsilon : 0.06786999999999999\n",
      "episode : 26720, reward mean : 0.26583608460158087, total_step : 832817, cur_epsilon : 0.06718299999999999\n",
      "episode : 26730, reward mean : 0.2657807774292678, total_step : 833406, cur_epsilon : 0.06659400000000004\n",
      "episode : 26740, reward mean : 0.2656073366058468, total_step : 834109, cur_epsilon : 0.06589100000000003\n",
      "episode : 26750, reward mean : 0.2655110348018809, total_step : 834608, cur_epsilon : 0.065392\n",
      "[EVAL] episode : 26750, reward mean : -0.6219999816268682, step mean : 82.4,\n",
      "synced target net\n",
      "episode : 26760, reward mean : 0.265233938008274, total_step : 835288, cur_epsilon : 0.06471199999999999\n",
      "episode : 26770, reward mean : 0.26498095559434276, total_step : 836005, cur_epsilon : 0.06399500000000002\n",
      "episode : 26780, reward mean : 0.2646131509135994, total_step : 836927, cur_epsilon : 0.06307300000000005\n",
      "episode : 26790, reward mean : 0.2644785435469115, total_step : 837629, cur_epsilon : 0.062371000000000065\n",
      "episode : 26800, reward mean : 0.26423881275356925, total_step : 838413, cur_epsilon : 0.06158700000000006\n",
      "[EVAL] episode : 26800, reward mean : -0.7899999778717757, step mean : 99.2,\n",
      "episode : 26810, reward mean : 0.2638317859754889, total_step : 839341, cur_epsilon : 0.06065900000000002\n",
      "synced target net\n",
      "episode : 26820, reward mean : 0.26355071521838497, total_step : 840035, cur_epsilon : 0.059965000000000046\n",
      "episode : 26830, reward mean : 0.26333470675799936, total_step : 840755, cur_epsilon : 0.05924499999999999\n",
      "episode : 26840, reward mean : 0.26301118414668606, total_step : 841564, cur_epsilon : 0.05843600000000004\n",
      "episode : 26850, reward mean : 0.26283352635426227, total_step : 842085, cur_epsilon : 0.05791500000000005\n",
      "[EVAL] episode : 26850, reward mean : -0.9999999776482582, step mean : 100.0,\n",
      "episode : 26860, reward mean : 0.26270886756374334, total_step : 842763, cur_epsilon : 0.05723699999999998\n",
      "episode : 26870, reward mean : 0.26234909501169357, total_step : 843671, cur_epsilon : 0.05632900000000007\n",
      "episode : 26880, reward mean : 0.26219829550252705, total_step : 844319, cur_epsilon : 0.05568099999999998\n",
      "episode : 26890, reward mean : 0.262160289446124, total_step : 844767, cur_epsilon : 0.05523299999999998\n",
      "synced target net\n",
      "episode : 26900, reward mean : 0.2619817912032253, total_step : 845490, cur_epsilon : 0.05451000000000006\n",
      "[EVAL] episode : 26900, reward mean : 0.2280000127851963, step mean : 58.0,\n",
      "episode : 26910, reward mean : 0.26180342562262704, total_step : 846114, cur_epsilon : 0.05388599999999999\n",
      "episode : 26920, reward mean : 0.26156278542563993, total_step : 846805, cur_epsilon : 0.05319499999999999\n",
      "episode : 26930, reward mean : 0.26140141789114946, total_step : 847483, cur_epsilon : 0.052517000000000036\n",
      "episode : 26940, reward mean : 0.26125872991557364, total_step : 848014, cur_epsilon : 0.05198599999999998\n",
      "episode : 26950, reward mean : 0.26107792890863923, total_step : 848745, cur_epsilon : 0.05125500000000005\n",
      "[EVAL] episode : 26950, reward mean : -0.2859999891370535, step mean : 49.2,\n",
      "episode : 26960, reward mean : 0.2607733747878358, total_step : 849608, cur_epsilon : 0.05039199999999999\n",
      "synced target net\n",
      "episode : 26970, reward mean : 0.26057360713502403, total_step : 850292, cur_epsilon : 0.05\n",
      "episode : 26980, reward mean : 0.2603458185544896, total_step : 851050, cur_epsilon : 0.05\n",
      "episode : 26990, reward mean : 0.2601063425259657, total_step : 851840, cur_epsilon : 0.05\n",
      "episode : 27000, reward mean : 0.2599185253674233, total_step : 852491, cur_epsilon : 0.05\n",
      "[EVAL] episode : 27000, reward mean : 0.2760000117123127, step mean : 53.2,\n",
      "episode : 27010, reward mean : 0.2595631316224212, total_step : 853393, cur_epsilon : 0.05\n",
      "episode : 27020, reward mean : 0.259358630099056, total_step : 854092, cur_epsilon : 0.05\n",
      "episode : 27030, reward mean : 0.2589718899546626, total_step : 854981, cur_epsilon : 0.05\n",
      "synced target net\n",
      "episode : 27040, reward mean : 0.25868676722100686, total_step : 855796, cur_epsilon : 0.05\n",
      "episode : 27050, reward mean : 0.2584037037277784, total_step : 856606, cur_epsilon : 0.05\n",
      "[EVAL] episode : 27050, reward mean : -0.6179999817162752, step mean : 82.0,\n",
      "episode : 27060, reward mean : 0.2581674126389864, total_step : 857391, cur_epsilon : 0.05\n",
      "episode : 27070, reward mean : 0.2579058066568512, total_step : 858245, cur_epsilon : 0.05\n",
      "episode : 27080, reward mean : 0.2577008931446364, total_step : 858948, cur_epsilon : 0.05\n",
      "episode : 27090, reward mean : 0.2572997484872535, total_step : 859680, cur_epsilon : 0.05\n",
      "synced target net\n",
      "episode : 27100, reward mean : 0.2570405972938696, total_step : 860331, cur_epsilon : 0.05\n",
      "[EVAL] episode : 27100, reward mean : -0.7959999777376652, step mean : 99.8,\n",
      "episode : 27110, reward mean : 0.2569159050827639, total_step : 860919, cur_epsilon : 0.05\n",
      "episode : 27120, reward mean : 0.2566814228232983, total_step : 861702, cur_epsilon : 0.05\n",
      "episode : 27130, reward mean : 0.2564238918958238, total_step : 862447, cur_epsilon : 0.05\n",
      "episode : 27140, reward mean : 0.256144811618008, total_step : 863253, cur_epsilon : 0.05\n",
      "episode : 27150, reward mean : 0.2560294728346073, total_step : 863916, cur_epsilon : 0.05\n",
      "[EVAL] episode : 27150, reward mean : -0.6139999818056822, step mean : 81.6,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m         done_reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done_reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m             episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 51\u001b[0m, in \u001b[0;36mAgent.play_step\u001b[0;34m(self, epsilon, sync_target)\u001b[0m\n\u001b[1;32m     48\u001b[0m     _, act_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(q_vals_v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_v\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 51\u001b[0m next_state, reward, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_buffer\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state, action, reward, is_done, next_state\n\u001b[1;32m     56\u001b[0m )\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgResizeGrayWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 11\u001b[0m         next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         next_state \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(next_state[:, :, \u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         return next_state, reward, is_done, info\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(next_state[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), reward, is_done, info\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/envs/unity_gym_env.py:200\u001b[0m, in \u001b[0;36mUnityToGymWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    197\u001b[0m     action_tuple\u001b[38;5;241m.\u001b[39madd_discrete(action)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mset_actions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, action_tuple)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m decision_step, terminal_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_steps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_agents(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(decision_step), \u001b[38;5;28mlen\u001b[39m(terminal_step)))\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/environment.py:348\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m step_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_step_input(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicator.exchange\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_communicator.py:142\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    140\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll_for_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_communicator.py:106\u001b[0m, in \u001b[0;36mRpcCommunicator.poll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    104\u001b[0m callback_timeout_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout_wait \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m<\u001b[39m deadline:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_timeout_wait\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# Got an acknowledgment from the connection\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m poll_callback:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# Fire the callback - if it detects something wrong, it should raise an exception.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# exp_buffer = NStepPriorityReplayBuffer(\n",
    "#     max_size=30000,\n",
    "#     prob_alpha=0.6,\n",
    "#     beta_start=0.4,\n",
    "#     beta_frames=500000,\n",
    "#     n_step=4,\n",
    "#     gamma=0.99,\n",
    "# )\n",
    "# agent = Agent(\n",
    "#     env=env,\n",
    "#     exp_buffer=exp_buffer,\n",
    "#     net=net,\n",
    "#     epsilon_start=0.9,\n",
    "#     epsilon_final=0.05,\n",
    "#     epsilon_decay_last_step=1000000,\n",
    "#     tgt_sync_steps=5000,\n",
    "#     learning_rate=1e-4,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "episode = 0\n",
    "reward_history = []\n",
    "spisode_steps_history = []\n",
    "spisode_start_steps = 0\n",
    "PRINT_EPISODE_INTERVAL = 10\n",
    "print_episode_rewards = []\n",
    "EVAL_EPISODE_INTERVAL = 50\n",
    "N_EVAL_EPISODES = 5\n",
    "\n",
    "while True:\n",
    "\n",
    "    for stp in range(20):\n",
    "        done_reward = agent.play_step()\n",
    "        if done_reward is not None:\n",
    "            episode += 1\n",
    "            reward_history.append(done_reward)\n",
    "            print_episode_rewards.append(done_reward)\n",
    "            spisode_steps_history.append(agent._total_step-spisode_start_steps)\n",
    "            spisode_start_steps = agent._total_step\n",
    "            if episode % PRINT_EPISODE_INTERVAL == 0:\n",
    "                rm = sum(print_episode_rewards) / len(print_episode_rewards)\n",
    "                print(f'episode : {episode}, reward mean : {rm}, total_step : {agent._total_step}, cur_epsilon : {agent._epsilon}')\n",
    "            if episode % EVAL_EPISODE_INTERVAL == 0:\n",
    "                eval_episode_rewards = []\n",
    "                eval_episode_steps = []\n",
    "                for i in range(N_EVAL_EPISODES):\n",
    "                    reward, steps = agent.simulate_episode()\n",
    "                    eval_episode_rewards.append(reward)\n",
    "                    eval_episode_steps.append(steps)\n",
    "                mr = sum(eval_episode_rewards) / len(eval_episode_rewards)\n",
    "                ms = sum(eval_episode_steps) / len(eval_episode_steps)\n",
    "                print(f'[EVAL] episode : {episode}, reward mean : {mr}, step mean : {ms},')\n",
    "\n",
    "    agent.train(n_iter=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21bbcc0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 0, done reward : 0.6300000082701445, total_step : 1, cur_epsilon : 0.6999799999999999\n",
      "episode : 1, done reward : 0.9100000020116568, total_step : 11, cur_epsilon : 0.69978\n",
      "episode : 2, done reward : 0.09000002034008503, total_step : 103, cur_epsilon : 0.69794\n",
      "episode : 3, done reward : 0.9900000002235174, total_step : 105, cur_epsilon : 0.6979\n",
      "episode : 4, done reward : 0.9600000008940697, total_step : 110, cur_epsilon : 0.6978\n",
      "episode : 5, done reward : 0.3200000151991844, total_step : 179, cur_epsilon : 0.6964199999999999\n",
      "episode : 6, done reward : 0.9300000015646219, total_step : 187, cur_epsilon : 0.69626\n",
      "episode : 7, done reward : 0.8300000037997961, total_step : 205, cur_epsilon : 0.6959\n",
      "episode : 8, done reward : -0.9999999776482582, total_step : 305, cur_epsilon : 0.6939\n",
      "episode : 9, done reward : 0.8200000040233135, total_step : 324, cur_epsilon : 0.6935199999999999\n",
      "episode : 10, done reward : 0.9600000008940697, total_step : 329, cur_epsilon : 0.6934199999999999\n",
      "episode : 11, done reward : 0.880000002682209, total_step : 342, cur_epsilon : 0.69316\n",
      "episode : 12, done reward : 0.9700000006705523, total_step : 346, cur_epsilon : 0.6930799999999999\n",
      "episode : 13, done reward : 0.9100000020116568, total_step : 356, cur_epsilon : 0.6928799999999999\n",
      "episode : 14, done reward : 0.9400000013411045, total_step : 363, cur_epsilon : 0.6927399999999999\n",
      "episode : 15, done reward : 0.9000000022351742, total_step : 374, cur_epsilon : 0.6925199999999999\n",
      "episode : 16, done reward : 0.3200000151991844, total_step : 443, cur_epsilon : 0.69114\n",
      "episode : 17, done reward : 0.9200000017881393, total_step : 452, cur_epsilon : 0.6909599999999999\n",
      "episode : 18, done reward : 0.5500000100582838, total_step : 498, cur_epsilon : 0.69004\n",
      "episode : 19, done reward : 1.0, total_step : 499, cur_epsilon : 0.69002\n",
      "episode : 20, done reward : 0.9800000004470348, total_step : 502, cur_epsilon : 0.6899599999999999\n",
      "episode : 21, done reward : 1.0, total_step : 503, cur_epsilon : 0.68994\n",
      "episode : 22, done reward : 0.7300000060349703, total_step : 531, cur_epsilon : 0.68938\n",
      "episode : 23, done reward : 0.9700000006705523, total_step : 535, cur_epsilon : 0.6892999999999999\n",
      "episode : 24, done reward : 0.9500000011175871, total_step : 541, cur_epsilon : 0.6891799999999999\n",
      "episode : 25, done reward : 1.0, total_step : 542, cur_epsilon : 0.68916\n",
      "episode : 26, done reward : 0.9200000017881393, total_step : 551, cur_epsilon : 0.6889799999999999\n",
      "episode : 27, done reward : 0.6100000087171793, total_step : 591, cur_epsilon : 0.6881799999999999\n",
      "episode : 28, done reward : 0.880000002682209, total_step : 604, cur_epsilon : 0.68792\n",
      "episode : 29, done reward : 0.7500000055879354, total_step : 630, cur_epsilon : 0.6874\n",
      "episode : 30, done reward : 0.42000001296401024, total_step : 689, cur_epsilon : 0.6862199999999999\n",
      "episode : 31, done reward : 0.5100000109523535, total_step : 739, cur_epsilon : 0.6852199999999999\n",
      "episode : 32, done reward : 0.7000000067055225, total_step : 770, cur_epsilon : 0.6846\n",
      "episode : 33, done reward : 0.8300000037997961, total_step : 788, cur_epsilon : 0.68424\n",
      "episode : 34, done reward : 0.37000001408159733, total_step : 852, cur_epsilon : 0.68296\n",
      "episode : 35, done reward : 0.9900000002235174, total_step : 854, cur_epsilon : 0.68292\n",
      "episode : 36, done reward : 0.3400000147521496, total_step : 921, cur_epsilon : 0.68158\n",
      "episode : 37, done reward : 0.9900000002235174, total_step : 923, cur_epsilon : 0.6815399999999999\n",
      "synced target net\n",
      "episode : 38, done reward : -0.9999999776482582, total_step : 1023, cur_epsilon : 0.6795399999999999\n",
      "episode : 39, done reward : 0.9000000022351742, total_step : 1034, cur_epsilon : 0.6793199999999999\n",
      "episode : 40, done reward : 0.9800000004470348, total_step : 1037, cur_epsilon : 0.67926\n",
      "episode : 41, done reward : 1.0, total_step : 1038, cur_epsilon : 0.67924\n",
      "episode : 42, done reward : 0.5800000093877316, total_step : 1081, cur_epsilon : 0.67838\n",
      "episode : 43, done reward : 0.9200000017881393, total_step : 1090, cur_epsilon : 0.6781999999999999\n",
      "episode : 44, done reward : 0.23000001721084118, total_step : 1168, cur_epsilon : 0.6766399999999999\n",
      "episode : 45, done reward : 0.5600000098347664, total_step : 1213, cur_epsilon : 0.67574\n",
      "episode : 46, done reward : 0.880000002682209, total_step : 1226, cur_epsilon : 0.67548\n",
      "episode : 47, done reward : 0.47000001184642315, total_step : 1280, cur_epsilon : 0.6744\n",
      "episode : 48, done reward : 0.8400000035762787, total_step : 1297, cur_epsilon : 0.67406\n",
      "episode : 49, done reward : 0.69000000692904, total_step : 1329, cur_epsilon : 0.6734199999999999\n",
      "episode : 50, done reward : 0.9400000013411045, total_step : 1336, cur_epsilon : 0.67328\n",
      "episode : 51, done reward : 0.9400000013411045, total_step : 1343, cur_epsilon : 0.67314\n",
      "episode : 52, done reward : 0.640000008046627, total_step : 1380, cur_epsilon : 0.6724\n",
      "episode : 53, done reward : 0.6300000082701445, total_step : 1418, cur_epsilon : 0.6716399999999999\n",
      "episode : 54, done reward : 0.7200000062584877, total_step : 1447, cur_epsilon : 0.67106\n",
      "episode : 55, done reward : 0.7100000064820051, total_step : 1477, cur_epsilon : 0.67046\n",
      "episode : 56, done reward : 0.9400000013411045, total_step : 1484, cur_epsilon : 0.6703199999999999\n",
      "episode : 57, done reward : 0.9600000008940697, total_step : 1489, cur_epsilon : 0.6702199999999999\n",
      "episode : 58, done reward : 0.6800000071525574, total_step : 1522, cur_epsilon : 0.6695599999999999\n",
      "episode : 59, done reward : 0.9600000008940697, total_step : 1527, cur_epsilon : 0.6694599999999999\n",
      "episode : 60, done reward : 0.7000000067055225, total_step : 1558, cur_epsilon : 0.66884\n",
      "episode : 61, done reward : 0.9300000015646219, total_step : 1566, cur_epsilon : 0.6686799999999999\n",
      "episode : 62, done reward : 0.7500000055879354, total_step : 1592, cur_epsilon : 0.66816\n",
      "episode : 63, done reward : 0.9700000006705523, total_step : 1596, cur_epsilon : 0.66808\n",
      "episode : 64, done reward : 0.6800000071525574, total_step : 1629, cur_epsilon : 0.6674199999999999\n",
      "episode : 65, done reward : 0.8000000044703484, total_step : 1650, cur_epsilon : 0.6669999999999999\n",
      "episode : 66, done reward : 0.9500000011175871, total_step : 1656, cur_epsilon : 0.6668799999999999\n",
      "episode : 67, done reward : 0.9000000022351742, total_step : 1667, cur_epsilon : 0.6666599999999999\n",
      "episode : 68, done reward : 0.8200000040233135, total_step : 1686, cur_epsilon : 0.66628\n",
      "episode : 69, done reward : 0.9100000020116568, total_step : 1696, cur_epsilon : 0.66608\n",
      "episode : 70, done reward : 0.8500000033527613, total_step : 1712, cur_epsilon : 0.6657599999999999\n",
      "episode : 71, done reward : 0.9500000011175871, total_step : 1718, cur_epsilon : 0.66564\n",
      "episode : 72, done reward : 0.8000000044703484, total_step : 1739, cur_epsilon : 0.6652199999999999\n",
      "episode : 73, done reward : 0.6600000075995922, total_step : 1774, cur_epsilon : 0.66452\n",
      "episode : 74, done reward : 0.1700000185519457, total_step : 1858, cur_epsilon : 0.66284\n",
      "episode : 75, done reward : 0.8500000033527613, total_step : 1874, cur_epsilon : 0.66252\n",
      "episode : 76, done reward : 0.9700000006705523, total_step : 1878, cur_epsilon : 0.6624399999999999\n",
      "episode : 77, done reward : 0.9800000004470348, total_step : 1881, cur_epsilon : 0.66238\n",
      "episode : 78, done reward : 0.5100000109523535, total_step : 1931, cur_epsilon : 0.66138\n",
      "episode : 79, done reward : 0.9000000022351742, total_step : 1942, cur_epsilon : 0.66116\n",
      "episode : 80, done reward : 0.7700000051409006, total_step : 1966, cur_epsilon : 0.6606799999999999\n",
      "episode : 81, done reward : 0.9900000002235174, total_step : 1968, cur_epsilon : 0.66064\n",
      "episode : 82, done reward : 0.9300000015646219, total_step : 1976, cur_epsilon : 0.66048\n",
      "episode : 83, done reward : 1.0, total_step : 1977, cur_epsilon : 0.6604599999999999\n",
      "episode : 84, done reward : 0.9500000011175871, total_step : 1983, cur_epsilon : 0.6603399999999999\n",
      "episode : 85, done reward : 0.9400000013411045, total_step : 1990, cur_epsilon : 0.6601999999999999\n",
      "synced target net\n",
      "episode : 86, done reward : 0.760000005364418, total_step : 2015, cur_epsilon : 0.6597\n",
      "episode : 87, done reward : 0.2600000165402889, total_step : 2090, cur_epsilon : 0.6582\n",
      "episode : 88, done reward : 0.9800000004470348, total_step : 2093, cur_epsilon : 0.65814\n",
      "episode : 89, done reward : 0.9000000022351742, total_step : 2104, cur_epsilon : 0.65792\n",
      "episode : 90, done reward : 0.6100000087171793, total_step : 2144, cur_epsilon : 0.6571199999999999\n",
      "episode : 91, done reward : 0.9400000013411045, total_step : 2151, cur_epsilon : 0.6569799999999999\n",
      "episode : 92, done reward : 0.9500000011175871, total_step : 2157, cur_epsilon : 0.65686\n",
      "episode : 93, done reward : 0.9300000015646219, total_step : 2165, cur_epsilon : 0.6567\n",
      "episode : 94, done reward : 0.6000000089406967, total_step : 2206, cur_epsilon : 0.6558799999999999\n",
      "episode : 95, done reward : 0.8600000031292439, total_step : 2221, cur_epsilon : 0.6555799999999999\n",
      "episode : 96, done reward : 0.9700000006705523, total_step : 2225, cur_epsilon : 0.6555\n",
      "episode : 97, done reward : 1.0, total_step : 2226, cur_epsilon : 0.65548\n",
      "episode : 98, done reward : 0.7400000058114529, total_step : 2253, cur_epsilon : 0.65494\n",
      "episode : 99, done reward : 0.8900000024586916, total_step : 2265, cur_epsilon : 0.6547\n",
      "episode : 100, done reward : -0.9999999776482582, total_step : 2365, cur_epsilon : 0.6527\n",
      "episode : 101, done reward : 0.6300000082701445, total_step : 2403, cur_epsilon : 0.65194\n",
      "episode : 102, done reward : 0.8100000042468309, total_step : 2423, cur_epsilon : 0.65154\n",
      "episode : 103, done reward : 1.0, total_step : 2424, cur_epsilon : 0.65152\n",
      "episode : 104, done reward : 0.7700000051409006, total_step : 2448, cur_epsilon : 0.65104\n",
      "episode : 105, done reward : 0.8700000029057264, total_step : 2462, cur_epsilon : 0.65076\n",
      "episode : 106, done reward : 0.9000000022351742, total_step : 2473, cur_epsilon : 0.65054\n",
      "episode : 107, done reward : 0.31000001542270184, total_step : 2543, cur_epsilon : 0.6491399999999999\n",
      "episode : 108, done reward : 0.8200000040233135, total_step : 2562, cur_epsilon : 0.64876\n",
      "episode : 109, done reward : 0.9600000008940697, total_step : 2567, cur_epsilon : 0.64866\n",
      "episode : 110, done reward : 0.8600000031292439, total_step : 2582, cur_epsilon : 0.6483599999999999\n",
      "episode : 111, done reward : 0.9700000006705523, total_step : 2586, cur_epsilon : 0.64828\n",
      "episode : 112, done reward : 0.9900000002235174, total_step : 2588, cur_epsilon : 0.6482399999999999\n",
      "episode : 113, done reward : 0.8900000024586916, total_step : 2600, cur_epsilon : 0.6479999999999999\n",
      "episode : 114, done reward : 1.0, total_step : 2601, cur_epsilon : 0.64798\n",
      "episode : 115, done reward : 0.7200000062584877, total_step : 2630, cur_epsilon : 0.6474\n",
      "episode : 116, done reward : 0.5600000098347664, total_step : 2675, cur_epsilon : 0.6465\n",
      "episode : 117, done reward : 0.9500000011175871, total_step : 2681, cur_epsilon : 0.64638\n",
      "episode : 118, done reward : 0.9900000002235174, total_step : 2683, cur_epsilon : 0.6463399999999999\n",
      "episode : 119, done reward : 0.9000000022351742, total_step : 2694, cur_epsilon : 0.6461199999999999\n",
      "episode : 120, done reward : 0.8400000035762787, total_step : 2711, cur_epsilon : 0.6457799999999999\n",
      "episode : 121, done reward : 0.6800000071525574, total_step : 2744, cur_epsilon : 0.6451199999999999\n",
      "episode : 122, done reward : 0.8100000042468309, total_step : 2764, cur_epsilon : 0.64472\n",
      "episode : 123, done reward : 0.7000000067055225, total_step : 2795, cur_epsilon : 0.6441\n",
      "episode : 124, done reward : -0.9999999776482582, total_step : 2895, cur_epsilon : 0.6421\n",
      "episode : 125, done reward : 0.9900000002235174, total_step : 2897, cur_epsilon : 0.64206\n",
      "episode : 126, done reward : 0.640000008046627, total_step : 2934, cur_epsilon : 0.64132\n",
      "episode : 127, done reward : 0.640000008046627, total_step : 2971, cur_epsilon : 0.6405799999999999\n",
      "synced target net\n",
      "episode : 128, done reward : 0.6300000082701445, total_step : 3009, cur_epsilon : 0.6398199999999999\n",
      "episode : 129, done reward : 1.0, total_step : 3010, cur_epsilon : 0.6397999999999999\n",
      "episode : 130, done reward : 0.9600000008940697, total_step : 3015, cur_epsilon : 0.6396999999999999\n",
      "episode : 131, done reward : 0.6800000071525574, total_step : 3048, cur_epsilon : 0.6390399999999999\n",
      "episode : 132, done reward : 0.6800000071525574, total_step : 3081, cur_epsilon : 0.63838\n",
      "episode : 133, done reward : 0.9600000008940697, total_step : 3086, cur_epsilon : 0.63828\n",
      "episode : 134, done reward : 0.5900000091642141, total_step : 3128, cur_epsilon : 0.63744\n",
      "episode : 135, done reward : 0.9800000004470348, total_step : 3131, cur_epsilon : 0.63738\n",
      "episode : 136, done reward : 0.9200000017881393, total_step : 3140, cur_epsilon : 0.6372\n",
      "episode : 137, done reward : 0.9900000002235174, total_step : 3142, cur_epsilon : 0.63716\n",
      "episode : 138, done reward : 0.9200000017881393, total_step : 3151, cur_epsilon : 0.63698\n",
      "episode : 139, done reward : 0.9200000017881393, total_step : 3160, cur_epsilon : 0.6367999999999999\n",
      "episode : 140, done reward : 0.9400000013411045, total_step : 3167, cur_epsilon : 0.63666\n",
      "episode : 141, done reward : 0.9800000004470348, total_step : 3170, cur_epsilon : 0.6365999999999999\n",
      "episode : 142, done reward : 0.9800000004470348, total_step : 3173, cur_epsilon : 0.63654\n",
      "episode : 143, done reward : 1.0, total_step : 3174, cur_epsilon : 0.63652\n",
      "episode : 144, done reward : 0.880000002682209, total_step : 3187, cur_epsilon : 0.6362599999999999\n",
      "episode : 145, done reward : 0.8200000040233135, total_step : 3206, cur_epsilon : 0.63588\n",
      "episode : 146, done reward : 0.9800000004470348, total_step : 3209, cur_epsilon : 0.6358199999999999\n",
      "episode : 147, done reward : 0.880000002682209, total_step : 3222, cur_epsilon : 0.6355599999999999\n",
      "episode : 148, done reward : 0.9700000006705523, total_step : 3226, cur_epsilon : 0.6354799999999999\n",
      "episode : 149, done reward : 0.9800000004470348, total_step : 3229, cur_epsilon : 0.63542\n",
      "episode : 150, done reward : 0.9800000004470348, total_step : 3232, cur_epsilon : 0.6353599999999999\n",
      "episode : 151, done reward : 0.8600000031292439, total_step : 3247, cur_epsilon : 0.63506\n",
      "episode : 152, done reward : 0.7100000064820051, total_step : 3277, cur_epsilon : 0.6344599999999999\n",
      "episode : 153, done reward : 0.9600000008940697, total_step : 3282, cur_epsilon : 0.6343599999999999\n",
      "episode : 154, done reward : 0.7200000062584877, total_step : 3311, cur_epsilon : 0.63378\n",
      "episode : 155, done reward : 0.9300000015646219, total_step : 3319, cur_epsilon : 0.63362\n",
      "episode : 156, done reward : 0.9400000013411045, total_step : 3326, cur_epsilon : 0.6334799999999999\n",
      "episode : 157, done reward : 0.030000021681189537, total_step : 3424, cur_epsilon : 0.63152\n",
      "episode : 158, done reward : 0.760000005364418, total_step : 3449, cur_epsilon : 0.6310199999999999\n",
      "episode : 159, done reward : 0.6300000082701445, total_step : 3487, cur_epsilon : 0.6302599999999999\n",
      "episode : 160, done reward : 0.5400000102818012, total_step : 3534, cur_epsilon : 0.62932\n",
      "episode : 161, done reward : 0.9800000004470348, total_step : 3537, cur_epsilon : 0.6292599999999999\n",
      "episode : 162, done reward : 1.0, total_step : 3538, cur_epsilon : 0.6292399999999999\n",
      "episode : 163, done reward : 0.8100000042468309, total_step : 3558, cur_epsilon : 0.62884\n",
      "episode : 164, done reward : 0.19000001810491085, total_step : 3640, cur_epsilon : 0.6272\n",
      "episode : 165, done reward : 1.0, total_step : 3641, cur_epsilon : 0.62718\n",
      "episode : 166, done reward : 0.3800000138580799, total_step : 3704, cur_epsilon : 0.6259199999999999\n",
      "episode : 167, done reward : 0.5800000093877316, total_step : 3747, cur_epsilon : 0.62506\n",
      "episode : 168, done reward : 0.8400000035762787, total_step : 3764, cur_epsilon : 0.6247199999999999\n",
      "episode : 169, done reward : 0.6800000071525574, total_step : 3797, cur_epsilon : 0.62406\n",
      "episode : 170, done reward : 0.9900000002235174, total_step : 3799, cur_epsilon : 0.6240199999999999\n",
      "episode : 171, done reward : 0.9000000022351742, total_step : 3810, cur_epsilon : 0.6237999999999999\n",
      "episode : 172, done reward : 0.7700000051409006, total_step : 3834, cur_epsilon : 0.62332\n",
      "episode : 173, done reward : 0.9100000020116568, total_step : 3844, cur_epsilon : 0.6231199999999999\n",
      "episode : 174, done reward : 0.8200000040233135, total_step : 3863, cur_epsilon : 0.62274\n",
      "episode : 175, done reward : 0.7900000046938658, total_step : 3885, cur_epsilon : 0.6223\n",
      "episode : 176, done reward : 0.8200000040233135, total_step : 3904, cur_epsilon : 0.6219199999999999\n",
      "episode : 177, done reward : 0.9400000013411045, total_step : 3911, cur_epsilon : 0.62178\n",
      "episode : 178, done reward : 0.9500000011175871, total_step : 3917, cur_epsilon : 0.62166\n",
      "episode : 179, done reward : 0.880000002682209, total_step : 3930, cur_epsilon : 0.6214\n",
      "episode : 180, done reward : 0.7100000064820051, total_step : 3960, cur_epsilon : 0.6207999999999999\n",
      "episode : 181, done reward : 0.9100000020116568, total_step : 3970, cur_epsilon : 0.6205999999999999\n",
      "episode : 182, done reward : 0.9900000002235174, total_step : 3972, cur_epsilon : 0.62056\n",
      "episode : 183, done reward : 0.8700000029057264, total_step : 3986, cur_epsilon : 0.6202799999999999\n",
      "synced target net\n",
      "episode : 184, done reward : 0.30000001564621925, total_step : 4057, cur_epsilon : 0.61886\n",
      "episode : 185, done reward : -0.9999999776482582, total_step : 4157, cur_epsilon : 0.61686\n",
      "episode : 186, done reward : 0.7800000049173832, total_step : 4180, cur_epsilon : 0.6164\n",
      "episode : 187, done reward : 0.8400000035762787, total_step : 4197, cur_epsilon : 0.6160599999999999\n",
      "episode : 188, done reward : 0.47000001184642315, total_step : 4251, cur_epsilon : 0.61498\n",
      "episode : 189, done reward : 0.9900000002235174, total_step : 4253, cur_epsilon : 0.6149399999999999\n",
      "episode : 190, done reward : 0.7200000062584877, total_step : 4282, cur_epsilon : 0.61436\n",
      "episode : 191, done reward : 0.9500000011175871, total_step : 4288, cur_epsilon : 0.6142399999999999\n",
      "episode : 192, done reward : 0.47000001184642315, total_step : 4342, cur_epsilon : 0.6131599999999999\n",
      "episode : 193, done reward : 0.7500000055879354, total_step : 4368, cur_epsilon : 0.61264\n",
      "episode : 194, done reward : 0.8500000033527613, total_step : 4384, cur_epsilon : 0.61232\n",
      "episode : 195, done reward : 0.9300000015646219, total_step : 4392, cur_epsilon : 0.6121599999999999\n",
      "episode : 196, done reward : 0.8600000031292439, total_step : 4407, cur_epsilon : 0.61186\n",
      "episode : 197, done reward : 0.6700000073760748, total_step : 4441, cur_epsilon : 0.61118\n",
      "episode : 198, done reward : 0.8300000037997961, total_step : 4459, cur_epsilon : 0.6108199999999999\n",
      "episode : 199, done reward : 0.8600000031292439, total_step : 4474, cur_epsilon : 0.61052\n",
      "episode : 200, done reward : 0.8700000029057264, total_step : 4488, cur_epsilon : 0.6102399999999999\n",
      "episode : 201, done reward : 0.7000000067055225, total_step : 4519, cur_epsilon : 0.6096199999999999\n",
      "episode : 202, done reward : 0.9800000004470348, total_step : 4522, cur_epsilon : 0.60956\n",
      "episode : 203, done reward : 0.8400000035762787, total_step : 4539, cur_epsilon : 0.60922\n",
      "episode : 204, done reward : 0.8600000031292439, total_step : 4554, cur_epsilon : 0.6089199999999999\n",
      "episode : 205, done reward : 0.8900000024586916, total_step : 4566, cur_epsilon : 0.60868\n",
      "episode : 206, done reward : 0.8700000029057264, total_step : 4580, cur_epsilon : 0.6083999999999999\n",
      "episode : 207, done reward : 0.9400000013411045, total_step : 4587, cur_epsilon : 0.6082599999999999\n",
      "episode : 208, done reward : 0.7800000049173832, total_step : 4610, cur_epsilon : 0.6077999999999999\n",
      "episode : 209, done reward : 0.8000000044703484, total_step : 4631, cur_epsilon : 0.6073799999999999\n",
      "episode : 210, done reward : 0.7300000060349703, total_step : 4659, cur_epsilon : 0.6068199999999999\n",
      "episode : 211, done reward : 0.8400000035762787, total_step : 4676, cur_epsilon : 0.6064799999999999\n",
      "episode : 212, done reward : 0.8600000031292439, total_step : 4691, cur_epsilon : 0.6061799999999999\n",
      "episode : 213, done reward : 0.7400000058114529, total_step : 4718, cur_epsilon : 0.60564\n",
      "episode : 214, done reward : 0.9300000015646219, total_step : 4726, cur_epsilon : 0.6054799999999999\n",
      "episode : 215, done reward : 0.8900000024586916, total_step : 4738, cur_epsilon : 0.60524\n",
      "episode : 216, done reward : 0.7900000046938658, total_step : 4760, cur_epsilon : 0.6048\n",
      "episode : 217, done reward : 0.7500000055879354, total_step : 4786, cur_epsilon : 0.6042799999999999\n",
      "episode : 218, done reward : 0.6300000082701445, total_step : 4824, cur_epsilon : 0.60352\n",
      "episode : 219, done reward : 0.9700000006705523, total_step : 4828, cur_epsilon : 0.60344\n",
      "episode : 220, done reward : 0.13000001944601536, total_step : 4916, cur_epsilon : 0.60168\n",
      "episode : 221, done reward : 0.6600000075995922, total_step : 4951, cur_epsilon : 0.60098\n",
      "episode : 222, done reward : 0.9400000013411045, total_step : 4958, cur_epsilon : 0.6008399999999999\n",
      "episode : 223, done reward : 0.8300000037997961, total_step : 4976, cur_epsilon : 0.6004799999999999\n",
      "synced target net\n",
      "episode : 224, done reward : 0.04000002145767212, total_step : 5073, cur_epsilon : 0.59854\n",
      "episode : 225, done reward : 0.8500000033527613, total_step : 5089, cur_epsilon : 0.59822\n",
      "episode : 226, done reward : 0.5800000093877316, total_step : 5132, cur_epsilon : 0.59736\n",
      "episode : 227, done reward : 0.9500000011175871, total_step : 5138, cur_epsilon : 0.59724\n",
      "episode : 228, done reward : 0.3900000136345625, total_step : 5200, cur_epsilon : 0.596\n",
      "episode : 229, done reward : 0.9600000008940697, total_step : 5205, cur_epsilon : 0.5959\n",
      "episode : 230, done reward : 0.8200000040233135, total_step : 5224, cur_epsilon : 0.5955199999999999\n",
      "episode : 231, done reward : 0.9800000004470348, total_step : 5227, cur_epsilon : 0.59546\n",
      "episode : 232, done reward : 0.9600000008940697, total_step : 5232, cur_epsilon : 0.59536\n",
      "episode : 233, done reward : 0.7700000051409006, total_step : 5256, cur_epsilon : 0.59488\n",
      "episode : 234, done reward : 0.5200000107288361, total_step : 5305, cur_epsilon : 0.5939\n",
      "episode : 235, done reward : 0.46000001206994057, total_step : 5360, cur_epsilon : 0.5928\n",
      "episode : 236, done reward : 0.8000000044703484, total_step : 5381, cur_epsilon : 0.5923799999999999\n",
      "episode : 237, done reward : 0.7700000051409006, total_step : 5405, cur_epsilon : 0.5919\n",
      "episode : 238, done reward : 0.9400000013411045, total_step : 5412, cur_epsilon : 0.59176\n",
      "episode : 239, done reward : 1.0, total_step : 5413, cur_epsilon : 0.5917399999999999\n",
      "episode : 240, done reward : 0.6300000082701445, total_step : 5451, cur_epsilon : 0.59098\n",
      "episode : 241, done reward : 0.8200000040233135, total_step : 5470, cur_epsilon : 0.5906\n",
      "episode : 242, done reward : -0.9999999776482582, total_step : 5570, cur_epsilon : 0.5886\n",
      "episode : 243, done reward : 0.9600000008940697, total_step : 5575, cur_epsilon : 0.5884999999999999\n",
      "episode : 244, done reward : 0.9000000022351742, total_step : 5586, cur_epsilon : 0.5882799999999999\n",
      "episode : 245, done reward : 0.8100000042468309, total_step : 5606, cur_epsilon : 0.58788\n",
      "episode : 246, done reward : 0.8100000042468309, total_step : 5626, cur_epsilon : 0.58748\n",
      "episode : 247, done reward : 0.8200000040233135, total_step : 5645, cur_epsilon : 0.5871\n",
      "episode : 248, done reward : 0.5500000100582838, total_step : 5691, cur_epsilon : 0.5861799999999999\n",
      "episode : 249, done reward : 0.9700000006705523, total_step : 5695, cur_epsilon : 0.5861\n",
      "episode : 250, done reward : 0.9600000008940697, total_step : 5700, cur_epsilon : 0.586\n",
      "episode : 251, done reward : 0.5500000100582838, total_step : 5746, cur_epsilon : 0.5850799999999999\n",
      "episode : 252, done reward : 0.9500000011175871, total_step : 5752, cur_epsilon : 0.5849599999999999\n",
      "episode : 253, done reward : 0.4300000127404928, total_step : 5810, cur_epsilon : 0.5838\n",
      "episode : 254, done reward : 0.9700000006705523, total_step : 5814, cur_epsilon : 0.58372\n",
      "episode : 255, done reward : 1.0, total_step : 5815, cur_epsilon : 0.5837\n",
      "episode : 256, done reward : 0.36000001430511475, total_step : 5880, cur_epsilon : 0.5823999999999999\n",
      "episode : 257, done reward : 0.48000001162290573, total_step : 5933, cur_epsilon : 0.58134\n",
      "episode : 258, done reward : 0.9800000004470348, total_step : 5936, cur_epsilon : 0.5812799999999999\n",
      "episode : 259, done reward : 0.7900000046938658, total_step : 5958, cur_epsilon : 0.5808399999999999\n",
      "synced target net\n",
      "episode : 260, done reward : 0.2800000160932541, total_step : 6031, cur_epsilon : 0.57938\n",
      "episode : 261, done reward : 0.880000002682209, total_step : 6044, cur_epsilon : 0.57912\n",
      "episode : 262, done reward : 0.9700000006705523, total_step : 6048, cur_epsilon : 0.57904\n",
      "episode : 263, done reward : 0.8000000044703484, total_step : 6069, cur_epsilon : 0.5786199999999999\n",
      "episode : 264, done reward : 0.9200000017881393, total_step : 6078, cur_epsilon : 0.57844\n",
      "episode : 265, done reward : 0.9300000015646219, total_step : 6086, cur_epsilon : 0.5782799999999999\n",
      "episode : 266, done reward : 0.6100000087171793, total_step : 6126, cur_epsilon : 0.57748\n",
      "episode : 267, done reward : 0.6100000087171793, total_step : 6166, cur_epsilon : 0.57668\n",
      "episode : 268, done reward : 0.020000021904706955, total_step : 6265, cur_epsilon : 0.5747\n",
      "episode : 269, done reward : 0.9600000008940697, total_step : 6270, cur_epsilon : 0.5746\n",
      "episode : 270, done reward : 0.9500000011175871, total_step : 6276, cur_epsilon : 0.57448\n",
      "episode : 271, done reward : 0.9100000020116568, total_step : 6286, cur_epsilon : 0.5742799999999999\n",
      "episode : 272, done reward : 0.7700000051409006, total_step : 6310, cur_epsilon : 0.5738\n",
      "episode : 273, done reward : 0.7400000058114529, total_step : 6337, cur_epsilon : 0.57326\n",
      "episode : 274, done reward : 0.2700000163167715, total_step : 6411, cur_epsilon : 0.57178\n",
      "episode : 275, done reward : 0.2800000160932541, total_step : 6484, cur_epsilon : 0.5703199999999999\n",
      "episode : 276, done reward : 0.8400000035762787, total_step : 6501, cur_epsilon : 0.5699799999999999\n",
      "episode : 277, done reward : 0.6100000087171793, total_step : 6541, cur_epsilon : 0.56918\n",
      "episode : 278, done reward : 0.9900000002235174, total_step : 6543, cur_epsilon : 0.56914\n",
      "episode : 279, done reward : 0.8500000033527613, total_step : 6559, cur_epsilon : 0.56882\n",
      "episode : 280, done reward : 0.5400000102818012, total_step : 6606, cur_epsilon : 0.5678799999999999\n",
      "episode : 281, done reward : 0.8400000035762787, total_step : 6623, cur_epsilon : 0.5675399999999999\n",
      "episode : 282, done reward : 0.9300000015646219, total_step : 6631, cur_epsilon : 0.56738\n",
      "episode : 283, done reward : 1.0, total_step : 6632, cur_epsilon : 0.56736\n",
      "episode : 284, done reward : 1.0, total_step : 6633, cur_epsilon : 0.56734\n",
      "episode : 285, done reward : 0.9700000006705523, total_step : 6637, cur_epsilon : 0.56726\n",
      "episode : 286, done reward : 0.8200000040233135, total_step : 6656, cur_epsilon : 0.5668799999999999\n",
      "episode : 287, done reward : 0.5400000102818012, total_step : 6703, cur_epsilon : 0.5659399999999999\n",
      "episode : 288, done reward : 0.7500000055879354, total_step : 6729, cur_epsilon : 0.5654199999999999\n",
      "episode : 289, done reward : 0.9400000013411045, total_step : 6736, cur_epsilon : 0.56528\n",
      "episode : 290, done reward : 0.6300000082701445, total_step : 6774, cur_epsilon : 0.5645199999999999\n",
      "episode : 291, done reward : 0.6100000087171793, total_step : 6814, cur_epsilon : 0.56372\n",
      "episode : 292, done reward : 0.9600000008940697, total_step : 6819, cur_epsilon : 0.56362\n",
      "episode : 293, done reward : 0.8500000033527613, total_step : 6835, cur_epsilon : 0.5632999999999999\n",
      "episode : 294, done reward : 0.9900000002235174, total_step : 6837, cur_epsilon : 0.56326\n",
      "episode : 295, done reward : 0.9200000017881393, total_step : 6846, cur_epsilon : 0.56308\n",
      "episode : 296, done reward : 0.8700000029057264, total_step : 6860, cur_epsilon : 0.5628\n",
      "episode : 297, done reward : 0.8900000024586916, total_step : 6872, cur_epsilon : 0.56256\n",
      "episode : 298, done reward : 0.9100000020116568, total_step : 6882, cur_epsilon : 0.56236\n",
      "episode : 299, done reward : 0.8200000040233135, total_step : 6901, cur_epsilon : 0.5619799999999999\n",
      "episode : 300, done reward : 0.8600000031292439, total_step : 6916, cur_epsilon : 0.56168\n",
      "episode : 301, done reward : 0.9100000020116568, total_step : 6926, cur_epsilon : 0.56148\n",
      "episode : 302, done reward : 0.9900000002235174, total_step : 6928, cur_epsilon : 0.5614399999999999\n",
      "episode : 303, done reward : 0.9300000015646219, total_step : 6936, cur_epsilon : 0.56128\n",
      "episode : 304, done reward : 0.8100000042468309, total_step : 6956, cur_epsilon : 0.5608799999999999\n",
      "episode : 305, done reward : 0.6300000082701445, total_step : 6994, cur_epsilon : 0.56012\n",
      "synced target net\n",
      "episode : 306, done reward : 0.24000001698732376, total_step : 7071, cur_epsilon : 0.55858\n",
      "episode : 307, done reward : 0.8300000037997961, total_step : 7089, cur_epsilon : 0.5582199999999999\n",
      "episode : 308, done reward : 1.0, total_step : 7090, cur_epsilon : 0.5581999999999999\n",
      "episode : 309, done reward : 0.6100000087171793, total_step : 7130, cur_epsilon : 0.5573999999999999\n",
      "episode : 310, done reward : 0.9500000011175871, total_step : 7136, cur_epsilon : 0.55728\n",
      "episode : 311, done reward : 0.7100000064820051, total_step : 7166, cur_epsilon : 0.55668\n",
      "episode : 312, done reward : 0.7000000067055225, total_step : 7197, cur_epsilon : 0.55606\n",
      "episode : 313, done reward : -0.9999999776482582, total_step : 7297, cur_epsilon : 0.55406\n",
      "episode : 314, done reward : 0.6600000075995922, total_step : 7332, cur_epsilon : 0.55336\n",
      "episode : 315, done reward : 0.8100000042468309, total_step : 7352, cur_epsilon : 0.5529599999999999\n",
      "episode : 316, done reward : 0.9600000008940697, total_step : 7357, cur_epsilon : 0.5528599999999999\n",
      "episode : 317, done reward : 0.7300000060349703, total_step : 7385, cur_epsilon : 0.5523\n",
      "episode : 318, done reward : 0.42000001296401024, total_step : 7444, cur_epsilon : 0.5511199999999999\n",
      "episode : 319, done reward : 0.5900000091642141, total_step : 7486, cur_epsilon : 0.55028\n",
      "episode : 320, done reward : 0.760000005364418, total_step : 7511, cur_epsilon : 0.5497799999999999\n",
      "episode : 321, done reward : 0.8400000035762787, total_step : 7528, cur_epsilon : 0.5494399999999999\n",
      "episode : 322, done reward : 1.0, total_step : 7529, cur_epsilon : 0.54942\n",
      "episode : 323, done reward : 0.7100000064820051, total_step : 7559, cur_epsilon : 0.54882\n",
      "episode : 324, done reward : 0.7900000046938658, total_step : 7581, cur_epsilon : 0.54838\n",
      "episode : 325, done reward : 0.7900000046938658, total_step : 7603, cur_epsilon : 0.54794\n",
      "episode : 326, done reward : 0.5300000105053186, total_step : 7651, cur_epsilon : 0.54698\n",
      "episode : 327, done reward : 0.7300000060349703, total_step : 7679, cur_epsilon : 0.5464199999999999\n",
      "episode : 328, done reward : 0.4900000113993883, total_step : 7731, cur_epsilon : 0.54538\n",
      "episode : 329, done reward : 0.9700000006705523, total_step : 7735, cur_epsilon : 0.5452999999999999\n",
      "episode : 330, done reward : 0.2700000163167715, total_step : 7809, cur_epsilon : 0.54382\n",
      "episode : 331, done reward : 0.8900000024586916, total_step : 7821, cur_epsilon : 0.54358\n",
      "episode : 332, done reward : 0.47000001184642315, total_step : 7875, cur_epsilon : 0.5425\n",
      "episode : 333, done reward : 0.9200000017881393, total_step : 7884, cur_epsilon : 0.5423199999999999\n",
      "episode : 334, done reward : 0.7800000049173832, total_step : 7907, cur_epsilon : 0.54186\n",
      "episode : 335, done reward : 0.6200000084936619, total_step : 7946, cur_epsilon : 0.54108\n",
      "episode : 336, done reward : 0.9400000013411045, total_step : 7953, cur_epsilon : 0.54094\n",
      "episode : 337, done reward : 0.9500000011175871, total_step : 7959, cur_epsilon : 0.54082\n",
      "episode : 338, done reward : 1.0, total_step : 7960, cur_epsilon : 0.5408\n",
      "synced target net\n",
      "episode : 339, done reward : 0.08000002056360245, total_step : 8053, cur_epsilon : 0.53894\n",
      "episode : 340, done reward : -0.9999999776482582, total_step : 8153, cur_epsilon : 0.53694\n",
      "episode : 341, done reward : 0.9700000006705523, total_step : 8157, cur_epsilon : 0.5368599999999999\n",
      "episode : 342, done reward : 0.5500000100582838, total_step : 8203, cur_epsilon : 0.53594\n",
      "episode : 343, done reward : 0.5000000111758709, total_step : 8254, cur_epsilon : 0.53492\n",
      "episode : 344, done reward : 0.9400000013411045, total_step : 8261, cur_epsilon : 0.5347799999999999\n",
      "episode : 345, done reward : 0.8100000042468309, total_step : 8281, cur_epsilon : 0.53438\n",
      "episode : 346, done reward : 0.8400000035762787, total_step : 8298, cur_epsilon : 0.53404\n",
      "episode : 347, done reward : 0.7700000051409006, total_step : 8322, cur_epsilon : 0.5335599999999999\n",
      "episode : 348, done reward : 0.9000000022351742, total_step : 8333, cur_epsilon : 0.5333399999999999\n",
      "episode : 349, done reward : 0.8900000024586916, total_step : 8345, cur_epsilon : 0.5330999999999999\n",
      "episode : 350, done reward : 0.450000012293458, total_step : 8401, cur_epsilon : 0.5319799999999999\n",
      "episode : 351, done reward : 0.7300000060349703, total_step : 8429, cur_epsilon : 0.53142\n",
      "episode : 352, done reward : 0.8700000029057264, total_step : 8443, cur_epsilon : 0.53114\n",
      "episode : 353, done reward : 0.5800000093877316, total_step : 8486, cur_epsilon : 0.53028\n",
      "episode : 354, done reward : 0.8600000031292439, total_step : 8501, cur_epsilon : 0.5299799999999999\n",
      "episode : 355, done reward : 0.8900000024586916, total_step : 8513, cur_epsilon : 0.52974\n",
      "episode : 356, done reward : 0.9000000022351742, total_step : 8524, cur_epsilon : 0.52952\n",
      "episode : 357, done reward : 0.9600000008940697, total_step : 8529, cur_epsilon : 0.52942\n",
      "episode : 358, done reward : 0.9400000013411045, total_step : 8536, cur_epsilon : 0.52928\n",
      "episode : 359, done reward : 0.7000000067055225, total_step : 8567, cur_epsilon : 0.5286599999999999\n",
      "episode : 360, done reward : 1.0, total_step : 8568, cur_epsilon : 0.52864\n",
      "episode : 361, done reward : 0.29000001586973667, total_step : 8640, cur_epsilon : 0.5271999999999999\n",
      "episode : 362, done reward : 0.8900000024586916, total_step : 8652, cur_epsilon : 0.52696\n",
      "episode : 363, done reward : 0.1100000198930502, total_step : 8742, cur_epsilon : 0.52516\n",
      "episode : 364, done reward : 0.6100000087171793, total_step : 8782, cur_epsilon : 0.5243599999999999\n",
      "episode : 365, done reward : 0.29000001586973667, total_step : 8854, cur_epsilon : 0.5229199999999999\n",
      "episode : 366, done reward : 0.15000001899898052, total_step : 8940, cur_epsilon : 0.5212\n",
      "episode : 367, done reward : 0.8600000031292439, total_step : 8955, cur_epsilon : 0.5208999999999999\n",
      "episode : 368, done reward : 0.8200000040233135, total_step : 8974, cur_epsilon : 0.52052\n",
      "synced target net\n",
      "episode : 369, done reward : 0.6600000075995922, total_step : 9009, cur_epsilon : 0.51982\n",
      "episode : 370, done reward : 0.09000002034008503, total_step : 9101, cur_epsilon : 0.51798\n",
      "episode : 371, done reward : 0.880000002682209, total_step : 9114, cur_epsilon : 0.51772\n",
      "episode : 372, done reward : -0.9999999776482582, total_step : 9214, cur_epsilon : 0.51572\n",
      "episode : 373, done reward : 0.0500000212341547, total_step : 9310, cur_epsilon : 0.5137999999999999\n",
      "episode : 374, done reward : 0.7400000058114529, total_step : 9337, cur_epsilon : 0.5132599999999999\n",
      "episode : 375, done reward : 0.8600000031292439, total_step : 9352, cur_epsilon : 0.51296\n",
      "episode : 376, done reward : 0.5100000109523535, total_step : 9402, cur_epsilon : 0.51196\n",
      "episode : 377, done reward : 0.9400000013411045, total_step : 9409, cur_epsilon : 0.5118199999999999\n",
      "episode : 378, done reward : 0.10000002011656761, total_step : 9500, cur_epsilon : 0.51\n",
      "episode : 379, done reward : 0.9700000006705523, total_step : 9504, cur_epsilon : 0.5099199999999999\n",
      "episode : 380, done reward : -0.9999999776482582, total_step : 9604, cur_epsilon : 0.5079199999999999\n",
      "episode : 381, done reward : 0.9100000020116568, total_step : 9614, cur_epsilon : 0.50772\n",
      "episode : 382, done reward : 1.0, total_step : 9615, cur_epsilon : 0.5076999999999999\n",
      "episode : 383, done reward : 0.41000001318752766, total_step : 9675, cur_epsilon : 0.5065\n",
      "episode : 384, done reward : 0.8400000035762787, total_step : 9692, cur_epsilon : 0.5061599999999999\n",
      "episode : 385, done reward : -0.9999999776482582, total_step : 9792, cur_epsilon : 0.5041599999999999\n",
      "episode : 386, done reward : 0.6600000075995922, total_step : 9827, cur_epsilon : 0.50346\n",
      "episode : 387, done reward : 0.4900000113993883, total_step : 9879, cur_epsilon : 0.50242\n",
      "episode : 388, done reward : 0.8100000042468309, total_step : 9899, cur_epsilon : 0.5020199999999999\n",
      "episode : 389, done reward : 0.880000002682209, total_step : 9912, cur_epsilon : 0.50176\n",
      "episode : 390, done reward : 0.7300000060349703, total_step : 9940, cur_epsilon : 0.5012\n",
      "episode : 391, done reward : 0.7300000060349703, total_step : 9968, cur_epsilon : 0.50064\n",
      "episode : 392, done reward : 0.9100000020116568, total_step : 9978, cur_epsilon : 0.50044\n",
      "episode : 393, done reward : 0.8500000033527613, total_step : 9994, cur_epsilon : 0.5001199999999999\n",
      "synced target net\n",
      "episode : 394, done reward : 0.5000000111758709, total_step : 10045, cur_epsilon : 0.4991\n",
      "episode : 395, done reward : 0.3800000138580799, total_step : 10108, cur_epsilon : 0.49783999999999995\n",
      "episode : 396, done reward : 0.8700000029057264, total_step : 10122, cur_epsilon : 0.49755999999999995\n",
      "episode : 397, done reward : -0.9999999776482582, total_step : 10222, cur_epsilon : 0.49555999999999994\n",
      "episode : 398, done reward : -0.9999999776482582, total_step : 10322, cur_epsilon : 0.49355999999999994\n",
      "episode : 399, done reward : 0.8100000042468309, total_step : 10342, cur_epsilon : 0.49315999999999993\n",
      "episode : 400, done reward : 0.7400000058114529, total_step : 10369, cur_epsilon : 0.49261999999999995\n",
      "episode : 401, done reward : 0.69000000692904, total_step : 10401, cur_epsilon : 0.49198\n",
      "episode : 402, done reward : 0.7100000064820051, total_step : 10431, cur_epsilon : 0.49137999999999993\n",
      "episode : 403, done reward : 0.07000002078711987, total_step : 10525, cur_epsilon : 0.48949999999999994\n",
      "episode : 404, done reward : 0.6600000075995922, total_step : 10560, cur_epsilon : 0.48879999999999996\n",
      "episode : 405, done reward : 0.7300000060349703, total_step : 10588, cur_epsilon : 0.48823999999999995\n",
      "episode : 406, done reward : -0.9999999776482582, total_step : 10688, cur_epsilon : 0.48623999999999995\n",
      "episode : 407, done reward : 0.7000000067055225, total_step : 10719, cur_epsilon : 0.48561999999999994\n",
      "episode : 408, done reward : 0.880000002682209, total_step : 10732, cur_epsilon : 0.48535999999999996\n",
      "episode : 409, done reward : 0.6200000084936619, total_step : 10771, cur_epsilon : 0.48457999999999996\n",
      "episode : 410, done reward : 0.8700000029057264, total_step : 10785, cur_epsilon : 0.48429999999999995\n",
      "episode : 411, done reward : 0.9600000008940697, total_step : 10790, cur_epsilon : 0.48419999999999996\n",
      "episode : 412, done reward : -0.9999999776482582, total_step : 10890, cur_epsilon : 0.48219999999999996\n",
      "episode : 413, done reward : 0.8100000042468309, total_step : 10910, cur_epsilon : 0.48179999999999995\n",
      "episode : 414, done reward : 0.9300000015646219, total_step : 10918, cur_epsilon : 0.48163999999999996\n",
      "synced target net\n",
      "episode : 415, done reward : 0.10000002011656761, total_step : 11009, cur_epsilon : 0.47981999999999997\n",
      "episode : 416, done reward : 0.020000021904706955, total_step : 11108, cur_epsilon : 0.47783999999999993\n",
      "episode : 417, done reward : 0.7800000049173832, total_step : 11131, cur_epsilon : 0.4773799999999999\n",
      "episode : 418, done reward : 0.9600000008940697, total_step : 11136, cur_epsilon : 0.4772799999999999\n",
      "episode : 419, done reward : 0.20000001788139343, total_step : 11217, cur_epsilon : 0.47565999999999997\n",
      "episode : 420, done reward : 0.47000001184642315, total_step : 11271, cur_epsilon : 0.47457999999999995\n",
      "episode : 421, done reward : 0.6600000075995922, total_step : 11306, cur_epsilon : 0.47387999999999997\n",
      "episode : 422, done reward : 0.9800000004470348, total_step : 11309, cur_epsilon : 0.47381999999999996\n",
      "episode : 423, done reward : 0.6800000071525574, total_step : 11342, cur_epsilon : 0.4731599999999999\n",
      "episode : 424, done reward : 0.5100000109523535, total_step : 11392, cur_epsilon : 0.47215999999999997\n",
      "episode : 425, done reward : 0.9700000006705523, total_step : 11396, cur_epsilon : 0.47207999999999994\n",
      "episode : 426, done reward : 0.8200000040233135, total_step : 11415, cur_epsilon : 0.47169999999999995\n",
      "episode : 427, done reward : -0.9999999776482582, total_step : 11515, cur_epsilon : 0.46969999999999995\n",
      "episode : 428, done reward : 0.6300000082701445, total_step : 11553, cur_epsilon : 0.46893999999999997\n",
      "episode : 429, done reward : -0.9999999776482582, total_step : 11653, cur_epsilon : 0.46693999999999997\n",
      "episode : 430, done reward : 0.8600000031292439, total_step : 11668, cur_epsilon : 0.46663999999999994\n",
      "episode : 431, done reward : 0.8200000040233135, total_step : 11687, cur_epsilon : 0.46625999999999995\n",
      "episode : 432, done reward : 0.6500000078231096, total_step : 11723, cur_epsilon : 0.46553999999999995\n",
      "episode : 433, done reward : 0.9900000002235174, total_step : 11725, cur_epsilon : 0.46549999999999997\n",
      "episode : 434, done reward : 0.880000002682209, total_step : 11738, cur_epsilon : 0.46524\n",
      "episode : 435, done reward : 0.9200000017881393, total_step : 11747, cur_epsilon : 0.4650599999999999\n",
      "episode : 436, done reward : 0.29000001586973667, total_step : 11819, cur_epsilon : 0.4636199999999999\n",
      "episode : 437, done reward : 0.3900000136345625, total_step : 11881, cur_epsilon : 0.46237999999999996\n",
      "episode : 438, done reward : 0.9900000002235174, total_step : 11883, cur_epsilon : 0.46234\n",
      "episode : 439, done reward : 0.9500000011175871, total_step : 11889, cur_epsilon : 0.46221999999999996\n",
      "episode : 440, done reward : 0.4900000113993883, total_step : 11941, cur_epsilon : 0.4611799999999999\n",
      "episode : 441, done reward : 0.7100000064820051, total_step : 11971, cur_epsilon : 0.46058\n",
      "episode : 442, done reward : 0.9600000008940697, total_step : 11976, cur_epsilon : 0.46047999999999994\n",
      "synced target net\n",
      "episode : 443, done reward : -0.9999999776482582, total_step : 12076, cur_epsilon : 0.45847999999999994\n",
      "episode : 444, done reward : 0.7900000046938658, total_step : 12098, cur_epsilon : 0.45803999999999995\n",
      "episode : 445, done reward : 0.13000001944601536, total_step : 12186, cur_epsilon : 0.45627999999999996\n",
      "episode : 446, done reward : 0.48000001162290573, total_step : 12239, cur_epsilon : 0.45521999999999996\n",
      "episode : 447, done reward : 0.8100000042468309, total_step : 12259, cur_epsilon : 0.45481999999999995\n",
      "episode : 448, done reward : 0.8900000024586916, total_step : 12271, cur_epsilon : 0.45458\n",
      "episode : 449, done reward : 0.7800000049173832, total_step : 12294, cur_epsilon : 0.45411999999999997\n",
      "episode : 450, done reward : 0.3800000138580799, total_step : 12357, cur_epsilon : 0.45285999999999993\n",
      "episode : 451, done reward : 0.6000000089406967, total_step : 12398, cur_epsilon : 0.45203999999999994\n",
      "episode : 452, done reward : -0.9999999776482582, total_step : 12498, cur_epsilon : 0.45004\n",
      "episode : 453, done reward : 0.6600000075995922, total_step : 12533, cur_epsilon : 0.44933999999999996\n",
      "episode : 454, done reward : 0.8500000033527613, total_step : 12549, cur_epsilon : 0.44902\n",
      "episode : 455, done reward : 1.0, total_step : 12550, cur_epsilon : 0.44899999999999995\n",
      "episode : 456, done reward : 0.20000001788139343, total_step : 12631, cur_epsilon : 0.44737999999999994\n",
      "episode : 457, done reward : 0.14000001922249794, total_step : 12718, cur_epsilon : 0.44564\n",
      "episode : 458, done reward : 1.0, total_step : 12719, cur_epsilon : 0.44561999999999996\n",
      "episode : 459, done reward : 0.12000001966953278, total_step : 12808, cur_epsilon : 0.44383999999999996\n",
      "episode : 460, done reward : 0.29000001586973667, total_step : 12880, cur_epsilon : 0.44239999999999996\n",
      "episode : 461, done reward : -0.9999999776482582, total_step : 12980, cur_epsilon : 0.44039999999999996\n",
      "episode : 462, done reward : 0.9800000004470348, total_step : 12983, cur_epsilon : 0.44033999999999995\n",
      "synced target net\n",
      "episode : 463, done reward : 0.6100000087171793, total_step : 13023, cur_epsilon : 0.43953999999999993\n",
      "episode : 464, done reward : 0.8000000044703484, total_step : 13044, cur_epsilon : 0.43911999999999995\n",
      "episode : 465, done reward : 0.5000000111758709, total_step : 13095, cur_epsilon : 0.43809999999999993\n",
      "episode : 466, done reward : -0.9999999776482582, total_step : 13195, cur_epsilon : 0.43609999999999993\n",
      "episode : 467, done reward : 0.19000001810491085, total_step : 13277, cur_epsilon : 0.43445999999999996\n",
      "episode : 468, done reward : 0.570000009611249, total_step : 13321, cur_epsilon : 0.43357999999999997\n",
      "episode : 469, done reward : 0.9300000015646219, total_step : 13329, cur_epsilon : 0.43341999999999997\n",
      "episode : 470, done reward : 0.5500000100582838, total_step : 13375, cur_epsilon : 0.43249999999999994\n",
      "episode : 471, done reward : 0.21000001765787601, total_step : 13455, cur_epsilon : 0.43089999999999995\n",
      "episode : 472, done reward : 0.5500000100582838, total_step : 13501, cur_epsilon : 0.42998\n",
      "episode : 473, done reward : 0.7400000058114529, total_step : 13528, cur_epsilon : 0.42943999999999993\n",
      "episode : 474, done reward : 0.7900000046938658, total_step : 13550, cur_epsilon : 0.42899999999999994\n",
      "episode : 475, done reward : 0.7500000055879354, total_step : 13576, cur_epsilon : 0.42847999999999997\n",
      "episode : 476, done reward : 0.640000008046627, total_step : 13613, cur_epsilon : 0.42773999999999995\n",
      "episode : 477, done reward : 0.25000001676380634, total_step : 13689, cur_epsilon : 0.42621999999999993\n",
      "episode : 478, done reward : 0.9200000017881393, total_step : 13698, cur_epsilon : 0.42604\n",
      "episode : 479, done reward : 0.3200000151991844, total_step : 13767, cur_epsilon : 0.42466\n",
      "episode : 480, done reward : 0.9200000017881393, total_step : 13776, cur_epsilon : 0.42447999999999997\n",
      "episode : 481, done reward : 0.8900000024586916, total_step : 13788, cur_epsilon : 0.42423999999999995\n",
      "episode : 482, done reward : -0.9999999776482582, total_step : 13888, cur_epsilon : 0.42223999999999995\n",
      "episode : 483, done reward : 0.5900000091642141, total_step : 13930, cur_epsilon : 0.42139999999999994\n",
      "episode : 484, done reward : 0.6200000084936619, total_step : 13969, cur_epsilon : 0.42061999999999994\n",
      "synced target net\n",
      "episode : 485, done reward : 0.4000000134110451, total_step : 14030, cur_epsilon : 0.41939999999999994\n",
      "episode : 486, done reward : 0.08000002056360245, total_step : 14123, cur_epsilon : 0.41753999999999997\n",
      "episode : 487, done reward : 0.880000002682209, total_step : 14136, cur_epsilon : 0.41727999999999993\n",
      "episode : 488, done reward : -0.9999999776482582, total_step : 14236, cur_epsilon : 0.41528\n",
      "episode : 489, done reward : 0.880000002682209, total_step : 14249, cur_epsilon : 0.41501999999999994\n",
      "episode : 490, done reward : 0.04000002145767212, total_step : 14346, cur_epsilon : 0.41307999999999995\n",
      "episode : 491, done reward : 0.7500000055879354, total_step : 14372, cur_epsilon : 0.41256\n",
      "episode : 492, done reward : 0.7200000062584877, total_step : 14401, cur_epsilon : 0.41197999999999996\n",
      "episode : 493, done reward : 0.9200000017881393, total_step : 14410, cur_epsilon : 0.41179999999999994\n",
      "episode : 494, done reward : 1.0, total_step : 14411, cur_epsilon : 0.41178\n",
      "episode : 495, done reward : 0.7200000062584877, total_step : 14440, cur_epsilon : 0.41119999999999995\n",
      "episode : 496, done reward : 0.5000000111758709, total_step : 14491, cur_epsilon : 0.41017999999999993\n",
      "episode : 497, done reward : 0.5100000109523535, total_step : 14541, cur_epsilon : 0.40917999999999993\n",
      "episode : 498, done reward : 0.8300000037997961, total_step : 14559, cur_epsilon : 0.40881999999999996\n",
      "episode : 499, done reward : 0.8600000031292439, total_step : 14574, cur_epsilon : 0.40851999999999994\n",
      "episode : 500, done reward : -0.9999999776482582, total_step : 14674, cur_epsilon : 0.40651999999999994\n",
      "episode : 501, done reward : -0.9999999776482582, total_step : 14774, cur_epsilon : 0.40451999999999994\n",
      "episode : 502, done reward : 0.9200000017881393, total_step : 14783, cur_epsilon : 0.40434\n",
      "episode : 503, done reward : -0.9999999776482582, total_step : 14883, cur_epsilon : 0.40234\n",
      "episode : 504, done reward : 0.9700000006705523, total_step : 14887, cur_epsilon : 0.40225999999999995\n",
      "episode : 505, done reward : 0.8400000035762787, total_step : 14904, cur_epsilon : 0.40191999999999994\n",
      "episode : 506, done reward : 0.9100000020116568, total_step : 14914, cur_epsilon : 0.40171999999999997\n",
      "synced target net\n",
      "episode : 507, done reward : -0.9999999776482582, total_step : 15014, cur_epsilon : 0.39971999999999996\n",
      "episode : 508, done reward : 0.04000002145767212, total_step : 15111, cur_epsilon : 0.39777999999999997\n",
      "episode : 509, done reward : -0.9999999776482582, total_step : 15211, cur_epsilon : 0.39577999999999997\n",
      "episode : 510, done reward : 0.9500000011175871, total_step : 15217, cur_epsilon : 0.39565999999999996\n",
      "episode : 511, done reward : 0.3200000151991844, total_step : 15286, cur_epsilon : 0.39427999999999996\n",
      "episode : 512, done reward : 0.7200000062584877, total_step : 15315, cur_epsilon : 0.39369999999999994\n",
      "episode : 513, done reward : 0.29000001586973667, total_step : 15387, cur_epsilon : 0.39225999999999994\n",
      "episode : 514, done reward : 0.3400000147521496, total_step : 15454, cur_epsilon : 0.39091999999999993\n",
      "episode : 515, done reward : 0.9300000015646219, total_step : 15462, cur_epsilon : 0.39075999999999994\n",
      "episode : 516, done reward : -0.9999999776482582, total_step : 15562, cur_epsilon : 0.38875999999999994\n",
      "episode : 517, done reward : 0.48000001162290573, total_step : 15615, cur_epsilon : 0.38769999999999993\n",
      "episode : 518, done reward : 0.7000000067055225, total_step : 15646, cur_epsilon : 0.38708\n",
      "episode : 519, done reward : 0.6200000084936619, total_step : 15685, cur_epsilon : 0.3863\n",
      "episode : 520, done reward : 0.5100000109523535, total_step : 15735, cur_epsilon : 0.3853\n",
      "episode : 521, done reward : 0.8900000024586916, total_step : 15747, cur_epsilon : 0.38505999999999996\n",
      "episode : 522, done reward : 1.0, total_step : 15748, cur_epsilon : 0.38503999999999994\n",
      "episode : 523, done reward : 0.640000008046627, total_step : 15785, cur_epsilon : 0.3843\n",
      "episode : 524, done reward : 0.8900000024586916, total_step : 15797, cur_epsilon : 0.38405999999999996\n",
      "episode : 525, done reward : -0.9999999776482582, total_step : 15897, cur_epsilon : 0.38205999999999996\n",
      "episode : 526, done reward : 0.8900000024586916, total_step : 15909, cur_epsilon : 0.38181999999999994\n",
      "episode : 527, done reward : 0.4400000125169754, total_step : 15966, cur_epsilon : 0.38067999999999996\n",
      "synced target net\n",
      "episode : 528, done reward : -0.9999999776482582, total_step : 16066, cur_epsilon : 0.37867999999999996\n",
      "episode : 529, done reward : 0.9100000020116568, total_step : 16076, cur_epsilon : 0.37848\n",
      "episode : 530, done reward : 0.36000001430511475, total_step : 16141, cur_epsilon : 0.37717999999999996\n",
      "episode : 531, done reward : -0.9999999776482582, total_step : 16241, cur_epsilon : 0.37517999999999996\n",
      "episode : 532, done reward : 0.9900000002235174, total_step : 16243, cur_epsilon : 0.37514\n",
      "episode : 533, done reward : 0.880000002682209, total_step : 16256, cur_epsilon : 0.37487999999999994\n",
      "episode : 534, done reward : 0.9100000020116568, total_step : 16266, cur_epsilon : 0.37467999999999996\n",
      "episode : 535, done reward : 0.5400000102818012, total_step : 16313, cur_epsilon : 0.37373999999999996\n",
      "episode : 536, done reward : 0.6100000087171793, total_step : 16353, cur_epsilon : 0.37293999999999994\n",
      "episode : 537, done reward : 0.7000000067055225, total_step : 16384, cur_epsilon : 0.37231999999999993\n",
      "episode : 538, done reward : 0.9500000011175871, total_step : 16390, cur_epsilon : 0.3722\n",
      "episode : 539, done reward : 0.8300000037997961, total_step : 16408, cur_epsilon : 0.37183999999999995\n",
      "episode : 540, done reward : 0.9800000004470348, total_step : 16411, cur_epsilon : 0.37177999999999994\n",
      "episode : 541, done reward : 0.7700000051409006, total_step : 16435, cur_epsilon : 0.37129999999999996\n",
      "episode : 542, done reward : 0.3200000151991844, total_step : 16504, cur_epsilon : 0.36991999999999997\n",
      "episode : 543, done reward : 0.7300000060349703, total_step : 16532, cur_epsilon : 0.36935999999999997\n",
      "episode : 544, done reward : 0.9700000006705523, total_step : 16536, cur_epsilon : 0.36927999999999994\n",
      "episode : 545, done reward : -0.9999999776482582, total_step : 16636, cur_epsilon : 0.36727999999999994\n",
      "episode : 546, done reward : 0.7800000049173832, total_step : 16659, cur_epsilon : 0.36682\n",
      "episode : 547, done reward : 0.19000001810491085, total_step : 16741, cur_epsilon : 0.36517999999999995\n",
      "episode : 548, done reward : 0.8400000035762787, total_step : 16758, cur_epsilon : 0.36483999999999994\n",
      "episode : 549, done reward : 0.760000005364418, total_step : 16783, cur_epsilon : 0.36433999999999994\n",
      "episode : 550, done reward : 1.0, total_step : 16784, cur_epsilon : 0.36432\n",
      "episode : 551, done reward : -0.9999999776482582, total_step : 16884, cur_epsilon : 0.36232\n",
      "episode : 552, done reward : 0.7300000060349703, total_step : 16912, cur_epsilon : 0.36175999999999997\n",
      "episode : 553, done reward : 0.570000009611249, total_step : 16956, cur_epsilon : 0.36088\n",
      "synced target net\n",
      "episode : 554, done reward : -0.9999999776482582, total_step : 17056, cur_epsilon : 0.35888\n",
      "episode : 555, done reward : 0.9500000011175871, total_step : 17062, cur_epsilon : 0.35875999999999997\n",
      "episode : 556, done reward : 0.6500000078231096, total_step : 17098, cur_epsilon : 0.35803999999999997\n",
      "episode : 557, done reward : 0.9300000015646219, total_step : 17106, cur_epsilon : 0.35788\n",
      "episode : 558, done reward : 0.8900000024586916, total_step : 17118, cur_epsilon : 0.35763999999999996\n",
      "episode : 559, done reward : 0.9300000015646219, total_step : 17126, cur_epsilon : 0.35747999999999996\n",
      "episode : 560, done reward : -0.9999999776482582, total_step : 17226, cur_epsilon : 0.35547999999999996\n",
      "episode : 561, done reward : -0.9999999776482582, total_step : 17326, cur_epsilon : 0.35347999999999996\n",
      "episode : 562, done reward : 0.8900000024586916, total_step : 17338, cur_epsilon : 0.35323999999999994\n",
      "episode : 563, done reward : -0.9999999776482582, total_step : 17438, cur_epsilon : 0.35123999999999994\n",
      "episode : 564, done reward : 0.6000000089406967, total_step : 17479, cur_epsilon : 0.35041999999999995\n",
      "episode : 565, done reward : 0.8500000033527613, total_step : 17495, cur_epsilon : 0.35009999999999997\n",
      "episode : 566, done reward : 0.9600000008940697, total_step : 17500, cur_epsilon : 0.35\n",
      "episode : 567, done reward : 0.5600000098347664, total_step : 17545, cur_epsilon : 0.34909999999999997\n",
      "episode : 568, done reward : 0.7900000046938658, total_step : 17567, cur_epsilon : 0.34865999999999997\n",
      "episode : 569, done reward : 0.8100000042468309, total_step : 17587, cur_epsilon : 0.34825999999999996\n",
      "episode : 570, done reward : 0.6200000084936619, total_step : 17626, cur_epsilon : 0.34747999999999996\n",
      "episode : 571, done reward : -0.9999999776482582, total_step : 17726, cur_epsilon : 0.34547999999999995\n",
      "episode : 572, done reward : 0.8600000031292439, total_step : 17741, cur_epsilon : 0.34517999999999993\n",
      "episode : 573, done reward : 0.8400000035762787, total_step : 17758, cur_epsilon : 0.34484\n",
      "episode : 574, done reward : -0.9999999776482582, total_step : 17858, cur_epsilon : 0.34284\n",
      "episode : 575, done reward : 0.2600000165402889, total_step : 17933, cur_epsilon : 0.34134\n",
      "episode : 576, done reward : 0.6500000078231096, total_step : 17969, cur_epsilon : 0.34062\n",
      "synced target net\n",
      "episode : 577, done reward : -0.9999999776482582, total_step : 18069, cur_epsilon : 0.33862\n",
      "episode : 578, done reward : 0.8500000033527613, total_step : 18085, cur_epsilon : 0.33829999999999993\n",
      "episode : 579, done reward : 0.8400000035762787, total_step : 18102, cur_epsilon : 0.33796\n",
      "episode : 580, done reward : 0.8000000044703484, total_step : 18123, cur_epsilon : 0.33753999999999995\n",
      "episode : 581, done reward : 0.37000001408159733, total_step : 18187, cur_epsilon : 0.33625999999999995\n",
      "episode : 582, done reward : -0.9999999776482582, total_step : 18287, cur_epsilon : 0.33425999999999995\n",
      "episode : 583, done reward : -0.9999999776482582, total_step : 18387, cur_epsilon : 0.33225999999999994\n",
      "episode : 584, done reward : 0.7200000062584877, total_step : 18416, cur_epsilon : 0.33168\n",
      "episode : 585, done reward : 0.7800000049173832, total_step : 18439, cur_epsilon : 0.33121999999999996\n",
      "episode : 586, done reward : -0.9999999776482582, total_step : 18539, cur_epsilon : 0.32921999999999996\n",
      "episode : 587, done reward : -0.9999999776482582, total_step : 18639, cur_epsilon : 0.32721999999999996\n",
      "episode : 588, done reward : 0.9200000017881393, total_step : 18648, cur_epsilon : 0.32703999999999994\n",
      "episode : 589, done reward : 0.7200000062584877, total_step : 18677, cur_epsilon : 0.32646\n",
      "episode : 590, done reward : 0.29000001586973667, total_step : 18749, cur_epsilon : 0.32502\n",
      "episode : 591, done reward : 0.1100000198930502, total_step : 18839, cur_epsilon : 0.32321999999999995\n",
      "episode : 592, done reward : 0.6300000082701445, total_step : 18877, cur_epsilon : 0.32245999999999997\n",
      "episode : 593, done reward : -0.9999999776482582, total_step : 18977, cur_epsilon : 0.32045999999999997\n",
      "synced target net\n",
      "episode : 594, done reward : 0.5000000111758709, total_step : 19028, cur_epsilon : 0.31943999999999995\n",
      "episode : 595, done reward : 0.14000001922249794, total_step : 19115, cur_epsilon : 0.3177\n",
      "episode : 596, done reward : 1.0, total_step : 19116, cur_epsilon : 0.31767999999999996\n",
      "episode : 597, done reward : 0.9200000017881393, total_step : 19125, cur_epsilon : 0.31749999999999995\n",
      "episode : 598, done reward : 0.10000002011656761, total_step : 19216, cur_epsilon : 0.31567999999999996\n",
      "episode : 599, done reward : 0.47000001184642315, total_step : 19270, cur_epsilon : 0.31459999999999994\n",
      "episode : 600, done reward : 0.4000000134110451, total_step : 19331, cur_epsilon : 0.31337999999999994\n",
      "episode : 601, done reward : 0.9000000022351742, total_step : 19342, cur_epsilon : 0.31315999999999994\n",
      "episode : 602, done reward : 0.570000009611249, total_step : 19386, cur_epsilon : 0.31227999999999995\n",
      "episode : 603, done reward : 0.9000000022351742, total_step : 19397, cur_epsilon : 0.31205999999999995\n",
      "episode : 604, done reward : -0.9999999776482582, total_step : 19497, cur_epsilon : 0.31005999999999995\n",
      "episode : 605, done reward : 0.9200000017881393, total_step : 19506, cur_epsilon : 0.30987999999999993\n",
      "episode : 606, done reward : -0.9999999776482582, total_step : 19606, cur_epsilon : 0.30787999999999993\n",
      "episode : 607, done reward : 0.570000009611249, total_step : 19650, cur_epsilon : 0.30699999999999994\n",
      "episode : 608, done reward : 0.9000000022351742, total_step : 19661, cur_epsilon : 0.30677999999999994\n",
      "episode : 609, done reward : -0.9999999776482582, total_step : 19761, cur_epsilon : 0.30477999999999994\n",
      "episode : 610, done reward : 1.0, total_step : 19762, cur_epsilon : 0.30476\n",
      "episode : 611, done reward : 0.7700000051409006, total_step : 19786, cur_epsilon : 0.30427999999999994\n",
      "episode : 612, done reward : -0.9999999776482582, total_step : 19886, cur_epsilon : 0.30227999999999994\n",
      "episode : 613, done reward : -0.9999999776482582, total_step : 19986, cur_epsilon : 0.30027999999999994\n",
      "synced target net\n",
      "episode : 614, done reward : 0.7700000051409006, total_step : 20010, cur_epsilon : 0.29979999999999996\n",
      "episode : 615, done reward : 0.6200000084936619, total_step : 20049, cur_epsilon : 0.29901999999999995\n",
      "episode : 616, done reward : -0.9999999776482582, total_step : 20149, cur_epsilon : 0.29701999999999995\n",
      "episode : 617, done reward : 0.15000001899898052, total_step : 20235, cur_epsilon : 0.29529999999999995\n",
      "episode : 618, done reward : 0.8000000044703484, total_step : 20256, cur_epsilon : 0.29488\n",
      "episode : 619, done reward : 0.7700000051409006, total_step : 20280, cur_epsilon : 0.29439999999999994\n",
      "episode : 620, done reward : 1.0, total_step : 20281, cur_epsilon : 0.29438\n",
      "episode : 621, done reward : 0.8600000031292439, total_step : 20296, cur_epsilon : 0.29407999999999995\n",
      "episode : 622, done reward : 0.37000001408159733, total_step : 20360, cur_epsilon : 0.29279999999999995\n",
      "episode : 623, done reward : 0.9500000011175871, total_step : 20366, cur_epsilon : 0.29267999999999994\n",
      "episode : 624, done reward : 0.9600000008940697, total_step : 20371, cur_epsilon : 0.29257999999999995\n",
      "episode : 625, done reward : 0.8200000040233135, total_step : 20390, cur_epsilon : 0.29219999999999996\n",
      "episode : 626, done reward : 0.9500000011175871, total_step : 20396, cur_epsilon : 0.29207999999999995\n",
      "episode : 627, done reward : 0.5900000091642141, total_step : 20438, cur_epsilon : 0.29123999999999994\n",
      "episode : 628, done reward : 0.9600000008940697, total_step : 20443, cur_epsilon : 0.29113999999999995\n",
      "episode : 629, done reward : 0.8300000037997961, total_step : 20461, cur_epsilon : 0.29078\n",
      "episode : 630, done reward : 0.9400000013411045, total_step : 20468, cur_epsilon : 0.29063999999999995\n",
      "episode : 631, done reward : 0.8900000024586916, total_step : 20480, cur_epsilon : 0.29039999999999994\n",
      "episode : 632, done reward : 0.12000001966953278, total_step : 20569, cur_epsilon : 0.28861999999999993\n",
      "episode : 633, done reward : 0.47000001184642315, total_step : 20623, cur_epsilon : 0.28753999999999996\n",
      "episode : 634, done reward : 0.7700000051409006, total_step : 20647, cur_epsilon : 0.28706\n",
      "episode : 635, done reward : 0.640000008046627, total_step : 20684, cur_epsilon : 0.28631999999999996\n",
      "episode : 636, done reward : 0.04000002145767212, total_step : 20781, cur_epsilon : 0.28437999999999997\n",
      "episode : 637, done reward : -0.9999999776482582, total_step : 20881, cur_epsilon : 0.28237999999999996\n",
      "episode : 638, done reward : 0.570000009611249, total_step : 20925, cur_epsilon : 0.2815\n",
      "synced target net\n",
      "episode : 639, done reward : 0.09000002034008503, total_step : 21017, cur_epsilon : 0.27965999999999996\n",
      "episode : 640, done reward : 0.9100000020116568, total_step : 21027, cur_epsilon : 0.27945999999999993\n",
      "episode : 641, done reward : 0.1600000187754631, total_step : 21112, cur_epsilon : 0.27775999999999995\n",
      "episode : 642, done reward : -0.9999999776482582, total_step : 21212, cur_epsilon : 0.27575999999999995\n",
      "episode : 643, done reward : 0.4400000125169754, total_step : 21269, cur_epsilon : 0.27462\n",
      "episode : 644, done reward : 0.5300000105053186, total_step : 21317, cur_epsilon : 0.27365999999999996\n",
      "episode : 645, done reward : 0.3800000138580799, total_step : 21380, cur_epsilon : 0.2724\n",
      "episode : 646, done reward : 0.8500000033527613, total_step : 21396, cur_epsilon : 0.27207999999999993\n",
      "episode : 647, done reward : 0.9500000011175871, total_step : 21402, cur_epsilon : 0.27196\n",
      "episode : 648, done reward : -0.9999999776482582, total_step : 21502, cur_epsilon : 0.26996\n",
      "episode : 649, done reward : -0.9999999776482582, total_step : 21602, cur_epsilon : 0.26796\n",
      "episode : 650, done reward : 0.8000000044703484, total_step : 21623, cur_epsilon : 0.26753999999999994\n",
      "episode : 651, done reward : -0.9999999776482582, total_step : 21723, cur_epsilon : 0.26553999999999994\n",
      "episode : 652, done reward : 0.9100000020116568, total_step : 21733, cur_epsilon : 0.26533999999999996\n",
      "episode : 653, done reward : 0.9700000006705523, total_step : 21737, cur_epsilon : 0.26525999999999994\n",
      "episode : 654, done reward : 0.9500000011175871, total_step : 21743, cur_epsilon : 0.26513999999999993\n",
      "episode : 655, done reward : -0.9999999776482582, total_step : 21843, cur_epsilon : 0.26313999999999993\n",
      "episode : 656, done reward : 0.8600000031292439, total_step : 21858, cur_epsilon : 0.26283999999999996\n",
      "episode : 657, done reward : 0.06000002101063728, total_step : 21953, cur_epsilon : 0.26093999999999995\n",
      "episode : 658, done reward : 0.9400000013411045, total_step : 21960, cur_epsilon : 0.2608\n",
      "episode : 659, done reward : 0.7100000064820051, total_step : 21990, cur_epsilon : 0.26019999999999993\n",
      "synced target net\n",
      "episode : 660, done reward : 0.31000001542270184, total_step : 22060, cur_epsilon : 0.2588\n",
      "episode : 661, done reward : 0.6800000071525574, total_step : 22093, cur_epsilon : 0.25814\n",
      "episode : 662, done reward : -0.9999999776482582, total_step : 22193, cur_epsilon : 0.25614\n",
      "episode : 663, done reward : 0.3800000138580799, total_step : 22256, cur_epsilon : 0.25487999999999994\n",
      "episode : 664, done reward : 0.8000000044703484, total_step : 22277, cur_epsilon : 0.25445999999999996\n",
      "episode : 665, done reward : -0.9999999776482582, total_step : 22377, cur_epsilon : 0.25245999999999996\n",
      "episode : 666, done reward : 0.20000001788139343, total_step : 22458, cur_epsilon : 0.25083999999999995\n",
      "episode : 667, done reward : 0.5200000107288361, total_step : 22507, cur_epsilon : 0.24985999999999997\n",
      "episode : 668, done reward : 0.7800000049173832, total_step : 22530, cur_epsilon : 0.24939999999999996\n",
      "episode : 669, done reward : 0.42000001296401024, total_step : 22589, cur_epsilon : 0.24821999999999994\n",
      "episode : 670, done reward : 0.4000000134110451, total_step : 22650, cur_epsilon : 0.24699999999999994\n",
      "episode : 671, done reward : -0.9999999776482582, total_step : 22750, cur_epsilon : 0.24499999999999994\n",
      "episode : 672, done reward : 0.9700000006705523, total_step : 22754, cur_epsilon : 0.24491999999999997\n",
      "episode : 673, done reward : -0.9999999776482582, total_step : 22854, cur_epsilon : 0.24291999999999997\n",
      "episode : 674, done reward : 0.640000008046627, total_step : 22891, cur_epsilon : 0.24217999999999995\n",
      "episode : 675, done reward : 0.9500000011175871, total_step : 22897, cur_epsilon : 0.24205999999999994\n",
      "episode : 676, done reward : 0.9200000017881393, total_step : 22906, cur_epsilon : 0.24187999999999993\n",
      "synced target net\n",
      "episode : 677, done reward : -0.9999999776482582, total_step : 23006, cur_epsilon : 0.23987999999999998\n",
      "episode : 678, done reward : -0.9999999776482582, total_step : 23106, cur_epsilon : 0.23787999999999998\n",
      "episode : 679, done reward : 0.14000001922249794, total_step : 23193, cur_epsilon : 0.23613999999999996\n",
      "episode : 680, done reward : -0.9999999776482582, total_step : 23293, cur_epsilon : 0.23413999999999996\n",
      "episode : 681, done reward : 1.0, total_step : 23294, cur_epsilon : 0.23411999999999994\n",
      "episode : 682, done reward : 0.46000001206994057, total_step : 23349, cur_epsilon : 0.23301999999999995\n",
      "episode : 683, done reward : -0.9999999776482582, total_step : 23449, cur_epsilon : 0.23101999999999995\n",
      "episode : 684, done reward : -0.9999999776482582, total_step : 23549, cur_epsilon : 0.22901999999999995\n",
      "episode : 685, done reward : -0.9999999776482582, total_step : 23649, cur_epsilon : 0.22701999999999994\n",
      "episode : 686, done reward : 0.14000001922249794, total_step : 23736, cur_epsilon : 0.22527999999999998\n",
      "episode : 687, done reward : -0.9999999776482582, total_step : 23836, cur_epsilon : 0.22327999999999998\n",
      "episode : 688, done reward : 0.4400000125169754, total_step : 23893, cur_epsilon : 0.22213999999999995\n",
      "episode : 689, done reward : -0.9999999776482582, total_step : 23993, cur_epsilon : 0.22013999999999995\n",
      "synced target net\n",
      "episode : 690, done reward : -0.9999999776482582, total_step : 24093, cur_epsilon : 0.21813999999999995\n",
      "episode : 691, done reward : 0.7200000062584877, total_step : 24122, cur_epsilon : 0.21755999999999998\n",
      "episode : 692, done reward : 0.6700000073760748, total_step : 24156, cur_epsilon : 0.21687999999999996\n",
      "episode : 693, done reward : -0.9999999776482582, total_step : 24256, cur_epsilon : 0.21487999999999996\n",
      "episode : 694, done reward : 0.69000000692904, total_step : 24288, cur_epsilon : 0.21423999999999993\n",
      "episode : 695, done reward : 0.15000001899898052, total_step : 24374, cur_epsilon : 0.21251999999999993\n",
      "episode : 696, done reward : 0.5000000111758709, total_step : 24425, cur_epsilon : 0.21149999999999997\n",
      "episode : 697, done reward : -0.9999999776482582, total_step : 24525, cur_epsilon : 0.20949999999999996\n",
      "episode : 698, done reward : 0.25000001676380634, total_step : 24601, cur_epsilon : 0.20797999999999994\n",
      "episode : 699, done reward : 0.04000002145767212, total_step : 24698, cur_epsilon : 0.20603999999999995\n",
      "episode : 700, done reward : 0.7700000051409006, total_step : 24722, cur_epsilon : 0.20555999999999996\n",
      "episode : 701, done reward : 0.9200000017881393, total_step : 24731, cur_epsilon : 0.20537999999999995\n",
      "episode : 702, done reward : 0.330000014975667, total_step : 24799, cur_epsilon : 0.20401999999999998\n",
      "episode : 703, done reward : 0.7400000058114529, total_step : 24826, cur_epsilon : 0.20347999999999994\n",
      "episode : 704, done reward : 0.69000000692904, total_step : 24858, cur_epsilon : 0.20283999999999996\n",
      "episode : 705, done reward : 0.9500000011175871, total_step : 24864, cur_epsilon : 0.20271999999999996\n",
      "episode : 706, done reward : -0.9999999776482582, total_step : 24964, cur_epsilon : 0.20071999999999995\n",
      "episode : 707, done reward : 0.8300000037997961, total_step : 24982, cur_epsilon : 0.20035999999999998\n",
      "synced target net\n",
      "episode : 708, done reward : 0.42000001296401024, total_step : 25041, cur_epsilon : 0.1991799999999999\n",
      "episode : 709, done reward : 0.3200000151991844, total_step : 25110, cur_epsilon : 0.19779999999999998\n",
      "episode : 710, done reward : 0.760000005364418, total_step : 25135, cur_epsilon : 0.19729999999999992\n",
      "episode : 711, done reward : 0.5300000105053186, total_step : 25183, cur_epsilon : 0.19633999999999996\n",
      "episode : 712, done reward : -0.9999999776482582, total_step : 25283, cur_epsilon : 0.19433999999999996\n",
      "episode : 713, done reward : 0.25000001676380634, total_step : 25359, cur_epsilon : 0.19282\n",
      "episode : 714, done reward : 0.8700000029057264, total_step : 25373, cur_epsilon : 0.19253999999999993\n",
      "episode : 715, done reward : 0.0500000212341547, total_step : 25469, cur_epsilon : 0.1906199999999999\n",
      "episode : 716, done reward : 0.47000001184642315, total_step : 25523, cur_epsilon : 0.18953999999999993\n",
      "episode : 717, done reward : 0.6100000087171793, total_step : 25563, cur_epsilon : 0.1887399999999999\n",
      "episode : 718, done reward : 0.9100000020116568, total_step : 25573, cur_epsilon : 0.18853999999999993\n",
      "episode : 719, done reward : 0.07000002078711987, total_step : 25667, cur_epsilon : 0.18665999999999994\n",
      "episode : 720, done reward : 0.3900000136345625, total_step : 25729, cur_epsilon : 0.18541999999999992\n",
      "episode : 721, done reward : 0.6200000084936619, total_step : 25768, cur_epsilon : 0.18463999999999992\n",
      "episode : 722, done reward : 0.6100000087171793, total_step : 25808, cur_epsilon : 0.18384\n",
      "episode : 723, done reward : -0.9999999776482582, total_step : 25908, cur_epsilon : 0.18184\n",
      "episode : 724, done reward : 0.29000001586973667, total_step : 25980, cur_epsilon : 0.1804\n",
      "synced target net\n",
      "episode : 725, done reward : -0.9999999776482582, total_step : 26080, cur_epsilon : 0.1784\n",
      "episode : 726, done reward : 0.760000005364418, total_step : 26105, cur_epsilon : 0.17789999999999995\n",
      "episode : 727, done reward : 0.9500000011175871, total_step : 26111, cur_epsilon : 0.17777999999999994\n",
      "episode : 728, done reward : 0.5600000098347664, total_step : 26156, cur_epsilon : 0.17687999999999993\n",
      "episode : 729, done reward : -0.9999999776482582, total_step : 26256, cur_epsilon : 0.17487999999999992\n",
      "episode : 730, done reward : 0.25000001676380634, total_step : 26332, cur_epsilon : 0.17335999999999996\n",
      "episode : 731, done reward : -0.9999999776482582, total_step : 26432, cur_epsilon : 0.17135999999999996\n",
      "episode : 732, done reward : 0.6800000071525574, total_step : 26465, cur_epsilon : 0.17069999999999996\n",
      "episode : 733, done reward : 0.6100000087171793, total_step : 26505, cur_epsilon : 0.16989999999999994\n",
      "episode : 734, done reward : 0.18000001832842827, total_step : 26588, cur_epsilon : 0.16823999999999995\n",
      "episode : 735, done reward : 0.7900000046938658, total_step : 26610, cur_epsilon : 0.16779999999999995\n",
      "episode : 736, done reward : 0.20000001788139343, total_step : 26691, cur_epsilon : 0.16618\n",
      "episode : 737, done reward : 0.9400000013411045, total_step : 26698, cur_epsilon : 0.16603999999999997\n",
      "episode : 738, done reward : -0.9999999776482582, total_step : 26798, cur_epsilon : 0.16403999999999996\n",
      "episode : 739, done reward : 0.29000001586973667, total_step : 26870, cur_epsilon : 0.16259999999999997\n",
      "episode : 740, done reward : 0.3200000151991844, total_step : 26939, cur_epsilon : 0.16121999999999992\n",
      "episode : 741, done reward : 0.9000000022351742, total_step : 26950, cur_epsilon : 0.16099999999999992\n",
      "synced target net\n",
      "episode : 742, done reward : -0.9999999776482582, total_step : 27050, cur_epsilon : 0.15899999999999992\n",
      "episode : 743, done reward : 0.8400000035762787, total_step : 27067, cur_epsilon : 0.1586599999999999\n",
      "episode : 744, done reward : 0.8200000040233135, total_step : 27086, cur_epsilon : 0.15827999999999998\n",
      "episode : 745, done reward : -0.9999999776482582, total_step : 27186, cur_epsilon : 0.15627999999999997\n",
      "episode : 746, done reward : 0.7400000058114529, total_step : 27213, cur_epsilon : 0.15574\n",
      "episode : 747, done reward : 0.450000012293458, total_step : 27269, cur_epsilon : 0.15461999999999998\n",
      "episode : 748, done reward : 0.9400000013411045, total_step : 27276, cur_epsilon : 0.15447999999999995\n",
      "episode : 749, done reward : 0.450000012293458, total_step : 27332, cur_epsilon : 0.15335999999999994\n",
      "episode : 750, done reward : 0.0500000212341547, total_step : 27428, cur_epsilon : 0.1514399999999999\n",
      "episode : 751, done reward : 0.7300000060349703, total_step : 27456, cur_epsilon : 0.1508799999999999\n",
      "episode : 752, done reward : 0.6000000089406967, total_step : 27497, cur_epsilon : 0.15005999999999997\n",
      "episode : 753, done reward : 0.8000000044703484, total_step : 27518, cur_epsilon : 0.14964\n",
      "episode : 754, done reward : 0.35000001452863216, total_step : 27584, cur_epsilon : 0.14832\n",
      "episode : 755, done reward : -0.9999999776482582, total_step : 27684, cur_epsilon : 0.14632\n",
      "episode : 756, done reward : -0.9999999776482582, total_step : 27784, cur_epsilon : 0.14432\n",
      "episode : 757, done reward : 0.47000001184642315, total_step : 27838, cur_epsilon : 0.14323999999999992\n",
      "episode : 758, done reward : -0.9999999776482582, total_step : 27938, cur_epsilon : 0.14123999999999992\n",
      "synced target net\n",
      "episode : 759, done reward : -0.9999999776482582, total_step : 28038, cur_epsilon : 0.13923999999999992\n",
      "episode : 760, done reward : 0.570000009611249, total_step : 28082, cur_epsilon : 0.13835999999999993\n",
      "episode : 761, done reward : 0.48000001162290573, total_step : 28135, cur_epsilon : 0.13729999999999998\n",
      "episode : 762, done reward : -0.9999999776482582, total_step : 28235, cur_epsilon : 0.13529999999999998\n",
      "episode : 763, done reward : 0.6100000087171793, total_step : 28275, cur_epsilon : 0.13449999999999995\n",
      "episode : 764, done reward : 0.8100000042468309, total_step : 28295, cur_epsilon : 0.1341\n",
      "episode : 765, done reward : -0.9999999776482582, total_step : 28395, cur_epsilon : 0.1321\n",
      "episode : 766, done reward : 0.7900000046938658, total_step : 28417, cur_epsilon : 0.13166\n",
      "episode : 767, done reward : 0.9600000008940697, total_step : 28422, cur_epsilon : 0.13156\n",
      "episode : 768, done reward : 0.8700000029057264, total_step : 28436, cur_epsilon : 0.13127999999999995\n",
      "episode : 769, done reward : -0.9999999776482582, total_step : 28536, cur_epsilon : 0.12927999999999995\n",
      "episode : 770, done reward : -0.9999999776482582, total_step : 28636, cur_epsilon : 0.12727999999999995\n",
      "episode : 771, done reward : -0.9999999776482582, total_step : 28736, cur_epsilon : 0.12527999999999995\n",
      "episode : 772, done reward : 0.7300000060349703, total_step : 28764, cur_epsilon : 0.12471999999999994\n",
      "episode : 773, done reward : 0.24000001698732376, total_step : 28841, cur_epsilon : 0.12317999999999996\n",
      "episode : 774, done reward : 0.5800000093877316, total_step : 28884, cur_epsilon : 0.12231999999999998\n",
      "episode : 775, done reward : 0.760000005364418, total_step : 28909, cur_epsilon : 0.12181999999999993\n",
      "synced target net\n",
      "episode : 776, done reward : -0.9999999776482582, total_step : 29009, cur_epsilon : 0.11981999999999993\n",
      "episode : 777, done reward : -0.9999999776482582, total_step : 29109, cur_epsilon : 0.11781999999999992\n",
      "episode : 778, done reward : 0.880000002682209, total_step : 29122, cur_epsilon : 0.11756\n",
      "episode : 779, done reward : -0.9999999776482582, total_step : 29222, cur_epsilon : 0.11556\n",
      "episode : 780, done reward : -0.9999999776482582, total_step : 29322, cur_epsilon : 0.11356\n",
      "episode : 781, done reward : -0.9999999776482582, total_step : 29422, cur_epsilon : 0.11155999999999999\n",
      "episode : 782, done reward : -0.9999999776482582, total_step : 29522, cur_epsilon : 0.10955999999999999\n",
      "episode : 783, done reward : 0.4300000127404928, total_step : 29580, cur_epsilon : 0.10839999999999994\n",
      "episode : 784, done reward : -0.9999999776482582, total_step : 29680, cur_epsilon : 0.10639999999999994\n",
      "episode : 785, done reward : -0.9999999776482582, total_step : 29780, cur_epsilon : 0.10439999999999994\n",
      "episode : 786, done reward : 0.4000000134110451, total_step : 29841, cur_epsilon : 0.10317999999999994\n",
      "episode : 787, done reward : -0.9999999776482582, total_step : 29941, cur_epsilon : 0.10117999999999994\n",
      "synced target net\n",
      "episode : 788, done reward : 0.2700000163167715, total_step : 30015, cur_epsilon : 0.09970000000000001\n",
      "episode : 789, done reward : -0.9999999776482582, total_step : 30115, cur_epsilon : 0.09770000000000001\n",
      "episode : 790, done reward : -0.9999999776482582, total_step : 30215, cur_epsilon : 0.09570000000000001\n",
      "episode : 791, done reward : -0.9999999776482582, total_step : 30315, cur_epsilon : 0.0937\n",
      "episode : 792, done reward : 0.9200000017881393, total_step : 30324, cur_epsilon : 0.09351999999999994\n",
      "episode : 793, done reward : -0.9999999776482582, total_step : 30424, cur_epsilon : 0.09151999999999993\n",
      "episode : 794, done reward : -0.9999999776482582, total_step : 30524, cur_epsilon : 0.08951999999999993\n",
      "episode : 795, done reward : -0.9999999776482582, total_step : 30624, cur_epsilon : 0.08751999999999993\n",
      "episode : 796, done reward : 0.760000005364418, total_step : 30649, cur_epsilon : 0.08701999999999999\n",
      "episode : 797, done reward : 0.020000021904706955, total_step : 30748, cur_epsilon : 0.08504\n",
      "episode : 798, done reward : -0.9999999776482582, total_step : 30848, cur_epsilon : 0.08304\n",
      "episode : 799, done reward : -0.9999999776482582, total_step : 30948, cur_epsilon : 0.08104\n",
      "episode : 800, done reward : 0.9900000002235174, total_step : 30950, cur_epsilon : 0.08099999999999996\n",
      "synced target net\n",
      "episode : 801, done reward : -0.9999999776482582, total_step : 31050, cur_epsilon : 0.07899999999999996\n",
      "episode : 802, done reward : -0.9999999776482582, total_step : 31150, cur_epsilon : 0.07699999999999996\n",
      "episode : 803, done reward : -0.9999999776482582, total_step : 31250, cur_epsilon : 0.07499999999999996\n",
      "episode : 804, done reward : -0.9999999776482582, total_step : 31350, cur_epsilon : 0.07299999999999995\n",
      "episode : 805, done reward : 0.31000001542270184, total_step : 31420, cur_epsilon : 0.0716\n",
      "episode : 806, done reward : 0.2200000174343586, total_step : 31499, cur_epsilon : 0.07001999999999997\n",
      "episode : 807, done reward : -0.9999999776482582, total_step : 31599, cur_epsilon : 0.06801999999999997\n",
      "episode : 808, done reward : -0.9999999776482582, total_step : 31699, cur_epsilon : 0.06601999999999997\n",
      "episode : 809, done reward : 0.46000001206994057, total_step : 31754, cur_epsilon : 0.06491999999999998\n",
      "episode : 810, done reward : -0.9999999776482582, total_step : 31854, cur_epsilon : 0.06291999999999998\n",
      "episode : 811, done reward : -0.9999999776482582, total_step : 31954, cur_epsilon : 0.060919999999999974\n",
      "synced target net\n",
      "episode : 812, done reward : -0.9999999776482582, total_step : 32054, cur_epsilon : 0.05891999999999997\n",
      "episode : 813, done reward : -0.9999999776482582, total_step : 32154, cur_epsilon : 0.05691999999999997\n",
      "episode : 814, done reward : 0.41000001318752766, total_step : 32214, cur_epsilon : 0.05571999999999999\n",
      "episode : 815, done reward : 0.4900000113993883, total_step : 32266, cur_epsilon : 0.05467999999999995\n",
      "episode : 816, done reward : -0.9999999776482582, total_step : 32366, cur_epsilon : 0.05267999999999995\n",
      "episode : 817, done reward : -0.9999999776482582, total_step : 32466, cur_epsilon : 0.05067999999999995\n",
      "episode : 818, done reward : 0.37000001408159733, total_step : 32530, cur_epsilon : 0.0494\n",
      "episode : 819, done reward : 0.36000001430511475, total_step : 32595, cur_epsilon : 0.04809999999999992\n",
      "episode : 820, done reward : -0.9999999776482582, total_step : 32695, cur_epsilon : 0.04609999999999992\n",
      "episode : 821, done reward : 0.570000009611249, total_step : 32739, cur_epsilon : 0.04521999999999993\n",
      "episode : 822, done reward : -0.9999999776482582, total_step : 32839, cur_epsilon : 0.043219999999999925\n",
      "episode : 823, done reward : -0.9999999776482582, total_step : 32939, cur_epsilon : 0.04121999999999992\n",
      "synced target net\n",
      "episode : 824, done reward : -0.9999999776482582, total_step : 33039, cur_epsilon : 0.03921999999999992\n",
      "episode : 825, done reward : 0.7500000055879354, total_step : 33065, cur_epsilon : 0.03869999999999996\n",
      "episode : 826, done reward : -0.9999999776482582, total_step : 33165, cur_epsilon : 0.036699999999999955\n",
      "episode : 827, done reward : -0.9999999776482582, total_step : 33265, cur_epsilon : 0.03469999999999995\n",
      "episode : 828, done reward : 0.6200000084936619, total_step : 33304, cur_epsilon : 0.03391999999999995\n",
      "episode : 829, done reward : -0.9999999776482582, total_step : 33404, cur_epsilon : 0.03191999999999995\n",
      "episode : 830, done reward : -0.9999999776482582, total_step : 33504, cur_epsilon : 0.029919999999999947\n",
      "episode : 831, done reward : -0.9999999776482582, total_step : 33604, cur_epsilon : 0.027919999999999945\n",
      "episode : 832, done reward : -0.9999999776482582, total_step : 33704, cur_epsilon : 0.025919999999999943\n",
      "episode : 833, done reward : -0.9999999776482582, total_step : 33804, cur_epsilon : 0.02391999999999994\n",
      "episode : 834, done reward : -0.9999999776482582, total_step : 33904, cur_epsilon : 0.02191999999999994\n",
      "synced target net\n",
      "episode : 835, done reward : -0.9999999776482582, total_step : 34004, cur_epsilon : 0.019919999999999938\n",
      "episode : 836, done reward : 0.46000001206994057, total_step : 34059, cur_epsilon : 0.018819999999999948\n",
      "episode : 837, done reward : -0.9999999776482582, total_step : 34159, cur_epsilon : 0.016819999999999946\n",
      "episode : 838, done reward : -0.9999999776482582, total_step : 34259, cur_epsilon : 0.014819999999999944\n",
      "episode : 839, done reward : -0.9999999776482582, total_step : 34359, cur_epsilon : 0.012819999999999943\n",
      "episode : 840, done reward : -0.9999999776482582, total_step : 34459, cur_epsilon : 0.01081999999999994\n",
      "episode : 841, done reward : -0.9999999776482582, total_step : 34559, cur_epsilon : 0.01\n",
      "episode : 842, done reward : 0.1600000187754631, total_step : 34644, cur_epsilon : 0.01\n",
      "episode : 843, done reward : -0.9999999776482582, total_step : 34744, cur_epsilon : 0.01\n",
      "episode : 844, done reward : -0.9999999776482582, total_step : 34844, cur_epsilon : 0.01\n",
      "episode : 845, done reward : -0.9999999776482582, total_step : 34944, cur_epsilon : 0.01\n",
      "synced target net\n",
      "episode : 846, done reward : -0.9999999776482582, total_step : 35044, cur_epsilon : 0.01\n",
      "episode : 847, done reward : -0.9999999776482582, total_step : 35144, cur_epsilon : 0.01\n",
      "episode : 848, done reward : -0.9999999776482582, total_step : 35244, cur_epsilon : 0.01\n",
      "episode : 849, done reward : -0.9999999776482582, total_step : 35344, cur_epsilon : 0.01\n",
      "episode : 850, done reward : -0.9999999776482582, total_step : 35444, cur_epsilon : 0.01\n",
      "episode : 851, done reward : -0.9999999776482582, total_step : 35544, cur_epsilon : 0.01\n",
      "episode : 852, done reward : -0.9999999776482582, total_step : 35644, cur_epsilon : 0.01\n",
      "episode : 853, done reward : -0.9999999776482582, total_step : 35744, cur_epsilon : 0.01\n",
      "episode : 854, done reward : -0.9999999776482582, total_step : 35844, cur_epsilon : 0.01\n",
      "episode : 855, done reward : -0.9999999776482582, total_step : 35944, cur_epsilon : 0.01\n",
      "synced target net\n",
      "episode : 856, done reward : -0.9999999776482582, total_step : 36044, cur_epsilon : 0.01\n",
      "episode : 857, done reward : -0.9999999776482582, total_step : 36144, cur_epsilon : 0.01\n",
      "episode : 858, done reward : -0.9999999776482582, total_step : 36244, cur_epsilon : 0.01\n",
      "episode : 859, done reward : -0.9999999776482582, total_step : 36344, cur_epsilon : 0.01\n",
      "episode : 860, done reward : -0.9999999776482582, total_step : 36444, cur_epsilon : 0.01\n",
      "episode : 861, done reward : -0.9999999776482582, total_step : 36544, cur_epsilon : 0.01\n",
      "episode : 862, done reward : -0.9999999776482582, total_step : 36644, cur_epsilon : 0.01\n",
      "episode : 863, done reward : -0.9999999776482582, total_step : 36744, cur_epsilon : 0.01\n",
      "episode : 864, done reward : -0.9999999776482582, total_step : 36844, cur_epsilon : 0.01\n",
      "episode : 865, done reward : -0.9999999776482582, total_step : 36944, cur_epsilon : 0.01\n",
      "episode : 866, done reward : 0.760000005364418, total_step : 36969, cur_epsilon : 0.01\n",
      "synced target net\n",
      "episode : 867, done reward : -0.9999999776482582, total_step : 37069, cur_epsilon : 0.01\n",
      "episode : 868, done reward : 0.31000001542270184, total_step : 37139, cur_epsilon : 0.01\n",
      "episode : 869, done reward : -0.9999999776482582, total_step : 37239, cur_epsilon : 0.01\n",
      "episode : 870, done reward : -0.9999999776482582, total_step : 37339, cur_epsilon : 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m         done_reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done_reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, done reward : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdone_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total_step : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39m_total_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cur_epsilon : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39m_epsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m, in \u001b[0;36mAgent.play_step\u001b[0;34m(self, epsilon, sync_target)\u001b[0m\n\u001b[1;32m     48\u001b[0m     _, act_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(q_vals_v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_v\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 51\u001b[0m next_state, reward, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_buffer\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state, action, reward, is_done, next_state\n\u001b[1;32m     56\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgResizeGrayWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 11\u001b[0m         next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         next_state \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(next_state[:, :, \u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         return next_state, reward, is_done, info\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mUnityToGymNumpyImgWrapper.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(next_state[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), reward, is_done, info\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/envs/unity_gym_env.py:200\u001b[0m, in \u001b[0;36mUnityToGymWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    197\u001b[0m     action_tuple\u001b[38;5;241m.\u001b[39madd_discrete(action)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mset_actions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, action_tuple)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m decision_step, terminal_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_steps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_agents(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(decision_step), \u001b[38;5;28mlen\u001b[39m(terminal_step)))\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/environment.py:348\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m step_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_step_input(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicator.exchange\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_communicator.py:142\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    140\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll_for_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m/mnt/d/Program Files/Unity/ml-agents-release_20/ml-agents-envs/mlagents_envs/rpc_communicator.py:106\u001b[0m, in \u001b[0;36mRpcCommunicator.poll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    104\u001b[0m callback_timeout_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout_wait \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m<\u001b[39m deadline:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_timeout_wait\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# Got an acknowledgment from the connection\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m poll_callback:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# Fire the callback - if it detects something wrong, it should raise an exception.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# exp_buffer = NStepPriorityReplayBuffer(\n",
    "#     max_size=30000,\n",
    "#     prob_alpha=0.6,\n",
    "#     beta_start=0.4,\n",
    "#     beta_frames=50000, #100000,\n",
    "#     n_step=4,\n",
    "#     gamma=0.99,\n",
    "# )\n",
    "# agent = Agent(\n",
    "#     env=env,\n",
    "#     exp_buffer=exp_buffer,\n",
    "#     net=net,\n",
    "#     epsilon_start=0.7,\n",
    "#     epsilon_final=0.01,\n",
    "#     epsilon_decay_last_step=50000, #200000,\n",
    "#     tgt_sync_steps=1000,\n",
    "#     learning_rate=1e-4,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "episode = 0\n",
    "reward_history = []\n",
    "spisode_steps_history = []\n",
    "spisode_start_steps = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    for stp in range(20):\n",
    "        done_reward = agent.play_step()\n",
    "        if done_reward is not None:\n",
    "            print(f'episode : {episode}, done reward : {done_reward}, total_step : {agent._total_step}, cur_epsilon : {agent._epsilon}')\n",
    "            episode += 1\n",
    "            reward_history.append(done_reward)\n",
    "            spisode_steps_history.append(agent._total_step-spisode_start_steps)\n",
    "            spisode_start_steps = agent._total_step\n",
    "\n",
    "    agent.train(n_iter=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a57eed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'episode total reward')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('episode total reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "421205c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'episode steps')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(spisode_steps_history)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('episode steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0f144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "281d5718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3680, 1.2494, 1.4766, 1.3485, 1.3401]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(state).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4e92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc090428",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dde473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
