{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72143cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n",
    "from mlagents_envs.environment import ActionTuple, UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import \\\n",
    "    EngineConfigurationChannel\n",
    "from mlagents_envs.exception import (\n",
    "    UnityEnvironmentException,\n",
    "    UnityCommunicationException,\n",
    "    UnityCommunicatorStoppedException,\n",
    ")\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cda46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Union\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastprogress import progress_bar as pb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4a7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpBuffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int = 1024,\n",
    "        prob_alpha: float = 0.6,\n",
    "        beta_start: float = 0.4,\n",
    "        beta_frames: float = 100000,\n",
    "        n_step: int = 4,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        self._prob_alpha = prob_alpha\n",
    "        self._max_size = max_size\n",
    "        self._buf = []\n",
    "        self._pos = 0\n",
    "        self._beta_start = beta_start\n",
    "        self._beta = beta_start\n",
    "        self._beta_frames = beta_frames\n",
    "        self._n_step = n_step\n",
    "        self._gamma = gamma\n",
    "        self._total_discounted_rewards = np.array([np.nan]*max_size)\n",
    "        self._last_states = [np.nan]*max_size\n",
    "\n",
    "    def update_bata(self, idx) -> None:\n",
    "        beta = self._beta_start + idx * (1.0 - self._beta_start) / self._beta_frames\n",
    "        self._beta = min(1.0, beta)\n",
    "        return self._beta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buf)\n",
    "\n",
    "    def append(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: Union[int, float],\n",
    "        done: bool,\n",
    "        next_state: np.ndarray,\n",
    "    ) -> None:\n",
    "        if len(self._buf) < self._max_size:\n",
    "            self._buf.append(\n",
    "                (state, action, reward, done, next_state)\n",
    "            )\n",
    "        else:\n",
    "            self._buf[self._pos] = (state, action, reward, done, next_state)\n",
    "\n",
    "        if len(self._buf) >= self._n_step:\n",
    "            dis_r = 0.0\n",
    "            last_state = self._buf[self._pos][0]\n",
    "            for i in range(self._n_step):\n",
    "                state, _, r, done, _ = self._buf[self._pos - i]\n",
    "                dis_r = r + self._gamma * dis_r\n",
    "                if done:\n",
    "                    last_state = state\n",
    "                    dis_r = r  # ※\n",
    "                self._total_discounted_rewards[self._pos - i] = dis_r\n",
    "                self._last_states[self._pos - i] = last_state\n",
    "            \n",
    "            for i in range(self._n_step-1):\n",
    "                done = self._buf[self._pos - i][3]\n",
    "                if done:\n",
    "                    break\n",
    "                self._total_discounted_rewards[self._pos - i] = np.nan\n",
    "                self._last_states[self._pos - i] = np.nan\n",
    "\n",
    "        self._pos = (self._pos + 1) % self._max_size\n",
    "\n",
    "    def get_latest_n(self, n: int):\n",
    "        if len(self._buf) < self._max_size:\n",
    "            if len(self._buf) < n+self._n_step:\n",
    "                raise RuntimeError('get_latest_n : len(self._buf) < n+self._n_step')\n",
    "            s_idx = self._pos - self._n_step - n \n",
    "            e_idx = self._pos - self._n_step\n",
    "            latest_exps = self._buf[s_idx:e_idx]\n",
    "            latest_total_rewards = self._total_discounted_rewards[s_idx:e_idx]\n",
    "            latest_last_states = self._last_states[s_idx:e_idx]\n",
    "        else:\n",
    "            s_idx = self._pos + self._max_size - self._n_step - n\n",
    "            e_idx = self._pos + self._max_size - self._n_step\n",
    "            latest_exps = (self._buf*2)[s_idx:e_idx]\n",
    "            latest_total_rewards = np.concatenate([self._total_discounted_rewards]*2)[s_idx:e_idx]\n",
    "            latest_last_states = (self._last_states*2)[s_idx:e_idx]\n",
    "        if len(latest_total_rewards) != n:\n",
    "            print(f'{s_idx} : {e_idx} : {e_idx-s_idx} : {len((self._total_discounted_rewards*2))}')\n",
    "        assert len(latest_exps) == n\n",
    "        assert len(latest_total_rewards) == n\n",
    "        assert len(latest_last_states) == n\n",
    "        states, actions, rewards, dones, next_states = zip(*latest_exps)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        next_states = np.array(next_states)\n",
    "        last_states = np.stack(latest_last_states)\n",
    "        return states, actions, rewards, dones, latest_total_rewards, last_states\n",
    "\n",
    "    def update_priorities(self, sample_indices: np.ndarray, sample_priorities: np.ndarray) -> None:\n",
    "        self._priorities[sample_indices] = sample_priorities\n",
    "\n",
    "    @property\n",
    "    def gamma(self) -> float:\n",
    "        return self._gamma\n",
    "\n",
    "    @property\n",
    "    def n_step(self) -> float:\n",
    "        return self._n_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfec9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        w = torch.full((out_features, in_features), sigma_init)\n",
    "        self._sigma_weight = nn.Parameter(w)\n",
    "        z = torch.zeros(out_features, in_features)\n",
    "        self.register_buffer(\"epsilon_weight\", z)\n",
    "        if bias:\n",
    "            w = torch.full((out_features,), sigma_init)\n",
    "            self._sigma_bias = nn.Parameter(w)\n",
    "            z = torch.zeros(out_features)\n",
    "            self.register_buffer(\"epsilon_bias\", z)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.epsilon_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.epsilon_bias.normal_()\n",
    "            bias = bias + self._sigma_bias * \\\n",
    "                   self.epsilon_bias.data\n",
    "        v = self._sigma_weight * self.epsilon_weight.data + \\\n",
    "            self.weight\n",
    "        return F.linear(input, v, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bb1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(A2C, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.policy = nn.Sequential(\n",
    "            NoisyLinear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.float() / 256\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc5c85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        exp_buffer: ExpBuffer,\n",
    "        net: nn.Module,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_final: float = 0.01,\n",
    "        epsilon_decay_last_step: int = 200000,\n",
    "        tgt_sync_steps: int = 10000,\n",
    "        learning_rate: float = 1e-4,\n",
    "        adam_eps: float = None,\n",
    "        clip_grad: float = 0.1,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        use_tgt_net: bool = False,\n",
    "    ):\n",
    "        self._env = env\n",
    "        self._exp_buffer = exp_buffer\n",
    "        self._net = net\n",
    "        self._tgt_net = deepcopy(net)\n",
    "        for p in self._tgt_net.parameters():\n",
    "            p.requires_grad = False\n",
    "        self._epsilon_start = epsilon_start\n",
    "        self._epsilon_final = epsilon_final\n",
    "        self._epsilon_decay_last_step = epsilon_decay_last_step\n",
    "        self._epsilon = epsilon_start\n",
    "        self._device = device\n",
    "        self._total_step = 0\n",
    "        self._total_trained_samples = 0\n",
    "        self._clip_grad = clip_grad\n",
    "        self._tgt_sync_steps = tgt_sync_steps\n",
    "        adam_kwargs = {}\n",
    "        if adam_eps is not None:\n",
    "            adam_kwargs['eps'] = adam_eps\n",
    "        self._optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, **adam_kwargs)\n",
    "        self._reset_episode()\n",
    "        self._use_tgt_net = use_tgt_net\n",
    "        self._episode = 0\n",
    "        self._episode_history = []\n",
    "        self._print_episode_rewards = []\n",
    "\n",
    "    def _reset_episode(self):\n",
    "        self._state = self._env.reset()\n",
    "        self._total_reward = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(self, epsilon: Optional[float] = None, sync_target: bool = True):\n",
    "        if epsilon is None:\n",
    "            epsilon = self._epsilon\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self._env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self._state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(self._device)\n",
    "            logits_v, value_v = self._net(state_v)\n",
    "            # _, act_v = torch.max(logits_v, dim=1)\n",
    "            # action = int(act_v.item())\n",
    "            probs_v = F.softmax(logits_v, dim=1)\n",
    "            prob = probs_v.data.cpu().numpy()[0]\n",
    "            action = np.random.choice(len(prob), p=prob)\n",
    "\n",
    "        next_state, reward, is_done, _ = self._env.step(action)\n",
    "        self._total_reward += reward\n",
    "\n",
    "        self._exp_buffer.append(\n",
    "            self._state, action, reward, is_done, next_state\n",
    "        )\n",
    "        self._state = next_state\n",
    "        if is_done:\n",
    "            done_reward = self._total_reward\n",
    "            self._reset_episode()\n",
    "\n",
    "        self._total_step += 1\n",
    "        self._update_epsilon(self._total_step)\n",
    "        self._exp_buffer.update_bata(self._total_step)\n",
    "\n",
    "        if self._total_step % self._tgt_sync_steps == 0 and sync_target and self._use_tgt_net:\n",
    "            self._tgt_net.load_state_dict(self._net.state_dict())\n",
    "            print(f'synced target net')\n",
    "\n",
    "        return done_reward\n",
    "\n",
    "    def train(self, n_iter: int = 1, batch_size: int = 32, print_episode_steps: int = 1) -> None:\n",
    "        n_step_gamma = self._exp_buffer.gamma ** self._exp_buffer.n_step\n",
    "        for i in range(n_iter):\n",
    "            for s in range(batch_size):\n",
    "                done_reward = self.play_step()\n",
    "                if done_reward is not None:\n",
    "                    self._print_episode_rewards.append(done_reward)\n",
    "                    if self._episode % print_episode_steps == 0:\n",
    "                        drm = sum(self._print_episode_rewards) / len(self._print_episode_rewards)\n",
    "                        print(f'episode : {self._episode}, done reward mean : {drm}, total_step : {self._total_step}, cur_epsilon : {self._epsilon}')\n",
    "                        self._print_episode_rewards.clear()\n",
    "                    self._episode_history.append((self._episode, done_reward, self._total_step, self._epsilon))\n",
    "                    self._episode += 1\n",
    "            states, actions, rewards, dones, total_discounted_rewards, \\\n",
    "                last_states = self._exp_buffer.get_latest_n(batch_size)\n",
    "            states_v = torch.tensor(states).to(self._device)\n",
    "            actions_v = torch.tensor(actions).to(self._device)\n",
    "            discounted_rewards_v = torch.tensor(total_discounted_rewards).to(self._device)\n",
    "            done_mask = torch.BoolTensor(dones).to(self._device)\n",
    "            # weights_v = torch.tensor(weights).to(self._device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                last_states_v = torch.tensor(last_states).to(self._device)\n",
    "                if self._use_tgt_net:\n",
    "                    last_state_values = self._tgt_net(last_states_v)[1].squeeze(-1)\n",
    "                else:\n",
    "                    last_state_values = self._net(last_states_v)[1].squeeze(-1)\n",
    "                last_state_values[done_mask] = 0.0\n",
    "                # print(f'[1] {last_state_values.shape} : {discounted_rewards_v.shape}')\n",
    "                expected_state_values = last_state_values.detach() * n_step_gamma + discounted_rewards_v\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            logits_v, value_v = self._net(states_v)\n",
    "            # print(f'[2] {value_v.shape} : {expected_state_values.shape}')\n",
    "            loss_value_v = F.mse_loss(expected_state_values.float(), value_v.squeeze(-1).float())  # (R(s,a) - V(s))^2\n",
    "\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "            adv_v = expected_state_values - value_v.detach()  # A(s, a) = R(s,a) - V(s)\n",
    "            log_prob_actions_v = adv_v * log_prob_v[range(batch_size), actions_v]\n",
    "            # log_prob_actions_v *= weights_v\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_loss_v = batch_size * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "            loss_policy_v.backward(retain_graph=True)\n",
    "            grads = np.concatenate([p.grad.data.cpu().numpy().flatten()\n",
    "                                    for p in self._net.parameters()\n",
    "                                    if p.grad is not None])\n",
    "\n",
    "            loss_v = entropy_loss_v + loss_value_v\n",
    "            loss_v.backward()\n",
    "            nn_utils.clip_grad_norm_(self._net.parameters(), self._clip_grad)\n",
    "            self._optimizer.step()\n",
    "\n",
    "            loss_v += loss_policy_v\n",
    "\n",
    "            # self._exp_buffer.update_priorities(sampled_indices, prios)  # 不要?\n",
    "            self._total_trained_samples += batch_size\n",
    "\n",
    "    def initial_exploration(self, n_steps: int = 10000, epsilon: float = 1.0) -> None:\n",
    "        eps_bak = self._epsilon\n",
    "        for i in pb(range(n_steps)):\n",
    "            self._epsilon = epsilon\n",
    "            self.play_step(sync_target=False)\n",
    "        self._total_step = 0\n",
    "        self._epsilon = eps_bak\n",
    "\n",
    "    def _update_epsilon(self, step_index: int) -> None:\n",
    "        self._epsilon = max(\n",
    "            self._epsilon_final,\n",
    "            self._epsilon_start - step_index / self._epsilon_decay_last_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10cb10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityToGymNumpyImgWrapper(UnityToGymWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnityToGymNumpyImgWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        state = super().reset(*args, **kwargs)\n",
    "        return np.array(state[0]).transpose(2, 0, 1)\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        next_state, reward, is_done, info = super().step(*args, **kwargs)\n",
    "        return np.array(next_state[0]).transpose(2, 0, 1), reward, is_done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97690a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityToGymNumpyImgResizeWrapper(UnityToGymNumpyImgWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnityToGymNumpyImgResizeWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        state = super().reset(*args, **kwargs)\n",
    "#         return cv2.resize(state[:, :, 10:-10].transpose(1, 2, 0), (224, 224)).transpose(2, 0, 1)\n",
    "        return state[:, :, 10:-10]\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        next_state, reward, is_done, info = super().step(*args, **kwargs)\n",
    "        next_state = cv2.resize(next_state[:, :, 10:-10].transpose(1, 2, 0), (224, 224)).transpose(2, 0, 1)\n",
    "#         return next_state, reward, is_done, info\n",
    "        return next_state[:, :, 10:-10], reward, is_done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db1c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityToGymNumpyImgResizeGrayWrapper(UnityToGymNumpyImgWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnityToGymNumpyImgResizeGrayWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        state = super().reset(*args, **kwargs)\n",
    "        return cv2.cvtColor(state[:, :, 10:-10].transpose(1, 2, 0), cv2.COLOR_RGB2GRAY).reshape(1, 64, 64)\n",
    "#         return state[:, :, 10:-10]\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        next_state, reward, is_done, info = super().step(*args, **kwargs)\n",
    "        next_state = cv2.cvtColor(next_state[:, :, 10:-10].transpose(1, 2, 0), cv2.COLOR_RGB2GRAY).reshape(1, 64, 64)\n",
    "#         return next_state, reward, is_done, info\n",
    "        return next_state, reward, is_done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1cf24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578fe53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e2ed681",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    unity_env.close()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ced14eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_env = UnityEnvironment(file_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74eb9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5a8df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UnityToGymWrapper(unity_env, allow_multiple_obs=True)\n",
    "env = UnityToGymNumpyImgResizeGrayWrapper(unity_env, allow_multiple_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b546b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior_names: ['GridWorld?team=0']\n",
      "\n",
      "== BehaviorSpecの情報の確認 ==\n",
      "observation_specs: [ObservationSpec(shape=(64, 84, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'), ObservationSpec(shape=(2,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.GOAL_SIGNAL: 1>, name='VectorSensor')]\n",
      "action_spec: Continuous: 0, Discrete: (5,)\n"
     ]
    }
   ],
   "source": [
    "# Unity環境のリセット\n",
    "unity_env.reset()\n",
    "\n",
    "# BehaviorNameのリストの取得\n",
    "behavior_names = list(unity_env.behavior_specs.keys())\n",
    "print('behavior_names:', behavior_names)\n",
    "\n",
    "# BehaviorSpecの取得\n",
    "behavior_spec = unity_env.behavior_specs[behavior_names[0]]\n",
    "\n",
    "# BehaviorSpecの情報の確認\n",
    "print('\\n== BehaviorSpecの情報の確認 ==')\n",
    "print('observation_specs:', behavior_spec.observation_specs)\n",
    "print('action_spec:', behavior_spec.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "509d354c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BehaviorSpec(observation_specs=[ObservationSpec(shape=(64, 84, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'), ObservationSpec(shape=(2,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.GOAL_SIGNAL: 1>, name='VectorSensor')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(5,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a817cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== DecisionStepsの情報の確認 ==\n",
      "obj: [array([[[[0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54901963, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]],\n",
      "\n",
      "        [[0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         ...,\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434],\n",
      "         [0.5372549 , 0.54509807, 0.56078434]]]], dtype=float32), array([[1., 0.]], dtype=float32)]\n",
      "reward: [-0.01]\n",
      "agent_id: [0]\n",
      "action_mask: [array([[False, False, False, False, False]])]\n",
      "\n",
      "== TerminalStepsの情報の確認 ==\n",
      "obs: [array([], shape=(0, 64, 84, 3), dtype=float32), array([], shape=(0, 2), dtype=float32)]\n",
      "reward: []\n",
      "agent_id: []\n",
      "interrupted: []\n"
     ]
    }
   ],
   "source": [
    "# 現在のステップの情報の取得\n",
    "decision_steps, terminal_steps = unity_env.get_steps(behavior_names[0])\n",
    "\n",
    "# DecisionStepsの情報の確認\n",
    "print('\\n== DecisionStepsの情報の確認 ==')\n",
    "print('obj:', decision_steps.obs)\n",
    "print('reward:', decision_steps.reward)\n",
    "print('agent_id:', decision_steps.agent_id)\n",
    "print('action_mask:', decision_steps.action_mask)\n",
    "\n",
    "# TerminalStepsの情報の確認\n",
    "print('\\n== TerminalStepsの情報の確認 ==')\n",
    "print('obs:', terminal_steps.obs)\n",
    "print('reward:', terminal_steps.reward)\n",
    "print('agent_id:', terminal_steps.agent_id)\n",
    "print('interrupted:', terminal_steps.interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bde7cf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decision_steps.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4213ef4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_steps.obs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b8b6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 84, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_steps.obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "571deb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AGENTS = decision_steps.obs[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "facfbef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43d59827",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = behavior_spec.action_spec.discrete_branches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cc340d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3e113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e8d0b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57aa05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "141c2141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Box(0.0, 1.0, (64, 84, 3), float32), Box(-inf, inf, (2,), float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0cbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "143a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "260b3271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b1e1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc7d45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fea500261f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(state.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6e688ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array(state)).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02833629",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, is_done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff06fbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bcceb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f4128ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8df86aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "554cb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "19358419",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6bbce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = A2C(input_shape=OBS_DIM, n_actions=N_ACTIONS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57226a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0285, -0.0030, -0.0026, -0.0016, -0.0351]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[0.0139]], device='cuda:0', grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(state).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5eac5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_buffer = ExpBuffer(\n",
    "    # max_size=1024,\n",
    "    prob_alpha=0.6,\n",
    "    beta_start=0.4,\n",
    "    beta_frames=30000, #100000,\n",
    "    n_step=4,\n",
    "    gamma=0.99,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "998b3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env=env,\n",
    "    exp_buffer=exp_buffer,\n",
    "    net=net,\n",
    "    epsilon_start=0.0,\n",
    "    epsilon_final=0.0,\n",
    "    epsilon_decay_last_step=30000, #200000,\n",
    "    tgt_sync_steps=1000,\n",
    "    learning_rate=5e-4,\n",
    "    adam_eps=1e-3,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64f11bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='100' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [100/100 00:01&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.initial_exploration(n_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ff6e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, dones, next_states = zip(*agent._exp_buffer._buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e49e2312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe9cc1f40d0>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(rewards[:200])\n",
    "plt.plot(agent._exp_buffer._total_discounted_rewards[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21bbcc0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 0, done reward mean : 0.3800000138580799, total_step : 36, cur_epsilon : 0.0\n",
      "episode : 100, done reward mean : -0.12889999331906438, total_step : 3118, cur_epsilon : 0.0\n",
      "episode : 200, done reward mean : -0.15879999443888665, total_step : 5703, cur_epsilon : 0.0\n",
      "episode : 300, done reward mean : -0.2806999928317964, total_step : 9000, cur_epsilon : 0.0\n",
      "episode : 400, done reward mean : -0.589499994199723, total_step : 11694, cur_epsilon : 0.0\n",
      "episode : 500, done reward mean : -0.34429999297484754, total_step : 14930, cur_epsilon : 0.0\n",
      "episode : 600, done reward mean : -0.2590999944321811, total_step : 17516, cur_epsilon : 0.0\n",
      "episode : 700, done reward mean : -0.14649999359622598, total_step : 20477, cur_epsilon : 0.0\n",
      "episode : 800, done reward mean : -0.414999993853271, total_step : 23325, cur_epsilon : 0.0\n",
      "episode : 900, done reward mean : -0.41319999389350415, total_step : 26151, cur_epsilon : 0.0\n",
      "episode : 1000, done reward mean : -0.4460999924875796, total_step : 29601, cur_epsilon : 0.0\n",
      "episode : 1100, done reward mean : -0.24569999316707253, total_step : 32750, cur_epsilon : 0.0\n",
      "episode : 1200, done reward mean : -0.06909999443218112, total_step : 35339, cur_epsilon : 0.0\n",
      "episode : 1300, done reward mean : -0.2787999926507473, total_step : 38722, cur_epsilon : 0.0\n",
      "episode : 1400, done reward mean : -0.47439999274909495, total_step : 42057, cur_epsilon : 0.0\n",
      "episode : 1500, done reward mean : -0.3473999929055572, total_step : 45328, cur_epsilon : 0.0\n",
      "episode : 1600, done reward mean : -0.126099992711097, total_step : 48683, cur_epsilon : 0.0\n",
      "episode : 1700, done reward mean : -0.21269999323412775, total_step : 51805, cur_epsilon : 0.0\n",
      "episode : 1800, done reward mean : -0.2804999930597842, total_step : 55005, cur_epsilon : 0.0\n",
      "episode : 1900, done reward mean : -0.2941999916359782, total_step : 58841, cur_epsilon : 0.0\n",
      "episode : 2000, done reward mean : -0.17379999365657567, total_step : 61768, cur_epsilon : 0.0\n",
      "episode : 2100, done reward mean : -0.1865999935939908, total_step : 64732, cur_epsilon : 0.0\n",
      "episode : 2200, done reward mean : -0.27789999267086385, total_step : 68104, cur_epsilon : 0.0\n",
      "episode : 2300, done reward mean : -0.13199999369680882, total_step : 71021, cur_epsilon : 0.0\n",
      "episode : 2400, done reward mean : -0.3725999930128455, total_step : 74241, cur_epsilon : 0.0\n",
      "episode : 2500, done reward mean : -0.22319999299943447, total_step : 77468, cur_epsilon : 0.0\n",
      "episode : 2600, done reward mean : -0.3684999937750399, total_step : 80348, cur_epsilon : 0.0\n",
      "episode : 2700, done reward mean : -0.08059999372810125, total_step : 83250, cur_epsilon : 0.0\n",
      "episode : 2800, done reward mean : -0.46229999234899877, total_step : 86765, cur_epsilon : 0.0\n",
      "episode : 2900, done reward mean : -0.28059999261051416, total_step : 90164, cur_epsilon : 0.0\n",
      "episode : 3000, done reward mean : -0.4549999925121665, total_step : 93610, cur_epsilon : 0.0\n",
      "episode : 3100, done reward mean : -0.38619999270886185, total_step : 96964, cur_epsilon : 0.0\n",
      "episode : 3200, done reward mean : -0.21879999332129954, total_step : 100046, cur_epsilon : 0.0\n",
      "episode : 3300, done reward mean : -0.44439999252557755, total_step : 103483, cur_epsilon : 0.0\n",
      "episode : 3400, done reward mean : -0.21269999278709292, total_step : 106801, cur_epsilon : 0.0\n",
      "episode : 3500, done reward mean : -0.2186999942176044, total_step : 109486, cur_epsilon : 0.0\n",
      "episode : 3600, done reward mean : -0.4766999931447208, total_step : 112646, cur_epsilon : 0.0\n",
      "episode : 3700, done reward mean : -0.2545999938622117, total_step : 115486, cur_epsilon : 0.0\n",
      "episode : 3800, done reward mean : -0.5202999921701849, total_step : 119084, cur_epsilon : 0.0\n",
      "episode : 3900, done reward mean : -0.42039999417960644, total_step : 121784, cur_epsilon : 0.0\n",
      "episode : 4000, done reward mean : -0.3567999924719334, total_step : 125246, cur_epsilon : 0.0\n",
      "episode : 4100, done reward mean : -0.19599999226629733, total_step : 128797, cur_epsilon : 0.0\n",
      "episode : 4200, done reward mean : -0.22929999308660626, total_step : 131984, cur_epsilon : 0.0\n",
      "episode : 4300, done reward mean : -0.13879999332129955, total_step : 135064, cur_epsilon : 0.0\n",
      "episode : 4400, done reward mean : -0.17059999283403157, total_step : 138359, cur_epsilon : 0.0\n",
      "episode : 4500, done reward mean : -0.23849999288097024, total_step : 141636, cur_epsilon : 0.0\n",
      "episode : 4600, done reward mean : -0.3202999930642545, total_step : 144828, cur_epsilon : 0.0\n",
      "episode : 4700, done reward mean : -0.361099993493408, total_step : 147832, cur_epsilon : 0.0\n",
      "episode : 4800, done reward mean : -0.18889999398961663, total_step : 150617, cur_epsilon : 0.0\n",
      "episode : 4900, done reward mean : -0.39069999327883126, total_step : 153721, cur_epsilon : 0.0\n",
      "episode : 5000, done reward mean : -0.40609999338164926, total_step : 156773, cur_epsilon : 0.0\n",
      "episode : 5100, done reward mean : -0.23869999332353473, total_step : 159852, cur_epsilon : 0.0\n",
      "episode : 5200, done reward mean : -0.41299999322742226, total_step : 162977, cur_epsilon : 0.0\n",
      "episode : 5300, done reward mean : -0.23769999446347356, total_step : 165551, cur_epsilon : 0.0\n",
      "episode : 5400, done reward mean : -0.37579999316483736, total_step : 168702, cur_epsilon : 0.0\n",
      "episode : 5500, done reward mean : -0.24709999425336718, total_step : 171370, cur_epsilon : 0.0\n",
      "episode : 5600, done reward mean : -0.2882999928854406, total_step : 174646, cur_epsilon : 0.0\n",
      "episode : 5700, done reward mean : -0.3870999940298498, total_step : 177415, cur_epsilon : 0.0\n",
      "episode : 5800, done reward mean : -0.2362999933771789, total_step : 180472, cur_epsilon : 0.0\n",
      "episode : 5900, done reward mean : -0.3133999941125512, total_step : 183203, cur_epsilon : 0.0\n",
      "episode : 6000, done reward mean : -0.3565999929234385, total_step : 186461, cur_epsilon : 0.0\n",
      "episode : 6100, done reward mean : -0.11269999234005809, total_step : 189977, cur_epsilon : 0.0\n",
      "episode : 6200, done reward mean : -0.19299999300390483, total_step : 193199, cur_epsilon : 0.0\n",
      "episode : 6300, done reward mean : -0.25389999186620116, total_step : 196929, cur_epsilon : 0.0\n",
      "episode : 6400, done reward mean : -0.4126999927870929, total_step : 200249, cur_epsilon : 0.0\n",
      "episode : 6500, done reward mean : -0.17709999402984977, total_step : 203015, cur_epsilon : 0.0\n",
      "episode : 6600, done reward mean : -0.35639999225735663, total_step : 206572, cur_epsilon : 0.0\n",
      "episode : 6700, done reward mean : 0.0027000061981379987, total_step : 209439, cur_epsilon : 0.0\n",
      "episode : 6800, done reward mean : -0.27859999377280475, total_step : 212321, cur_epsilon : 0.0\n",
      "episode : 6900, done reward mean : -0.49719999335706233, total_step : 215389, cur_epsilon : 0.0\n",
      "episode : 7000, done reward mean : -0.4377999928966165, total_step : 218659, cur_epsilon : 0.0\n",
      "episode : 7100, done reward mean : -0.25719999358057977, total_step : 221628, cur_epsilon : 0.0\n",
      "episode : 7200, done reward mean : -0.3453999922797084, total_step : 225168, cur_epsilon : 0.0\n",
      "episode : 7300, done reward mean : -0.3419999939203262, total_step : 227985, cur_epsilon : 0.0\n",
      "episode : 7400, done reward mean : -0.3960999929346144, total_step : 231240, cur_epsilon : 0.0\n",
      "episode : 7500, done reward mean : -0.383799992762506, total_step : 234572, cur_epsilon : 0.0\n",
      "episode : 7600, done reward mean : -0.2679999915510416, total_step : 238441, cur_epsilon : 0.0\n",
      "episode : 7700, done reward mean : -0.3683999928832054, total_step : 241716, cur_epsilon : 0.0\n",
      "episode : 7800, done reward mean : -0.3261999936029315, total_step : 244674, cur_epsilon : 0.0\n",
      "episode : 7900, done reward mean : -0.34599999338388443, total_step : 247725, cur_epsilon : 0.0\n",
      "episode : 8000, done reward mean : -0.2889999935403466, total_step : 250713, cur_epsilon : 0.0\n",
      "episode : 8100, done reward mean : -0.3701999930664897, total_step : 253909, cur_epsilon : 0.0\n",
      "episode : 8200, done reward mean : -0.5044999936409295, total_step : 256852, cur_epsilon : 0.0\n",
      "episode : 8300, done reward mean : -0.1862999931536615, total_step : 260011, cur_epsilon : 0.0\n",
      "episode : 8400, done reward mean : -0.3679999926686287, total_step : 263385, cur_epsilon : 0.0\n",
      "episode : 8500, done reward mean : -0.2856999954022467, total_step : 265542, cur_epsilon : 0.0\n",
      "episode : 8600, done reward mean : -0.3952999929524958, total_step : 268791, cur_epsilon : 0.0\n",
      "episode : 8700, done reward mean : -0.2825999932363629, total_step : 271913, cur_epsilon : 0.0\n",
      "episode : 8800, done reward mean : -0.3122999934665859, total_step : 274934, cur_epsilon : 0.0\n",
      "episode : 8900, done reward mean : -0.23149999326094986, total_step : 278044, cur_epsilon : 0.0\n",
      "episode : 9000, done reward mean : -0.21519999206066132, total_step : 281686, cur_epsilon : 0.0\n",
      "episode : 9100, done reward mean : -0.22769999312236905, total_step : 284855, cur_epsilon : 0.0\n",
      "episode : 9200, done reward mean : -0.31509999295696617, total_step : 288102, cur_epsilon : 0.0\n",
      "episode : 9300, done reward mean : -0.3592999933101237, total_step : 291187, cur_epsilon : 0.0\n",
      "episode : 9400, done reward mean : -0.3675999926775694, total_step : 294557, cur_epsilon : 0.0\n",
      "episode : 9500, done reward mean : -0.29579999182373284, total_step : 298308, cur_epsilon : 0.0\n",
      "episode : 9600, done reward mean : -0.1947999931871891, total_step : 301451, cur_epsilon : 0.0\n",
      "episode : 9700, done reward mean : -0.3121999925747514, total_step : 304865, cur_epsilon : 0.0\n",
      "episode : 9800, done reward mean : -0.41199999280273913, total_step : 308180, cur_epsilon : 0.0\n",
      "episode : 9900, done reward mean : -0.3490999926440418, total_step : 311565, cur_epsilon : 0.0\n",
      "episode : 10000, done reward mean : -0.24249999368563294, total_step : 314486, cur_epsilon : 0.0\n",
      "episode : 10100, done reward mean : -0.3163999918103218, total_step : 318239, cur_epsilon : 0.0\n",
      "episode : 10200, done reward mean : -0.1768999924696982, total_step : 321702, cur_epsilon : 0.0\n",
      "episode : 10300, done reward mean : -0.37499999318271876, total_step : 324845, cur_epsilon : 0.0\n",
      "episode : 10400, done reward mean : -0.2818999930284917, total_step : 328057, cur_epsilon : 0.0\n",
      "episode : 10500, done reward mean : -0.10479999341070652, total_step : 331100, cur_epsilon : 0.0\n",
      "episode : 10600, done reward mean : -0.4487999948859215, total_step : 333486, cur_epsilon : 0.0\n",
      "episode : 10700, done reward mean : -0.2200999935157597, total_step : 336484, cur_epsilon : 0.0\n",
      "episode : 10800, done reward mean : -0.45849999310448764, total_step : 339664, cur_epsilon : 0.0\n",
      "episode : 10900, done reward mean : -0.32819999266415834, total_step : 343040, cur_epsilon : 0.0\n",
      "episode : 11000, done reward mean : -0.2159999942779541, total_step : 345696, cur_epsilon : 0.0\n",
      "episode : 11100, done reward mean : -0.26399999365210536, total_step : 348632, cur_epsilon : 0.0\n",
      "episode : 11200, done reward mean : -0.4327999923378229, total_step : 352151, cur_epsilon : 0.0\n",
      "episode : 11300, done reward mean : -0.11069999372586609, total_step : 355053, cur_epsilon : 0.0\n",
      "episode : 11400, done reward mean : -0.4026999930106103, total_step : 358275, cur_epsilon : 0.0\n",
      "episode : 11500, done reward mean : -0.5005999935045838, total_step : 361278, cur_epsilon : 0.0\n",
      "episode : 11600, done reward mean : -0.3493999944254756, total_step : 363868, cur_epsilon : 0.0\n",
      "episode : 11700, done reward mean : -0.3441999925300479, total_step : 367299, cur_epsilon : 0.0\n",
      "episode : 11800, done reward mean : -0.3568999922461808, total_step : 370861, cur_epsilon : 0.0\n",
      "episode : 11900, done reward mean : -0.36029999239370225, total_step : 374352, cur_epsilon : 0.0\n",
      "episode : 12000, done reward mean : -0.3392999942041934, total_step : 377041, cur_epsilon : 0.0\n",
      "episode : 12100, done reward mean : -0.5426999927870929, total_step : 380360, cur_epsilon : 0.0\n",
      "episode : 12200, done reward mean : -0.20499999228864907, total_step : 383900, cur_epsilon : 0.0\n",
      "episode : 12300, done reward mean : -0.25999999284744263, total_step : 387190, cur_epsilon : 0.0\n",
      "episode : 12400, done reward mean : -0.1192999935336411, total_step : 390174, cur_epsilon : 0.0\n",
      "episode : 12500, done reward mean : -0.0408999932743609, total_step : 393275, cur_epsilon : 0.0\n",
      "episode : 12600, done reward mean : -0.39549999294802546, total_step : 396522, cur_epsilon : 0.0\n",
      "episode : 12700, done reward mean : -0.3648999925144017, total_step : 399962, cur_epsilon : 0.0\n",
      "episode : 12800, done reward mean : -0.30009999262169, total_step : 403356, cur_epsilon : 0.0\n",
      "episode : 12900, done reward mean : -0.144999993853271, total_step : 406199, cur_epsilon : 0.0\n",
      "episode : 13000, done reward mean : -0.22899999331682921, total_step : 409284, cur_epsilon : 0.0\n",
      "episode : 13100, done reward mean : -0.28829999243840576, total_step : 412760, cur_epsilon : 0.0\n",
      "episode : 13200, done reward mean : -0.1823999934643507, total_step : 415775, cur_epsilon : 0.0\n",
      "episode : 13300, done reward mean : -0.4392999930866063, total_step : 418963, cur_epsilon : 0.0\n",
      "episode : 13400, done reward mean : -0.2284999935515225, total_step : 421944, cur_epsilon : 0.0\n",
      "episode : 13500, done reward mean : -0.32989999374374745, total_step : 424840, cur_epsilon : 0.0\n",
      "episode : 13600, done reward mean : -0.22959999307990075, total_step : 428032, cur_epsilon : 0.0\n",
      "episode : 13700, done reward mean : -0.3090999930910766, total_step : 431217, cur_epsilon : 0.0\n",
      "episode : 13800, done reward mean : -0.3742999934218824, total_step : 434254, cur_epsilon : 0.0\n",
      "episode : 13900, done reward mean : -0.10269999323412776, total_step : 437371, cur_epsilon : 0.0\n",
      "episode : 14000, done reward mean : -0.23869999332353473, total_step : 440452, cur_epsilon : 0.0\n",
      "episode : 14100, done reward mean : -0.2229999927803874, total_step : 443774, cur_epsilon : 0.0\n",
      "episode : 14200, done reward mean : -0.3704999941773713, total_step : 446474, cur_epsilon : 0.0\n",
      "episode : 14300, done reward mean : -0.44419999297708274, total_step : 449709, cur_epsilon : 0.0\n",
      "episode : 14400, done reward mean : -0.40009999306872485, total_step : 452903, cur_epsilon : 0.0\n",
      "episode : 14500, done reward mean : -0.2498999928496778, total_step : 456195, cur_epsilon : 0.0\n",
      "episode : 14600, done reward mean : -0.1326999936811626, total_step : 459115, cur_epsilon : 0.0\n",
      "episode : 14700, done reward mean : -0.15969999285414815, total_step : 462400, cur_epsilon : 0.0\n",
      "episode : 14800, done reward mean : -0.24199999324977398, total_step : 465516, cur_epsilon : 0.0\n",
      "episode : 14900, done reward mean : -0.10979999508708715, total_step : 467811, cur_epsilon : 0.0\n",
      "episode : 15000, done reward mean : -0.4503999928385019, total_step : 471110, cur_epsilon : 0.0\n",
      "episode : 15100, done reward mean : -0.23989999352023006, total_step : 474104, cur_epsilon : 0.0\n",
      "episode : 15200, done reward mean : -0.21859999243170022, total_step : 477582, cur_epsilon : 0.0\n",
      "episode : 15300, done reward mean : -0.34119999304413795, total_step : 480789, cur_epsilon : 0.0\n",
      "episode : 15400, done reward mean : -0.283499994110316, total_step : 483518, cur_epsilon : 0.0\n",
      "episode : 15500, done reward mean : -0.34199999369680883, total_step : 486430, cur_epsilon : 0.0\n",
      "episode : 15600, done reward mean : -0.3821999939158559, total_step : 489247, cur_epsilon : 0.0\n",
      "episode : 15700, done reward mean : -0.39409999253228306, total_step : 492682, cur_epsilon : 0.0\n",
      "episode : 15800, done reward mean : -0.2018999928049743, total_step : 495997, cur_epsilon : 0.0\n",
      "episode : 15900, done reward mean : -0.3880999924428761, total_step : 499469, cur_epsilon : 0.0\n",
      "episode : 16000, done reward mean : -0.32039999306201933, total_step : 502664, cur_epsilon : 0.0\n",
      "episode : 16100, done reward mean : -0.2640999927558005, total_step : 505995, cur_epsilon : 0.0\n",
      "episode : 16200, done reward mean : -0.392199992351234, total_step : 509512, cur_epsilon : 0.0\n",
      "episode : 16300, done reward mean : -0.4992999919690192, total_step : 513193, cur_epsilon : 0.0\n",
      "episode : 16400, done reward mean : -0.3537999938800931, total_step : 516027, cur_epsilon : 0.0\n",
      "episode : 16500, done reward mean : -0.3706999939493835, total_step : 518830, cur_epsilon : 0.0\n",
      "episode : 16600, done reward mean : -0.37249999368563297, total_step : 521748, cur_epsilon : 0.0\n",
      "episode : 16700, done reward mean : -0.3509999930486083, total_step : 524950, cur_epsilon : 0.0\n",
      "episode : 16800, done reward mean : -0.23569999204948544, total_step : 528595, cur_epsilon : 0.0\n",
      "episode : 16900, done reward mean : -0.3729999916628003, total_step : 532415, cur_epsilon : 0.0\n",
      "episode : 17000, done reward mean : -0.25299999278038743, total_step : 535738, cur_epsilon : 0.0\n",
      "episode : 17100, done reward mean : -0.3553999940678477, total_step : 538487, cur_epsilon : 0.0\n",
      "episode : 17200, done reward mean : -0.2195999924093485, total_step : 541977, cur_epsilon : 0.0\n",
      "episode : 17300, done reward mean : -0.3065999935939908, total_step : 544939, cur_epsilon : 0.0\n",
      "episode : 17400, done reward mean : -0.19829999400302767, total_step : 547719, cur_epsilon : 0.0\n",
      "episode : 17500, done reward mean : -0.4904999930597842, total_step : 550920, cur_epsilon : 0.0\n",
      "episode : 17600, done reward mean : -0.2967999938130379, total_step : 553786, cur_epsilon : 0.0\n",
      "episode : 17700, done reward mean : -0.026899993363767864, total_step : 556850, cur_epsilon : 0.0\n",
      "episode : 17800, done reward mean : -0.22409999364987016, total_step : 559785, cur_epsilon : 0.0\n",
      "episode : 17900, done reward mean : -0.32489999251440166, total_step : 563227, cur_epsilon : 0.0\n",
      "episode : 18000, done reward mean : -0.4403999930620193, total_step : 566424, cur_epsilon : 0.0\n",
      "episode : 18100, done reward mean : -0.20099999349564313, total_step : 569429, cur_epsilon : 0.0\n",
      "episode : 18200, done reward mean : -0.38629999248310926, total_step : 572887, cur_epsilon : 0.0\n",
      "episode : 18300, done reward mean : -0.444299993198365, total_step : 576022, cur_epsilon : 0.0\n",
      "episode : 18400, done reward mean : -0.4927999923378229, total_step : 579541, cur_epsilon : 0.0\n",
      "episode : 18500, done reward mean : -0.40949999352917077, total_step : 582530, cur_epsilon : 0.0\n",
      "episode : 18600, done reward mean : -0.047799993120133874, total_step : 585698, cur_epsilon : 0.0\n",
      "episode : 18700, done reward mean : -0.26299999345093966, total_step : 588721, cur_epsilon : 0.0\n",
      "episode : 18800, done reward mean : -0.3229999930039048, total_step : 591946, cur_epsilon : 0.0\n",
      "episode : 18900, done reward mean : -0.1667999929189682, total_step : 595205, cur_epsilon : 0.0\n",
      "episode : 19000, done reward mean : -0.3298999928496778, total_step : 598497, cur_epsilon : 0.0\n",
      "episode : 19100, done reward mean : -0.3356999929435551, total_step : 601746, cur_epsilon : 0.0\n",
      "episode : 19200, done reward mean : -0.23479999229311943, total_step : 605287, cur_epsilon : 0.0\n",
      "episode : 19300, done reward mean : -0.14289999300613998, total_step : 608513, cur_epsilon : 0.0\n",
      "episode : 19400, done reward mean : -0.22189999414607883, total_step : 611224, cur_epsilon : 0.0\n",
      "episode : 19500, done reward mean : -0.38959999307990073, total_step : 614414, cur_epsilon : 0.0\n",
      "episode : 19600, done reward mean : -0.18469999274238943, total_step : 617753, cur_epsilon : 0.0\n",
      "episode : 19700, done reward mean : -0.23599999338388444, total_step : 620809, cur_epsilon : 0.0\n",
      "episode : 19800, done reward mean : -0.2755999940633774, total_step : 623562, cur_epsilon : 0.0\n",
      "episode : 19900, done reward mean : -0.20689999425783753, total_step : 626228, cur_epsilon : 0.0\n",
      "episode : 20000, done reward mean : -0.2478999931178987, total_step : 629403, cur_epsilon : 0.0\n",
      "episode : 20100, done reward mean : -0.2498999926261604, total_step : 632798, cur_epsilon : 0.0\n",
      "episode : 20200, done reward mean : -0.27769999334588646, total_step : 635871, cur_epsilon : 0.0\n",
      "episode : 20300, done reward mean : -0.25379999320954083, total_step : 639004, cur_epsilon : 0.0\n",
      "episode : 20400, done reward mean : -0.3724999930150807, total_step : 642219, cur_epsilon : 0.0\n",
      "episode : 20500, done reward mean : -0.4216999932564795, total_step : 645330, cur_epsilon : 0.0\n",
      "episode : 20600, done reward mean : -0.3245999927446246, total_step : 648670, cur_epsilon : 0.0\n",
      "episode : 20700, done reward mean : -0.21799999356269836, total_step : 651645, cur_epsilon : 0.0\n",
      "episode : 20800, done reward mean : -0.38529999362304806, total_step : 654594, cur_epsilon : 0.0\n",
      "episode : 20900, done reward mean : -0.40609999315813183, total_step : 657749, cur_epsilon : 0.0\n",
      "episode : 21000, done reward mean : -0.33779999312013387, total_step : 660922, cur_epsilon : 0.0\n",
      "episode : 21100, done reward mean : -0.3557999936118722, total_step : 663875, cur_epsilon : 0.0\n",
      "episode : 21200, done reward mean : -0.39689999314025043, total_step : 667037, cur_epsilon : 0.0\n",
      "episode : 21300, done reward mean : -0.2390999928675592, total_step : 670322, cur_epsilon : 0.0\n",
      "episode : 21400, done reward mean : -0.2532999929971993, total_step : 673549, cur_epsilon : 0.0\n",
      "episode : 21500, done reward mean : -0.22469999318942427, total_step : 676690, cur_epsilon : 0.0\n",
      "episode : 21600, done reward mean : -0.2295999939739704, total_step : 679482, cur_epsilon : 0.0\n",
      "episode : 21700, done reward mean : -0.3814999925903976, total_step : 682892, cur_epsilon : 0.0\n",
      "episode : 21800, done reward mean : -0.23539999295026065, total_step : 686136, cur_epsilon : 0.0\n",
      "episode : 21900, done reward mean : -0.29809999311342833, total_step : 689312, cur_epsilon : 0.0\n",
      "episode : 22000, done reward mean : -0.29919999442994594, total_step : 691903, cur_epsilon : 0.0\n",
      "episode : 22100, done reward mean : -0.2982999926619232, total_step : 695275, cur_epsilon : 0.0\n",
      "episode : 22200, done reward mean : -0.35229999413713814, total_step : 697995, cur_epsilon : 0.0\n",
      "episode : 22300, done reward mean : -0.5253999931737781, total_step : 701145, cur_epsilon : 0.0\n",
      "episode : 22400, done reward mean : -0.20619999293237926, total_step : 704400, cur_epsilon : 0.0\n",
      "episode : 22500, done reward mean : -0.37539999317377803, total_step : 707549, cur_epsilon : 0.0\n",
      "episode : 22600, done reward mean : -0.33579999338835476, total_step : 710599, cur_epsilon : 0.0\n",
      "episode : 22700, done reward mean : -0.30789999378845095, total_step : 713475, cur_epsilon : 0.0\n",
      "episode : 22800, done reward mean : -0.1806999937258661, total_step : 716376, cur_epsilon : 0.0\n",
      "episode : 22900, done reward mean : -0.13219999391585588, total_step : 719194, cur_epsilon : 0.0\n",
      "episode : 23000, done reward mean : -0.2445999925211072, total_step : 722633, cur_epsilon : 0.0\n",
      "episode : 23100, done reward mean : -0.11049999305978417, total_step : 725834, cur_epsilon : 0.0\n",
      "episode : 23200, done reward mean : -0.2423999934643507, total_step : 728849, cur_epsilon : 0.0\n",
      "episode : 23300, done reward mean : -0.28439999252557757, total_step : 732288, cur_epsilon : 0.0\n",
      "episode : 23400, done reward mean : -0.21469999251887203, total_step : 735727, cur_epsilon : 0.0\n",
      "episode : 23500, done reward mean : -0.35439999386668203, total_step : 738563, cur_epsilon : 0.0\n",
      "episode : 23600, done reward mean : -0.20959999330341816, total_step : 741652, cur_epsilon : 0.0\n",
      "episode : 23700, done reward mean : -0.21579999294131993, total_step : 744906, cur_epsilon : 0.0\n",
      "episode : 23800, done reward mean : -0.44249999279156327, total_step : 748225, cur_epsilon : 0.0\n",
      "episode : 23900, done reward mean : -0.1389999930933118, total_step : 751410, cur_epsilon : 0.0\n",
      "episode : 24000, done reward mean : -0.2697999937459826, total_step : 754303, cur_epsilon : 0.0\n",
      "episode : 24100, done reward mean : -0.1912999921478331, total_step : 757906, cur_epsilon : 0.0\n",
      "episode : 24200, done reward mean : -0.3401999930664897, total_step : 761103, cur_epsilon : 0.0\n",
      "episode : 24300, done reward mean : -0.209699993301183, total_step : 764195, cur_epsilon : 0.0\n",
      "episode : 24400, done reward mean : -0.1608999930508435, total_step : 767399, cur_epsilon : 0.0\n",
      "episode : 24500, done reward mean : -0.22899999286979436, total_step : 770682, cur_epsilon : 0.0\n",
      "episode : 24600, done reward mean : -0.47349999165162443, total_step : 774511, cur_epsilon : 0.0\n",
      "episode : 24700, done reward mean : -0.41029999217018487, total_step : 778104, cur_epsilon : 0.0\n",
      "episode : 24800, done reward mean : -0.4008999928273261, total_step : 781405, cur_epsilon : 0.0\n",
      "episode : 24900, done reward mean : -0.3350999929569662, total_step : 784652, cur_epsilon : 0.0\n",
      "episode : 25000, done reward mean : -0.3105999932810664, total_step : 787751, cur_epsilon : 0.0\n",
      "episode : 25100, done reward mean : -0.4804999930597842, total_step : 790947, cur_epsilon : 0.0\n",
      "episode : 25200, done reward mean : -0.2223999923467636, total_step : 794465, cur_epsilon : 0.0\n",
      "episode : 25300, done reward mean : -0.2574999935738742, total_step : 797435, cur_epsilon : 0.0\n",
      "episode : 25400, done reward mean : -0.2136999929882586, total_step : 800662, cur_epsilon : 0.0\n",
      "episode : 25500, done reward mean : -0.2652999931760132, total_step : 803809, cur_epsilon : 0.0\n",
      "episode : 25600, done reward mean : -0.32289999367669225, total_step : 806732, cur_epsilon : 0.0\n",
      "episode : 25700, done reward mean : -0.28939999353140594, total_step : 809720, cur_epsilon : 0.0\n",
      "episode : 25800, done reward mean : -0.30389999410137536, total_step : 812453, cur_epsilon : 0.0\n",
      "episode : 25900, done reward mean : -0.2674999935738742, total_step : 815420, cur_epsilon : 0.0\n",
      "episode : 26000, done reward mean : -0.2848999940790236, total_step : 818163, cur_epsilon : 0.0\n",
      "episode : 26100, done reward mean : -0.5618999928049743, total_step : 821474, cur_epsilon : 0.0\n",
      "episode : 26200, done reward mean : -0.11149999303743244, total_step : 824681, cur_epsilon : 0.0\n",
      "episode : 26300, done reward mean : -0.35959999285638333, total_step : 827967, cur_epsilon : 0.0\n",
      "episode : 26400, done reward mean : -0.42219999369233846, total_step : 830887, cur_epsilon : 0.0\n",
      "episode : 26500, done reward mean : -0.24699999313801527, total_step : 834053, cur_epsilon : 0.0\n",
      "episode : 26600, done reward mean : -0.17089999305084347, total_step : 837252, cur_epsilon : 0.0\n",
      "episode : 26700, done reward mean : -0.26089999394491314, total_step : 840058, cur_epsilon : 0.0\n",
      "episode : 26800, done reward mean : -0.43739999379962685, total_step : 842928, cur_epsilon : 0.0\n",
      "episode : 26900, done reward mean : -0.21539999272674323, total_step : 846277, cur_epsilon : 0.0\n",
      "episode : 27000, done reward mean : -0.5650999927334488, total_step : 849620, cur_epsilon : 0.0\n",
      "episode : 27100, done reward mean : -0.37979999352246524, total_step : 852611, cur_epsilon : 0.0\n",
      "episode : 27200, done reward mean : -0.4046999936364591, total_step : 855552, cur_epsilon : 0.0\n",
      "episode : 27300, done reward mean : -0.30009999239817264, total_step : 859049, cur_epsilon : 0.0\n",
      "episode : 27400, done reward mean : -0.512599993236363, total_step : 862170, cur_epsilon : 0.0\n",
      "episode : 27500, done reward mean : -0.36299999367445707, total_step : 865092, cur_epsilon : 0.0\n",
      "episode : 27600, done reward mean : -0.17589999293908476, total_step : 868345, cur_epsilon : 0.0\n",
      "episode : 27700, done reward mean : -0.16249999368563295, total_step : 871266, cur_epsilon : 0.0\n",
      "episode : 27800, done reward mean : -0.2821999921277165, total_step : 874883, cur_epsilon : 0.0\n",
      "episode : 27900, done reward mean : -0.26699999269098046, total_step : 878247, cur_epsilon : 0.0\n",
      "episode : 28000, done reward mean : -0.2223999936878681, total_step : 881165, cur_epsilon : 0.0\n",
      "episode : 28100, done reward mean : -0.2625999927893281, total_step : 884483, cur_epsilon : 0.0\n",
      "episode : 28200, done reward mean : -0.3568999933637679, total_step : 887544, cur_epsilon : 0.0\n",
      "episode : 28300, done reward mean : -0.32259999234229325, total_step : 891062, cur_epsilon : 0.0\n",
      "episode : 28400, done reward mean : -0.203699993211776, total_step : 894193, cur_epsilon : 0.0\n",
      "episode : 28500, done reward mean : -0.2691999926418066, total_step : 897581, cur_epsilon : 0.0\n",
      "episode : 28600, done reward mean : -0.4637999934330583, total_step : 900612, cur_epsilon : 0.0\n",
      "episode : 28700, done reward mean : -0.3442999923042953, total_step : 904147, cur_epsilon : 0.0\n",
      "episode : 28800, done reward mean : -0.26379999455064534, total_step : 906681, cur_epsilon : 0.0\n",
      "episode : 28900, done reward mean : -0.3374999933503568, total_step : 909750, cur_epsilon : 0.0\n",
      "episode : 29000, done reward mean : -0.2922999936901033, total_step : 912670, cur_epsilon : 0.0\n",
      "episode : 29100, done reward mean : -0.3560999936051667, total_step : 915628, cur_epsilon : 0.0\n",
      "episode : 29200, done reward mean : -0.249599993750453, total_step : 918517, cur_epsilon : 0.0\n",
      "episode : 29300, done reward mean : -0.348299993108958, total_step : 921696, cur_epsilon : 0.0\n",
      "episode : 29400, done reward mean : -0.3547999918460846, total_step : 925433, cur_epsilon : 0.0\n",
      "episode : 29500, done reward mean : -0.028999994210898876, total_step : 928120, cur_epsilon : 0.0\n",
      "episode : 29600, done reward mean : -0.39169999392703175, total_step : 930935, cur_epsilon : 0.0\n",
      "episode : 29700, done reward mean : -0.3159999929368496, total_step : 934189, cur_epsilon : 0.0\n",
      "episode : 29800, done reward mean : -0.32699999380856754, total_step : 937056, cur_epsilon : 0.0\n",
      "episode : 29900, done reward mean : -0.2638999941013753, total_step : 939789, cur_epsilon : 0.0\n",
      "episode : 30000, done reward mean : -0.30029999328777196, total_step : 942882, cur_epsilon : 0.0\n",
      "episode : 30100, done reward mean : -0.32479999229311945, total_step : 946422, cur_epsilon : 0.0\n",
      "episode : 30200, done reward mean : -0.3034999920986593, total_step : 950050, cur_epsilon : 0.0\n",
      "episode : 30300, done reward mean : -0.4443999929726124, total_step : 953287, cur_epsilon : 0.0\n",
      "episode : 30400, done reward mean : -0.33639999248087404, total_step : 956741, cur_epsilon : 0.0\n",
      "episode : 30500, done reward mean : -0.2915999937057495, total_step : 959656, cur_epsilon : 0.0\n",
      "episode : 30600, done reward mean : -0.28449999475851656, total_step : 962100, cur_epsilon : 0.0\n",
      "episode : 30700, done reward mean : -0.2291999937593937, total_step : 964989, cur_epsilon : 0.0\n",
      "episode : 30800, done reward mean : -0.21349999411031603, total_step : 967721, cur_epsilon : 0.0\n",
      "episode : 30900, done reward mean : -0.24619999315589666, total_step : 970875, cur_epsilon : 0.0\n",
      "episode : 31000, done reward mean : -0.0786999928764999, total_step : 974158, cur_epsilon : 0.0\n",
      "episode : 31100, done reward mean : -0.17169999280944467, total_step : 977470, cur_epsilon : 0.0\n",
      "episode : 31200, done reward mean : -0.3928999925591052, total_step : 980891, cur_epsilon : 0.0\n",
      "episode : 31300, done reward mean : -0.30179999347776176, total_step : 983904, cur_epsilon : 0.0\n",
      "episode : 31400, done reward mean : -0.32769999423995616, total_step : 986576, cur_epsilon : 0.0\n",
      "episode : 31500, done reward mean : -0.2648999929614365, total_step : 989818, cur_epsilon : 0.0\n",
      "episode : 31600, done reward mean : -0.3714999925903976, total_step : 993227, cur_epsilon : 0.0\n",
      "episode : 31700, done reward mean : -0.34429999275133016, total_step : 996560, cur_epsilon : 0.0\n",
      "episode : 31800, done reward mean : -0.17269999278709292, total_step : 999882, cur_epsilon : 0.0\n",
      "episode : 31900, done reward mean : -0.16419999342411756, total_step : 1002917, cur_epsilon : 0.0\n",
      "episode : 32000, done reward mean : -0.2877999917790294, total_step : 1006685, cur_epsilon : 0.0\n",
      "episode : 32100, done reward mean : -0.11549999317154289, total_step : 1009835, cur_epsilon : 0.0\n",
      "episode : 32200, done reward mean : -0.26459999319165944, total_step : 1012977, cur_epsilon : 0.0\n",
      "episode : 32300, done reward mean : -0.3120999932475388, total_step : 1016091, cur_epsilon : 0.0\n",
      "episode : 32400, done reward mean : -0.24239999301731585, total_step : 1019308, cur_epsilon : 0.0\n",
      "episode : 32500, done reward mean : -0.39419999230653047, total_step : 1022843, cur_epsilon : 0.0\n",
      "episode : 32600, done reward mean : -0.3489999933168292, total_step : 1025930, cur_epsilon : 0.0\n",
      "episode : 32700, done reward mean : -0.3069999938085675, total_step : 1028795, cur_epsilon : 0.0\n",
      "episode : 32800, done reward mean : -0.17779999289661647, total_step : 1032069, cur_epsilon : 0.0\n",
      "episode : 32900, done reward mean : -0.1477999933436513, total_step : 1035142, cur_epsilon : 0.0\n",
      "episode : 33000, done reward mean : -0.278399992659688, total_step : 1038519, cur_epsilon : 0.0\n",
      "episode : 33100, done reward mean : -0.23829999377951025, total_step : 1041398, cur_epsilon : 0.0\n",
      "episode : 33200, done reward mean : -0.34419999320060013, total_step : 1044532, cur_epsilon : 0.0\n",
      "episode : 33300, done reward mean : -0.3146999934129417, total_step : 1047573, cur_epsilon : 0.0\n",
      "episode : 33400, done reward mean : -0.258799993544817, total_step : 1050554, cur_epsilon : 0.0\n",
      "episode : 33500, done reward mean : -0.3988999919779599, total_step : 1054237, cur_epsilon : 0.0\n",
      "episode : 33600, done reward mean : -0.38859999310225246, total_step : 1057417, cur_epsilon : 0.0\n",
      "episode : 33700, done reward mean : -0.2752999940700829, total_step : 1060165, cur_epsilon : 0.0\n",
      "episode : 33800, done reward mean : -0.23799999356269835, total_step : 1063142, cur_epsilon : 0.0\n",
      "episode : 33900, done reward mean : -0.23229999391362072, total_step : 1065959, cur_epsilon : 0.0\n",
      "episode : 34000, done reward mean : -0.30729999447241424, total_step : 1068530, cur_epsilon : 0.0\n",
      "episode : 34100, done reward mean : -0.4970999929122627, total_step : 1071795, cur_epsilon : 0.0\n",
      "episode : 34200, done reward mean : -0.29819999244064094, total_step : 1075269, cur_epsilon : 0.0\n",
      "episode : 34300, done reward mean : -0.3222999923489988, total_step : 1078784, cur_epsilon : 0.0\n",
      "episode : 34400, done reward mean : -0.3334999934397638, total_step : 1081815, cur_epsilon : 0.0\n",
      "episode : 34500, done reward mean : -0.4234999932162464, total_step : 1084940, cur_epsilon : 0.0\n",
      "episode : 34600, done reward mean : -0.519699993301183, total_step : 1088033, cur_epsilon : 0.0\n",
      "episode : 34700, done reward mean : -0.3442999920807779, total_step : 1091665, cur_epsilon : 0.0\n",
      "episode : 34800, done reward mean : -0.413899992313236, total_step : 1095195, cur_epsilon : 0.0\n",
      "episode : 34900, done reward mean : -0.42989999329671263, total_step : 1098289, cur_epsilon : 0.0\n",
      "episode : 35000, done reward mean : -0.3096999928541482, total_step : 1101577, cur_epsilon : 0.0\n",
      "episode : 35100, done reward mean : -0.25569999339058996, total_step : 1104632, cur_epsilon : 0.0\n",
      "episode : 35200, done reward mean : -0.22349999165162443, total_step : 1108456, cur_epsilon : 0.0\n",
      "episode : 35300, done reward mean : -0.22659999448806048, total_step : 1111016, cur_epsilon : 0.0\n",
      "episode : 35400, done reward mean : -0.41499999206513166, total_step : 1114658, cur_epsilon : 0.0\n",
      "episode : 35500, done reward mean : -0.4759999924898148, total_step : 1118110, cur_epsilon : 0.0\n",
      "episode : 35600, done reward mean : -0.30149999348446727, total_step : 1121118, cur_epsilon : 0.0\n",
      "episode : 35700, done reward mean : -0.46649999359622596, total_step : 1124077, cur_epsilon : 0.0\n",
      "episode : 35800, done reward mean : -0.2107999923825264, total_step : 1127578, cur_epsilon : 0.0\n",
      "episode : 35900, done reward mean : -0.3303999937325716, total_step : 1130477, cur_epsilon : 0.0\n",
      "episode : 36000, done reward mean : -0.1743999931961298, total_step : 1133614, cur_epsilon : 0.0\n",
      "episode : 36100, done reward mean : -0.2956999929435551, total_step : 1136865, cur_epsilon : 0.0\n",
      "episode : 36200, done reward mean : -0.38469999430701135, total_step : 1139507, cur_epsilon : 0.0\n",
      "episode : 36300, done reward mean : -0.2623999921232462, total_step : 1143124, cur_epsilon : 0.0\n",
      "episode : 36400, done reward mean : -0.284199993647635, total_step : 1146060, cur_epsilon : 0.0\n",
      "episode : 36500, done reward mean : -0.2751999938488007, total_step : 1148906, cur_epsilon : 0.0\n",
      "episode : 36600, done reward mean : -0.19069999305531382, total_step : 1152107, cur_epsilon : 0.0\n",
      "episode : 36700, done reward mean : -0.2780999937839806, total_step : 1154984, cur_epsilon : 0.0\n",
      "episode : 36800, done reward mean : -0.18349999299272896, total_step : 1158212, cur_epsilon : 0.0\n",
      "episode : 36900, done reward mean : -0.14939999286085368, total_step : 1161503, cur_epsilon : 0.0\n",
      "episode : 37000, done reward mean : -0.38389999320730567, total_step : 1164636, cur_epsilon : 0.0\n",
      "episode : 37100, done reward mean : -0.2023999934643507, total_step : 1167655, cur_epsilon : 0.0\n",
      "episode : 37200, done reward mean : -0.24609999360516668, total_step : 1170612, cur_epsilon : 0.0\n",
      "episode : 37300, done reward mean : -0.3965999938175082, total_step : 1173472, cur_epsilon : 0.0\n",
      "episode : 37400, done reward mean : -0.12039999350905418, total_step : 1176471, cur_epsilon : 0.0\n",
      "episode : 37500, done reward mean : -0.4503999937325716, total_step : 1179372, cur_epsilon : 0.0\n",
      "episode : 37600, done reward mean : -0.29599999360740187, total_step : 1182329, cur_epsilon : 0.0\n",
      "episode : 37700, done reward mean : -0.22929999308660626, total_step : 1185516, cur_epsilon : 0.0\n",
      "episode : 37800, done reward mean : -0.41939999397844074, total_step : 1188305, cur_epsilon : 0.0\n",
      "episode : 37900, done reward mean : -0.2842999923042953, total_step : 1191838, cur_epsilon : 0.0\n",
      "episode : 38000, done reward mean : -0.29779999289661646, total_step : 1195110, cur_epsilon : 0.0\n",
      "episode : 38100, done reward mean : -0.38769999401643873, total_step : 1197885, cur_epsilon : 0.0\n",
      "episode : 38200, done reward mean : -0.25649999180808664, total_step : 1201641, cur_epsilon : 0.0\n",
      "episode : 38300, done reward mean : -0.13439999274909498, total_step : 1204976, cur_epsilon : 0.0\n",
      "episode : 38400, done reward mean : -0.36369999321177604, total_step : 1208109, cur_epsilon : 0.0\n",
      "episode : 38500, done reward mean : -0.4919999930262566, total_step : 1211325, cur_epsilon : 0.0\n",
      "episode : 38600, done reward mean : -0.2490999939851463, total_step : 1214112, cur_epsilon : 0.0\n",
      "episode : 38700, done reward mean : -0.23839999355375766, total_step : 1217091, cur_epsilon : 0.0\n",
      "episode : 38800, done reward mean : -0.3483999948948622, total_step : 1219473, cur_epsilon : 0.0\n",
      "episode : 38900, done reward mean : -0.12239999279379844, total_step : 1222787, cur_epsilon : 0.0\n",
      "episode : 39000, done reward mean : -0.24239999301731585, total_step : 1226002, cur_epsilon : 0.0\n",
      "episode : 39100, done reward mean : -0.18269999457523226, total_step : 1228525, cur_epsilon : 0.0\n",
      "episode : 39200, done reward mean : -0.3871999938040972, total_step : 1231392, cur_epsilon : 0.0\n",
      "episode : 39300, done reward mean : -0.176399994045496, total_step : 1234155, cur_epsilon : 0.0\n",
      "episode : 39400, done reward mean : -0.34509999250993134, total_step : 1237599, cur_epsilon : 0.0\n",
      "episode : 39500, done reward mean : -0.41899999309331176, total_step : 1240782, cur_epsilon : 0.0\n",
      "episode : 39600, done reward mean : -0.32889999421313404, total_step : 1243468, cur_epsilon : 0.0\n",
      "episode : 39700, done reward mean : -0.3612999930419028, total_step : 1246678, cur_epsilon : 0.0\n",
      "episode : 39800, done reward mean : -0.33479999274015426, total_step : 1250021, cur_epsilon : 0.0\n",
      "episode : 39900, done reward mean : -0.31969999307766556, total_step : 1253211, cur_epsilon : 0.0\n",
      "episode : 40000, done reward mean : -0.2603999935090542, total_step : 1256210, cur_epsilon : 0.0\n",
      "episode : 40100, done reward mean : -0.22139999460428952, total_step : 1258722, cur_epsilon : 0.0\n",
      "episode : 40200, done reward mean : -0.2733999927714467, total_step : 1262049, cur_epsilon : 0.0\n",
      "episode : 40300, done reward mean : -0.22709999380633236, total_step : 1264915, cur_epsilon : 0.0\n",
      "episode : 40400, done reward mean : -0.33099999349564313, total_step : 1267921, cur_epsilon : 0.0\n",
      "episode : 40500, done reward mean : -0.5096999924071133, total_step : 1271411, cur_epsilon : 0.0\n",
      "episode : 40600, done reward mean : -0.2958999913744628, total_step : 1275359, cur_epsilon : 0.0\n",
      "episode : 40700, done reward mean : -0.27609999338164926, total_step : 1278414, cur_epsilon : 0.0\n",
      "episode : 40800, done reward mean : -0.580799992159009, total_step : 1282011, cur_epsilon : 0.0\n",
      "episode : 40900, done reward mean : -0.31609999293461444, total_step : 1285264, cur_epsilon : 0.0\n",
      "episode : 41000, done reward mean : -0.28519999340176583, total_step : 1288311, cur_epsilon : 0.0\n",
      "episode : 41100, done reward mean : -0.3212999919243157, total_step : 1292014, cur_epsilon : 0.0\n",
      "episode : 41200, done reward mean : -0.4157999929413199, total_step : 1295264, cur_epsilon : 0.0\n",
      "episode : 41300, done reward mean : -0.3462999922595918, total_step : 1298821, cur_epsilon : 0.0\n",
      "episode : 41400, done reward mean : -0.3366999929212034, total_step : 1302080, cur_epsilon : 0.0\n",
      "episode : 41500, done reward mean : -0.0954999933950603, total_step : 1305133, cur_epsilon : 0.0\n",
      "episode : 41600, done reward mean : -0.161999993249774, total_step : 1308247, cur_epsilon : 0.0\n",
      "episode : 41700, done reward mean : -0.2400999932922423, total_step : 1311340, cur_epsilon : 0.0\n",
      "episode : 41800, done reward mean : -0.3044999927468598, total_step : 1314677, cur_epsilon : 0.0\n",
      "episode : 41900, done reward mean : -0.25049999373033643, total_step : 1317577, cur_epsilon : 0.0\n",
      "episode : 42000, done reward mean : -0.22929999152198433, total_step : 1321457, cur_epsilon : 0.0\n",
      "episode : 42100, done reward mean : -0.2560999940522015, total_step : 1324211, cur_epsilon : 0.0\n",
      "episode : 42200, done reward mean : -0.2965999920293689, total_step : 1327869, cur_epsilon : 0.0\n",
      "episode : 42300, done reward mean : -0.2121999925747514, total_step : 1331281, cur_epsilon : 0.0\n",
      "episode : 42400, done reward mean : -0.34349999299272893, total_step : 1334507, cur_epsilon : 0.0\n",
      "episode : 42500, done reward mean : -0.2948999931849539, total_step : 1337649, cur_epsilon : 0.0\n",
      "episode : 42600, done reward mean : -0.24159999303519725, total_step : 1340858, cur_epsilon : 0.0\n",
      "episode : 42700, done reward mean : -0.5067999926954507, total_step : 1344220, cur_epsilon : 0.0\n",
      "episode : 42800, done reward mean : -0.35499999318271874, total_step : 1347367, cur_epsilon : 0.0\n",
      "episode : 42900, done reward mean : -0.4503999921679497, total_step : 1350963, cur_epsilon : 0.0\n",
      "episode : 43000, done reward mean : -0.31189999235793947, total_step : 1354477, cur_epsilon : 0.0\n",
      "episode : 43100, done reward mean : -0.215099993404001, total_step : 1357520, cur_epsilon : 0.0\n",
      "episode : 43200, done reward mean : -0.4090999930910766, total_step : 1360709, cur_epsilon : 0.0\n",
      "episode : 43300, done reward mean : -0.49749999267980455, total_step : 1364077, cur_epsilon : 0.0\n",
      "episode : 43400, done reward mean : -0.2601999935135245, total_step : 1367076, cur_epsilon : 0.0\n",
      "episode : 43500, done reward mean : -0.3287999922037125, total_step : 1370654, cur_epsilon : 0.0\n",
      "episode : 43600, done reward mean : -0.40919999219477177, total_step : 1374236, cur_epsilon : 0.0\n",
      "episode : 43700, done reward mean : -0.107299993801862, total_step : 1377106, cur_epsilon : 0.0\n",
      "episode : 43800, done reward mean : -0.1145999938622117, total_step : 1379946, cur_epsilon : 0.0\n",
      "episode : 43900, done reward mean : -0.362099992800504, total_step : 1383263, cur_epsilon : 0.0\n",
      "episode : 44000, done reward mean : -0.2107999935001135, total_step : 1386267, cur_epsilon : 0.0\n",
      "episode : 44100, done reward mean : -0.3063999933749437, total_step : 1389324, cur_epsilon : 0.0\n",
      "episode : 44200, done reward mean : -0.12139999326318503, total_step : 1392432, cur_epsilon : 0.0\n",
      "episode : 44300, done reward mean : -0.08899999286979438, total_step : 1395717, cur_epsilon : 0.0\n",
      "episode : 44400, done reward mean : -0.22809999311342835, total_step : 1398890, cur_epsilon : 0.0\n",
      "episode : 44500, done reward mean : -0.20919999420642854, total_step : 1401579, cur_epsilon : 0.0\n",
      "episode : 44600, done reward mean : -0.25929999286308886, total_step : 1404864, cur_epsilon : 0.0\n",
      "episode : 44700, done reward mean : -0.4172999931313097, total_step : 1408032, cur_epsilon : 0.0\n",
      "episode : 44800, done reward mean : -0.20479999296367168, total_step : 1411273, cur_epsilon : 0.0\n",
      "episode : 44900, done reward mean : -0.5559999929368495, total_step : 1414525, cur_epsilon : 0.0\n",
      "episode : 45000, done reward mean : -0.27659999292343856, total_step : 1417781, cur_epsilon : 0.0\n",
      "episode : 45100, done reward mean : -0.3577999920025468, total_step : 1421447, cur_epsilon : 0.0\n",
      "episode : 45200, done reward mean : -0.35609999248757956, total_step : 1424900, cur_epsilon : 0.0\n",
      "episode : 45300, done reward mean : -0.29149999348446726, total_step : 1427907, cur_epsilon : 0.0\n",
      "episode : 45400, done reward mean : -0.3733999927714467, total_step : 1431232, cur_epsilon : 0.0\n",
      "episode : 45500, done reward mean : -0.37039999328553674, total_step : 1434333, cur_epsilon : 0.0\n",
      "episode : 45600, done reward mean : -0.3269999933615327, total_step : 1437398, cur_epsilon : 0.0\n",
      "episode : 45700, done reward mean : -0.3443999934196472, total_step : 1440439, cur_epsilon : 0.0\n",
      "episode : 45800, done reward mean : -0.31789999378845096, total_step : 1443312, cur_epsilon : 0.0\n",
      "episode : 45900, done reward mean : -0.4301999928429723, total_step : 1446607, cur_epsilon : 0.0\n",
      "episode : 46000, done reward mean : -0.3701999928429723, total_step : 1449902, cur_epsilon : 0.0\n",
      "episode : 46100, done reward mean : -0.12279999300837517, total_step : 1453125, cur_epsilon : 0.0\n",
      "episode : 46200, done reward mean : -0.43589999360963705, total_step : 1456079, cur_epsilon : 0.0\n",
      "episode : 46300, done reward mean : -0.41479999385774136, total_step : 1458921, cur_epsilon : 0.0\n",
      "episode : 46400, done reward mean : -0.3350999936275184, total_step : 1461867, cur_epsilon : 0.0\n",
      "episode : 46500, done reward mean : -0.45029999351128935, total_step : 1464866, cur_epsilon : 0.0\n",
      "episode : 46600, done reward mean : -0.3676999937929213, total_step : 1467740, cur_epsilon : 0.0\n",
      "episode : 46700, done reward mean : -0.4415999932587147, total_step : 1470852, cur_epsilon : 0.0\n",
      "episode : 46800, done reward mean : -0.26329999366775153, total_step : 1473783, cur_epsilon : 0.0\n",
      "episode : 46900, done reward mean : -0.3490999917499721, total_step : 1477560, cur_epsilon : 0.0\n",
      "episode : 47000, done reward mean : -0.33659999337047336, total_step : 1480620, cur_epsilon : 0.0\n",
      "episode : 47100, done reward mean : -0.27229999436065555, total_step : 1483239, cur_epsilon : 0.0\n",
      "episode : 47200, done reward mean : -0.3131999925523996, total_step : 1486663, cur_epsilon : 0.0\n",
      "episode : 47300, done reward mean : -0.15709999380633236, total_step : 1489528, cur_epsilon : 0.0\n",
      "episode : 47400, done reward mean : -0.2480999935604632, total_step : 1492507, cur_epsilon : 0.0\n",
      "episode : 47500, done reward mean : -0.27269999323412775, total_step : 1495629, cur_epsilon : 0.0\n",
      "episode : 47600, done reward mean : -0.16449999364092946, total_step : 1498570, cur_epsilon : 0.0\n",
      "episode : 47700, done reward mean : -0.1530999938957393, total_step : 1501397, cur_epsilon : 0.0\n",
      "episode : 47800, done reward mean : -0.2834999927692115, total_step : 1504724, cur_epsilon : 0.0\n",
      "episode : 47900, done reward mean : -0.3770999926887453, total_step : 1508088, cur_epsilon : 0.0\n",
      "episode : 48000, done reward mean : -0.26999999307096006, total_step : 1511282, cur_epsilon : 0.0\n",
      "episode : 48100, done reward mean : -0.41279999412596224, total_step : 1514005, cur_epsilon : 0.0\n",
      "episode : 48200, done reward mean : -0.2814999932609499, total_step : 1517112, cur_epsilon : 0.0\n",
      "episode : 48300, done reward mean : -0.2897999930754304, total_step : 1520308, cur_epsilon : 0.0\n",
      "episode : 48400, done reward mean : -0.23299999233335256, total_step : 1523835, cur_epsilon : 0.0\n",
      "episode : 48500, done reward mean : -0.33999999307096007, total_step : 1527030, cur_epsilon : 0.0\n",
      "episode : 48600, done reward mean : -0.388499992210418, total_step : 1530607, cur_epsilon : 0.0\n",
      "episode : 48700, done reward mean : -0.24709999335929753, total_step : 1533671, cur_epsilon : 0.0\n",
      "episode : 48800, done reward mean : -0.3833999925479293, total_step : 1537098, cur_epsilon : 0.0\n",
      "episode : 48900, done reward mean : -0.3813999925926328, total_step : 1540505, cur_epsilon : 0.0\n",
      "episode : 49000, done reward mean : -0.3591999935358763, total_step : 1543494, cur_epsilon : 0.0\n",
      "episode : 49100, done reward mean : -0.43679999381303786, total_step : 1546358, cur_epsilon : 0.0\n",
      "episode : 49200, done reward mean : -0.5041999927535653, total_step : 1549694, cur_epsilon : 0.0\n",
      "episode : 49300, done reward mean : -0.24459999319165945, total_step : 1552836, cur_epsilon : 0.0\n",
      "episode : 49400, done reward mean : -0.3611999939382076, total_step : 1555643, cur_epsilon : 0.0\n",
      "episode : 49500, done reward mean : -0.28739999268203975, total_step : 1559009, cur_epsilon : 0.0\n",
      "episode : 49600, done reward mean : -0.41999999262392523, total_step : 1562404, cur_epsilon : 0.0\n",
      "episode : 49700, done reward mean : -0.21389999298378826, total_step : 1565641, cur_epsilon : 0.0\n",
      "episode : 49800, done reward mean : -0.33059999395161865, total_step : 1568445, cur_epsilon : 0.0\n",
      "episode : 49900, done reward mean : -0.11879999287426472, total_step : 1571727, cur_epsilon : 0.0\n",
      "episode : 50000, done reward mean : -0.433599993661046, total_step : 1574658, cur_epsilon : 0.0\n",
      "episode : 50100, done reward mean : -0.2394999928586185, total_step : 1577945, cur_epsilon : 0.0\n",
      "episode : 50200, done reward mean : -0.5110999919287861, total_step : 1581645, cur_epsilon : 0.0\n",
      "episode : 50300, done reward mean : -0.42589999271556733, total_step : 1584996, cur_epsilon : 0.0\n",
      "episode : 50400, done reward mean : -0.5881999926641583, total_step : 1588374, cur_epsilon : 0.0\n",
      "episode : 50500, done reward mean : -0.3525999918952584, total_step : 1592091, cur_epsilon : 0.0\n",
      "episode : 50600, done reward mean : -0.2532999932207167, total_step : 1595217, cur_epsilon : 0.0\n",
      "episode : 50700, done reward mean : -0.2646999945305288, total_step : 1597762, cur_epsilon : 0.0\n",
      "episode : 50800, done reward mean : -0.367299993801862, total_step : 1600630, cur_epsilon : 0.0\n",
      "episode : 50900, done reward mean : -0.32859999265521767, total_step : 1604014, cur_epsilon : 0.0\n",
      "episode : 51000, done reward mean : -0.4304999923892319, total_step : 1607508, cur_epsilon : 0.0\n",
      "episode : 51100, done reward mean : -0.39609999204054475, total_step : 1611157, cur_epsilon : 0.0\n",
      "episode : 51200, done reward mean : -0.13699999269098043, total_step : 1614522, cur_epsilon : 0.0\n",
      "episode : 51300, done reward mean : -0.23649999292567372, total_step : 1617781, cur_epsilon : 0.0\n",
      "episode : 51400, done reward mean : -0.35559999249875546, total_step : 1621231, cur_epsilon : 0.0\n",
      "episode : 51500, done reward mean : -0.20139999393373728, total_step : 1624042, cur_epsilon : 0.0\n",
      "episode : 51600, done reward mean : -0.467199991568923, total_step : 1627899, cur_epsilon : 0.0\n",
      "episode : 51700, done reward mean : -0.07079999394714832, total_step : 1630705, cur_epsilon : 0.0\n",
      "episode : 51800, done reward mean : -0.3876999926753342, total_step : 1634070, cur_epsilon : 0.0\n",
      "episode : 51900, done reward mean : -0.36849999444559217, total_step : 1636651, cur_epsilon : 0.0\n",
      "episode : 52000, done reward mean : -0.1555999945104122, total_step : 1639202, cur_epsilon : 0.0\n",
      "episode : 52100, done reward mean : -0.31149999326094985, total_step : 1642310, cur_epsilon : 0.0\n",
      "episode : 52200, done reward mean : -0.22099999327212572, total_step : 1645416, cur_epsilon : 0.0\n",
      "episode : 52300, done reward mean : -0.06729999223724008, total_step : 1648981, cur_epsilon : 0.0\n",
      "episode : 52400, done reward mean : -0.3678999920003116, total_step : 1652651, cur_epsilon : 0.0\n",
      "episode : 52500, done reward mean : -0.21389999298378826, total_step : 1655884, cur_epsilon : 0.0\n",
      "episode : 52600, done reward mean : -0.2711999937146902, total_step : 1658793, cur_epsilon : 0.0\n",
      "episode : 52700, done reward mean : -0.3423999930173159, total_step : 1662012, cur_epsilon : 0.0\n",
      "episode : 52800, done reward mean : -0.4098999919556081, total_step : 1665702, cur_epsilon : 0.0\n",
      "episode : 52900, done reward mean : -0.2426999936811626, total_step : 1668625, cur_epsilon : 0.0\n",
      "episode : 53000, done reward mean : -0.47879999242722987, total_step : 1672109, cur_epsilon : 0.0\n",
      "episode : 53100, done reward mean : -0.36309999322518705, total_step : 1675228, cur_epsilon : 0.0\n",
      "episode : 53200, done reward mean : -0.3723999934643507, total_step : 1678244, cur_epsilon : 0.0\n",
      "episode : 53300, done reward mean : -0.2814999939315021, total_step : 1681054, cur_epsilon : 0.0\n",
      "episode : 53400, done reward mean : -0.44669999336823824, total_step : 1684112, cur_epsilon : 0.0\n",
      "episode : 53500, done reward mean : -0.31189999347552655, total_step : 1687123, cur_epsilon : 0.0\n",
      "episode : 53600, done reward mean : -0.40569999227300285, total_step : 1690672, cur_epsilon : 0.0\n",
      "episode : 53700, done reward mean : -0.11689999246969819, total_step : 1694131, cur_epsilon : 0.0\n",
      "episode : 53800, done reward mean : -0.18709999425336718, total_step : 1696795, cur_epsilon : 0.0\n",
      "episode : 53900, done reward mean : -0.19099999438971282, total_step : 1699401, cur_epsilon : 0.0\n",
      "episode : 54000, done reward mean : -0.06839999265968799, total_step : 1702781, cur_epsilon : 0.0\n",
      "episode : 54100, done reward mean : -0.4239999940991402, total_step : 1705521, cur_epsilon : 0.0\n",
      "episode : 54200, done reward mean : -0.21299999255687, total_step : 1708943, cur_epsilon : 0.0\n",
      "episode : 54300, done reward mean : -0.28869999242946504, total_step : 1712425, cur_epsilon : 0.0\n",
      "episode : 54400, done reward mean : -0.28399999275803567, total_step : 1715755, cur_epsilon : 0.0\n",
      "episode : 54500, done reward mean : -0.17319999389350416, total_step : 1718581, cur_epsilon : 0.0\n",
      "episode : 54600, done reward mean : -0.40389999276027083, total_step : 1721918, cur_epsilon : 0.0\n",
      "episode : 54700, done reward mean : -0.40479999274015427, total_step : 1725260, cur_epsilon : 0.0\n",
      "episode : 54800, done reward mean : -0.3528999923355877, total_step : 1728784, cur_epsilon : 0.0\n",
      "episode : 54900, done reward mean : -0.15539999406784774, total_step : 1731533, cur_epsilon : 0.0\n",
      "episode : 55000, done reward mean : -0.21319999366998674, total_step : 1734460, cur_epsilon : 0.0\n",
      "episode : 55100, done reward mean : -0.4205999926105142, total_step : 1737859, cur_epsilon : 0.0\n",
      "episode : 55200, done reward mean : -0.21969999441877008, total_step : 1740453, cur_epsilon : 0.0\n",
      "episode : 55300, done reward mean : -0.41299999300390483, total_step : 1743677, cur_epsilon : 0.0\n",
      "episode : 55400, done reward mean : -0.41669999247416856, total_step : 1747136, cur_epsilon : 0.0\n",
      "episode : 55500, done reward mean : -0.3283999919891357, total_step : 1750811, cur_epsilon : 0.0\n",
      "episode : 55600, done reward mean : 0.005100005697458982, total_step : 1753456, cur_epsilon : 0.0\n",
      "episode : 55700, done reward mean : -0.1136999929882586, total_step : 1756687, cur_epsilon : 0.0\n",
      "episode : 55800, done reward mean : -0.34579999472945927, total_step : 1759142, cur_epsilon : 0.0\n",
      "episode : 55900, done reward mean : -0.4029999927803874, total_step : 1762468, cur_epsilon : 0.0\n",
      "episode : 56000, done reward mean : -0.5435999929904938, total_step : 1765701, cur_epsilon : 0.0\n",
      "episode : 56100, done reward mean : -0.3190999930910766, total_step : 1768885, cur_epsilon : 0.0\n",
      "episode : 56200, done reward mean : -0.4101999930664897, total_step : 1772081, cur_epsilon : 0.0\n",
      "episode : 56300, done reward mean : -0.27009999284520747, total_step : 1775375, cur_epsilon : 0.0\n",
      "episode : 56400, done reward mean : -0.42709999380633235, total_step : 1778241, cur_epsilon : 0.0\n",
      "episode : 56500, done reward mean : -0.3266999933682382, total_step : 1781303, cur_epsilon : 0.0\n",
      "episode : 56600, done reward mean : -0.1980999937839806, total_step : 1784182, cur_epsilon : 0.0\n",
      "episode : 56700, done reward mean : -0.15289999390020967, total_step : 1787005, cur_epsilon : 0.0\n",
      "episode : 56800, done reward mean : -0.43009999262169, total_step : 1790400, cur_epsilon : 0.0\n",
      "episode : 56900, done reward mean : -0.3172999929077923, total_step : 1793665, cur_epsilon : 0.0\n",
      "episode : 57000, done reward mean : -0.19699999336153268, total_step : 1796729, cur_epsilon : 0.0\n",
      "episode : 57100, done reward mean : -0.3112999928183854, total_step : 1800033, cur_epsilon : 0.0\n",
      "episode : 57200, done reward mean : -0.39439999230206013, total_step : 1803570, cur_epsilon : 0.0\n",
      "episode : 57300, done reward mean : -0.3491999937593937, total_step : 1806459, cur_epsilon : 0.0\n",
      "episode : 57400, done reward mean : -0.4989999933168292, total_step : 1809543, cur_epsilon : 0.0\n",
      "episode : 57500, done reward mean : -0.25509999295696617, total_step : 1812786, cur_epsilon : 0.0\n",
      "episode : 57600, done reward mean : -0.3534999934397638, total_step : 1815815, cur_epsilon : 0.0\n",
      "episode : 57700, done reward mean : -0.1822999939136207, total_step : 1818633, cur_epsilon : 0.0\n",
      "episode : 57800, done reward mean : -0.3112999921478331, total_step : 1822238, cur_epsilon : 0.0\n",
      "episode : 57900, done reward mean : -0.3216999932564795, total_step : 1825347, cur_epsilon : 0.0\n",
      "episode : 58000, done reward mean : -0.22799999356269837, total_step : 1828323, cur_epsilon : 0.0\n",
      "episode : 58100, done reward mean : -0.26729999179020525, total_step : 1832084, cur_epsilon : 0.0\n",
      "episode : 58200, done reward mean : -0.24369999388232827, total_step : 1834918, cur_epsilon : 0.0\n",
      "episode : 58300, done reward mean : -0.46659999269992114, total_step : 1838280, cur_epsilon : 0.0\n",
      "episode : 58400, done reward mean : -0.2887999942153692, total_step : 1840963, cur_epsilon : 0.0\n",
      "episode : 58500, done reward mean : -0.3088999924249947, total_step : 1844445, cur_epsilon : 0.0\n",
      "episode : 58600, done reward mean : -0.27219999324530364, total_step : 1847560, cur_epsilon : 0.0\n",
      "episode : 58700, done reward mean : -0.3182999942265451, total_step : 1850241, cur_epsilon : 0.0\n",
      "episode : 58800, done reward mean : -0.26599999360740184, total_step : 1853195, cur_epsilon : 0.0\n",
      "episode : 58900, done reward mean : -0.22459999341517686, total_step : 1856236, cur_epsilon : 0.0\n",
      "episode : 59000, done reward mean : -0.3581999919936061, total_step : 1859904, cur_epsilon : 0.0\n",
      "episode : 59100, done reward mean : -0.17759999334812165, total_step : 1862970, cur_epsilon : 0.0\n",
      "episode : 59200, done reward mean : -0.2758999929390848, total_step : 1866221, cur_epsilon : 0.0\n",
      "episode : 59300, done reward mean : -0.4156999927200377, total_step : 1869571, cur_epsilon : 0.0\n",
      "episode : 59400, done reward mean : -0.2631999945640564, total_step : 1872101, cur_epsilon : 0.0\n",
      "episode : 59500, done reward mean : -0.24179999370127916, total_step : 1875015, cur_epsilon : 0.0\n",
      "episode : 59600, done reward mean : -0.23519999407231806, total_step : 1877762, cur_epsilon : 0.0\n",
      "episode : 59700, done reward mean : -0.12529999228194355, total_step : 1881311, cur_epsilon : 0.0\n",
      "episode : 59800, done reward mean : -0.16679999381303787, total_step : 1884174, cur_epsilon : 0.0\n",
      "episode : 59900, done reward mean : -0.3381999931111932, total_step : 1887351, cur_epsilon : 0.0\n",
      "episode : 60000, done reward mean : -0.11379999298602343, total_step : 1890583, cur_epsilon : 0.0\n",
      "episode : 60100, done reward mean : -0.16219999302178623, total_step : 1893800, cur_epsilon : 0.0\n",
      "episode : 60200, done reward mean : -0.2067999940365553, total_step : 1896564, cur_epsilon : 0.0\n",
      "episode : 60300, done reward mean : -0.3877999915555119, total_step : 1900433, cur_epsilon : 0.0\n",
      "episode : 60400, done reward mean : -0.24409999342635275, total_step : 1903469, cur_epsilon : 0.0\n",
      "episode : 60500, done reward mean : -0.2939999932050705, total_step : 1906604, cur_epsilon : 0.0\n",
      "episode : 60600, done reward mean : -0.35539999362081287, total_step : 1909551, cur_epsilon : 0.0\n",
      "episode : 60700, done reward mean : -0.1589999933168292, total_step : 1912633, cur_epsilon : 0.0\n",
      "episode : 60800, done reward mean : -0.372599991671741, total_step : 1916451, cur_epsilon : 0.0\n",
      "episode : 60900, done reward mean : -0.11549999272450805, total_step : 1919799, cur_epsilon : 0.0\n",
      "episode : 61000, done reward mean : -0.22689999314025044, total_step : 1922962, cur_epsilon : 0.0\n",
      "episode : 61100, done reward mean : -0.13769999356940388, total_step : 1925936, cur_epsilon : 0.0\n",
      "episode : 61200, done reward mean : -0.19879999309778212, total_step : 1929119, cur_epsilon : 0.0\n",
      "episode : 61300, done reward mean : -0.2778999931178987, total_step : 1932291, cur_epsilon : 0.0\n",
      "episode : 61400, done reward mean : -0.30319999299943445, total_step : 1935520, cur_epsilon : 0.0\n",
      "episode : 61500, done reward mean : -0.3788999933190644, total_step : 1938605, cur_epsilon : 0.0\n",
      "episode : 61600, done reward mean : -0.45689999336376785, total_step : 1941668, cur_epsilon : 0.0\n",
      "episode : 61700, done reward mean : -0.4917999916896224, total_step : 1945476, cur_epsilon : 0.0\n",
      "episode : 61800, done reward mean : -0.21409999387338757, total_step : 1948313, cur_epsilon : 0.0\n",
      "episode : 61900, done reward mean : -0.22749999223276973, total_step : 1951884, cur_epsilon : 0.0\n",
      "episode : 62000, done reward mean : -0.38919999308884146, total_step : 1955070, cur_epsilon : 0.0\n",
      "episode : 62100, done reward mean : -0.31199999280273916, total_step : 1958385, cur_epsilon : 0.0\n",
      "episode : 62200, done reward mean : -0.10999999307096005, total_step : 1961577, cur_epsilon : 0.0\n",
      "episode : 62300, done reward mean : -0.1225999927893281, total_step : 1964895, cur_epsilon : 0.0\n",
      "episode : 62400, done reward mean : -0.2840999932028353, total_step : 1968034, cur_epsilon : 0.0\n",
      "episode : 62500, done reward mean : -0.42879999309778216, total_step : 1971216, cur_epsilon : 0.0\n",
      "episode : 62600, done reward mean : -0.26849999265745284, total_step : 1974589, cur_epsilon : 0.0\n",
      "episode : 62700, done reward mean : -0.23019999373704195, total_step : 1977486, cur_epsilon : 0.0\n",
      "episode : 62800, done reward mean : -0.18279999300837516, total_step : 1980705, cur_epsilon : 0.0\n",
      "episode : 62900, done reward mean : -0.1930999936722219, total_step : 1983629, cur_epsilon : 0.0\n",
      "episode : 63000, done reward mean : -0.40939999353140594, total_step : 1986619, cur_epsilon : 0.0\n",
      "episode : 63100, done reward mean : -0.32199999302625654, total_step : 1989830, cur_epsilon : 0.0\n",
      "episode : 63200, done reward mean : -0.435799994058907, total_step : 1992585, cur_epsilon : 0.0\n",
      "episode : 63300, done reward mean : -0.3477999924495816, total_step : 1996056, cur_epsilon : 0.0\n",
      "episode : 63400, done reward mean : -0.40519999250769617, total_step : 1999503, cur_epsilon : 0.0\n",
      "episode : 63500, done reward mean : -0.3541999938711524, total_step : 2002343, cur_epsilon : 0.0\n",
      "episode : 63600, done reward mean : -0.2549999934062362, total_step : 2005387, cur_epsilon : 0.0\n",
      "episode : 63700, done reward mean : -0.2294999928586185, total_step : 2008677, cur_epsilon : 0.0\n",
      "episode : 63800, done reward mean : -0.3854999931715429, total_step : 2011826, cur_epsilon : 0.0\n",
      "episode : 63900, done reward mean : -0.3603999923914671, total_step : 2015322, cur_epsilon : 0.0\n",
      "episode : 64000, done reward mean : -0.19399999342858792, total_step : 2018354, cur_epsilon : 0.0\n",
      "episode : 64100, done reward mean : -0.2187999926507473, total_step : 2021733, cur_epsilon : 0.0\n",
      "episode : 64200, done reward mean : -0.4458999931626022, total_step : 2024888, cur_epsilon : 0.0\n",
      "episode : 64300, done reward mean : -0.40919999353587627, total_step : 2027872, cur_epsilon : 0.0\n",
      "episode : 64400, done reward mean : -0.4002999930642545, total_step : 2031068, cur_epsilon : 0.0\n",
      "episode : 64500, done reward mean : -0.4551999931782484, total_step : 2034213, cur_epsilon : 0.0\n",
      "episode : 64600, done reward mean : -0.2580999926663935, total_step : 2037587, cur_epsilon : 0.0\n",
      "episode : 64700, done reward mean : -0.15299999322742225, total_step : 2040712, cur_epsilon : 0.0\n",
      "episode : 64800, done reward mean : -0.5452999931760132, total_step : 2043861, cur_epsilon : 0.0\n",
      "episode : 64900, done reward mean : -0.3352999931760132, total_step : 2047007, cur_epsilon : 0.0\n",
      "episode : 65000, done reward mean : -0.24979999285191298, total_step : 2050298, cur_epsilon : 0.0\n",
      "episode : 65100, done reward mean : -0.28219999369233845, total_step : 2053216, cur_epsilon : 0.0\n",
      "episode : 65200, done reward mean : -0.3208999937213957, total_step : 2056119, cur_epsilon : 0.0\n",
      "episode : 65300, done reward mean : -0.22139999259263277, total_step : 2059526, cur_epsilon : 0.0\n",
      "episode : 65400, done reward mean : -0.42119999170303346, total_step : 2063323, cur_epsilon : 0.0\n",
      "episode : 65500, done reward mean : -0.3734999934397638, total_step : 2066350, cur_epsilon : 0.0\n",
      "episode : 65600, done reward mean : -0.27479999274015426, total_step : 2069689, cur_epsilon : 0.0\n",
      "episode : 65700, done reward mean : -0.32279999345541, total_step : 2072710, cur_epsilon : 0.0\n",
      "episode : 65800, done reward mean : -0.407799992673099, total_step : 2076082, cur_epsilon : 0.0\n",
      "episode : 65900, done reward mean : -0.18989999374374747, total_step : 2078974, cur_epsilon : 0.0\n",
      "episode : 66000, done reward mean : -0.16059999372810124, total_step : 2081876, cur_epsilon : 0.0\n",
      "episode : 66100, done reward mean : -0.06769999289885163, total_step : 2085146, cur_epsilon : 0.0\n",
      "episode : 66200, done reward mean : -0.2877999931201339, total_step : 2088316, cur_epsilon : 0.0\n",
      "episode : 66300, done reward mean : -0.3645999938622117, total_step : 2091157, cur_epsilon : 0.0\n",
      "episode : 66400, done reward mean : -0.33469999296590686, total_step : 2094398, cur_epsilon : 0.0\n",
      "episode : 66500, done reward mean : -0.28099999282509086, total_step : 2097704, cur_epsilon : 0.0\n",
      "episode : 66600, done reward mean : -0.19329999255016447, total_step : 2101135, cur_epsilon : 0.0\n",
      "episode : 66700, done reward mean : -0.34289999390020964, total_step : 2103957, cur_epsilon : 0.0\n",
      "episode : 66800, done reward mean : -0.2029999930039048, total_step : 2107182, cur_epsilon : 0.0\n",
      "episode : 66900, done reward mean : -0.28619999293237924, total_step : 2110437, cur_epsilon : 0.0\n",
      "episode : 67000, done reward mean : -0.1751999934017658, total_step : 2113481, cur_epsilon : 0.0\n",
      "episode : 67100, done reward mean : -0.35939999263733624, total_step : 2116866, cur_epsilon : 0.0\n",
      "episode : 67200, done reward mean : -0.30559999227523804, total_step : 2120414, cur_epsilon : 0.0\n",
      "episode : 67300, done reward mean : -0.372299993019551, total_step : 2123629, cur_epsilon : 0.0\n",
      "episode : 67400, done reward mean : -0.24709999291226267, total_step : 2126893, cur_epsilon : 0.0\n",
      "episode : 67500, done reward mean : -0.3491999926418066, total_step : 2130277, cur_epsilon : 0.0\n",
      "episode : 67600, done reward mean : -0.24579999405890704, total_step : 2133033, cur_epsilon : 0.0\n",
      "episode : 67700, done reward mean : -0.22549999294802547, total_step : 2136279, cur_epsilon : 0.0\n",
      "episode : 67800, done reward mean : -0.2595999928563833, total_step : 2139569, cur_epsilon : 0.0\n",
      "episode : 67900, done reward mean : -0.4169999920204282, total_step : 2143225, cur_epsilon : 0.0\n",
      "episode : 68000, done reward mean : -0.388399992659688, total_step : 2146601, cur_epsilon : 0.0\n",
      "episode : 68100, done reward mean : -0.27279999345541, total_step : 2149623, cur_epsilon : 0.0\n",
      "episode : 68200, done reward mean : -0.42599999338388445, total_step : 2152678, cur_epsilon : 0.0\n",
      "episode : 68300, done reward mean : -0.3000999935157597, total_step : 2155674, cur_epsilon : 0.0\n",
      "episode : 68400, done reward mean : -0.34769999312236904, total_step : 2158841, cur_epsilon : 0.0\n",
      "episode : 68500, done reward mean : -0.15999999351799488, total_step : 2161838, cur_epsilon : 0.0\n",
      "episode : 68600, done reward mean : -0.30249999279156325, total_step : 2165157, cur_epsilon : 0.0\n",
      "episode : 68700, done reward mean : -0.24649999337270856, total_step : 2168217, cur_epsilon : 0.0\n",
      "episode : 68800, done reward mean : -0.418199993558228, total_step : 2171194, cur_epsilon : 0.0\n",
      "episode : 68900, done reward mean : -0.21559999383985995, total_step : 2174048, cur_epsilon : 0.0\n",
      "episode : 69000, done reward mean : -0.26529999339953064, total_step : 2177096, cur_epsilon : 0.0\n",
      "episode : 69100, done reward mean : -0.18509999295696616, total_step : 2180342, cur_epsilon : 0.0\n",
      "episode : 69200, done reward mean : -0.36719999358057975, total_step : 2183306, cur_epsilon : 0.0\n",
      "episode : 69300, done reward mean : -0.27719999201595785, total_step : 2186966, cur_epsilon : 0.0\n",
      "episode : 69400, done reward mean : -0.28159999325871465, total_step : 2190078, cur_epsilon : 0.0\n",
      "episode : 69500, done reward mean : -0.39979999262839555, total_step : 2193471, cur_epsilon : 0.0\n",
      "episode : 69600, done reward mean : -0.30029999239370225, total_step : 2196966, cur_epsilon : 0.0\n",
      "episode : 69700, done reward mean : -0.3458999933861196, total_step : 2200020, cur_epsilon : 0.0\n",
      "episode : 69800, done reward mean : -0.17939999353140593, total_step : 2203007, cur_epsilon : 0.0\n",
      "episode : 69900, done reward mean : -0.24829999355599283, total_step : 2205986, cur_epsilon : 0.0\n",
      "episode : 70000, done reward mean : -0.3058999933861196, total_step : 2209042, cur_epsilon : 0.0\n",
      "episode : 70100, done reward mean : -0.24889999398961662, total_step : 2211827, cur_epsilon : 0.0\n",
      "episode : 70200, done reward mean : -0.28079999282956125, total_step : 2215131, cur_epsilon : 0.0\n",
      "episode : 70300, done reward mean : -0.27459999319165945, total_step : 2218272, cur_epsilon : 0.0\n",
      "episode : 70400, done reward mean : -0.19739999402314423, total_step : 2221043, cur_epsilon : 0.0\n",
      "episode : 70500, done reward mean : -0.27159999281167985, total_step : 2224350, cur_epsilon : 0.0\n",
      "episode : 70600, done reward mean : -0.3973999937996268, total_step : 2227220, cur_epsilon : 0.0\n",
      "episode : 70700, done reward mean : -0.2940999927558005, total_step : 2230554, cur_epsilon : 0.0\n",
      "episode : 70800, done reward mean : -0.3233999927714467, total_step : 2233882, cur_epsilon : 0.0\n",
      "episode : 70900, done reward mean : -0.2779999922215939, total_step : 2237453, cur_epsilon : 0.0\n",
      "episode : 71000, done reward mean : -0.19039999328553678, total_step : 2240550, cur_epsilon : 0.0\n",
      "episode : 71100, done reward mean : -0.2587999937683344, total_step : 2243434, cur_epsilon : 0.0\n",
      "episode : 71200, done reward mean : -0.2679999922215939, total_step : 2247008, cur_epsilon : 0.0\n",
      "episode : 71300, done reward mean : -0.23399999365210533, total_step : 2249941, cur_epsilon : 0.0\n",
      "episode : 71400, done reward mean : -0.3273999937996268, total_step : 2252810, cur_epsilon : 0.0\n",
      "episode : 71500, done reward mean : -0.3626999941281974, total_step : 2255533, cur_epsilon : 0.0\n",
      "episode : 71600, done reward mean : -0.4147999922931194, total_step : 2259068, cur_epsilon : 0.0\n",
      "episode : 71700, done reward mean : -0.25209999391809107, total_step : 2261885, cur_epsilon : 0.0\n",
      "episode : 71800, done reward mean : -0.33599999248981477, total_step : 2265337, cur_epsilon : 0.0\n",
      "episode : 71900, done reward mean : -0.2152999916113913, total_step : 2269172, cur_epsilon : 0.0\n",
      "episode : 72000, done reward mean : -0.30779999289661647, total_step : 2272443, cur_epsilon : 0.0\n",
      "episode : 72100, done reward mean : -0.4242999920807779, total_step : 2276077, cur_epsilon : 0.0\n",
      "episode : 72200, done reward mean : -0.3702999941818416, total_step : 2278777, cur_epsilon : 0.0\n",
      "episode : 72300, done reward mean : -0.20269999301061034, total_step : 2281997, cur_epsilon : 0.0\n",
      "episode : 72400, done reward mean : -0.25669999269768595, total_step : 2285359, cur_epsilon : 0.0\n",
      "episode : 72500, done reward mean : -0.11519999407231808, total_step : 2288106, cur_epsilon : 0.0\n",
      "episode : 72600, done reward mean : -0.2602999926172197, total_step : 2291506, cur_epsilon : 0.0\n",
      "episode : 72700, done reward mean : -0.19909999264404177, total_step : 2294890, cur_epsilon : 0.0\n",
      "episode : 72800, done reward mean : -0.46499999407678844, total_step : 2297634, cur_epsilon : 0.0\n",
      "episode : 72900, done reward mean : -0.1639999932050705, total_step : 2300770, cur_epsilon : 0.0\n",
      "episode : 73000, done reward mean : -0.2780999933369458, total_step : 2303845, cur_epsilon : 0.0\n",
      "episode : 73100, done reward mean : -0.22459999430924654, total_step : 2306488, cur_epsilon : 0.0\n",
      "episode : 73200, done reward mean : -0.19989999396726488, total_step : 2309282, cur_epsilon : 0.0\n",
      "episode : 73300, done reward mean : -0.3292999935336411, total_step : 2312265, cur_epsilon : 0.0\n",
      "episode : 73400, done reward mean : -0.20769999334588646, total_step : 2315337, cur_epsilon : 0.0\n",
      "episode : 73500, done reward mean : -0.27999999418854715, total_step : 2318033, cur_epsilon : 0.0\n",
      "episode : 73600, done reward mean : -0.2859999920427799, total_step : 2321684, cur_epsilon : 0.0\n",
      "episode : 73700, done reward mean : -0.5060999920405448, total_step : 2325334, cur_epsilon : 0.0\n",
      "episode : 73800, done reward mean : -0.17289999367669226, total_step : 2328258, cur_epsilon : 0.0\n",
      "episode : 73900, done reward mean : -0.3839999929815531, total_step : 2331491, cur_epsilon : 0.0\n",
      "episode : 74000, done reward mean : -0.3245999936386943, total_step : 2334433, cur_epsilon : 0.0\n",
      "episode : 74100, done reward mean : -0.2719999934732914, total_step : 2337449, cur_epsilon : 0.0\n",
      "episode : 74200, done reward mean : -0.3411999932676554, total_step : 2340557, cur_epsilon : 0.0\n",
      "episode : 74300, done reward mean : -0.3306999928317964, total_step : 2343855, cur_epsilon : 0.0\n",
      "episode : 74400, done reward mean : -0.4216999932564795, total_step : 2346966, cur_epsilon : 0.0\n",
      "episode : 74500, done reward mean : -0.3910999923758209, total_step : 2350468, cur_epsilon : 0.0\n",
      "episode : 74600, done reward mean : -0.16339999388903378, total_step : 2353297, cur_epsilon : 0.0\n",
      "episode : 74700, done reward mean : -0.26849999310448763, total_step : 2356474, cur_epsilon : 0.0\n",
      "episode : 74800, done reward mean : -0.2348999938555062, total_step : 2359317, cur_epsilon : 0.0\n",
      "episode : 74900, done reward mean : -0.4222999925725162, total_step : 2362733, cur_epsilon : 0.0\n",
      "episode : 75000, done reward mean : -0.3912999919243157, total_step : 2366437, cur_epsilon : 0.0\n",
      "episode : 75100, done reward mean : -0.45239999413490295, total_step : 2369158, cur_epsilon : 0.0\n",
      "episode : 75200, done reward mean : -0.2124999941326678, total_step : 2371880, cur_epsilon : 0.0\n",
      "episode : 75300, done reward mean : -0.17099999371916055, total_step : 2374783, cur_epsilon : 0.0\n",
      "episode : 75400, done reward mean : -0.16319999277591704, total_step : 2378105, cur_epsilon : 0.0\n",
      "episode : 75500, done reward mean : -0.22829999377951027, total_step : 2380981, cur_epsilon : 0.0\n",
      "episode : 75600, done reward mean : -0.4645999927446246, total_step : 2384321, cur_epsilon : 0.0\n",
      "episode : 75700, done reward mean : -0.3360999929346144, total_step : 2387576, cur_epsilon : 0.0\n",
      "episode : 75800, done reward mean : -0.2982999933324754, total_step : 2390653, cur_epsilon : 0.0\n",
      "episode : 75900, done reward mean : -0.14249999301508068, total_step : 2393871, cur_epsilon : 0.0\n",
      "episode : 76000, done reward mean : -0.20399999298155308, total_step : 2397102, cur_epsilon : 0.0\n",
      "episode : 76100, done reward mean : -0.34379999298602343, total_step : 2400333, cur_epsilon : 0.0\n",
      "episode : 76200, done reward mean : -0.1445999925211072, total_step : 2403774, cur_epsilon : 0.0\n",
      "episode : 76300, done reward mean : -0.5027999936789275, total_step : 2406698, cur_epsilon : 0.0\n",
      "episode : 76400, done reward mean : -0.32889999309554696, total_step : 2409877, cur_epsilon : 0.0\n",
      "episode : 76500, done reward mean : -0.24899999354034663, total_step : 2412859, cur_epsilon : 0.0\n",
      "episode : 76600, done reward mean : -0.4777999931201339, total_step : 2416032, cur_epsilon : 0.0\n",
      "episode : 76700, done reward mean : -0.17889999421313404, total_step : 2418719, cur_epsilon : 0.0\n",
      "episode : 76800, done reward mean : -0.35469999341294167, total_step : 2421762, cur_epsilon : 0.0\n",
      "episode : 76900, done reward mean : -0.4139999923110008, total_step : 2425293, cur_epsilon : 0.0\n",
      "episode : 77000, done reward mean : -0.2548999940790236, total_step : 2428039, cur_epsilon : 0.0\n",
      "episode : 77100, done reward mean : -0.21389999343082308, total_step : 2431072, cur_epsilon : 0.0\n",
      "episode : 77200, done reward mean : -0.2898999937437475, total_step : 2433968, cur_epsilon : 0.0\n",
      "episode : 77300, done reward mean : -0.341699994597584, total_step : 2436481, cur_epsilon : 0.0\n",
      "episode : 77400, done reward mean : -0.35439999341964723, total_step : 2439519, cur_epsilon : 0.0\n",
      "episode : 77500, done reward mean : -0.15929999308660625, total_step : 2442705, cur_epsilon : 0.0\n",
      "episode : 77600, done reward mean : -0.2755999931693077, total_step : 2445856, cur_epsilon : 0.0\n",
      "episode : 77700, done reward mean : -0.5146999929659068, total_step : 2449093, cur_epsilon : 0.0\n",
      "episode : 77800, done reward mean : -0.11339999321848154, total_step : 2452222, cur_epsilon : 0.0\n",
      "episode : 77900, done reward mean : -0.21679999336600303, total_step : 2455286, cur_epsilon : 0.0\n",
      "episode : 78000, done reward mean : -0.3587999937683344, total_step : 2458168, cur_epsilon : 0.0\n",
      "episode : 78100, done reward mean : -0.39769999334588646, total_step : 2461237, cur_epsilon : 0.0\n",
      "episode : 78200, done reward mean : -0.29649999225512147, total_step : 2464793, cur_epsilon : 0.0\n",
      "episode : 78300, done reward mean : -0.12589999383315445, total_step : 2467649, cur_epsilon : 0.0\n",
      "episode : 78400, done reward mean : -0.20309999300166964, total_step : 2470869, cur_epsilon : 0.0\n",
      "episode : 78500, done reward mean : -0.26989999307319523, total_step : 2474062, cur_epsilon : 0.0\n",
      "episode : 78600, done reward mean : -0.35969999330118296, total_step : 2477151, cur_epsilon : 0.0\n",
      "episode : 78700, done reward mean : -0.19329999232664705, total_step : 2480673, cur_epsilon : 0.0\n",
      "episode : 78800, done reward mean : -0.18689999403432012, total_step : 2483436, cur_epsilon : 0.0\n",
      "episode : 78900, done reward mean : -0.44719999268651006, total_step : 2486802, cur_epsilon : 0.0\n",
      "episode : 79000, done reward mean : -0.28749999290332195, total_step : 2490070, cur_epsilon : 0.0\n",
      "episode : 79100, done reward mean : -0.2929999938979745, total_step : 2492898, cur_epsilon : 0.0\n",
      "episode : 79200, done reward mean : -0.13239999301731586, total_step : 2496116, cur_epsilon : 0.0\n",
      "episode : 79300, done reward mean : -0.38069999260827897, total_step : 2499518, cur_epsilon : 0.0\n",
      "episode : 79400, done reward mean : -0.3319999936968088, total_step : 2502435, cur_epsilon : 0.0\n",
      "episode : 79500, done reward mean : -0.39289999278262255, total_step : 2505755, cur_epsilon : 0.0\n",
      "episode : 79600, done reward mean : -0.256799993366003, total_step : 2508819, cur_epsilon : 0.0\n",
      "episode : 79700, done reward mean : -0.39629999293014406, total_step : 2512076, cur_epsilon : 0.0\n",
      "episode : 79800, done reward mean : -0.2623999939113855, total_step : 2514895, cur_epsilon : 0.0\n",
      "episode : 79900, done reward mean : -0.2454999933950603, total_step : 2517945, cur_epsilon : 0.0\n",
      "episode : 80000, done reward mean : -0.18979999307543038, total_step : 2521137, cur_epsilon : 0.0\n",
      "episode : 80100, done reward mean : -0.5052999931760133, total_step : 2524284, cur_epsilon : 0.0\n",
      "episode : 80200, done reward mean : -0.17249999368563296, total_step : 2527202, cur_epsilon : 0.0\n",
      "episode : 80300, done reward mean : -0.32939999308437107, total_step : 2530386, cur_epsilon : 0.0\n",
      "episode : 80400, done reward mean : -0.2127999943494797, total_step : 2533008, cur_epsilon : 0.0\n",
      "episode : 80500, done reward mean : -0.2986999931000173, total_step : 2536186, cur_epsilon : 0.0\n",
      "episode : 80600, done reward mean : -0.32509999385103583, total_step : 2539030, cur_epsilon : 0.0\n",
      "episode : 80700, done reward mean : -0.26479999341070654, total_step : 2542071, cur_epsilon : 0.0\n",
      "episode : 80800, done reward mean : -0.2847999931871891, total_step : 2545215, cur_epsilon : 0.0\n",
      "episode : 80900, done reward mean : -0.257499992903322, total_step : 2548482, cur_epsilon : 0.0\n",
      "episode : 81000, done reward mean : -0.423799992762506, total_step : 2551814, cur_epsilon : 0.0\n",
      "episode : 81100, done reward mean : -0.31849999288097025, total_step : 2555093, cur_epsilon : 0.0\n",
      "episode : 81200, done reward mean : -0.42809999400749804, total_step : 2557872, cur_epsilon : 0.0\n",
      "episode : 81300, done reward mean : -0.23639999248087407, total_step : 2561330, cur_epsilon : 0.0\n",
      "episode : 81400, done reward mean : -0.3960999938286841, total_step : 2564185, cur_epsilon : 0.0\n",
      "episode : 81500, done reward mean : -0.40389999164268375, total_step : 2568013, cur_epsilon : 0.0\n",
      "episode : 81600, done reward mean : -0.1574999933503568, total_step : 2571080, cur_epsilon : 0.0\n",
      "episode : 81700, done reward mean : -0.2714999939315021, total_step : 2573891, cur_epsilon : 0.0\n",
      "episode : 81800, done reward mean : -0.26589999249204993, total_step : 2577343, cur_epsilon : 0.0\n",
      "episode : 81900, done reward mean : -0.20399999164044857, total_step : 2581172, cur_epsilon : 0.0\n",
      "episode : 82000, done reward mean : -0.42909999219700695, total_step : 2584755, cur_epsilon : 0.0\n",
      "episode : 82100, done reward mean : -0.25559999383986, total_step : 2587605, cur_epsilon : 0.0\n",
      "episode : 82200, done reward mean : -0.3396999930776656, total_step : 2590793, cur_epsilon : 0.0\n",
      "episode : 82300, done reward mean : -0.35349999321624637, total_step : 2593925, cur_epsilon : 0.0\n",
      "episode : 82400, done reward mean : -0.2832999941147864, total_step : 2596654, cur_epsilon : 0.0\n",
      "episode : 82500, done reward mean : -0.2131999945640564, total_step : 2599183, cur_epsilon : 0.0\n",
      "episode : 82600, done reward mean : -0.02409999320283532, total_step : 2602314, cur_epsilon : 0.0\n",
      "episode : 82700, done reward mean : -0.3513999928161502, total_step : 2605623, cur_epsilon : 0.0\n",
      "episode : 82800, done reward mean : -0.18739999402314425, total_step : 2608391, cur_epsilon : 0.0\n",
      "episode : 82900, done reward mean : -0.11009999440982937, total_step : 2610988, cur_epsilon : 0.0\n",
      "episode : 83000, done reward mean : -0.267499992903322, total_step : 2614256, cur_epsilon : 0.0\n",
      "episode : 83100, done reward mean : -0.18779999267309905, total_step : 2617630, cur_epsilon : 0.0\n",
      "episode : 83200, done reward mean : -0.1854999933950603, total_step : 2620680, cur_epsilon : 0.0\n",
      "episode : 83300, done reward mean : -0.3651999931782484, total_step : 2623828, cur_epsilon : 0.0\n",
      "episode : 83400, done reward mean : -0.38069999260827897, total_step : 2627224, cur_epsilon : 0.0\n",
      "episode : 83500, done reward mean : -0.341299993712455, total_step : 2630133, cur_epsilon : 0.0\n",
      "episode : 83600, done reward mean : -0.1299999926239252, total_step : 2633525, cur_epsilon : 0.0\n",
      "episode : 83700, done reward mean : -0.36249999234452845, total_step : 2637042, cur_epsilon : 0.0\n",
      "episode : 83800, done reward mean : -0.4718999928049743, total_step : 2640356, cur_epsilon : 0.0\n",
      "episode : 83900, done reward mean : -0.27619999248534444, total_step : 2643808, cur_epsilon : 0.0\n",
      "episode : 84000, done reward mean : -0.15709999246522785, total_step : 2647271, cur_epsilon : 0.0\n",
      "episode : 84100, done reward mean : -0.3599999924004078, total_step : 2650761, cur_epsilon : 0.0\n",
      "episode : 84200, done reward mean : -0.26199999257922174, total_step : 2654174, cur_epsilon : 0.0\n",
      "episode : 84300, done reward mean : -0.27129999371245506, total_step : 2657084, cur_epsilon : 0.0\n",
      "episode : 84400, done reward mean : -0.2098999941907823, total_step : 2659780, cur_epsilon : 0.0\n",
      "episode : 84500, done reward mean : -0.3618999925814569, total_step : 2663192, cur_epsilon : 0.0\n",
      "episode : 84600, done reward mean : -0.2529999943450093, total_step : 2665818, cur_epsilon : 0.0\n",
      "episode : 84700, done reward mean : -0.4053999927267432, total_step : 2669162, cur_epsilon : 0.0\n",
      "episode : 84800, done reward mean : -0.273799992762506, total_step : 2672491, cur_epsilon : 0.0\n",
      "episode : 84900, done reward mean : -0.3463999929279089, total_step : 2675748, cur_epsilon : 0.0\n",
      "episode : 85000, done reward mean : -0.1808999946154654, total_step : 2678255, cur_epsilon : 0.0\n",
      "episode : 85100, done reward mean : -0.39909999309107663, total_step : 2681439, cur_epsilon : 0.0\n",
      "episode : 85200, done reward mean : -0.18619999181479216, total_step : 2685191, cur_epsilon : 0.0\n",
      "episode : 85300, done reward mean : -0.19229999279603363, total_step : 2688509, cur_epsilon : 0.0\n",
      "episode : 85400, done reward mean : -0.22939999330788852, total_step : 2691596, cur_epsilon : 0.0\n",
      "episode : 85500, done reward mean : -0.49499999295920133, total_step : 2694842, cur_epsilon : 0.0\n",
      "episode : 85600, done reward mean : -0.37829999333247544, total_step : 2697919, cur_epsilon : 0.0\n",
      "episode : 85700, done reward mean : -0.26139999393373725, total_step : 2700730, cur_epsilon : 0.0\n",
      "episode : 85800, done reward mean : -0.4026999930106103, total_step : 2703950, cur_epsilon : 0.0\n",
      "episode : 85900, done reward mean : -0.3571999929100275, total_step : 2707220, cur_epsilon : 0.0\n",
      "episode : 86000, done reward mean : -0.3907999926060438, total_step : 2710618, cur_epsilon : 0.0\n",
      "episode : 86100, done reward mean : -0.551699991915375, total_step : 2714326, cur_epsilon : 0.0\n",
      "episode : 86200, done reward mean : -0.19359999433159827, total_step : 2716958, cur_epsilon : 0.0\n",
      "episode : 86300, done reward mean : -0.32679999314248565, total_step : 2720118, cur_epsilon : 0.0\n",
      "episode : 86400, done reward mean : -0.4110999939404428, total_step : 2722925, cur_epsilon : 0.0\n",
      "episode : 86500, done reward mean : -0.29299999345093963, total_step : 2725949, cur_epsilon : 0.0\n",
      "episode : 86600, done reward mean : -0.33399999253451823, total_step : 2729385, cur_epsilon : 0.0\n",
      "episode : 86700, done reward mean : -0.2715999932587147, total_step : 2732494, cur_epsilon : 0.0\n",
      "episode : 86800, done reward mean : -0.3772999915666878, total_step : 2736357, cur_epsilon : 0.0\n",
      "episode : 86900, done reward mean : -0.2737999934330583, total_step : 2739391, cur_epsilon : 0.0\n",
      "episode : 87000, done reward mean : -0.4439999938756227, total_step : 2742226, cur_epsilon : 0.0\n",
      "episode : 87100, done reward mean : -0.39989999352023003, total_step : 2745218, cur_epsilon : 0.0\n",
      "episode : 87200, done reward mean : -0.21229999301955105, total_step : 2748437, cur_epsilon : 0.0\n",
      "episode : 87300, done reward mean : -0.3271999935805798, total_step : 2751399, cur_epsilon : 0.0\n",
      "episode : 87400, done reward mean : -0.3440999927558005, total_step : 2754734, cur_epsilon : 0.0\n",
      "episode : 87500, done reward mean : -0.2571999940276146, total_step : 2757503, cur_epsilon : 0.0\n",
      "episode : 87600, done reward mean : -0.15819999378174543, total_step : 2760383, cur_epsilon : 0.0\n",
      "episode : 87700, done reward mean : -0.21079999327659607, total_step : 2763486, cur_epsilon : 0.0\n",
      "episode : 87800, done reward mean : -0.4059999933838844, total_step : 2766543, cur_epsilon : 0.0\n",
      "episode : 87900, done reward mean : -0.48189999235793946, total_step : 2770052, cur_epsilon : 0.0\n",
      "episode : 88000, done reward mean : -0.3759999927133322, total_step : 2773405, cur_epsilon : 0.0\n",
      "episode : 88100, done reward mean : -0.2151999931782484, total_step : 2776550, cur_epsilon : 0.0\n",
      "episode : 88200, done reward mean : -0.14239999257028102, total_step : 2779965, cur_epsilon : 0.0\n",
      "episode : 88300, done reward mean : -0.365199992954731, total_step : 2783212, cur_epsilon : 0.0\n",
      "episode : 88400, done reward mean : -0.2903999935090542, total_step : 2786214, cur_epsilon : 0.0\n",
      "episode : 88500, done reward mean : -0.45999999329447744, total_step : 2789310, cur_epsilon : 0.0\n",
      "episode : 88600, done reward mean : -0.21789999401196838, total_step : 2792086, cur_epsilon : 0.0\n",
      "episode : 88700, done reward mean : -0.3870999926887453, total_step : 2795451, cur_epsilon : 0.0\n",
      "episode : 88800, done reward mean : -0.42549999250099063, total_step : 2798897, cur_epsilon : 0.0\n",
      "episode : 88900, done reward mean : -0.36979999240487815, total_step : 2802386, cur_epsilon : 0.0\n",
      "episode : 89000, done reward mean : -0.39989999374374746, total_step : 2805281, cur_epsilon : 0.0\n",
      "episode : 89100, done reward mean : -0.31969999397173526, total_step : 2808073, cur_epsilon : 0.0\n",
      "episode : 89200, done reward mean : -0.36379999231547117, total_step : 2811603, cur_epsilon : 0.0\n",
      "episode : 89300, done reward mean : -0.2327999932318926, total_step : 2814720, cur_epsilon : 0.0\n",
      "episode : 89400, done reward mean : -0.250499993506819, total_step : 2817723, cur_epsilon : 0.0\n",
      "episode : 89500, done reward mean : -0.28949999263510107, total_step : 2821110, cur_epsilon : 0.0\n",
      "episode : 89600, done reward mean : -0.2991999926418066, total_step : 2824493, cur_epsilon : 0.0\n",
      "episode : 89700, done reward mean : -0.383699993211776, total_step : 2827626, cur_epsilon : 0.0\n",
      "episode : 89800, done reward mean : -0.04699999358505011, total_step : 2830590, cur_epsilon : 0.0\n",
      "episode : 89900, done reward mean : -0.33959999330341817, total_step : 2833678, cur_epsilon : 0.0\n",
      "episode : 90000, done reward mean : -0.17089999349787832, total_step : 2836681, cur_epsilon : 0.0\n",
      "episode : 90100, done reward mean : -0.4301999923959374, total_step : 2840174, cur_epsilon : 0.0\n",
      "episode : 90200, done reward mean : -0.5634999920986593, total_step : 2843800, cur_epsilon : 0.0\n",
      "episode : 90300, done reward mean : -0.3032999929971993, total_step : 2847026, cur_epsilon : 0.0\n",
      "episode : 90400, done reward mean : -0.35369999254122375, total_step : 2850455, cur_epsilon : 0.0\n",
      "episode : 90500, done reward mean : -0.36739999290555714, total_step : 2853720, cur_epsilon : 0.0\n",
      "episode : 90600, done reward mean : -0.41629999270662665, total_step : 2857074, cur_epsilon : 0.0\n",
      "episode : 90700, done reward mean : -0.4519999923557043, total_step : 2860585, cur_epsilon : 0.0\n",
      "episode : 90800, done reward mean : -0.31339999232441185, total_step : 2864110, cur_epsilon : 0.0\n",
      "episode : 90900, done reward mean : -0.45699999447911976, total_step : 2866677, cur_epsilon : 0.0\n",
      "episode : 91000, done reward mean : -0.381499992813915, total_step : 2869986, cur_epsilon : 0.0\n",
      "episode : 91100, done reward mean : -0.5279999933391809, total_step : 2873061, cur_epsilon : 0.0\n",
      "episode : 91200, done reward mean : -0.20579999405890703, total_step : 2875817, cur_epsilon : 0.0\n",
      "episode : 91300, done reward mean : -0.354699992518872, total_step : 2879258, cur_epsilon : 0.0\n",
      "episode : 91400, done reward mean : -0.17029999395832418, total_step : 2882055, cur_epsilon : 0.0\n",
      "episode : 91500, done reward mean : -0.2672999924607575, total_step : 2885517, cur_epsilon : 0.0\n",
      "episode : 91600, done reward mean : -0.31629999293014405, total_step : 2888770, cur_epsilon : 0.0\n",
      "episode : 91700, done reward mean : -0.3500999932922423, total_step : 2891866, cur_epsilon : 0.0\n",
      "episode : 91800, done reward mean : -0.28479999251663685, total_step : 2895303, cur_epsilon : 0.0\n",
      "episode : 91900, done reward mean : -0.16969999374821781, total_step : 2898195, cur_epsilon : 0.0\n",
      "episode : 92000, done reward mean : -0.4004999932833016, total_step : 2901294, cur_epsilon : 0.0\n",
      "episode : 92100, done reward mean : -0.15419999431818723, total_step : 2903930, cur_epsilon : 0.0\n",
      "episode : 92200, done reward mean : -0.3971999929100275, total_step : 2907196, cur_epsilon : 0.0\n",
      "episode : 92300, done reward mean : -0.4811999925971031, total_step : 2910601, cur_epsilon : 0.0\n",
      "episode : 92400, done reward mean : -0.35499999273568394, total_step : 2913946, cur_epsilon : 0.0\n",
      "episode : 92500, done reward mean : -0.30209999369457363, total_step : 2916863, cur_epsilon : 0.0\n",
      "episode : 92600, done reward mean : -0.2189999933168292, total_step : 2919949, cur_epsilon : 0.0\n",
      "episode : 92700, done reward mean : -0.25569999249652026, total_step : 2923398, cur_epsilon : 0.0\n",
      "episode : 92800, done reward mean : -0.3679999915510416, total_step : 2927267, cur_epsilon : 0.0\n",
      "episode : 92900, done reward mean : -0.14319999277591705, total_step : 2930589, cur_epsilon : 0.0\n",
      "episode : 93000, done reward mean : -0.16829999333247542, total_step : 2933665, cur_epsilon : 0.0\n",
      "episode : 93100, done reward mean : -0.17039999373257161, total_step : 2936566, cur_epsilon : 0.0\n",
      "episode : 93200, done reward mean : -0.4484999937750399, total_step : 2939448, cur_epsilon : 0.0\n",
      "episode : 93300, done reward mean : -0.3392999935336411, total_step : 2942438, cur_epsilon : 0.0\n",
      "episode : 93400, done reward mean : -0.14209999369457363, total_step : 2945351, cur_epsilon : 0.0\n",
      "episode : 93500, done reward mean : -0.16809999311342835, total_step : 2948522, cur_epsilon : 0.0\n",
      "episode : 93600, done reward mean : -0.3623999936878681, total_step : 2951442, cur_epsilon : 0.0\n",
      "episode : 93700, done reward mean : -0.3544999927468598, total_step : 2954776, cur_epsilon : 0.0\n",
      "episode : 93800, done reward mean : -0.10939999219030141, total_step : 2958362, cur_epsilon : 0.0\n",
      "episode : 93900, done reward mean : -0.3380999937839806, total_step : 2961237, cur_epsilon : 0.0\n",
      "episode : 94000, done reward mean : -0.10919999331235886, total_step : 2964320, cur_epsilon : 0.0\n",
      "episode : 94100, done reward mean : -0.3265999926999211, total_step : 2967678, cur_epsilon : 0.0\n",
      "episode : 94200, done reward mean : -0.3411999925971031, total_step : 2971083, cur_epsilon : 0.0\n",
      "episode : 94300, done reward mean : -0.36439999274909496, total_step : 2974421, cur_epsilon : 0.0\n",
      "episode : 94400, done reward mean : -0.3156999927200377, total_step : 2977769, cur_epsilon : 0.0\n",
      "episode : 94500, done reward mean : -0.3172999929077923, total_step : 2981036, cur_epsilon : 0.0\n",
      "episode : 94600, done reward mean : -0.35909999242052437, total_step : 2984519, cur_epsilon : 0.0\n",
      "episode : 94700, done reward mean : -0.4043999920785427, total_step : 2988154, cur_epsilon : 0.0\n",
      "episode : 94800, done reward mean : -0.2855999933928251, total_step : 2991203, cur_epsilon : 0.0\n",
      "episode : 94900, done reward mean : -0.4603999928385019, total_step : 2994499, cur_epsilon : 0.0\n",
      "episode : 95000, done reward mean : -0.3714999934844673, total_step : 2997504, cur_epsilon : 0.0\n",
      "episode : 95100, done reward mean : -0.2184999935515225, total_step : 3000484, cur_epsilon : 0.0\n",
      "episode : 95200, done reward mean : -0.38279999457299707, total_step : 3003008, cur_epsilon : 0.0\n",
      "episode : 95300, done reward mean : -0.3784999924339354, total_step : 3006483, cur_epsilon : 0.0\n",
      "episode : 95400, done reward mean : -0.30559999294579027, total_step : 3009732, cur_epsilon : 0.0\n",
      "episode : 95500, done reward mean : -0.40579999271780254, total_step : 3013082, cur_epsilon : 0.0\n",
      "episode : 95600, done reward mean : -0.1537999938800931, total_step : 3015916, cur_epsilon : 0.0\n",
      "episode : 95700, done reward mean : -0.31619999293237927, total_step : 3019172, cur_epsilon : 0.0\n",
      "episode : 95800, done reward mean : -0.3626999934576452, total_step : 3022194, cur_epsilon : 0.0\n",
      "episode : 95900, done reward mean : -0.4639999936521053, total_step : 3025128, cur_epsilon : 0.0\n",
      "episode : 96000, done reward mean : -0.20719999313354492, total_step : 3028296, cur_epsilon : 0.0\n",
      "episode : 96100, done reward mean : -0.3210999932698905, total_step : 3031401, cur_epsilon : 0.0\n",
      "episode : 96200, done reward mean : -0.2800999932922423, total_step : 3034498, cur_epsilon : 0.0\n",
      "episode : 96300, done reward mean : -0.3246999934129417, total_step : 3037540, cur_epsilon : 0.0\n",
      "episode : 96400, done reward mean : -0.17309999277815222, total_step : 3040864, cur_epsilon : 0.0\n",
      "episode : 96500, done reward mean : -0.33779999289661644, total_step : 3044132, cur_epsilon : 0.0\n",
      "episode : 96600, done reward mean : -0.19199999257922173, total_step : 3047544, cur_epsilon : 0.0\n",
      "episode : 96700, done reward mean : -0.10759999379515647, total_step : 3050417, cur_epsilon : 0.0\n",
      "episode : 96800, done reward mean : -0.2638999927602708, total_step : 3053750, cur_epsilon : 0.0\n",
      "episode : 96900, done reward mean : -0.3700999930687249, total_step : 3056941, cur_epsilon : 0.0\n",
      "episode : 97000, done reward mean : -0.3422999936901033, total_step : 3059858, cur_epsilon : 0.0\n",
      "episode : 97100, done reward mean : -0.548799993544817, total_step : 3062842, cur_epsilon : 0.0\n",
      "episode : 97200, done reward mean : -0.3643999923020601, total_step : 3066378, cur_epsilon : 0.0\n",
      "episode : 97300, done reward mean : -0.280999993942678, total_step : 3069181, cur_epsilon : 0.0\n",
      "episode : 97400, done reward mean : -0.3608999939449131, total_step : 3071985, cur_epsilon : 0.0\n",
      "episode : 97500, done reward mean : -0.2336999929882586, total_step : 3075216, cur_epsilon : 0.0\n",
      "episode : 97600, done reward mean : -0.4115999934822321, total_step : 3078228, cur_epsilon : 0.0\n",
      "episode : 97700, done reward mean : -0.380899994391948, total_step : 3080834, cur_epsilon : 0.0\n",
      "episode : 97800, done reward mean : -0.20979999262839555, total_step : 3084226, cur_epsilon : 0.0\n",
      "episode : 97900, done reward mean : -0.5390999946556986, total_step : 3086711, cur_epsilon : 0.0\n",
      "episode : 98000, done reward mean : -0.22499999407678842, total_step : 3089457, cur_epsilon : 0.0\n",
      "episode : 98100, done reward mean : -0.3800999921746552, total_step : 3093051, cur_epsilon : 0.0\n",
      "episode : 98200, done reward mean : -0.2222999939136207, total_step : 3095869, cur_epsilon : 0.0\n",
      "episode : 98300, done reward mean : -0.4188999922014773, total_step : 3099451, cur_epsilon : 0.0\n",
      "episode : 98400, done reward mean : -0.45409999320283534, total_step : 3102589, cur_epsilon : 0.0\n",
      "episode : 98500, done reward mean : -0.3038999936543405, total_step : 3105524, cur_epsilon : 0.0\n",
      "episode : 98600, done reward mean : -0.35969999285414817, total_step : 3108815, cur_epsilon : 0.0\n",
      "episode : 98700, done reward mean : -0.56999999307096, total_step : 3112011, cur_epsilon : 0.0\n",
      "episode : 98800, done reward mean : -0.29299999434500934, total_step : 3114639, cur_epsilon : 0.0\n",
      "episode : 98900, done reward mean : -0.16049999171867968, total_step : 3118435, cur_epsilon : 0.0\n",
      "episode : 99000, done reward mean : -0.2800999923981726, total_step : 3121924, cur_epsilon : 0.0\n",
      "episode : 99100, done reward mean : -0.4448999929614365, total_step : 3125164, cur_epsilon : 0.0\n",
      "episode : 99200, done reward mean : -0.37179999347776177, total_step : 3128178, cur_epsilon : 0.0\n",
      "episode : 99300, done reward mean : -0.31489999340847136, total_step : 3131221, cur_epsilon : 0.0\n",
      "episode : 99400, done reward mean : -0.26929999286308887, total_step : 3134505, cur_epsilon : 0.0\n",
      "episode : 99500, done reward mean : -0.3687999926507473, total_step : 3137883, cur_epsilon : 0.0\n",
      "episode : 99600, done reward mean : -0.18969999285414815, total_step : 3141169, cur_epsilon : 0.0\n",
      "episode : 99700, done reward mean : -0.15529999339953066, total_step : 3144218, cur_epsilon : 0.0\n",
      "episode : 99800, done reward mean : -0.2895999933034182, total_step : 3147307, cur_epsilon : 0.0\n",
      "episode : 99900, done reward mean : -0.38119999259710313, total_step : 3150712, cur_epsilon : 0.0\n",
      "episode : 100000, done reward mean : -0.39029999328777193, total_step : 3153810, cur_epsilon : 0.0\n",
      "episode : 100100, done reward mean : -0.3027999930083752, total_step : 3157033, cur_epsilon : 0.0\n",
      "episode : 100200, done reward mean : -0.3513999930396676, total_step : 3160239, cur_epsilon : 0.0\n",
      "episode : 100300, done reward mean : -0.4226999936811626, total_step : 3163160, cur_epsilon : 0.0\n",
      "episode : 100400, done reward mean : -0.42899999309331177, total_step : 3166344, cur_epsilon : 0.0\n",
      "episode : 100500, done reward mean : -0.42269999189302326, total_step : 3170059, cur_epsilon : 0.0\n",
      "episode : 100600, done reward mean : -0.2595999928563833, total_step : 3173351, cur_epsilon : 0.0\n",
      "episode : 100700, done reward mean : -0.3206999930553138, total_step : 3176553, cur_epsilon : 0.0\n",
      "episode : 100800, done reward mean : -0.08899999376386404, total_step : 3179440, cur_epsilon : 0.0\n",
      "episode : 100900, done reward mean : -0.29959999352693556, total_step : 3182431, cur_epsilon : 0.0\n",
      "episode : 101000, done reward mean : -0.3292999921925366, total_step : 3186016, cur_epsilon : 0.0\n",
      "episode : 101100, done reward mean : -0.33059999350458386, total_step : 3189018, cur_epsilon : 0.0\n",
      "episode : 101200, done reward mean : -0.22959999196231365, total_step : 3192707, cur_epsilon : 0.0\n",
      "episode : 101300, done reward mean : -0.22739999290555715, total_step : 3195976, cur_epsilon : 0.0\n",
      "episode : 101400, done reward mean : -0.3673999935761094, total_step : 3198944, cur_epsilon : 0.0\n",
      "episode : 101500, done reward mean : -0.3671999940276146, total_step : 3201710, cur_epsilon : 0.0\n",
      "episode : 101600, done reward mean : -0.21799999333918094, total_step : 3204786, cur_epsilon : 0.0\n",
      "episode : 101700, done reward mean : -0.5133999932184815, total_step : 3207915, cur_epsilon : 0.0\n",
      "episode : 101800, done reward mean : -0.2600999946333468, total_step : 3210414, cur_epsilon : 0.0\n",
      "episode : 101900, done reward mean : -0.14529999317601322, total_step : 3213561, cur_epsilon : 0.0\n",
      "episode : 102000, done reward mean : -0.1464999938197434, total_step : 3216421, cur_epsilon : 0.0\n",
      "episode : 102100, done reward mean : -0.37329999255016444, total_step : 3219846, cur_epsilon : 0.0\n",
      "episode : 102200, done reward mean : -0.47169999236240984, total_step : 3223354, cur_epsilon : 0.0\n",
      "episode : 102300, done reward mean : -0.20939999263733625, total_step : 3226742, cur_epsilon : 0.0\n",
      "episode : 102400, done reward mean : -0.30159999303519724, total_step : 3229951, cur_epsilon : 0.0\n",
      "episode : 102500, done reward mean : -0.3713999930396676, total_step : 3233155, cur_epsilon : 0.0\n",
      "episode : 102600, done reward mean : -0.39929999398067595, total_step : 3235947, cur_epsilon : 0.0\n",
      "episode : 102700, done reward mean : -0.4035999934375286, total_step : 3238976, cur_epsilon : 0.0\n",
      "episode : 102800, done reward mean : -0.3160999924875796, total_step : 3242427, cur_epsilon : 0.0\n",
      "episode : 102900, done reward mean : -0.37329999411478637, total_step : 3245155, cur_epsilon : 0.0\n",
      "episode : 103000, done reward mean : -0.2465999924764037, total_step : 3248612, cur_epsilon : 0.0\n",
      "episode : 103100, done reward mean : -0.18999999284744262, total_step : 3251907, cur_epsilon : 0.0\n",
      "episode : 103200, done reward mean : -0.2420999921299517, total_step : 3255521, cur_epsilon : 0.0\n",
      "episode : 103300, done reward mean : -0.25489999340847136, total_step : 3258566, cur_epsilon : 0.0\n",
      "episode : 103400, done reward mean : -0.3741999938711524, total_step : 3261404, cur_epsilon : 0.0\n",
      "episode : 103500, done reward mean : -0.4397999930754304, total_step : 3264595, cur_epsilon : 0.0\n",
      "episode : 103600, done reward mean : -0.08199999302625656, total_step : 3267810, cur_epsilon : 0.0\n",
      "episode : 103700, done reward mean : -0.43549999272450807, total_step : 3271156, cur_epsilon : 0.0\n",
      "episode : 103800, done reward mean : -0.3084999924339354, total_step : 3274636, cur_epsilon : 0.0\n",
      "episode : 103900, done reward mean : -0.30509999362751844, total_step : 3277577, cur_epsilon : 0.0\n",
      "episode : 104000, done reward mean : -0.22899999264627696, total_step : 3280959, cur_epsilon : 0.0\n",
      "episode : 104100, done reward mean : -0.3229999918863177, total_step : 3284677, cur_epsilon : 0.0\n",
      "episode : 104200, done reward mean : -0.2080999935604632, total_step : 3287652, cur_epsilon : 0.0\n",
      "episode : 104300, done reward mean : -0.5608999932743609, total_step : 3290755, cur_epsilon : 0.0\n",
      "episode : 104400, done reward mean : -0.46219999235123393, total_step : 3294273, cur_epsilon : 0.0\n",
      "episode : 104500, done reward mean : -0.21589999405667185, total_step : 3297025, cur_epsilon : 0.0\n",
      "episode : 104600, done reward mean : -0.19639999359846116, total_step : 3299986, cur_epsilon : 0.0\n",
      "episode : 104700, done reward mean : -0.2606999921612442, total_step : 3303584, cur_epsilon : 0.0\n",
      "episode : 104800, done reward mean : -0.253599993661046, total_step : 3306515, cur_epsilon : 0.0\n",
      "episode : 104900, done reward mean : -0.35309999300166967, total_step : 3309738, cur_epsilon : 0.0\n",
      "episode : 105000, done reward mean : -0.27559999383985995, total_step : 3312592, cur_epsilon : 0.0\n",
      "episode : 105100, done reward mean : -0.4005999928340316, total_step : 3315890, cur_epsilon : 0.0\n",
      "episode : 105200, done reward mean : -0.297599992454052, total_step : 3319360, cur_epsilon : 0.0\n",
      "episode : 105300, done reward mean : -0.13619999337941407, total_step : 3322418, cur_epsilon : 0.0\n",
      "episode : 105400, done reward mean : -0.1737999938800931, total_step : 3325252, cur_epsilon : 0.0\n",
      "episode : 105500, done reward mean : -0.26529999228194356, total_step : 3328797, cur_epsilon : 0.0\n",
      "episode : 105600, done reward mean : -0.3704999928362668, total_step : 3332097, cur_epsilon : 0.0\n",
      "episode : 105700, done reward mean : -0.4255999927222729, total_step : 3335443, cur_epsilon : 0.0\n",
      "episode : 105800, done reward mean : -0.2736999943293631, total_step : 3338076, cur_epsilon : 0.0\n",
      "episode : 105900, done reward mean : -0.3997999919578433, total_step : 3341766, cur_epsilon : 0.0\n",
      "episode : 106000, done reward mean : -0.35479999385774136, total_step : 3344610, cur_epsilon : 0.0\n",
      "episode : 106100, done reward mean : -0.38529999228194356, total_step : 3348159, cur_epsilon : 0.0\n",
      "episode : 106200, done reward mean : -0.316499993596226, total_step : 3351117, cur_epsilon : 0.0\n",
      "episode : 106300, done reward mean : -0.15599999405443668, total_step : 3353872, cur_epsilon : 0.0\n",
      "episode : 106400, done reward mean : -0.18209999235346913, total_step : 3357383, cur_epsilon : 0.0\n",
      "episode : 106500, done reward mean : -0.45429999319836495, total_step : 3360523, cur_epsilon : 0.0\n",
      "episode : 106600, done reward mean : -0.28019999351352454, total_step : 3363520, cur_epsilon : 0.0\n",
      "episode : 106700, done reward mean : -0.24429999319836498, total_step : 3366657, cur_epsilon : 0.0\n",
      "episode : 106800, done reward mean : -0.3103999944031239, total_step : 3369259, cur_epsilon : 0.0\n",
      "episode : 106900, done reward mean : -0.4125999925658107, total_step : 3372677, cur_epsilon : 0.0\n",
      "episode : 107000, done reward mean : -0.3224999941326678, total_step : 3375398, cur_epsilon : 0.0\n",
      "episode : 107100, done reward mean : -0.1348999940790236, total_step : 3378138, cur_epsilon : 0.0\n",
      "episode : 107200, done reward mean : -0.05099999394267798, total_step : 3380944, cur_epsilon : 0.0\n",
      "episode : 107300, done reward mean : -0.3949999927356839, total_step : 3384289, cur_epsilon : 0.0\n",
      "episode : 107400, done reward mean : -0.3641999938711524, total_step : 3387128, cur_epsilon : 0.0\n",
      "episode : 107500, done reward mean : -0.10179999437183142, total_step : 3389743, cur_epsilon : 0.0\n",
      "episode : 107600, done reward mean : -0.36079999305307864, total_step : 3392944, cur_epsilon : 0.0\n",
      "episode : 107700, done reward mean : -0.3593999930843711, total_step : 3396131, cur_epsilon : 0.0\n",
      "episode : 107800, done reward mean : -0.2914999937079847, total_step : 3399039, cur_epsilon : 0.0\n",
      "episode : 107900, done reward mean : -0.34439999297261237, total_step : 3402278, cur_epsilon : 0.0\n",
      "episode : 108000, done reward mean : -0.4516999930329621, total_step : 3405489, cur_epsilon : 0.0\n",
      "episode : 108100, done reward mean : -0.26179999303072693, total_step : 3408700, cur_epsilon : 0.0\n",
      "episode : 108200, done reward mean : -0.23789999334141612, total_step : 3411775, cur_epsilon : 0.0\n",
      "episode : 108300, done reward mean : -0.20159999325871467, total_step : 3414885, cur_epsilon : 0.0\n",
      "episode : 108400, done reward mean : -0.3406999928317964, total_step : 3418186, cur_epsilon : 0.0\n",
      "episode : 108500, done reward mean : -0.23919999308884143, total_step : 3421371, cur_epsilon : 0.0\n",
      "episode : 108600, done reward mean : -0.2954999925009906, total_step : 3424820, cur_epsilon : 0.0\n",
      "episode : 108700, done reward mean : -0.2946999931894243, total_step : 3427962, cur_epsilon : 0.0\n",
      "episode : 108800, done reward mean : -0.17969999419525265, total_step : 3430657, cur_epsilon : 0.0\n",
      "episode : 108900, done reward mean : -0.2643999931961298, total_step : 3433793, cur_epsilon : 0.0\n",
      "episode : 109000, done reward mean : -0.3268999935872853, total_step : 3436760, cur_epsilon : 0.0\n",
      "episode : 109100, done reward mean : -0.4079999928921461, total_step : 3440033, cur_epsilon : 0.0\n",
      "episode : 109200, done reward mean : -0.4059999927133322, total_step : 3443385, cur_epsilon : 0.0\n",
      "episode : 109300, done reward mean : -0.3431999932229519, total_step : 3446513, cur_epsilon : 0.0\n",
      "episode : 109400, done reward mean : -0.18939999397844076, total_step : 3449301, cur_epsilon : 0.0\n",
      "episode : 109500, done reward mean : -0.3207999923825264, total_step : 3452805, cur_epsilon : 0.0\n",
      "episode : 109600, done reward mean : -0.3122999934665859, total_step : 3455824, cur_epsilon : 0.0\n",
      "episode : 109700, done reward mean : -0.2513999930396676, total_step : 3459032, cur_epsilon : 0.0\n",
      "episode : 109800, done reward mean : -0.17679999314248562, total_step : 3462193, cur_epsilon : 0.0\n",
      "episode : 109900, done reward mean : -0.35829999355599285, total_step : 3465171, cur_epsilon : 0.0\n",
      "episode : 110000, done reward mean : -0.25119999326765535, total_step : 3468274, cur_epsilon : 0.0\n",
      "episode : 110100, done reward mean : -0.19339999366551638, total_step : 3471203, cur_epsilon : 0.0\n",
      "episode : 110200, done reward mean : -0.3198999939672649, total_step : 3473997, cur_epsilon : 0.0\n",
      "episode : 110300, done reward mean : -0.1808999934978783, total_step : 3476999, cur_epsilon : 0.0\n",
      "episode : 110400, done reward mean : -0.38319999366998675, total_step : 3479927, cur_epsilon : 0.0\n",
      "episode : 110500, done reward mean : -0.3134999932162464, total_step : 3483057, cur_epsilon : 0.0\n",
      "episode : 110600, done reward mean : -0.29669999359175564, total_step : 3486017, cur_epsilon : 0.0\n",
      "episode : 110700, done reward mean : -0.2578999931178987, total_step : 3489189, cur_epsilon : 0.0\n",
      "episode : 110800, done reward mean : -0.3794999930821359, total_step : 3492379, cur_epsilon : 0.0\n",
      "episode : 110900, done reward mean : -0.15329999232664704, total_step : 3495905, cur_epsilon : 0.0\n",
      "episode : 111000, done reward mean : -0.3994999924115837, total_step : 3499392, cur_epsilon : 0.0\n",
      "episode : 111100, done reward mean : -0.41729999223724007, total_step : 3502956, cur_epsilon : 0.0\n",
      "episode : 111200, done reward mean : -0.351999993249774, total_step : 3506071, cur_epsilon : 0.0\n",
      "episode : 111300, done reward mean : -0.18089999305084348, total_step : 3509273, cur_epsilon : 0.0\n",
      "episode : 111400, done reward mean : -0.34439999230206014, total_step : 3512809, cur_epsilon : 0.0\n",
      "episode : 111500, done reward mean : -0.2829999927803874, total_step : 3516133, cur_epsilon : 0.0\n",
      "episode : 111600, done reward mean : -0.23149999169632793, total_step : 3519938, cur_epsilon : 0.0\n",
      "episode : 111700, done reward mean : -0.19199999347329139, total_step : 3522952, cur_epsilon : 0.0\n",
      "episode : 111800, done reward mean : -0.16689999202266337, total_step : 3526612, cur_epsilon : 0.0\n",
      "episode : 111900, done reward mean : -0.37879999332129954, total_step : 3529694, cur_epsilon : 0.0\n",
      "episode : 112000, done reward mean : -0.13939999420195817, total_step : 3532384, cur_epsilon : 0.0\n",
      "episode : 112100, done reward mean : -0.3987999928742647, total_step : 3535662, cur_epsilon : 0.0\n",
      "episode : 112200, done reward mean : -0.24729999313130974, total_step : 3538827, cur_epsilon : 0.0\n",
      "episode : 112300, done reward mean : -0.2756999933905899, total_step : 3541878, cur_epsilon : 0.0\n",
      "episode : 112400, done reward mean : -0.3386999931000173, total_step : 3545060, cur_epsilon : 0.0\n",
      "episode : 112500, done reward mean : -0.12509999362751842, total_step : 3548009, cur_epsilon : 0.0\n",
      "episode : 112600, done reward mean : -0.34589999293908474, total_step : 3551261, cur_epsilon : 0.0\n",
      "episode : 112700, done reward mean : -0.41689999202266337, total_step : 3554918, cur_epsilon : 0.0\n",
      "episode : 112800, done reward mean : -0.12609999360516666, total_step : 3557873, cur_epsilon : 0.0\n",
      "episode : 112900, done reward mean : -0.40199999414384363, total_step : 3560589, cur_epsilon : 0.0\n",
      "episode : 113000, done reward mean : -0.49329999210312964, total_step : 3564214, cur_epsilon : 0.0\n",
      "episode : 113100, done reward mean : -0.43549999272450807, total_step : 3567564, cur_epsilon : 0.0\n",
      "episode : 113200, done reward mean : -0.40289999367669227, total_step : 3570487, cur_epsilon : 0.0\n",
      "episode : 113300, done reward mean : -0.409699993301183, total_step : 3573575, cur_epsilon : 0.0\n",
      "episode : 113400, done reward mean : -0.2579999940097332, total_step : 3576352, cur_epsilon : 0.0\n",
      "episode : 113500, done reward mean : -0.37359999410808087, total_step : 3579085, cur_epsilon : 0.0\n",
      "episode : 113600, done reward mean : -0.41279999367892745, total_step : 3582008, cur_epsilon : 0.0\n",
      "episode : 113700, done reward mean : -0.2395999926328659, total_step : 3585395, cur_epsilon : 0.0\n",
      "episode : 113800, done reward mean : -0.1475999926775694, total_step : 3588765, cur_epsilon : 0.0\n",
      "episode : 113900, done reward mean : -0.3577999933436513, total_step : 3591839, cur_epsilon : 0.0\n",
      "episode : 114000, done reward mean : -0.3309999926015735, total_step : 3595239, cur_epsilon : 0.0\n",
      "episode : 114100, done reward mean : -0.19959999330341816, total_step : 3598331, cur_epsilon : 0.0\n",
      "episode : 114200, done reward mean : -0.2793999930843711, total_step : 3601518, cur_epsilon : 0.0\n",
      "episode : 114300, done reward mean : -0.38279999278485777, total_step : 3604840, cur_epsilon : 0.0\n",
      "episode : 114400, done reward mean : -0.28869999332353474, total_step : 3607922, cur_epsilon : 0.0\n",
      "episode : 114500, done reward mean : -0.15409999364987015, total_step : 3610856, cur_epsilon : 0.0\n",
      "episode : 114600, done reward mean : -0.19249999234452844, total_step : 3614374, cur_epsilon : 0.0\n",
      "episode : 114700, done reward mean : -0.34549999272450804, total_step : 3617723, cur_epsilon : 0.0\n",
      "episode : 114800, done reward mean : -0.32809999311342836, total_step : 3620898, cur_epsilon : 0.0\n",
      "episode : 114900, done reward mean : -0.22799999445676802, total_step : 3623474, cur_epsilon : 0.0\n",
      "episode : 115000, done reward mean : -0.4336999927647412, total_step : 3626804, cur_epsilon : 0.0\n",
      "episode : 115100, done reward mean : -0.3932999934442341, total_step : 3629831, cur_epsilon : 0.0\n",
      "episode : 115200, done reward mean : -0.32799999266862867, total_step : 3633203, cur_epsilon : 0.0\n",
      "episode : 115300, done reward mean : -0.21399999365210534, total_step : 3636138, cur_epsilon : 0.0\n",
      "episode : 115400, done reward mean : -0.20139999348670246, total_step : 3639145, cur_epsilon : 0.0\n",
      "episode : 115500, done reward mean : -0.3856999913789332, total_step : 3643092, cur_epsilon : 0.0\n",
      "episode : 115600, done reward mean : -0.4936999929882586, total_step : 3646321, cur_epsilon : 0.0\n",
      "episode : 115700, done reward mean : -0.28789999356493357, total_step : 3649292, cur_epsilon : 0.0\n",
      "episode : 115800, done reward mean : -0.38059999372810127, total_step : 3652192, cur_epsilon : 0.0\n",
      "episode : 115900, done reward mean : -0.2208999934978783, total_step : 3655196, cur_epsilon : 0.0\n",
      "episode : 116000, done reward mean : -0.38049999283626673, total_step : 3658495, cur_epsilon : 0.0\n",
      "episode : 116100, done reward mean : -0.22539999429136515, total_step : 3661148, cur_epsilon : 0.0\n",
      "episode : 116200, done reward mean : -0.2521999932453036, total_step : 3664263, cur_epsilon : 0.0\n",
      "episode : 116300, done reward mean : -0.3502999935112894, total_step : 3667262, cur_epsilon : 0.0\n",
      "episode : 116400, done reward mean : -0.283799992762506, total_step : 3670594, cur_epsilon : 0.0\n",
      "episode : 116500, done reward mean : -0.31239999368786814, total_step : 3673511, cur_epsilon : 0.0\n",
      "episode : 116600, done reward mean : -0.24379999365657568, total_step : 3676443, cur_epsilon : 0.0\n",
      "episode : 116700, done reward mean : -0.1880999937839806, total_step : 3679317, cur_epsilon : 0.0\n",
      "episode : 116800, done reward mean : -0.4248999927379191, total_step : 3682658, cur_epsilon : 0.0\n",
      "episode : 116900, done reward mean : -0.24119999304413794, total_step : 3685865, cur_epsilon : 0.0\n",
      "episode : 117000, done reward mean : -0.33799999333918096, total_step : 3688939, cur_epsilon : 0.0\n",
      "episode : 117100, done reward mean : -0.29069999350234865, total_step : 3691940, cur_epsilon : 0.0\n",
      "episode : 117200, done reward mean : -0.29579999383538963, total_step : 3694796, cur_epsilon : 0.0\n",
      "episode : 117300, done reward mean : -0.43539999317377803, total_step : 3697945, cur_epsilon : 0.0\n",
      "episode : 117400, done reward mean : -0.3696999928541482, total_step : 3701237, cur_epsilon : 0.0\n",
      "episode : 117500, done reward mean : -0.26609999226406217, total_step : 3704788, cur_epsilon : 0.0\n",
      "episode : 117600, done reward mean : -0.16609999315813184, total_step : 3707941, cur_epsilon : 0.0\n",
      "episode : 117700, done reward mean : -0.37499999318271876, total_step : 3711086, cur_epsilon : 0.0\n",
      "episode : 117800, done reward mean : -0.3804999932833016, total_step : 3714185, cur_epsilon : 0.0\n",
      "episode : 117900, done reward mean : -0.29089999394491317, total_step : 3716988, cur_epsilon : 0.0\n",
      "episode : 118000, done reward mean : -0.3381999917700887, total_step : 3720759, cur_epsilon : 0.0\n",
      "episode : 118100, done reward mean : -0.2752999929524958, total_step : 3724008, cur_epsilon : 0.0\n",
      "episode : 118200, done reward mean : -0.5126999930106103, total_step : 3727231, cur_epsilon : 0.0\n",
      "episode : 118300, done reward mean : -0.32209999369457365, total_step : 3730146, cur_epsilon : 0.0\n",
      "episode : 118400, done reward mean : -0.2179999937862158, total_step : 3733022, cur_epsilon : 0.0\n",
      "episode : 118500, done reward mean : -0.438399992659688, total_step : 3736397, cur_epsilon : 0.0\n",
      "episode : 118600, done reward mean : -0.5132999918796122, total_step : 3740121, cur_epsilon : 0.0\n",
      "episode : 118700, done reward mean : -0.2963999931514263, total_step : 3743280, cur_epsilon : 0.0\n",
      "episode : 118800, done reward mean : -0.2664999929256737, total_step : 3746538, cur_epsilon : 0.0\n",
      "episode : 118900, done reward mean : -0.40219999235123394, total_step : 3750054, cur_epsilon : 0.0\n",
      "episode : 119000, done reward mean : -0.3317999919131398, total_step : 3753759, cur_epsilon : 0.0\n",
      "episode : 119100, done reward mean : -0.1750999936275184, total_step : 3756703, cur_epsilon : 0.0\n",
      "episode : 119200, done reward mean : -0.3163999931514263, total_step : 3759862, cur_epsilon : 0.0\n",
      "episode : 119300, done reward mean : -0.2546999934129417, total_step : 3762903, cur_epsilon : 0.0\n",
      "episode : 119400, done reward mean : -0.08329999344423414, total_step : 3765931, cur_epsilon : 0.0\n",
      "episode : 119500, done reward mean : -0.12549999272450804, total_step : 3769280, cur_epsilon : 0.0\n",
      "episode : 119600, done reward mean : -0.34499999340623616, total_step : 3772323, cur_epsilon : 0.0\n",
      "episode : 119700, done reward mean : -0.35079999282956126, total_step : 3775626, cur_epsilon : 0.0\n",
      "episode : 119800, done reward mean : -0.2697999930754304, total_step : 3778820, cur_epsilon : 0.0\n",
      "episode : 119900, done reward mean : -0.2513999939337373, total_step : 3781630, cur_epsilon : 0.0\n",
      "episode : 120000, done reward mean : -0.2600999932922423, total_step : 3784725, cur_epsilon : 0.0\n",
      "episode : 120100, done reward mean : -0.32409999320283533, total_step : 3787860, cur_epsilon : 0.0\n",
      "episode : 120200, done reward mean : -0.36619999270886183, total_step : 3791216, cur_epsilon : 0.0\n",
      "episode : 120300, done reward mean : -0.2045999925211072, total_step : 3794651, cur_epsilon : 0.0\n",
      "episode : 120400, done reward mean : -0.19879999287426472, total_step : 3797933, cur_epsilon : 0.0\n",
      "episode : 120500, done reward mean : -0.2609999930486083, total_step : 3801132, cur_epsilon : 0.0\n",
      "episode : 120600, done reward mean : -0.17139999326318503, total_step : 3804239, cur_epsilon : 0.0\n",
      "episode : 120700, done reward mean : -0.433599993661046, total_step : 3807170, cur_epsilon : 0.0\n",
      "episode : 120800, done reward mean : -0.2682999928854406, total_step : 3810446, cur_epsilon : 0.0\n",
      "episode : 120900, done reward mean : -0.2538999936543405, total_step : 3813380, cur_epsilon : 0.0\n",
      "episode : 121000, done reward mean : -0.3454999938420951, total_step : 3816232, cur_epsilon : 0.0\n",
      "episode : 121100, done reward mean : -0.3982999940030277, total_step : 3819008, cur_epsilon : 0.0\n",
      "episode : 121200, done reward mean : -0.36069999283179643, total_step : 3822309, cur_epsilon : 0.0\n",
      "episode : 121300, done reward mean : -0.30469999318942426, total_step : 3825452, cur_epsilon : 0.0\n",
      "episode : 121400, done reward mean : -0.2565999938175082, total_step : 3828314, cur_epsilon : 0.0\n",
      "episode : 121500, done reward mean : -0.3237999923154712, total_step : 3831842, cur_epsilon : 0.0\n",
      "episode : 121600, done reward mean : -0.3620999930240214, total_step : 3835058, cur_epsilon : 0.0\n",
      "episode : 121700, done reward mean : -0.31379999343305826, total_step : 3838090, cur_epsilon : 0.0\n",
      "episode : 121800, done reward mean : -0.38359999276697637, total_step : 3841420, cur_epsilon : 0.0\n",
      "episode : 121900, done reward mean : -0.19909999309107662, total_step : 3844600, cur_epsilon : 0.0\n",
      "episode : 122000, done reward mean : -0.22629999360069633, total_step : 3847557, cur_epsilon : 0.0\n",
      "episode : 122100, done reward mean : -0.17939999263733625, total_step : 3850946, cur_epsilon : 0.0\n",
      "episode : 122200, done reward mean : -0.2476999924518168, total_step : 3854416, cur_epsilon : 0.0\n",
      "episode : 122300, done reward mean : -0.1974999933503568, total_step : 3857483, cur_epsilon : 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_episode_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 136\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, n_iter, batch_size, print_episode_steps)\u001b[0m\n\u001b[1;32m    131\u001b[0m grads \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    132\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_net\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m    133\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m    135\u001b[0m loss_v \u001b[38;5;241m=\u001b[39m entropy_loss_v \u001b[38;5;241m+\u001b[39m loss_value_v\n\u001b[0;32m--> 136\u001b[0m \u001b[43mloss_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m nn_utils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_grad)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/mlagent-learn-20-test-54-2MrXv-py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# exp_buffer = ExpBuffer(\n",
    "#     # max_size=1024,\n",
    "#     prob_alpha=0.6,\n",
    "#     beta_start=0.4,\n",
    "#     beta_frames=30000, #100000,\n",
    "#     n_step=4,\n",
    "#     gamma=0.99,\n",
    "# )\n",
    "# agent = Agent(\n",
    "#     env=env,\n",
    "#     exp_buffer=exp_buffer,\n",
    "#     net=net,\n",
    "#     epsilon_start=0.0,\n",
    "#     epsilon_final=0.0,\n",
    "#     epsilon_decay_last_step=30000, #200000,\n",
    "#     tgt_sync_steps=1000,\n",
    "#     learning_rate=5e-4,\n",
    "#     adam_eps=1e-3,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    agent.train(n_iter=20, batch_size=32, print_episode_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0f144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dc090428",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dde473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e351856",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # // to be implemented by the developer\n",
    "    # public override void OnEpisodeBegin()\n",
    "    # {\n",
    "    #     area.AreaReset();\n",
    "    #     Array values = Enum.GetValues(typeof(GridGoal));\n",
    "    #     // if (m_GoalSensor is object)\n",
    "    #     // {\n",
    "    #     //     CurrentGoal = (GridGoal)values.GetValue(UnityEngine.Random.Range(0, values.Length));\n",
    "    #     // }\n",
    "    #     // else\n",
    "    #     // {\n",
    "    #     //     CurrentGoal = GridGoal.GreenPlus;\n",
    "    #     // }\n",
    "    #     CurrentGoal = GridGoal.GreenPlus;\n",
    "    # }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
